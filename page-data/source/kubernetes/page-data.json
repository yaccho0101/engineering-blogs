{"componentChunkName":"component---src-templates-blog-post-list-by-source-tsx","path":"/source/kubernetes","result":{"data":{"allPost":{"edges":[{"node":{"ID":4724,"Title":"Blog: Announcing the 2023 Steering Committee Election Results","Description":"<p><strong>Author</strong>: Kaslin Fields</p>\n<p>The <a href=\"https://github.com/kubernetes/community/tree/master/events/elections/2023\">2023 Steering Committee Election</a> is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2023. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.</p>\n<p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee‚Äôs role in their <a href=\"https://github.com/kubernetes/steering/blob/master/charter.md\">charter</a>.</p>\n<p>Thank you to everyone who voted in the election; your participation helps support the community‚Äôs continued health and success.</p>\n<h2 id=\"results\">Results</h2>\n<p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):</p>\n<ul>\n<li><strong>Stephen Augustus (<a href=\"https://github.com/justaugustus\">@justaugustus</a>), Cisco</strong></li>\n<li><strong>Paco Xu Âæê‰øäÊù∞ (<a href=\"https://github.com/pacoxu\">@pacoxu</a>), DaoCloud</strong></li>\n<li><strong>Patrick Ohly (<a href=\"https://github.com/pohly\">@pohly</a>), Intel</strong></li>\n<li><strong>Maciej Szulik (<a href=\"https://github.com/soltysh\">@soltysh</a>), Red Hat</strong></li>\n</ul>\n<p>They join continuing members:</p>\n<ul>\n<li><strong>Benjamin Elder (<a href=\"https://github.com/bentheelder\">@bentheelder</a>), Google</strong></li>\n<li><strong>Bob Killen (<a href=\"https://github.com/mrbobbytables\">@mrbobbytables</a>), Google</strong></li>\n<li><strong>Nabarun Pal (<a href=\"https://github.com/palnabarun\">@palnabarun</a>, VMware</strong></li>\n</ul>\n<p>Stephen Augustus is a returning Steering Committee Member.</p>\n<h2 id=\"big-thanks\">Big Thanks!</h2>\n<p>Thank you and congratulations on a successful election to this round‚Äôs election officers:</p>\n<ul>\n<li>Bridget Kromhout (<a href=\"https://github.com/bridgetkromhout\">@bridgetkromhout</a>)</li>\n<li>Davanum Srinavas (<a href=\"https://github.com/dims\">@dims</a>)</li>\n<li>Kaslin Fields (<a href=\"https://github.com/kaslin\">@kaslin</a>)</li>\n</ul>\n<p>Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community:</p>\n<ul>\n<li>Christoph Blecker (<a href=\"https://github.com/cblecker\">@cblecker</a>)</li>\n<li>Carlos Tadeu Panato Jr. (<a href=\"https://github.com/cpanato\">@cpanato</a>)</li>\n<li>Tim Pepper (<a href=\"https://github.com/tpepper\">@tpepper</a>)</li>\n</ul>\n<p>And thank you to all the candidates who came forward to run for election.</p>\n<h2 id=\"get-involved-with-the-steering-committee\">Get Involved with the Steering Committee</h2>\n<p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee <a href=\"https://github.com/orgs/kubernetes/projects/40\">backlog items</a> and weigh in by filing an issue or creating a PR against their <a href=\"https://github.com/kubernetes/steering\">repo</a>. They have an open meeting on <a href=\"https://github.com/kubernetes/steering\">the first Monday at 9:30am PT of every month</a>. They can also be contacted at their public mailing list <a href=\"mailto:steering@kubernetes.io\">steering@kubernetes.io</a>.</p>\n<p>You can see what the Steering Committee meetings are all about by watching past meetings on the <a href=\"https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM\">YouTube Playlist</a>.</p>\n<p>If you want to meet some of the newly elected Steering Committee members, join us for the Steering AMA at the <a href=\"https://k8s.dev/summit\">Kubernetes Contributor Summit in Chicago</a>.</p>\n<hr>\n<p><em>This post was written by the <a href=\"https://github.com/kubernetes/community/tree/master/communication/contributor-comms\">Contributor Comms Subproject</a>. If you want to write stories about the Kubernetes community, learn more about us.</em></p>","PublishedAt":"2023-10-02 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/10/02/steering-committee-results-2023/","SourceName":"Kubernetes"}},{"node":{"ID":4652,"Title":"Blog: Happy 7th Birthday kubeadm!","Description":"<p><strong>Author:</strong> Fabrizio Pandini (VMware)</p>\n<p>What a journey so far!</p>\n<p>Starting from the initial blog post <a href=\"https://kubernetes.io/blog/2016/09/how-we-made-kubernetes-easy-to-install/\">‚ÄúHow we made Kubernetes insanely easy to install‚Äù</a> in September 2016, followed by an exciting growth that lead to general availability / <a href=\"https://kubernetes.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/\">‚ÄúProduction-Ready Kubernetes Cluster Creation with kubeadm‚Äù</a> two years later.</p>\n<p>And later on a continuous, steady and reliable flow of small improvements that is still going on as of today.</p>\n<h2 id=\"what-is-kubeadm-quick-refresher\">What is kubeadm? (quick refresher)</h2>\n<p>kubeadm is focused on bootstrapping Kubernetes clusters on existing infrastructure and performing an essential set of maintenance tasks. The core of the kubeadm interface is quite simple: new control plane nodes\nare created by running <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/\"><code>kubeadm init</code></a> and\nworker nodes are joined to the control plane by running\n<a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/\"><code>kubeadm join</code></a>.\nAlso included are utilities for managing already bootstrapped clusters, such as control plane upgrades\nand token and certificate renewal.</p>\n<p>To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of its scope:</p>\n<ul>\n<li>Infrastructure provisioning</li>\n<li>Third-party networking</li>\n<li>Non-critical add-ons, e.g. for monitoring, logging, and visualization</li>\n<li>Specific cloud provider integrations</li>\n</ul>\n<p>Infrastructure provisioning, for example, is left to other SIG Cluster Lifecycle projects, such as the\n<a href=\"https://cluster-api.sigs.k8s.io/\">Cluster API</a>. Instead, kubeadm covers only the common denominator\nin every Kubernetes cluster: the\n<a href=\"https://kubernetes.io/docs/concepts/overview/components/#control-plane-components\">control plane</a>.\nThe user may install their preferred networking solution and other add-ons on top of Kubernetes\n<em>after</em> cluster creation.</p>\n<p>Behind the scenes, kubeadm does a lot. The tool makes sure you have all the key components:\netcd, the API server, the scheduler, the controller manager. You can join more control plane nodes\nfor improving resiliency or join worker nodes for running your workloads. You get cluster DNS\nand kube-proxy set up for you. TLS between components is enabled and used for encryption in transit.</p>\n<h2 id=\"let-s-celebrate-past-present-and-future-of-kubeadm\">Let's celebrate! Past, present and future of kubeadm</h2>\n<p>In all and for all kubeadm's story is tightly coupled with Kubernetes' story, and with this amazing community.</p>\n<p>Therefore celebrating kubeadm is first of all celebrating this community, a set of people, who joined forces in finding a common ground, a minimum viable tool, for bootstrapping Kubernetes clusters.</p>\n<p>This tool, was instrumental to the Kubernetes success back in time as well as it is today, and the silver line of kubeadm's value proposition can be summarized in two points</p>\n<ul>\n<li>\n<p>An obsession in making things deadly simple for the majority of the users: kubeadm init &amp; kubeadm join, that's all you need!</p>\n</li>\n<li>\n<p>A sharp focus on a well-defined problem scope: bootstrapping Kubernetes clusters on existing infrastructure. As our slogan says: <em>keep it simple, keep it extensible!</em></p>\n</li>\n</ul>\n<p>This silver line, this clear contract, is the foundation the entire kubeadm user base relies on, and this post is a celebration for kubeadm's users as well.</p>\n<p>We are deeply thankful for any feedback from our users, for the enthusiasm that they are continuously showing for this tool via Slack, GitHub, social media, blogs, in person at every KubeCon or at the various meet ups around the world. Keep going!</p>\n<p>What continues to amaze me after all those years is the great things people are building on top of kubeadm, and as of today there is a strong and very active list of projects doing so:</p>\n<ul>\n<li><a href=\"https://minikube.sigs.k8s.io/\">minikube</a></li>\n<li><a href=\"https://kind.sigs.k8s.io/\">kind</a></li>\n<li><a href=\"https://cluster-api.sigs.k8s.io/\">Cluster API</a></li>\n<li><a href=\"https://kubespray.io/\">Kubespray</a></li>\n<li>and many more; if you are using Kubernetes today, there is a good chance that you are using kubeadm even without knowing it üòú</li>\n</ul>\n<p>This community, the kubeadm‚Äôs users, the projects building on top of kubeadm are the highlights of kubeadm‚Äôs 7th birthday celebration and the foundation for what will come next!</p>\n<p>Stay tuned, and feel free to reach out to us!</p>\n<ul>\n<li>Try <a href=\"https://kubernetes.io/docs/setup/\">kubeadm</a> to install Kubernetes today</li>\n<li>Get involved with the Kubernetes project on <a href=\"https://github.com/kubernetes/kubernetes\">GitHub</a></li>\n<li>Connect with the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for latest updates</li>\n</ul>","PublishedAt":"2023-09-26 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/09/26/happy-7th-birthday-kubeadm/","SourceName":"Kubernetes"}},{"node":{"ID":4640,"Title":"Blog: kubeadm: Use etcd Learner to Join a Control Plane Node Safely","Description":"<p><strong>Author:</strong> Paco Xu (DaoCloud)</p>\n<p>The <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/\"><code>kubeadm</code></a> tool now supports etcd learner mode, which\nallows you to enhance the resilience and stability\nof your Kubernetes clusters by leveraging the <a href=\"https://etcd.io/docs/v3.4/learning/design-learner/#appendix-learner-implementation-in-v34\">learner mode</a>\nfeature introduced in etcd version 3.4.\nThis guide will walk you through using etcd learner mode with kubeadm. By default, kubeadm runs\na local etcd instance on each control plane node.</p>\n<p>In v1.27, kubeadm introduced a new feature gate <code>EtcdLearnerMode</code>. With this feature gate enabled,\nwhen joining a new control plane node, a new etcd member will be created as a learner and\npromoted to a voting member only after the etcd data are fully aligned.</p>\n<h2 id=\"what-are-the-advantages-of-using-etcd-learner-mode\">What are the advantages of using etcd learner mode?</h2>\n<p>etcd learner mode offers several compelling reasons to consider its adoption\nin Kubernetes clusters:</p>\n<ol>\n<li><strong>Enhanced Resilience</strong>: etcd learner nodes are non-voting members that catch up with\nthe leader's logs before becoming fully operational. This prevents new cluster members\nfrom disrupting the quorum or causing leader elections, making the cluster more resilient\nduring membership changes.</li>\n<li><strong>Reduced Cluster Unavailability</strong>: Traditional approaches to adding new members often\nresult in cluster unavailability periods, especially in slow infrastructure or misconfigurations.\netcd learner mode minimizes such disruptions.</li>\n<li><strong>Simplified Maintenance</strong>: Learner nodes provide a safer and reversible way to add or replace\ncluster members. This reduces the risk of accidental cluster outages due to misconfigurations or\nmissteps during member additions.</li>\n<li><strong>Improved Network Tolerance</strong>: In scenarios involving network partitions, learner mode allows\nfor more graceful handling. Depending on the partition a new member lands, it can seamlessly\nintegrate with the existing cluster without causing disruptions.</li>\n</ol>\n<p>In summary, the etcd learner mode improves the reliability and manageability of Kubernetes clusters\nduring member additions and changes, making it a valuable feature for cluster operators.</p>\n<h2 id=\"how-nodes-join-a-cluster-that-s-using-the-new-mode\">How nodes join a cluster that's using the new mode</h2>\n<h3 id=\"create-K8s-cluster-etcd-learner-mode\">Create a Kubernetes cluster backed by etcd in learner mode</h3>\n<p>For a general explanation about creating highly available clusters with kubeadm, you can refer to\n<a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/\">Creating Highly Available Clusters with kubeadm</a>.</p>\n<p>To create a Kubernetes cluster, backed by etcd in learner mode, using kubeadm, follow these steps:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># kubeadm init --feature-gates=EtcdLearnerMode=true ...</span>\n</span></span><span style=\"display:flex;\"><span>kubeadm init --config<span style=\"color:#666\">=</span>kubeadm-config.yaml\n</span></span></code></pre></div><p>The kubeadm configuration file is like below:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta3<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ClusterConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">featureGates</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">EtcdLearnerMode</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The kubeadm tool deploys a single-node Kubernetes cluster with etcd set to use learner mode.</p>\n<h3 id=\"join-nodes-to-the-kubernetes-cluster\">Join nodes to the Kubernetes cluster</h3>\n<p>Before joining a control-plane node to the new Kubernetes cluster, ensure that the existing control plane nodes\nand all etcd members are healthy.</p>\n<p>Check the cluster health with <code>etcdctl</code>. If <code>etcdctl</code> isn't available, you can run this tool inside a container image.\nYou would do that directly with your container runtime using a tool such as <code>crictl run</code> and not through Kubernetes</p>\n<p>Here is an example on a client command that uses secure communication to check the cluster health of the etcd cluster:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#b8860b\">ETCDCTL_API</span><span style=\"color:#666\">=</span><span style=\"color:#666\">3</span> etcdctl --endpoints 127.0.0.1:2379 <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --cert<span style=\"color:#666\">=</span>/etc/kubernetes/pki/etcd/server.crt <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --key<span style=\"color:#666\">=</span>/etc/kubernetes/pki/etcd/server.key <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --cacert<span style=\"color:#666\">=</span>/etc/kubernetes/pki/etcd/ca.crt <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> member list\n</span></span><span style=\"display:flex;\"><span>...\n</span></span><span style=\"display:flex;\"><span>dc543c4d307fadb9, started, node1, https://10.6.177.40:2380, https://10.6.177.40:2379, <span style=\"color:#a2f\">false</span>\n</span></span></code></pre></div><p>To check if the Kubernetes control plane is healthy, run <code>kubectl get node -l node-role.kubernetes.io/control-plane=</code>\nand check if the nodes are ready.</p>\n<p>Note: It is recommended to have an odd number of members in a etcd cluster.</p>\n<p>Before joining a worker node to the new Kubernetes cluster, ensure that the control plane nodes are healthy.</p>\n<h2 id=\"what-s-next\">What's next</h2>\n<ul>\n<li>The feature gate <code>EtcdLearnerMode</code> is alpha in v1.27 and we expect it to graduate to beta in the next\nminor release of Kubernetes (v1.29).</li>\n<li>etcd has an open issue that may make the process more automatic:\n<a href=\"https://github.com/etcd-io/etcd/issues/15107\">Support auto-promoting a learner member to a voting member</a>.</li>\n<li>Learn more about the kubeadm <a href=\"https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/\">configuration format</a> here.</li>\n</ul>\n<h2 id=\"feedback\">Feedback</h2>\n<p>Was this guide helpful? If you have any feedback or encounter any issues, please let us know.\nYour feedback is always welcome! Join the bi-weekly <a href=\"https://docs.google.com/document/d/1Gmc7LyCIL_148a9Tft7pdhdee0NBHdOfHS1SAF0duI4/edit\">SIG Cluster Lifecycle meeting</a>\nor weekly <a href=\"https://docs.google.com/document/d/130_kiXjG7graFNSnIAgtMS1G8zPDwpkshgfRYS0nggo/edit\">kubeadm office hours</a>. Or reach us via <a href=\"https://slack.k8s.io/\">Slack</a> (channel <strong>#kubeadm</strong>), or the <a href=\"https://groups.google.com/g/kubernetes-sig-cluster-lifecycle\">SIG's mailing list</a>.</p>","PublishedAt":"2023-09-25 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/09/25/kubeadm-use-etcd-learner-mode/","SourceName":"Kubernetes"}},{"node":{"ID":4575,"Title":"Blog: User Namespaces: Now Supports Running Stateful Pods in Alpha!","Description":"<p><strong>Authors:</strong> Rodrigo Campos Catelin (Microsoft), Giuseppe Scrivano (Red Hat), Sascha Grunert (Red Hat)</p>\n<p>Kubernetes v1.25 introduced support for user namespaces for only stateless\npods. Kubernetes 1.28 lifted that restriction, after some design changes were\ndone in 1.27.</p>\n<p>The beauty of this feature is that:</p>\n<ul>\n<li>it is trivial to adopt (you just need to set a bool in the pod spec)</li>\n<li>doesn't need any changes for <strong>most</strong> applications</li>\n<li>improves security by <em>drastically</em> enhancing the isolation of containers and\nmitigating CVEs rated HIGH and CRITICAL.</li>\n</ul>\n<p>This post explains the basics of user namespaces and also shows:</p>\n<ul>\n<li>the changes that arrived in the recent Kubernetes v1.28 release</li>\n<li>a <strong>demo of a vulnerability rated as HIGH</strong> that is not exploitable with user namespaces</li>\n<li>the runtime requirements to use this feature</li>\n<li>what you can expect in future releases regarding user namespaces.</li>\n</ul>\n<h2 id=\"what-is-a-user-namespace\">What is a user namespace?</h2>\n<p>A user namespace is a Linux feature that isolates the user and group identifiers\n(UIDs and GIDs) of the containers from the ones on the host. The indentifiers\nin the container can be mapped to indentifiers on the host in a way where the\nhost UID/GIDs used for different containers never overlap. Even more, the\nidentifiers can be mapped to <em>unprivileged</em> non-overlapping UIDs and GIDs on the\nhost. This basically means two things:</p>\n<ul>\n<li>\n<p>As the UIDs and GIDs for different containers are mapped to different UIDs\nand GIDs on the host, containers have a harder time to attack each other even\nif they escape the container boundaries. For example, if container A is running\nwith different UIDs and GIDs on the host than container B, the operations it\ncan do on container B's files and process are limited: only read/write what a\nfile allows to others, as it will never have permission for the owner or\ngroup (the UIDs/GIDs on the host are guaranteed to be different for\ndifferent containers).</p>\n</li>\n<li>\n<p>As the UIDs and GIDs are mapped to unprivileged users on the host, if a\ncontainer escapes the container boundaries, even if it is running as root\ninside the container, it has no privileges on the host. This greatly\nprotects what host files it can read/write, which process it can send signals\nto, etc.</p>\n</li>\n</ul>\n<p>Furthermore, capabilities granted are only valid inside the user namespace and\nnot on the host.</p>\n<p>Without using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capabilities\nwere granted to the container, the capabilities are valid on the host too. None\nof this is true when using user namespaces (modulo bugs, of course üôÇ).</p>\n<h2 id=\"changes-in-1-28\">Changes in 1.28</h2>\n<p>As already mentioned, starting from 1.28, Kubernetes supports user namespaces\nwith stateful pods. This means that pods with user namespaces can use any type\nof volume, they are no longer limited to only some volume types as before.</p>\n<p>The feature gate to activate this feature was renamed, it is no longer\n<code>UserNamespacesStatelessPodsSupport</code> but from 1.28 onwards you should use\n<code>UserNamespacesSupport</code>. There were many changes done and the requirements on\nthe node hosts changed. So with Kubernetes 1.28 the feature flag was renamed to\nreflect this.</p>\n<h2 id=\"demo\">Demo</h2>\n<p>Rodrigo created a demo which exploits <a href=\"https://unit42.paloaltonetworks.com/cve-2022-0492-cgroups/\">CVE 2022-0492</a> and shows how\nthe exploit can occur without user namespaces. He also shows how it is not\npossible to use this exploit from a Pod where the containers are using this\nfeature.</p>\n<p>This vulnerability is rated <strong>HIGH</strong> and allows <strong>a container with no special\nprivileges to read/write to any path on the host</strong> and launch processes as root\non the host too.</p>\n<div style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\">\n<iframe src=\"https://www.youtube.com/embed/M4a2b4KkXN8\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"Mitigation of CVE-2022-0492 on Kubernetes by enabling User Namespace support\"></iframe>\n</div>\n<p>Most applications in containers run as root today, or as a semi-predictable\nnon-root user (user ID 65534 is a somewhat popular choice). When you run a Pod\nwith containers using a userns, Kubernetes runs those containers as unprivileged\nusers, with no changes needed in your app.</p>\n<p>This means two containers running as user 65534 will effectively be mapped to\ndifferent users on the host, limiting what they can do to each other in case of\nan escape, and if they are running as root, the privileges on the host are\nreduced to the one of an unprivileged user.</p>\n<h2 id=\"node-system-requirements\">Node system requirements</h2>\n<p>There are requirements on the Linux kernel version as well as the container\nruntime to use this feature.</p>\n<p>On Linux you need Linux 6.3 or greater. This is because the feature relies on a\nkernel feature named idmap mounts, and support to use idmap mounts with tmpfs\nwas merged in Linux 6.3.</p>\n<p>If you are using CRI-O with crun, this is <a href=\"https://github.com/cri-o/cri-o/releases/tag/v1.28.1\">supported in CRI-O\n1.28.1</a> and crun 1.9 or greater. If you are using CRI-O with runc,\nthis is still not supported.</p>\n<p>containerd support is currently targeted for containerd 2.0; it is likely that\nit won't matter if you use it with crun or runc.</p>\n<p>Please note that containerd 1.7 added <em>experimental</em> support for user\nnamespaces as implemented in Kubernetes 1.25 and 1.26. The redesign done in 1.27\nis not supported by containerd 1.7, therefore it only works, in terms of user\nnamespaces support, with Kubernetes 1.25 and 1.26.</p>\n<p>One limitation present in containerd 1.7 is that it needs to change the\nownership of every file and directory inside the container image, during Pod\nstartup. This means it has a storage overhead and can significantly impact the\ncontainer startup latency. Containerd 2.0 will probably include a implementation\nthat will eliminate the startup latency added and the storage overhead. Take\nthis into account if you plan to use containerd 1.7 with user namespaces in\nproduction.</p>\n<p>None of these containerd limitations apply to <a href=\"https://github.com/cri-o/cri-o/releases/tag/v1.28.1\">CRI-O 1.28</a>.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>Looking ahead to Kubernetes 1.29, the plan is to work with SIG Auth to integrate user\nnamespaces to Pod Security Standards (PSS) and the Pod Security Admission. For\nthe time being, the plan is to relax checks in PSS policies when user namespaces are\nin use. This means that the fields <code>spec[.*].securityContext</code> <code>runAsUser</code>,\n<code>runAsNonRoot</code>, <code>allowPrivilegeEscalation</code> and <code>capabilities</code> will not trigger a\nviolation if user namespaces are in use. The behavior will probably be controlled by\nutilizing a API Server feature gate, like <code>UserNamespacesPodSecurityStandards</code>\nor similar.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>You can reach SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnode\">Open Community Issues/PRs</a></li>\n</ul>\n<p>You can also contact us directly:</p>\n<ul>\n<li>GitHub: @rata @giuseppe @saschagrunert</li>\n<li>Slack: @rata @giuseppe @sascha</li>\n</ul>","PublishedAt":"2023-09-13 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/09/13/userns-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":4561,"Title":"Blog: Comparing Local Kubernetes Development Tools: Telepresence, Gefyra, and mirrord","Description":"<p><strong>Author:</strong> Eyal Bukchin (MetalBear)</p>\n<p>The Kubernetes development cycle is an evolving landscape with a myriad of tools seeking to streamline the process. Each tool has its unique approach, and the choice often comes down to individual project requirements, the team's expertise, and the preferred workflow.</p>\n<p>Among the various solutions, a category we dubbed ‚ÄúLocal K8S Development tools‚Äù has emerged, which seeks to enhance the Kubernetes development experience by connecting locally running components to the Kubernetes cluster. This facilitates rapid testing of new code in cloud conditions, circumventing the traditional cycle of Dockerization, CI, and deployment.</p>\n<p>In this post, we compare three solutions in this category: Telepresence, Gefyra, and our own contender, mirrord.</p>\n<h2 id=\"telepresence\">Telepresence</h2>\n<p>The oldest and most well-established solution in the category, <a href=\"https://www.telepresence.io/\">Telepresence</a> uses a VPN (or more specifically, a <code>tun</code> device) to connect the user's machine (or a locally running container) and the cluster's network. It then supports the interception of incoming traffic to a specific service in the cluster, and its redirection to a local port. The traffic being redirected can also be filtered to avoid completely disrupting the remote service. It also offers complementary features to support file access (by locally mounting a volume mounted to a pod) and importing environment variables.\nTelepresence requires the installation of a local daemon on the user's machine (which requires root privileges) and a Traffic Manager component on the cluster. Additionally, it runs an Agent as a sidecar on the pod to intercept the desired traffic.</p>\n<h2 id=\"gefyra\">Gefyra</h2>\n<p><a href=\"https://gefyra.dev/\">Gefyra</a>, similar to Telepresence, employs a VPN to connect to the cluster. However, it only supports connecting locally running Docker containers to the cluster. This approach enhances portability across different OSes and local setups. However, the downside is that it does not support natively run uncontainerized code.</p>\n<p>Gefyra primarily focuses on network traffic, leaving file access and environment variables unsupported. Unlike Telepresence, it doesn't alter the workloads in the cluster, ensuring a straightforward clean-up process if things go awry.</p>\n<h2 id=\"mirrord\">mirrord</h2>\n<p>The newest of the three tools, <a href=\"https://mirrord.dev/\">mirrord</a> adopts a different approach by injecting itself\ninto the local binary (utilizing <code>LD_PRELOAD</code> on Linux or <code>DYLD_INSERT_LIBRARIES</code> on macOS),\nand overriding libc function calls, which it then proxies a temporary agent it runs in the cluster.\nFor example, when the local process tries to read a file mirrord intercepts that call and sends it\nto the agent, which then reads the file from the remote pod. This method allows mirrord to cover\nall inputs and outputs to the process ‚Äì covering network access, file access, and\nenvironment variables uniformly.</p>\n<p>By working at the process level, mirrord supports running multiple local processes simultaneously, each in the context of their respective pod in the cluster, without requiring them to be containerized and without needing root permissions on the user‚Äôs machine.</p>\n<h2 id=\"summary\">Summary</h2>\n<table>\n<caption>Comparison of Telepresence, Gefyra, and mirrord</caption>\n<thead>\n<tr>\n<td class=\"empty\"></td>\n<th>Telepresence</th>\n<th>Gefyra</th>\n<th>mirrord</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<th scope=\"row\">Cluster connection scope</th>\n<td>Entire machine or container</td>\n<td>Container</td>\n<td>Process</td>\n</tr>\n<tr>\n<th scope=\"row\">Developer OS support</th>\n<td>Linux, macOS, Windows</td>\n<td>Linux, macOS, Windows</td>\n<td>Linux, macOS, Windows (WSL)</td>\n</tr>\n<tr>\n<th scope=\"row\">Incoming traffic features</th>\n<td>Interception</td>\n<td>Interception</td>\n<td>Interception or mirroring</td>\n</tr>\n<tr>\n<th scope=\"row\">File access</th>\n<td>Supported</td>\n<td>Unsupported</td>\n<td>Supported</td>\n</tr>\n<tr>\n<th scope=\"row\">Environment variables</th>\n<td>Supported</td>\n<td>Unsupported</td>\n<td>Supported</td>\n</tr>\n<tr>\n<th scope=\"row\">Requires local root</th>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n</tr>\n<tr>\n<th scope=\"row\">How to use</th>\n<td><ul><li>CLI</li><li>Docker Desktop extension</li></ul></td>\n<td><ul><li>CLI</li><li>Docker Desktop extension</li></ul></td>\n<td><ul><li>CLI</li><li>Visual Studio Code extension</li><li>IntelliJ plugin</li></ul></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Telepresence, Gefyra, and mirrord each offer unique approaches to streamline the Kubernetes development cycle, each having its strengths and weaknesses. Telepresence is feature-rich but comes with complexities, mirrord offers a seamless experience and supports various functionalities, while Gefyra aims for simplicity and robustness.</p>\n<p>Your choice between them should depend on the specific requirements of your project, your team's familiarity with the tools, and the desired development workflow. Whichever tool you choose, we believe the local Kubernetes development approach can provide an easy, effective, and cheap solution to the bottlenecks of the Kubernetes development cycle, and will become even more prevalent as these tools continue to innovate and evolve.</p>","PublishedAt":"2023-09-12 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/09/12/local-k8s-development-tools/","SourceName":"Kubernetes"}},{"node":{"ID":4504,"Title":"Blog: Kubernetes Legacy Package Repositories Will Be Frozen On September 13, 2023","Description":"<p><strong>Authors</strong>: Bob Killen (Google), Chris Short (AWS), Jeremy Rickard (Microsoft), Marko Mudriniƒá (Kubermatic), Tim Bannister (The Scale Factory)</p>\n<p>On August 15, 2023, the Kubernetes project announced the general availability of\nthe community-owned package repositories for Debian and RPM packages available\nat <code>pkgs.k8s.io</code>. The new package repositories are replacement for the legacy\nGoogle-hosted package repositories: <code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>.\nThe\n<a href=\"https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/\">announcement blog post for <code>pkgs.k8s.io</code></a>\nhighlighted that we will stop publishing packages to the legacy repositories in\nthe future.</p>\n<p>Today, we're formally deprecating the legacy package repositories (<code>apt.kubernetes.io</code>\nand <code>yum.kubernetes.io</code>), and we're announcing our plans to freeze the contents of\nthe repositories as of <strong>September 13, 2023</strong>.</p>\n<p>Please continue reading in order to learn what does this mean for you as an user or\ndistributor, and what steps you may need to take.</p>\n<h2 id=\"how-does-this-affect-me-as-a-kubernetes-end-user\">How does this affect me as a Kubernetes end user?</h2>\n<p>This change affects users <strong>directly installing upstream versions of Kubernetes</strong>,\neither manually by following the official\n<a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\">installation</a> and\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\">upgrade</a> instructions, or\nby <strong>using a Kubernetes installer</strong> that's using packages provided by the Kubernetes\nproject.</p>\n<p><strong>This change also affects you if you run Linux on your own PC and have installed <code>kubectl</code> using the legacy package repositories</strong>.\nWe'll explain later on how to <a href=\"#check-if-affected\">check</a> if you're affected.</p>\n<p>If you use <strong>fully managed</strong> Kubernetes, for example through a service from a cloud\nprovider, you would only be affected by this change if you also installed <code>kubectl</code>\non your Linux PC using packages from the legacy repositories. Cloud providers are\ngenerally using their own Kubernetes distributions and therefore they don't use\npackages provided by the Kubernetes project; more importantly, if someone else is\nmanaging Kubernetes for you, then they would usually take responsibility for that check.</p>\n<p>If you have a managed <a href=\"https://kubernetes.io/docs/concepts/overview/components/#control-plane-components\">control plane</a>\nbut you are responsible for <strong>managing the nodes yourself</strong>, and any of those nodes run Linux,\nyou should <a href=\"#check-if-affected\">check</a> whether you are affected.</p>\n<p>If you're managing your clusters on your own by following the official installation\nand upgrade instructions, please follow the instructions in this blog post to migrate\nto the (new) community-owned package repositories.</p>\n<p>If you're using a Kubernetes installer that's using packages provided by the\nKubernetes project, please check the installer tool's communication channels for\ninformation about what steps you need to take, and eventually if needed, follow up\nwith maintainers to let them know about this change.</p>\n<h2 id=\"how-does-this-affect-me-as-a-kubernetes-distributor\">How does this affect me as a Kubernetes distributor?</h2>\n<p>If you're using the legacy repositories as part of your project (e.g. a Kubernetes\ninstaller tool), you should migrate to the community-owned repositories as soon as\npossible and inform your users about this change and what steps they need to take.</p>\n<h2 id=\"timeline-of-changes\">Timeline of changes</h2>\n<!-- note to maintainers - the trailing whitespace is significant -->\n<ul>\n<li><strong>15th August 2023:</strong><br>\nKubernetes announces a new, community-managed source for Linux software packages of Kubernetes components</li>\n<li><strong>31st August 2023:</strong><br>\n<em>(this announcement)</em> Kubernetes formally deprecates the legacy\npackage repositories</li>\n<li><strong>13th September 2023</strong> (approximately):<br>\nKubernetes will freeze the legacy package repositories,\n(<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>).\nThe freeze will happen immediately following the patch releases that are scheduled for September, 2023.</li>\n</ul>\n<p>The Kubernetes patch releases scheduled for September 2023 (v1.28.2, v1.27.6,\nv1.26.9, v1.25.14) will have packages published <strong>both</strong> to the community-owned and\nthe legacy repositories.</p>\n<p>We'll freeze the legacy repositories after cutting the patch releases for September\nwhich means that we'll completely stop publishing packages to the legacy repositories\nat that point.</p>\n<p>For the v1.28, v1.27, v1.26, and v1.25 patch releases from October 2023 and onwards,\nwe'll only publish packages to the new package repositories (<code>pkgs.k8s.io</code>).</p>\n<h3 id=\"what-about-future-minor-releases\">What about future minor releases?</h3>\n<p>Kubernetes 1.29 and onwards will have packages published <strong>only</strong> to the\ncommunity-owned repositories (<code>pkgs.k8s.io</code>).</p>\n<h2 id=\"can-i-continue-to-use-the-legacy-package-repositories\">Can I continue to use the legacy package repositories?</h2>\n<p>The existing packages in the legacy repositories will be available for the foreseeable\nfuture. However, the Kubernetes project can't provide <em>any</em> guarantees on how long\nis that going to be. The deprecated legacy repositories, and their contents, might\nbe removed at any time in the future and without a further notice period.</p>\n<p>The Kubernetes project <strong>strongly recommends</strong> migrating to the new community-owned\nrepositories <strong>as soon as possible</strong>.</p>\n<p>Given that no new releases will be published to the legacy repositories <strong>after the September 13, 2023</strong>\ncut-off point, <strong>you will not be able to upgrade to any patch or minor release made from that date onwards.</strong></p>\n<p>Whilst the project makes every effort to release secure software, there may one\nday be a high-severity vulnerability in Kubernetes, and consequently an important\nrelease to upgrade to. The advice we're announcing will help you be as prepared for\nany future security update, whether trivial or urgent.</p>\n<h2 id=\"check-if-affected\">How can I check if I'm using the legacy repositories?</h2>\n<p>The steps to check if you're using the legacy repositories depend on whether you're\nusing Debian-based distributions (Debian, Ubuntu, and more) or RPM-based distributions\n(CentOS, RHEL, Rocky Linux, and more) in your cluster.</p>\n<p>Run these instructions on one of your nodes in the cluster.</p>\n<h3 id=\"debian-based-linux-distributions\">Debian-based Linux distributions</h3>\n<p>The repository definitions (sources) are located in <code>/etc/apt/sources.list</code> and <code>/etc/apt/sources.list.d/</code>\non Debian-based distributions. Inspect these two locations and try to locate a\npackage repository definition that looks like:</p>\n<pre tabindex=\"0\"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\n</code></pre><p><strong>If you find a repository definition that looks like this, you're using the legacy repository and you need to migrate.</strong></p>\n<p>If the repository definition uses <code>pkgs.k8s.io</code>, you're already using the\ncommunity-hosted repositories and you don't need to take any action.</p>\n<p>On most systems, this repository definition should be located in\n<code>/etc/apt/sources.list.d/kubernetes.list</code> (as recommended by the Kubernetes\ndocumentation), but on some systems it might be in a different location.</p>\n<p>If you can't find a repository definition related to Kubernetes, it's likely that you\ndon't use package managers to install Kubernetes and you don't need to take any action.</p>\n<h3 id=\"rpm-based-linux-distributions\">RPM-based Linux distributions</h3>\n<p>The repository definitions are located in <code>/etc/yum.repos.d</code> if you're using the\n<code>yum</code> package manager, or <code>/etc/dnf/dnf.conf</code> and <code>/etc/dnf/repos.d/</code> if you're using\n<code>dnf</code> package manager. Inspect those locations and try to locate a package repository\ndefinition that looks like this:</p>\n<pre tabindex=\"0\"><code>[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nexclude=kubelet kubeadm kubectl\n</code></pre><p><strong>If you find a repository definition that looks like this, you're using the legacy repository and you need to migrate.</strong></p>\n<p>If the repository definition uses <code>pkgs.k8s.io</code>, you're already using the\ncommunity-hosted repositories and you don't need to take any action.</p>\n<p>On most systems, that repository definition should be located in <code>/etc/yum.repos.d/kubernetes.repo</code>\n(as recommended by the Kubernetes documentation), but on some systems it might be\nin a different location.</p>\n<p>If you can't find a repository definition related to Kubernetes, it's likely that you\ndon't use package managers to install Kubernetes and you don't need to take any action.</p>\n<h2 id=\"how-can-i-migrate-to-the-new-community-operated-repositories\">How can I migrate to the new community-operated repositories?</h2>\n<p>For more information on how to migrate to the new community\nmanaged packages, please refer to the\n<a href=\"https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/\">announcement blog post for <code>pkgs.k8s.io</code></a>.</p>\n<h2 id=\"why-is-the-kubernetes-project-making-this-change\">Why is the Kubernetes project making this change?</h2>\n<p>Kubernetes has been publishing packages solely to the Google-hosted repository\nsince Kubernetes v1.5, or the past <strong>seven</strong> years! Following in the footsteps of\nmigrating to our community-managed registry, <code>registry.k8s.io</code>, we are now migrating the\nKubernetes package repositories to our own community-managed infrastructure. We‚Äôre\nthankful to Google for their continuous hosting and support all these years, but\nthis transition marks another big milestone for the project‚Äôs goal of migrating\nto complete community-owned infrastructure.</p>\n<h2 id=\"is-there-a-kubernetes-tool-to-help-me-migrate\">Is there a Kubernetes tool to help me migrate?</h2>\n<p>We don't have any announcement to make about tooling there. As a Kubernetes user, you\nhave to manually modify your configuration to use the new repositories. Automating\nthe migration from the legacy to the community-owned repositories is technically\nchallenging and we want to avoid any potential risks associated with this.</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>First of all, we want to acknowledge the contributions from Alphabet. Staff at Google\nhave provided their time; Google as a business has provided both the infrastructure\nto serve packages, and the security context for giving those packages trustworthy\ndigital signatures.\nThese have been important to the adoption and growth of Kubernetes.</p>\n<p>Releasing software might not be glamorous but it's important. Many people within\nthe Kubernetes contributor community have contributed to the new way that we, as a\nproject, have for building and publishing packages.</p>\n<p>And finally, we want to once again acknowledge the help from SUSE. OpenBuildService,\nfrom SUSE, is the technology that the powers the new community-managed package repositories.</p>","PublishedAt":"2023-08-31 22:30:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/","SourceName":"Kubernetes"}},{"node":{"ID":4479,"Title":"Blog: Gateway API v0.8.0: Introducing Service Mesh Support","Description":"<p><em><strong>Authors:</strong></em> Flynn (Buoyant), John Howard (Google), Keith Mattix (Microsoft), Michael Beaumont (Kong), Mike Morris (independent), Rob Scott (Google)</p>\n<p>We are thrilled to announce the v0.8.0 release of Gateway API! With this\nrelease, Gateway API support for service mesh has reached <a href=\"https://gateway-api.sigs.k8s.io/geps/overview/#status\">Experimental\nstatus</a>. We look forward to your feedback!</p>\n<p>We're especially delighted to announce that Kuma 2.3+, Linkerd 2.14+, and Istio\n1.16+ are all fully-conformant implementations of Gateway API service mesh\nsupport.</p>\n<h2 id=\"service-mesh-support-in-gateway-api\">Service mesh support in Gateway API</h2>\n<p>While the initial focus of Gateway API was always ingress (north-south)\ntraffic, it was clear almost from the beginning that the same basic routing\nconcepts should also be applicable to service mesh (east-west) traffic. In\n2022, the Gateway API subproject started the <a href=\"https://gateway-api.sigs.k8s.io/concepts/gamma/\">GAMMA initiative</a>, a\ndedicated vendor-neutral workstream, specifically to examine how best to fit\nservice mesh support into the framework of the Gateway API resources, without\nrequiring users of Gateway API to relearn everything they understand about the\nAPI.</p>\n<p>Over the last year, GAMMA has dug deeply into the challenges and possible\nsolutions around using Gateway API for service mesh. The end result is a small\nnumber of <a href=\"https://gateway-api.sigs.k8s.io/contributing/enhancement-requests/\">enhancement proposals</a> that subsume many hours of thought and\ndebate, and provide a minimum viable path to allow Gateway API to be used for\nservice mesh.</p>\n<h3 id=\"how-will-mesh-routing-work-when-using-gateway-api\">How will mesh routing work when using Gateway API?</h3>\n<p>You can find all the details in the <a href=\"https://gateway-api.sigs.k8s.io/concepts/gamma/#how-the-gateway-api-works-for-service-mesh\">Gateway API Mesh routing\ndocumentation</a> and <a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1426/\">GEP-1426</a>, but the short version for Gateway\nAPI v0.8.0 is that an HTTPRoute can now have a <code>parentRef</code> that is a Service,\nrather than just a Gateway. We anticipate future GEPs in this area as we gain\nmore experience with service mesh use cases -- binding to a Service makes it\npossible to use the Gateway API with a service mesh, but there are several\ninteresting use cases that remain difficult to cover.</p>\n<p>As an example, you might use an HTTPRoute to do an A-B test in the mesh as\nfollows:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>gateway.networking.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>HTTPRoute<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>bar-route<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">parentRefs</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">group</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>demo-app<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">matches</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">headers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Exact<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>env<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">backendRefs</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>demo-app-v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">backendRefs</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>demo-app-v2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5000</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Any request to port 5000 of the <code>demo-app</code> Service that has the header <code>env: v1</code> will be routed to <code>demo-app-v1</code>, while any request without that header\nwill be routed to <code>demo-app-v2</code> -- and since this is being handled by the\nservice mesh, not the ingress controller, the A/B test can happen anywhere in\nthe application's call graph.</p>\n<h3 id=\"how-do-i-know-this-will-be-truly-portable\">How do I know this will be truly portable?</h3>\n<p>Gateway API has been investing heavily in conformance tests across all\nfeatures it supports, and mesh is no exception. One of the challenges that the\nGAMMA initiative ran into is that many of these tests were strongly tied to\nthe idea that a given implementation provides an ingress controller. Many\nservice meshes don't, and requiring a GAMMA-conformant mesh to also implement\nan ingress controller seemed impractical at best. This resulted in work\nrestarting on Gateway API <em>conformance profiles</em>, as discussed in <a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1709/\">GEP-1709</a>.</p>\n<p>The basic idea of conformance profiles is that we can define subsets of the\nGateway API, and allow implementations to choose (and document) which subsets\nthey conform to. GAMMA is adding a new profile, named <code>Mesh</code> and described in\n<a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1686/\">GEP-1686</a>, which checks only the mesh functionality as defined by GAMMA. At\nthis point, Kuma 2.3+, Linkerd 2.14+, and Istio 1.16+ are all conformant with\nthe <code>Mesh</code> profile.</p>\n<h2 id=\"what-else-is-in-gateway-api-v0-8-0\">What else is in Gateway API v0.8.0?</h2>\n<p>This release is all about preparing Gateway API for the upcoming v1.0 release\nwhere HTTPRoute, Gateway, and GatewayClass will graduate to GA. There are two\nmain changes related to this: CEL validation and API version changes.</p>\n<h3 id=\"cel-validation\">CEL Validation</h3>\n<p>The first major change is that Gateway API v0.8.0 is the start of a transition\nfrom webhook validation to <a href=\"https://kubernetes.io/docs/reference/using-api/cel/\">CEL validation</a> using information built into\nthe CRDs. That will mean different things depending on the version of\nKubernetes you're using:</p>\n<h4 id=\"kubernetes-1-25\">Kubernetes 1.25+</h4>\n<p>CEL validation is fully supported, and almost all validation is implemented in\nCEL. (The sole exception is that header names in header modifier filters can\nonly do case-insensitive validation. There is more information in <a href=\"https://github.com/kubernetes-sigs/gateway-api/issues/2277\">issue\n2277</a>.)</p>\n<p>We recommend <em>not</em> using the validating webhook on these Kubernetes versions.</p>\n<h4 id=\"kubernetes-1-23-and-1-24\">Kubernetes 1.23 and 1.24</h4>\n<p>CEL validation is not supported, but Gateway API v0.8.0 CRDs can still be\ninstalled. When you upgrade to Kubernetes 1.25+, the validation included in\nthese CRDs will automatically take effect.</p>\n<p>We recommend continuing to use the validating webhook on these Kubernetes\nversions.</p>\n<h4 id=\"kubernetes-1-22-and-older\">Kubernetes 1.22 and older</h4>\n<p>Gateway API only commits to support for <a href=\"https://gateway-api.sigs.k8s.io/concepts/versioning/#supported-versions\">5 most recent versions of\nKubernetes</a>. As such, these versions are no longer\nsupported by Gateway API, and unfortunately Gateway API v0.8.0 cannot be\ninstalled on them, since CRDs containing CEL validation will be rejected.</p>\n<h3 id=\"api-version-changes\">API Version Changes</h3>\n<p>As we prepare for a v1.0 release that will graduate Gateway, GatewayClass, and\nHTTPRoute to the <code>v1</code> API Version from <code>v1beta1</code>, we are continuing the process\nof moving away from <code>v1alpha2</code> for resources that have graduated to <code>v1beta1</code>.\nFor more information on this change and everything else included in this\nrelease, refer to the <a href=\"https://github.com/kubernetes-sigs/gateway-api/releases/tag/v0.8.0\">v0.8.0 release notes</a>.</p>\n<h2 id=\"how-can-i-get-started-with-gateway-api\">How can I get started with Gateway API?</h2>\n<p>Gateway API represents the future of load balancing, routing, and service mesh\nAPIs in Kubernetes. There are already more than 20 <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">implementations</a>\navailable (including both ingress controllers and service meshes) and the list\nkeeps growing.</p>\n<p>If you're interested in getting started with Gateway API, take a look at the\n<a href=\"https://gateway-api.sigs.k8s.io/concepts/api-overview/\">API concepts documentation</a> and check out some of the\n<a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/\">Guides</a> to try it out. Because this is a CRD-based API, you can\ninstall the latest version on any Kubernetes 1.23+ cluster.</p>\n<p>If you're specifically interested in helping to contribute to Gateway API, we\nwould love to have you! Please feel free to <a href=\"https://github.com/kubernetes-sigs/gateway-api/issues/new/choose\">open a new issue</a> on the\nrepository, or join in the <a href=\"https://github.com/kubernetes-sigs/gateway-api/discussions\">discussions</a>. Also check out the <a href=\"https://gateway-api.sigs.k8s.io/contributing/community/\">community\npage</a> which includes links to the Slack channel and community\nmeetings. We look forward to seeing you!!</p>\n<h2 id=\"further-reading\">Further Reading:</h2>\n<ul>\n<li><a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1324/\">GEP-1324</a> provides an overview of the GAMMA goals and some important\ndefinitions. This GEP is well worth a read for its discussion of the problem\nspace.</li>\n<li><a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1426/\">GEP-1426</a> defines how to use Gateway API route resources, such as\nHTTPRoute, to manage traffic within a service mesh.</li>\n<li><a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1686/\">GEP-1686</a> builds on the work of <a href=\"https://gateway-api.sigs.k8s.io/geps/gep-1709/\">GEP-1709</a> to define a <em>conformance\nprofile</em> for service meshes to be declared conformant with Gateway API.</li>\n</ul>\n<p>Although these are <a href=\"https://gateway-api.sigs.k8s.io/geps/overview/#status\">Experimental</a> patterns, note that they are available\nin the <a href=\"https://gateway-api.sigs.k8s.io/concepts/versioning/#release-channels-eg-experimental-standard\"><code>standard</code> release channel</a>, since the GAMMA initiative has not\nneeded to introduce new resources or fields to date.</p>","PublishedAt":"2023-08-29 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/29/gateway-api-v0-8/","SourceName":"Kubernetes"}},{"node":{"ID":4468,"Title":"Blog: Kubernetes 1.28: A New (alpha) Mechanism For Safer Cluster Upgrades","Description":"<p><strong>Author:</strong> Richa Banker (Google)</p>\n<p>This blog describes the <em>mixed version proxy</em>, a new alpha feature in Kubernetes 1.28. The\nmixed version proxy enables an HTTP request for a resource to be served by the correct API server\nin cases where there are multiple API servers at varied versions in a cluster. For example,\nthis is useful during a cluster upgrade, or when you're rolling out the runtime configuration of\nthe cluster's control plane.</p>\n<h2 id=\"what-problem-does-this-solve\">What problem does this solve?</h2>\n<p>When a cluster undergoes an upgrade, the kube-apiservers existing at different versions in that scenario can serve different sets (groups, versions, resources) of built-in resources. A resource request made in this scenario may be served by any of the available apiservers, potentially resulting in the request ending up at an apiserver that may not be aware of the requested resource; consequently it being served a 404 not found error which is incorrect. Furthermore, incorrect serving of the 404 errors can lead to serious consequences such as namespace deletion being blocked incorrectly or objects being garbage collected mistakenly.</p>\n<h2 id=\"how-do-we-solve-the-problem\">How do we solve the problem?</h2>\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/images/blog/2023-08-28-a-new-alpha-mechanism-for-safer-cluster-upgrades/mvp-flow-diagram.svg\"/>\n</figure>\n<p>The new feature ‚ÄúMixed Version Proxy‚Äù provides the kube-apiserver with the capability to proxy a request to a peer kube-apiserver which is aware of the requested resource and hence can serve the request. To do this, a new filter has been added to the handler chain in the API server's aggregation layer.</p>\n<ol>\n<li>The new filter in the handler chain checks if the request is for a group/version/resource that the apiserver doesn't know about (using the existing <a href=\"https://github.com/kubernetes/kubernetes/blob/release-1.28/pkg/apis/apiserverinternal/types.go#L25-L37\">StorageVersion API</a>). If so, it proxies the request to one of the apiservers that is listed in the ServerStorageVersion object. If the identified peer apiserver fails to respond (due to reasons like network connectivity, race between the request being received and the controller registering the apiserver-resource info in ServerStorageVersion object), then error 503(&quot;Service Unavailable&quot;) is served.</li>\n<li>To prevent indefinite proxying of the request, a (new for v1.28) HTTP header <code>X-Kubernetes-APIServer-Rerouted: true</code> is added to the original request once it is determined that the request cannot be served by the original API server. Setting that to true marks that the original API server couldn't handle the request and it should therefore be proxied. If a destination peer API server sees this header, it never proxies the request further.</li>\n<li>To set the network location of a kube-apiserver that peers will use to proxy requests, the value passed in <code>--advertise-address</code> or (when <code>--advertise-address</code> is unspecified) the <code>--bind-address</code> flag is used. For users with network configurations that would not allow communication between peer kube-apiservers using the addresses specified in these flags, there is an option to pass in the correct peer address as <code>--peer-advertise-ip</code> and <code>--peer-advertise-port</code> flags that are introduced in this feature.</li>\n</ol>\n<h2 id=\"how-do-i-enable-this-feature\">How do I enable this feature?</h2>\n<p>Following are the required steps to enable the feature:</p>\n<ul>\n<li>Download the <a href=\"https://kubernetes.io/releases/download/\">latest Kubernetes project</a> (version <code>v1.28.0</code> or later)</li>\n<li>Switch on the feature gate with the command line flag <code>--feature-gates=UnknownVersionInteroperabilityProxy=true</code> on the kube-apiservers</li>\n<li>Pass the CA bundle that will be used by source kube-apiserver to authenticate destination kube-apiserver's serving certs using the flag <code>--peer-ca-file</code> on the kube-apiservers. Note: this is a required flag for this feature to work. There is no default value enabled for this flag.</li>\n<li>Pass the correct ip and port of the local kube-apiserver that will be used by peers to connect to this kube-apiserver while proxying a request. Use the flags <code>--peer-advertise-ip</code> and <code>peer-advertise-port</code> to the kube-apiservers upon startup. If unset, the value passed to either <code>--advertise-address</code> or <code>--bind-address</code> is used. If those too, are unset, the host's default interface will be used.</li>\n</ul>\n<h2 id=\"what-s-missing\">What‚Äôs missing?</h2>\n<p>Currently we only proxy resource requests to a peer kube-apiserver when its determined to do so. Next we need to address how to work discovery requests in such scenarios. Right now we are planning to have the following capabilities for beta</p>\n<ul>\n<li>Merged discovery across all kube-apiservers</li>\n<li>Use an egress dialer for network connections made to peer kube-apiservers</li>\n</ul>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>Read the <a href=\"https://kubernetes.io/docs/concepts/architecture/mixed-version-proxy\">Mixed Version Proxy documentation</a></li>\n<li>Read <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4020-unknown-version-interoperability-proxy\">KEP-4020: Unknown Version Interoperability Proxy</a></li>\n</ul>\n<h2 id=\"how-can-i-get-involved\">How can I get involved?</h2>\n<p>Reach us on <a href=\"https://slack.k8s.io/\">Slack</a>: <a href=\"https://kubernetes.slack.com/messages/sig-api-machinery\">#sig-api-machinery</a>, or through the <a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery\">mailing list</a>.</p>\n<p>Huge thanks to the contributors that have helped in the design, implementation, and review of this feature: Daniel Smith, Han Kang, Joe Betz, Jordan Liggit, Antonio Ojea, David Eads and Ben Luddy!</p>","PublishedAt":"2023-08-28 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/28/kubernetes-1-28-feature-mixed-version-proxy-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":4463,"Title":"Blog: Kubernetes v1.28: Introducing native sidecar containers","Description":"<p><em><strong>Authors:</strong></em> Todd Neal (AWS), Matthias Bertschy (ARMO), Sergey Kanzhelev (Google), Gunju Kim (NAVER), Shannon Kularathna (Google)</p>\n<p>This post explains how to use the new sidecar feature, which enables restartable init containers and is available in alpha in Kubernetes 1.28. We want your feedback so that we can graduate this feature as soon as possible.</p>\n<p>The concept of a ‚Äúsidecar‚Äù has been part of Kubernetes since nearly the very beginning. In 2015, sidecars were described in a <a href=\"https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/\">blog post</a> about composite containers as additional containers that ‚Äúextend and enhance the ‚Äòmain‚Äô container‚Äù. Sidecar containers have become a common Kubernetes deployment pattern and are often used for network proxies or as part of a logging system. Until now, sidecars were a concept that Kubernetes users applied without native support. The lack of native support has caused some usage friction, which this enhancement aims to resolve.</p>\n<h2 id=\"what-are-sidecar-containers-in-1-28\">What are sidecar containers in 1.28?</h2>\n<p>Kubernetes 1.28 adds a new <code>restartPolicy</code> field to <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\">init containers</a> that is available when the <code>SidecarContainers</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> is enabled.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">initContainers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>secret-fetch<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>secret-fetch:1.0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>network-proxy<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>network-proxy:1.0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">restartPolicy</span>:<span style=\"color:#bbb\"> </span>Always<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The field is optional and, if set, the only valid value is Always. Setting this field changes the behavior of init containers as follows:</p>\n<ul>\n<li>The container restarts if it exits</li>\n<li>Any subsequent init container starts immediately after the <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes\">startupProbe</a> has successfully completed instead of waiting for the restartable init container to exit</li>\n<li>The resource usage calculation changes for the pod as restartable init container resources are now added to the sum of the resource requests by the main containers</li>\n</ul>\n<p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination\">Pod termination</a> continues to only depend on the main containers. An init container with a <code>restartPolicy</code> of <code>Always</code> (named a sidecar) won't prevent the pod from terminating after the main containers exit.</p>\n<p>The following properties of restartable init containers make them ideal for the sidecar deployment pattern:</p>\n<ul>\n<li>Init containers have a well-defined startup order regardless of whether you set a <code>restartPolicy</code>, so you can ensure that your sidecar starts before any container declarations that come after the sidecar declaration in your manifest.</li>\n<li>Sidecar containers don't extend the lifetime of the Pod, so you can use them in short-lived Pods with no changes to the Pod lifecycle.</li>\n<li>Sidecar containers are restarted on exit, which improves resilience and lets you use sidecars to provide services that your main containers can more reliably consume.</li>\n</ul>\n<h2 id=\"when-to-use-sidecar-containers\">When to use sidecar containers</h2>\n<p>You might find built-in sidecar containers useful for workloads such as the following:</p>\n<ul>\n<li><strong>Batch or AI/ML workloads</strong>, or other Pods that run to completion. These workloads will experience the most significant benefits.</li>\n<li><strong>Network proxies</strong> that start up before any other container in the manifest. Every other container that runs can use the proxy container's services. For instructions, see the <a href=\"https://istio.io/latest/blog/2023/native-sidecars/\">Kubernetes Native sidecars in Istio blog post</a>.</li>\n<li><strong>Log collection containers</strong>, which can now start before any other container and run until the Pod terminates. This improves the reliability of log collection in your Pods.</li>\n<li><strong>Jobs</strong>, which can use sidecars for any purpose without Job completion being blocked by the running sidecar. No additional configuration is required to ensure this behavior.</li>\n</ul>\n<h2 id=\"how-did-users-get-sidecar-behavior-before-1-28\">How did users get sidecar behavior before 1.28?</h2>\n<p>Prior to the sidecar feature, the following options were available for implementing sidecar behavior depending on the desired lifetime of the sidecar container:</p>\n<ul>\n<li><strong>Lifetime of sidecar less than Pod lifetime</strong>: Use an init container, which provides well-defined startup order. However, the sidecar has to exit for other init containers and main Pod containers to start.</li>\n<li><strong>Lifetime of sidecar equal to Pod lifetime</strong>: Use a main container that runs alongside your workload containers in the Pod. This method doesn't give you control over startup order, and lets the sidecar container potentially block Pod termination after the workload containers exit.</li>\n</ul>\n<p>The built-in sidecar feature solves for the use case of having a lifetime equal to the Pod lifetime and has the following additional benefits:</p>\n<ul>\n<li>Provides control over startup order</li>\n<li>Doesn‚Äôt block Pod termination</li>\n</ul>\n<h2 id=\"transitioning-existing-sidecars-to-the-new-model\">Transitioning existing sidecars to the new model</h2>\n<p>We recommend only using the sidecars feature gate in <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages\">short lived testing clusters</a> at the alpha stage. If you have an existing sidecar that is configured as a main container so it can run for the lifetime of the pod, it can be moved to the <code>initContainers</code> section of the pod spec and given a <code>restartPolicy</code> of <code>Always</code>. In many cases, the sidecar should work as before with the added benefit of having a defined startup ordering and not prolonging the pod lifetime.</p>\n<h2 id=\"known-issues\">Known issues</h2>\n<p>The alpha release of built-in sidecar containers has the following known issues, which we'll resolve before graduating the feature to beta:</p>\n<ul>\n<li>The CPU, memory, device, and topology manager are unaware of the sidecar container lifetime and additional resource usage, and will operate as if the Pod had lower resource requests than it actually does.</li>\n<li>The output of <code>kubectl describe node</code> is incorrect when sidecars are in use. The output shows resource usage that's lower than the actual usage because it doesn't use the new resource usage calculation for sidecar containers.</li>\n</ul>\n<h2 id=\"we-need-your-feedback\">We need your feedback!</h2>\n<p>In the alpha stage, we want you to try out sidecar containers in your environments and open issues if you encounter bugs or friction points. We're especially interested in feedback about the following:</p>\n<ul>\n<li>The shutdown sequence, especially with multiple sidecars running</li>\n<li>The backoff timeout adjustment for crashing sidecars</li>\n<li>The behavior of Pod readiness and liveness probes when sidecars are running</li>\n</ul>\n<p>To open an issue, see the <a href=\"https://github.com/kubernetes/kubernetes/issues/new/choose\">Kubernetes GitHub repository</a>.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>In addition to the known issues that will be resolved, we're working on adding termination ordering for sidecar and main containers. This will ensure that sidecar containers only terminate after the Pod's main containers have exited.</p>\n<p>We‚Äôre excited to see the sidecar feature come to Kubernetes and are interested in feedback.</p>\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n<p>Many years have passed since the original KEP was written, so we apologize if we omit anyone who worked on this feature over the years. This is a best-effort attempt to recognize the people involved in this effort.</p>\n<ul>\n<li><a href=\"https://github.com/mrunalp/\">mrunalp</a> for design discussions and reviews</li>\n<li><a href=\"https://github.com/thockin/\">thockin</a> for API discussions and support thru years</li>\n<li><a href=\"https://github.com/bobbypage\">bobbypage</a> for reviews</li>\n<li><a href=\"https://github.com/smarterclayton\">smarterclayton</a> for detailed review and feedback</li>\n<li><a href=\"https://github.com/howardjohn\">howardjohn</a> for feedback over years and trying it early during implementation</li>\n<li><a href=\"https://github.com/derekwaynecarr\">derekwaynecarr</a> and <a href=\"https://github.com/dchen1107\">dchen1107</a> for leadership</li>\n<li><a href=\"https://github.com/Jpbetz\">jpbetz</a> for API and termination ordering designs as well as code reviews</li>\n<li><a href=\"https://github.com/Joseph-Irving\">Joseph-Irving</a> and <a href=\"https://github.com/rata\">rata</a> for the early iterations design and reviews years back</li>\n<li><a href=\"https://github.com/swatisehgal\">swatisehgal</a> and <a href=\"https://github.com/ffromani\">ffromani</a> for early feedback on resource managers impact</li>\n<li><a href=\"https://github.com/Alculquicondor\">alculquicondor</a> for feedback on addressing the version skew of the scheduler</li>\n<li><a href=\"https://github.com/Wojtek-t\">wojtek-t</a> for PRR review of a KEP</li>\n<li><a href=\"https://github.com/ahg-g\">ahg-g</a> for reviewing the scheduler portion of a KEP</li>\n<li><a href=\"https://github.com/Adisky\">adisky</a> for the Job completion issue</li>\n</ul>\n<h2 id=\"more-information\">More Information</h2>\n<ul>\n<li>Read <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#api-for-sidecar-containers\">API for sidecar containers</a> in the Kubernetes documentation</li>\n<li>Read the <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/753-sidecar-containers/README.md\">Sidecar KEP</a></li>\n</ul>","PublishedAt":"2023-08-25 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/25/native-sidecar-containers/","SourceName":"Kubernetes"}},{"node":{"ID":4464,"Title":"Blog: Kubernetes 1.28: Beta support for using swap on Linux","Description":"<p><strong>Author:</strong> Itamar Holder (Red Hat)</p>\n<p>The 1.22 release <a href=\"https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/\">introduced Alpha support</a>\nfor configuring swap memory usage for Kubernetes workloads running on Linux on a per-node basis.\nNow, in release 1.28, support for swap on Linux nodes has graduated to Beta, along with many\nnew improvements.</p>\n<p>Prior to version 1.22, Kubernetes did not provide support for swap memory on Linux systems.\nThis was due to the inherent difficulty in guaranteeing and accounting for pod memory utilization\nwhen swap memory was involved. As a result, swap support was deemed out of scope in the initial\ndesign of Kubernetes, and the default behavior of a kubelet was to fail to start if swap memory\nwas detected on a node.</p>\n<p>In version 1.22, the swap feature for Linux was initially introduced in its Alpha stage. This represented\na significant advancement, providing Linux users with the opportunity to experiment with the swap\nfeature for the first time. However, as an Alpha version, it was not fully developed and had\nseveral issues, including inadequate support for cgroup v2, insufficient metrics and summary\nAPI statistics, inadequate testing, and more.</p>\n<p>Swap in Kubernetes has numerous <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md#user-stories\">use cases</a>\nfor a wide range of users. As a result, the node special interest group within the Kubernetes project\nhas invested significant effort into supporting swap on Linux nodes for beta.\nCompared to the alpha, the kubelet's support for running with swap enabled is more stable and\nrobust, more user-friendly, and addresses many known shortcomings. This graduation to beta\nrepresents a crucial step towards achieving the goal of fully supporting swap in Kubernetes.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>The utilization of swap memory on a node where it has already been provisioned can be\nfacilitated by the activation of the <code>NodeSwap</code> feature gate on the kubelet.\nAdditionally, you must disable the <code>failSwapOn</code> configuration setting, or the deprecated\n<code>--fail-swap-on</code> command line flag must be deactivated.</p>\n<p>It is possible to configure the <code>memorySwap.swapBehavior</code> option to define the manner in which a node utilizes swap memory. For instance,</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># this fragment goes into the kubelet&#39;s configuration file</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">memorySwap</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">swapBehavior</span>:<span style=\"color:#bbb\"> </span>UnlimitedSwap<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The available configuration options for <code>swapBehavior</code> are:</p>\n<ul>\n<li><code>UnlimitedSwap</code> (default): Kubernetes workloads can use as much swap memory as they\nrequest, up to the system limit.</li>\n<li><code>LimitedSwap</code>: The utilization of swap memory by Kubernetes workloads is subject to limitations.\nOnly Pods of <a href=\"docs/concepts/workloads/pods/pod-qos/#burstable\">Burstable</a> QoS are permitted to employ swap.</li>\n</ul>\n<p>If configuration for <code>memorySwap</code> is not specified and the feature gate is\nenabled, by default the kubelet will apply the same behaviour as the\n<code>UnlimitedSwap</code> setting.</p>\n<p>Note that <code>NodeSwap</code> is supported for <strong>cgroup v2</strong> only. For Kubernetes v1.28,\nusing swap along with cgroup v1 is no longer supported.</p>\n<h2 id=\"install-a-swap-enabled-cluster-with-kubeadm\">Install a swap-enabled cluster with kubeadm</h2>\n<h3 id=\"before-you-begin\">Before you begin</h3>\n<p>It is required for this demo that the kubeadm tool be installed, following the steps outlined in the\n<a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm\">kubeadm installation guide</a>.\nIf swap is already enabled on the node, cluster creation may\nproceed. If swap is not enabled, please refer to the provided instructions for enabling swap.</p>\n<h3 id=\"create-a-swap-file-and-turn-swap-on\">Create a swap file and turn swap on</h3>\n<p>I'll demonstrate creating 4GiB of unencrypted swap.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>dd <span style=\"color:#a2f;font-weight:bold\">if</span><span style=\"color:#666\">=</span>/dev/zero <span style=\"color:#b8860b\">of</span><span style=\"color:#666\">=</span>/swapfile <span style=\"color:#b8860b\">bs</span><span style=\"color:#666\">=</span>128M <span style=\"color:#b8860b\">count</span><span style=\"color:#666\">=</span><span style=\"color:#666\">32</span>\n</span></span><span style=\"display:flex;\"><span>chmod <span style=\"color:#666\">600</span> /swapfile\n</span></span><span style=\"display:flex;\"><span>mkswap /swapfile\n</span></span><span style=\"display:flex;\"><span>swapon /swapfile\n</span></span><span style=\"display:flex;\"><span>swapon -s <span style=\"color:#080;font-style:italic\"># enable the swap file only until this node is rebooted</span>\n</span></span></code></pre></div><p>To start the swap file at boot time, add line like <code>/swapfile swap swap defaults 0 0</code> to <code>/etc/fstab</code> file.</p>\n<h3 id=\"set-up-a-kubernetes-cluster-that-uses-swap-enabled-nodes\">Set up a Kubernetes cluster that uses swap-enabled nodes</h3>\n<p>To make things clearer, here is an example kubeadm configuration file <code>kubeadm-config.yaml</code> for the swap enabled cluster.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;kubeadm.k8s.io/v1beta3&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>InitConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeletConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">failSwapOn</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">false</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">featureGates</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">NodeSwap</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">memorySwap</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">swapBehavior</span>:<span style=\"color:#bbb\"> </span>LimitedSwap<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Then create a single-node cluster using <code>kubeadm init --config kubeadm-config.yaml</code>.\nDuring init, there is a warning that swap is enabled on the node and in case the kubelet\n<code>failSwapOn</code> is set to true. We plan to remove this warning in a future release.</p>\n<h2 id=\"how-is-the-swap-limit-being-determined-with-limitedswap\">How is the swap limit being determined with LimitedSwap?</h2>\n<p>The configuration of swap memory, including its limitations, presents a significant\nchallenge. Not only is it prone to misconfiguration, but as a system-level property, any\nmisconfiguration could potentially compromise the entire node rather than just a specific\nworkload. To mitigate this risk and ensure the health of the node, we have implemented\nSwap in Beta with automatic configuration of limitations.</p>\n<p>With <code>LimitedSwap</code>, Pods that do not fall under the Burstable QoS classification (i.e.\n<code>BestEffort</code>/<code>Guaranteed</code> Qos Pods) are prohibited from utilizing swap memory.\n<code>BestEffort</code> QoS Pods exhibit unpredictable memory consumption patterns and lack\ninformation regarding their memory usage, making it difficult to determine a safe\nallocation of swap memory. Conversely, <code>Guaranteed</code> QoS Pods are typically employed for\napplications that rely on the precise allocation of resources specified by the workload,\nwith memory being immediately available. To maintain the aforementioned security and node\nhealth guarantees, these Pods are not permitted to use swap memory when <code>LimitedSwap</code> is\nin effect.</p>\n<p>Prior to detailing the calculation of the swap limit, it is necessary to define the following terms:</p>\n<ul>\n<li><code>nodeTotalMemory</code>: The total amount of physical memory available on the node.</li>\n<li><code>totalPodsSwapAvailable</code>: The total amount of swap memory on the node that is available for use by Pods (some swap memory may be reserved for system use).</li>\n<li><code>containerMemoryRequest</code>: The container's memory request.</li>\n</ul>\n<p>Swap limitation is configured as:\n<code>(containerMemoryRequest / nodeTotalMemory) √ó totalPodsSwapAvailable</code></p>\n<p>In other words, the amount of swap that a container is able to use is proportionate to its\nmemory request, the node's total physical memory and the total amount of swap memory on\nthe node that is available for use by Pods.</p>\n<p>It is important to note that, for containers within Burstable QoS Pods, it is possible to\nopt-out of swap usage by specifying memory requests that are equal to memory limits.\nContainers configured in this manner will not have access to swap memory.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>There are a number of possible ways that one could envision swap use on a node.\nWhen swap is already provisioned and available on a node,\nSIG Node have <a href=\"https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#proposal\">proposed</a>\nthe kubelet should be able to be configured so that:</p>\n<ul>\n<li>It can start with swap on.</li>\n<li>It will direct the Container Runtime Interface to allocate zero swap memory\nto Kubernetes workloads by default.</li>\n</ul>\n<p>Swap configuration on a node is exposed to a cluster admin via the\n<a href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1\"><code>memorySwap</code> in the KubeletConfiguration</a>.\nAs a cluster administrator, you can specify the node's behaviour in the\npresence of swap memory by setting <code>memorySwap.swapBehavior</code>.</p>\n<p>The kubelet <a href=\"https://kubernetes.io/docs/concepts/architecture/cri/\">employs the CRI</a>\n(container runtime interface) API to direct the CRI to\nconfigure specific cgroup v2 parameters (such as <code>memory.swap.max</code>) in a manner that will\nenable the desired swap configuration for a container. The CRI is then responsible to\nwrite these settings to the container-level cgroup.</p>\n<h2 id=\"how-can-i-monitor-swap\">How can I monitor swap?</h2>\n<p>A notable deficiency in the Alpha version was the inability to monitor and introspect swap\nusage. This issue has been addressed in the Beta version introduced in Kubernetes 1.28, which now\nprovides the capability to monitor swap usage through several different methods.</p>\n<p>The beta version of kubelet now collects\n<a href=\"https://kubernetes.io/docs/reference/instrumentation/node-metrics/\">node-level metric statistics</a>,\nwhich can be accessed at the <code>/metrics/resource</code> and <code>/stats/summary</code> kubelet HTTP endpoints.\nThis allows clients who can directly interrogate the kubelet to\nmonitor swap usage and remaining swap memory when using LimitedSwap. Additionally, a\n<code>machine_swap_bytes</code> metric has been added to cadvisor to show the total physical swap capacity of the\nmachine.</p>\n<h2 id=\"caveats\">Caveats</h2>\n<p>Having swap available on a system reduces predictability. Swap's performance is\nworse than regular memory, sometimes by many orders of magnitude, which can\ncause unexpected performance regressions. Furthermore, swap changes a system's\nbehaviour under memory pressure. Since enabling swap permits\ngreater memory usage for workloads in Kubernetes that cannot be predictably\naccounted for, it also increases the risk of noisy neighbours and unexpected\npacking configurations, as the scheduler cannot account for swap memory usage.</p>\n<p>The performance of a node with swap memory enabled depends on the underlying\nphysical storage. When swap memory is in use, performance will be significantly\nworse in an I/O operations per second (IOPS) constrained environment, such as a\ncloud VM with I/O throttling, when compared to faster storage mediums like\nsolid-state drives or NVMe.</p>\n<p>As such, we do not advocate the utilization of swap memory for workloads or\nenvironments that are subject to performance constraints. Furthermore, it is\nrecommended to employ <code>LimitedSwap</code>, as this significantly mitigates the risks\nposed to the node.</p>\n<p>Cluster administrators and developers should benchmark their nodes and applications\nbefore using swap in production scenarios, and <a href=\"#how-do-i-get-involved\">we need your help</a> with that!</p>\n<h3 id=\"security-risk\">Security risk</h3>\n<p>Enabling swap on a system without encryption poses a security risk, as critical information,\nsuch as volumes that represent Kubernetes Secrets, <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/#information-security-for-secrets\">may be swapped out to the disk</a>.\nIf an unauthorized individual gains\naccess to the disk, they could potentially obtain these confidential data. To mitigate this risk, the\nKubernetes project strongly recommends that you encrypt your swap space.\nHowever, handling encrypted swap is not within the scope of\nkubelet; rather, it is a general OS configuration concern and should be addressed at that level.\nIt is the administrator's responsibility to provision encrypted swap to mitigate this risk.</p>\n<p>Furthermore, as previously mentioned, with <code>LimitedSwap</code> the user has the option to completely\ndisable swap usage for a container by specifying memory requests that are equal to memory limits.\nThis will prevent the corresponding containers from accessing swap memory.</p>\n<h2 id=\"looking-ahead\">Looking ahead</h2>\n<p>The Kubernetes 1.28 release introduced Beta support for swap memory on Linux nodes,\nand we will continue to work towards <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages\">general availability</a>\nfor this feature. I hope that this will include:</p>\n<ul>\n<li>Add the ability to set a system-reserved quantity of swap from what kubelet detects on the host.</li>\n<li>Adding support for controlling swap consumption at the Pod level via cgroups.\n<ul>\n<li>This point is still under discussion.</li>\n</ul>\n</li>\n<li>Collecting feedback from test user cases.\n<ul>\n<li>We will consider introducing new configuration modes for swap, such as a\nnode-wide swap limit for workloads.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>You can review the current <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory\">documentation</a>\nfor using swap with Kubernetes.</p>\n<p>For more information, and to assist with testing and provide feedback, please\nsee <a href=\"https://github.com/kubernetes/enhancements/issues/4128\">KEP-2400</a> and its\n<a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md\">design proposal</a>.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>Your feedback is always welcome! SIG Node <a href=\"https://github.com/kubernetes/community/tree/master/sig-node#meetings\">meets regularly</a>\nand <a href=\"https://github.com/kubernetes/community/tree/master/sig-node#contact\">can be reached</a>\nvia <a href=\"https://slack.k8s.io/\">Slack</a> (channel <strong>#sig-node</strong>), or the SIG's\n<a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">mailing list</a>. A Slack\nchannel dedicated to swap is also available at <strong>#sig-node-swap</strong>.</p>\n<p>Feel free to reach out to me, Itamar Holder (<strong>@iholder101</strong> on Slack and GitHub)\nif you'd like to help or ask further questions.</p>","PublishedAt":"2023-08-24 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/24/swap-linux-beta/","SourceName":"Kubernetes"}},{"node":{"ID":4434,"Title":"Blog: Kubernetes 1.28: Node podresources API Graduates to GA","Description":"<p><strong>Author:</strong>\nFrancesco Romani (Red Hat)</p>\n<p>The podresources API is an API served by the kubelet locally on the node, which exposes the compute resources exclusively\nallocated to containers. With the release of Kubernetes 1.28, that API is now Generally Available.</p>\n<h2 id=\"what-problem-does-it-solve\">What problem does it solve?</h2>\n<p>The kubelet can allocate exclusive resources to containers, like\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/\">CPUs, granting exclusive access to full cores</a>\nor <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/memory-manager/\">memory, either regions or hugepages</a>.\nWorkloads which require high performance, or low latency (or both) leverage these features.\nThe kubelet also can assign <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\">devices to containers</a>.\nCollectively, these features which enable exclusive assignments are known as &quot;resource managers&quot;.</p>\n<p>Without an API like podresources, the only possible option to learn about resource assignment was to read the state files the\nresource managers use. While done out of necessity, the problem with this approach is the path and the format of these file are\nboth internal implementation details. Albeit very stable, the project reserves the right to change them freely.\nConsuming the content of the state files is thus fragile and unsupported, and projects doing this are recommended to consider\nmoving to podresources API or to other supported APIs.</p>\n<h2 id=\"overview-of-the-api\">Overview of the API</h2>\n<p>The podresources API was <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources\">initially proposed to enable device monitoring</a>.\nIn order to enable monitoring agents, a key prerequisite is to enable introspection of device assignment, which is performed by the kubelet.\nServing this purpose was the initial goal of the API. The first iteration of the API only had a single function implemented, <code>List</code>,\nto return information about the assignment of devices to containers.\nThe API is used by <a href=\"https://github.com/k8snetworkplumbingwg/multus-cni\">multus CNI</a> and by\n<a href=\"https://github.com/NVIDIA/dcgm-exporter\">GPU monitoring tools</a>.</p>\n<p>Since its inception, the podresources API increased its scope to cover other resource managers than device manager.\nStarting from Kubernetes 1.20, the <code>List</code> API reports also CPU cores and memory regions (including hugepages); the API also\nreports the NUMA locality of the devices, while the locality of CPUs and memory can be inferred from the system.</p>\n<p>In Kubernetes 1.21, the API <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2403-pod-resources-allocatable-resources/README.md\">gained</a>\nthe <code>GetAllocatableResources</code> function.\nThis newer API complements the existing <code>List</code> API and enables monitoring agents to determine the unallocated resources,\nthus enabling new features built on top of the podresources API like a\n<a href=\"https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/pkg/noderesourcetopology/README.md\">NUMA-aware scheduler plugin</a>.</p>\n<p>Finally, in Kubernetes 1.27, another function, <code>Get</code> was introduced to be more friendly with CNI meta-plugins, to make it simpler to access resources\nallocated to a specific pod, rather than having to filter through resources for all pods on the node. The <code>Get</code> function is currently alpha level.</p>\n<h2 id=\"consuming-the-api\">Consuming the API</h2>\n<p>The podresources API is served by the kubelet locally, on the same node on which is running.\nOn unix flavors, the endpoint is served over a unix domain socket; the default path is <code>/var/lib/kubelet/pod-resources/kubelet.sock</code>.\nOn windows, the endpoint is served over a named pipe; the default path is <code>npipe://\\\\.\\pipe\\kubelet-pod-resources</code>.</p>\n<p>In order for the containerized monitoring application consume the API, the socket should be mounted inside the container.\nA good practice is to mount the directory on which the podresources socket endpoint sits rather than the socket directly.\nThis will ensure that after a kubelet restart, the containerized monitor application will be able to re-connect to the socket.</p>\n<p>An example manifest for a hypothetical monitoring agent consuming the podresources API and deployed as a DaemonSet could look like:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>apps/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>DaemonSet<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>podresources-monitoring-app<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>monitoring<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>podresources-monitoring<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>podresources-monitoring<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">args</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- --podresources-socket=unix:///host-podresources/kubelet.sock<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- /bin/podresources-monitor<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>podresources-monitor:latest <span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># just for an example</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMounts</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">mountPath</span>:<span style=\"color:#bbb\"> </span>/host-podresources<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>host-podresources<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">serviceAccountName</span>:<span style=\"color:#bbb\"> </span>podresources-monitor<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">hostPath</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">path</span>:<span style=\"color:#bbb\"> </span>/var/lib/kubelet/pod-resources<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Directory<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>host-podresources<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>I hope you find it straightforward to consume the podresources API programmatically.\nThe kubelet API package provides the protocol file and the go type definitions; however, a client package is not yet available from the project,\nand the existing code should not be used directly.\nThe <a href=\"https://github.com/kubernetes/kubernetes/blob/v1.28.0-rc.0/pkg/kubelet/apis/podresources/client.go#L32\">recommended</a>\napproach is to reimplement the client in your projects, copying and pasting the related functions like for example\nthe multus project is <a href=\"https://github.com/k8snetworkplumbingwg/multus-cni/blob/v4.0.2/pkg/kubeletclient/kubeletclient.go\">doing</a>.</p>\n<p>When operating the containerized monitoring application consuming the podresources API, few points are worth highlighting to prevent &quot;gotcha&quot; moments:</p>\n<ul>\n<li>Even though the API only exposes data, and doesn't allow by design clients to mutate the kubelet state, the gRPC request/response model requires\nread-write access to the podresources API socket. In other words, it is not possible to limit the container mount to <code>ReadOnly</code>.</li>\n<li>Multiple clients are allowed to connect to the podresources socket and consume the API, since it is stateless.</li>\n<li>The kubelet has <a href=\"https://github.com/kubernetes/kubernetes/pull/116459\">built-in rate limits</a> to mitigate local Denial of Service attacks from\nmisbehaving or malicious consumers. The consumers of the API must tolerate rate limit errors returned by the server. The rate limit is currently\nhardcoded and global, so misbehaving clients can consume all the quota and potentially starve correctly behaving clients.</li>\n</ul>\n<h2 id=\"future-enhancements\">Future enhancements</h2>\n<p>For historical reasons, the podresources API has a less precise specification than typical kubernetes APIs (such as the Kubernetes HTTP API, or the container runtime interface).\nThis leads to unspecified behavior in corner cases.\nAn <a href=\"https://issues.k8s.io/119423\">effort</a> is ongoing to rectify this state and to have a more precise specification.</p>\n<p>The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation\">Dynamic Resource Allocation (DRA)</a> infrastructure\nis a major overhaul of the resource management.\nThe <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3695-pod-resources-for-dra\">integration</a> with the podresources API\nis already ongoing.</p>\n<p>An <a href=\"https://issues.k8s.io/119817\">effort</a> is ongoing to recommend or create a reference client package ready to be consumed.</p>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>This feature is driven by <a href=\"https://github.com/Kubernetes/community/blob/master/sig-node/README.md\">SIG Node</a>.\nPlease join us to connect with the community and share your ideas and feedback around the above feature and\nbeyond. We look forward to hearing from you!</p>","PublishedAt":"2023-08-23 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/23/kubelet-podresources-api-ga/","SourceName":"Kubernetes"}},{"node":{"ID":4397,"Title":"Blog: Kubernetes 1.28: Improved failure handling for Jobs","Description":"<p><strong>Authors:</strong> Kevin Hannon (G-Research), Micha≈Ç Wo≈∫niak (Google)</p>\n<p>This blog discusses two new features in Kubernetes 1.28 to improve Jobs for batch\nusers: <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-replacement-policy\">Pod replacement policy</a>\nand <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#backoff-limit-per-index\">Backoff limit per index</a>.</p>\n<p>These features continue the effort started by the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy\">Pod failure policy</a>\nto improve the handling of Pod failures in a Job.</p>\n<h2 id=\"pod-replacement-policy\">Pod replacement policy</h2>\n<p>By default, when a pod enters a terminating state (e.g. due to preemption or\neviction), Kubernetes immediately creates a replacement Pod. Therefore, both Pods are running\nat the same time. In API terms, a pod is considered terminating when it has a\n<code>deletionTimestamp</code> and it has a phase <code>Pending</code> or <code>Running</code>.</p>\n<p>The scenario when two Pods are running at a given time is problematic for\nsome popular machine learning frameworks, such as\nTensorFlow and <a href=\"https://jax.readthedocs.io/en/latest/\">JAX</a>, which require at most one Pod running at the same time,\nfor a given index.\nTensorflow gives the following error if two pods are running for a given index.</p>\n<pre tabindex=\"0\"><code> /job:worker/task:4: Duplicate task registration with task_name=/job:worker/replica:0/task:4\n</code></pre><p>See more details in the (<a href=\"https://github.com/kubernetes/kubernetes/issues/115844\">issue</a>).</p>\n<p>Creating the replacement Pod before the previous one fully terminates can also\ncause problems in clusters with scarce resources or with tight budgets, such as:</p>\n<ul>\n<li>cluster resources can be difficult to obtain for Pods pending to be scheduled,\nas Kubernetes might take a long time to find available nodes until the existing\nPods are fully terminated.</li>\n<li>if cluster autoscaler is enabled, the replacement Pods might produce undesired\nscale ups.</li>\n</ul>\n<h3 id=\"pod-replacement-policy-how-to-use\">How can you use it?</h3>\n<p>This is an alpha feature, which you can enable by turning on <code>JobPodReplacementPolicy</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> in\nyour cluster.</p>\n<p>Once the feature is enabled in your cluster, you can use it by creating a new Job that specifies a\n<code>podReplacementPolicy</code> field as shown here:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">podReplacementPolicy</span>:<span style=\"color:#bbb\"> </span>Failed<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>In that Job, the Pods would only be replaced once they reached the <code>Failed</code> phase,\nand not when they are terminating.</p>\n<p>Additionally, you can inspect the <code>.status.terminating</code> field of a Job. The value\nof the field is the number of Pods owned by the Job that are currently terminating.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl get jobs/myjob -o<span style=\"color:#666\">=</span><span style=\"color:#b8860b\">jsonpath</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">&#39;{.items[*].status.terminating}&#39;</span>\n</span></span></code></pre></div><pre tabindex=\"0\"><code>3 # three Pods are terminating and have not yet reached the Failed phase\n</code></pre><p>This can be particularly useful for external queueing controllers, such as\n<a href=\"https://github.com/kubernetes-sigs/kueue\">Kueue</a>, that tracks quota\nfrom running Pods of a Job until the resources are reclaimed from\nthe currently terminating Job.</p>\n<p>Note that the <code>podReplacementPolicy: Failed</code> is the default when using a custom\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy\">Pod failure policy</a>.</p>\n<h2 id=\"backoff-limit-per-index\">Backoff limit per index</h2>\n<p>By default, Pod failures for <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode\">Indexed Jobs</a>\nare counted towards the global limit of retries, represented by <code>.spec.backoffLimit</code>.\nThis means, that if there is a consistently failing index, it is restarted\nrepeatedly until it exhausts the limit. Once the limit is reached the entire\nJob is marked failed and some indexes may never be even started.</p>\n<p>This is problematic for use cases where you want to handle Pod failures for\nevery index independently. For example, if you use Indexed Jobs for running\nintegration tests where each index corresponds to a testing suite. In that case,\nyou may want to account for possible flake tests allowing for 1 or 2 retries per\nsuite. There might be some buggy suites, making the corresponding\nindexes fail consistently. In that case you may prefer to limit retries for\nthe buggy suites, yet allowing other suites to complete.</p>\n<p>The feature allows you to:</p>\n<ul>\n<li>complete execution of all indexes, despite some indexes failing.</li>\n<li>better utilize the computational resources by avoiding unnecessary retries of consistently failing indexes.</li>\n</ul>\n<h3 id=\"backoff-limit-per-index-how-to-use\">How can you use it?</h3>\n<p>This is an alpha feature, which you can enable by turning on the\n<code>JobBackoffLimitPerIndex</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a>\nin your cluster.</p>\n<p>Once the feature is enabled in your cluster, you can create an Indexed Job with the\n<code>.spec.backoffLimitPerIndex</code> field specified.</p>\n<h4 id=\"example\">Example</h4>\n<p>The following example demonstrates how to use this feature to make sure the\nJob executes all indexes (provided there is no other reason for the early Job\ntermination, such as reaching the <code>activeDeadlineSeconds</code> timeout, or being\nmanually deleted by the user), and the number of failures is controlled per index.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>batch/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>job-backoff-limit-per-index-execute-all<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">completions</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">8</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">parallelism</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">2</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">completionMode</span>:<span style=\"color:#bbb\"> </span>Indexed<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">backoffLimitPerIndex</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">restartPolicy</span>:<span style=\"color:#bbb\"> </span>Never<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># this example container returns an error, and fails,</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># when it is run as the second or third index in any Job</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># (even after a retry) </span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>python<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- python3<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- -c<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- |<span style=\"color:#b44;font-style:italic\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> import os, sys, time\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> id = int(os.environ.get(&#34;JOB_COMPLETION_INDEX&#34;))\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> if id == 1 or id == 2:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> sys.exit(1)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> time.sleep(1)</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Now, inspect the Pods after the job is finished:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex;\"><span>kubectl get pods -l job-name<span style=\"color:#666\">=</span>job-backoff-limit-per-index-execute-all\n</span></span></code></pre></div><p>Returns output similar to this:</p>\n<pre tabindex=\"0\"><code>NAME READY STATUS RESTARTS AGE\njob-backoff-limit-per-index-execute-all-0-b26vc 0/1 Completed 0 49s\njob-backoff-limit-per-index-execute-all-1-6j5gd 0/1 Error 0 49s\njob-backoff-limit-per-index-execute-all-1-6wd82 0/1 Error 0 37s\njob-backoff-limit-per-index-execute-all-2-c66hg 0/1 Error 0 32s\njob-backoff-limit-per-index-execute-all-2-nf982 0/1 Error 0 43s\njob-backoff-limit-per-index-execute-all-3-cxmhf 0/1 Completed 0 33s\njob-backoff-limit-per-index-execute-all-4-9q6kq 0/1 Completed 0 28s\njob-backoff-limit-per-index-execute-all-5-z9hqf 0/1 Completed 0 28s\njob-backoff-limit-per-index-execute-all-6-tbkr8 0/1 Completed 0 23s\njob-backoff-limit-per-index-execute-all-7-hxjsq 0/1 Completed 0 22s\n</code></pre><p>Additionally, you can take a look at the status for that Job:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex;\"><span>kubectl get <span style=\"color:#a2f\">jobs</span> job-backoff-limit-per-index-fail-index -o yaml\n</span></span></code></pre></div><p>The output ends with a <code>status</code> similar to:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">status</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">completedIndexes</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">0</span>,<span style=\"color:#666\">3-7</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">failedIndexes</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span>,<span style=\"color:#666\">2</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">succeeded</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">6</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">failed</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">4</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">conditions</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Job has failed indexes<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">reason</span>:<span style=\"color:#bbb\"> </span>FailedIndexes<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">status</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;True&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Failed<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Here, indexes <code>1</code> and <code>2</code> were both retried once. After the second failure,\nin each of them, the specified <code>.spec.backoffLimitPerIndex</code> was exceeded, so\nthe retries were stopped. For comparison, if the per-index backoff was disabled,\nthen the buggy indexes would retry until the global <code>backoffLimit</code> was exceeded,\nand then the entire Job would be marked failed, before some of the higher\nindexes are started.</p>\n<h2 id=\"how-can-you-learn-more\">How can you learn more?</h2>\n<ul>\n<li>Read the user-facing documentation for <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-replacement-policy\">Pod replacement policy</a>,\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#backoff-limit-per-index\">Backoff limit per index</a>, and\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy\">Pod failure policy</a></li>\n<li>Read the KEPs for <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3939-allow-replacement-when-fully-terminated\">Pod Replacement Policy</a>,\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3850-backoff-limits-per-index-for-indexed-jobs\">Backoff limit per index</a>, and\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3329-retriable-and-non-retriable-failures\">Pod failure policy</a>.</li>\n</ul>\n<h2 id=\"getting-involved\">Getting Involved</h2>\n<p>These features were sponsored by <a href=\"https://github.com/kubernetes/community/tree/master/sig-apps\">SIG Apps</a>. Batch use cases are actively\nbeing improved for Kubernetes users in the\n<a href=\"https://github.com/kubernetes/community/tree/master/wg-batch\">batch working group</a>.\nWorking groups are relatively short-lived initiatives focused on specific goals.\nThe goal of the WG Batch is to improve experience for batch workload users, offer support for\nbatch processing use cases, and enhance the\nJob API for common use cases. If that interests you, please join the working\ngroup either by subscriping to our\n<a href=\"https://groups.google.com/a/kubernetes.io/g/wg-batch\">mailing list</a> or on\n<a href=\"https://kubernetes.slack.com/messages/wg-batch\">Slack</a>.</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>As with any Kubernetes feature, multiple people contributed to getting this\ndone, from testing and filing bugs to reviewing code.</p>\n<p>We would not have been able to achieve either of these features without Aldo\nCulquicondor (Google) providing excellent domain knowledge and expertise\nthroughout the Kubernetes ecosystem.</p>","PublishedAt":"2023-08-21 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/21/kubernetes-1-28-jobapi-update/","SourceName":"Kubernetes"}},{"node":{"ID":4389,"Title":"Blog: Kubernetes v1.28: Retroactive Default StorageClass move to GA","Description":"<p><strong>Author:</strong> Roman Bedn√°≈ô (Red Hat)</p>\n<p>Announcing graduation to General Availability (GA) - Retroactive Default StorageClass Assignment in Kubernetes v1.28!</p>\n<p>Kubernetes SIG Storage team is thrilled to announce that the &quot;Retroactive Default StorageClass Assignment&quot; feature,\nintroduced as an alpha in Kubernetes v1.25, has now graduated to GA and is officially part of the Kubernetes v1.28 release.\nThis enhancement brings a significant improvement to how default\n<a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/\">StorageClasses</a> are assigned to PersistentVolumeClaims (PVCs).</p>\n<p>With this feature enabled, you no longer need to create a default StorageClass first and then a PVC to assign the class.\nInstead, any PVCs without a StorageClass assigned will now be retroactively updated to include the default StorageClass.\nThis enhancement ensures that PVCs no longer get stuck in an unbound state, and storage provisioning works seamlessly,\neven when a default StorageClass is not defined at the time of PVC creation.</p>\n<h2 id=\"what-changed\">What changed?</h2>\n<p>The PersistentVolume (PV) controller has been modified to automatically assign a default StorageClass to any unbound\nPersistentVolumeClaim with the <code>storageClassName</code> not set. Additionally, the PersistentVolumeClaim\nadmission validation mechanism within\nthe API server has been adjusted to allow changing values from an unset state to an actual StorageClass name.</p>\n<h2 id=\"how-to-use-it\">How to use it?</h2>\n<p>As this feature has graduated to GA, there's no need to enable a feature gate anymore.\nSimply make sure you are running Kubernetes v1.28 or later, and the feature will be available for use.</p>\n<p>For more details, read about\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retroactive-default-storageclass-assignment\">default StorageClass assignment</a> in the Kubernetes documentation.\nYou can also read the previous <a href=\"https://kubernetes.io/blog/2023/01/05/retroactive-default-storage-class/\">blog post</a> announcing beta graduation in v1.26.</p>\n<p>To provide feedback, join our <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special-Interest-Group</a> (SIG)\nor participate in discussions on our <a href=\"https://app.slack.com/client/T09NY5SBT/C09QZFCE5\">public Slack channel</a>.</p>","PublishedAt":"2023-08-18 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/18/retroactive-default-storage-class-ga/","SourceName":"Kubernetes"}},{"node":{"ID":4380,"Title":"Blog: Kubernetes 1.28: Non-Graceful Node Shutdown Moves to GA","Description":"<p><strong>Authors:</strong> Xing Yang (VMware) and Ashutosh Kumar (Elastic)</p>\n<p>The Kubernetes Non-Graceful Node Shutdown feature is now GA in Kubernetes v1.28.\nIt was introduced as\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown\">alpha</a>\nin Kubernetes v1.24, and promoted to\n<a href=\"https://kubernetes.io/blog/2022/12/16/kubernetes-1-26-non-graceful-node-shutdown-beta/\">beta</a>\nin Kubernetes v1.26.\nThis feature allows stateful workloads to restart on a different node if the\noriginal node is shutdown unexpectedly or ends up in a non-recoverable state\nsuch as the hardware failure or unresponsive OS.</p>\n<h2 id=\"what-is-a-non-graceful-node-shutdown\">What is a Non-Graceful Node Shutdown</h2>\n<p>In a Kubernetes cluster, a node can be shutdown in a planned graceful way or\nunexpectedly because of reasons such as power outage or something else external.\nA node shutdown could lead to workload failure if the node is not drained\nbefore the shutdown. A node shutdown can be either graceful or non-graceful.</p>\n<p>The <a href=\"https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/\">Graceful Node Shutdown</a>\nfeature allows Kubelet to detect a node shutdown event, properly terminate the pods,\nand release resources, before the actual shutdown.</p>\n<p>When a node is shutdown but not detected by Kubelet's Node Shutdown Manager,\nthis becomes a non-graceful node shutdown.\nNon-graceful node shutdown is usually not a problem for stateless apps, however,\nit is a problem for stateful apps.\nThe stateful application cannot function properly if the pods are stuck on the\nshutdown node and are not restarting on a running node.</p>\n<p>In the case of a non-graceful node shutdown, you can manually add an <code>out-of-service</code> taint on the Node.</p>\n<pre tabindex=\"0\"><code>kubectl taint nodes &lt;node-name&gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute\n</code></pre><p>This taint triggers pods on the node to be forcefully deleted if there are no\nmatching tolerations on the pods. Persistent volumes attached to the shutdown node\nwill be detached, and new pods will be created successfully on a different running\nnode.</p>\n<p><strong>Note:</strong> Before applying the out-of-service taint, you must verify that a node is\nalready in shutdown or power-off state (not in the middle of restarting).</p>\n<p>Once all the workload pods that are linked to the out-of-service node are moved to\na new running node, and the shutdown node has been recovered, you should remove that\ntaint on the affected node after the node is recovered.</p>\n<h2 id=\"what-s-new-in-stable\">What‚Äôs new in stable</h2>\n<p>With the promotion of the Non-Graceful Node Shutdown feature to stable, the\nfeature gate <code>NodeOutOfServiceVolumeDetach</code> is locked to true on\n<code>kube-controller-manager</code> and cannot be disabled.</p>\n<p>Metrics <code>force_delete_pods_total</code> and <code>force_delete_pod_errors_total</code> in the\nPod GC Controller are enhanced to account for all forceful pods deletion.\nA reason is added to the metric to indicate whether the pod is forcefully deleted\nbecause it is terminated, orphaned, terminating with the <code>out-of-service</code> taint,\nor terminating and unscheduled.</p>\n<p>A &quot;reason&quot; is also added to the metric <code>attachdetach_controller_forced_detaches</code>\nin the Attach Detach Controller to indicate whether the force detach is caused by\nthe <code>out-of-service</code> taint or a timeout.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>This feature requires a user to manually add a taint to the node to trigger\nworkloads failover and remove the taint after the node is recovered.\nIn the future, we plan to find ways to automatically detect and fence nodes\nthat are shutdown/failed and automatically failover workloads to another node.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>Check out additional documentation on this feature\n<a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown\">here</a>.</p>\n<h2 id=\"how-to-get-involved\">How to get involved?</h2>\n<p>We offer a huge thank you to all the contributors who helped with design,\nimplementation, and review of this feature and helped move it from alpha, beta, to stable:</p>\n<ul>\n<li>Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>)</li>\n<li>Derek Carr (<a href=\"https://github.com/derekwaynecarr\">derekwaynecarr</a>)</li>\n<li>Danielle Endocrimes (<a href=\"https://github.com/endocrimes\">endocrimes</a>)</li>\n<li>Baofa Fan (<a href=\"https://github.com/carlory\">carlory</a>)</li>\n<li>Tim Hockin (<a href=\"https://github.com/thockin\">thockin</a>)</li>\n<li>Ashutosh Kumar (<a href=\"https://github.com/sonasingh46\">sonasingh46</a>)</li>\n<li>Hemant Kumar (<a href=\"https://github.com/gnufied\">gnufied</a>)</li>\n<li>Yuiko Mouri (<a href=\"https://github.com/YuikoTakada\">YuikoTakada</a>)</li>\n<li>Mrunal Patel (<a href=\"https://github.com/mrunalp\">mrunalp</a>)</li>\n<li>David Porter (<a href=\"https://github.com/bobbypage\">bobbypage</a>)</li>\n<li>Yassine Tijani (<a href=\"https://github.com/yastij\">yastij</a>)</li>\n<li>Jing Xu (<a href=\"https://github.com/jingxu97\">jingxu97</a>)</li>\n<li>Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>)</li>\n</ul>\n<p>This feature is a collaboration between SIG Storage and SIG Node.\nFor those interested in getting involved with the design and development of any\npart of the Kubernetes Storage system, join the Kubernetes Storage Special\nInterest Group (SIG).\nFor those interested in getting involved with the design and development of the\ncomponents that support the controlled interactions between pods and host\nresources, join the Kubernetes Node SIG.</p>","PublishedAt":"2023-08-16 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/16/kubernetes-1-28-non-graceful-node-shutdown-ga/","SourceName":"Kubernetes"}},{"node":{"ID":4372,"Title":"Blog: pkgs.k8s.io: Introducing Kubernetes Community-Owned Package Repositories","Description":"<p><strong>Author</strong>: Marko Mudriniƒá (Kubermatic)</p>\n<p>On behalf of Kubernetes SIG Release, I am very excited to introduce the\nKubernetes community-owned software\nrepositories for Debian and RPM packages: <code>pkgs.k8s.io</code>! The new package\nrepositories are replacement for the Google-hosted package repositories\n(<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) that we've been using since\nKubernetes v1.5.</p>\n<p>This blog post contains information about these new package repositories,\nwhat does it mean to you as an end user, and how to migrate to the new\nrepositories.</p>\n<h2 id=\"what-you-need-to-know-about-the-new-package-repositories\">What you need to know about the new package repositories?</h2>\n<ul>\n<li>This is an <strong>opt-in change</strong>; you're required to manually migrate from the\nGoogle-hosted repository to the Kubernetes community-owned repositories.\nSee <a href=\"#how-to-migrate\">how to migrate</a> later in this announcement for migration information\nand instructions.</li>\n<li>Access to the Google-hosted repository will remain intact for the foreseeable\nfuture. However, the Kubernetes project plans to stop publishing packages to\nthe Google-hosted repository in the future. The project strongly recommends\nmigrating to the Kubernetes package repositories going forward.</li>\n<li>The Kubernetes package repositories contain packages beginning with those\nKubernetes versions that were still under support when the community took\nover the package builds. This means that anything before v1.24.0 will only be\navailable in the Google-hosted repository.</li>\n<li>There's a dedicated package repository for each Kubernetes minor version.\nWhen upgrading to a different minor release, you must bear in mind that\nthe package repository details also change.</li>\n</ul>\n<h2 id=\"why-are-we-introducing-new-package-repositories\">Why are we introducing new package repositories?</h2>\n<p>As the Kubernetes project is growing, we want to ensure the best possible\nexperience for the end users. The Google-hosted repository has been serving\nus well for many years, but we started facing some problems that require\nsignificant changes to how we publish packages. Another goal that we have is to\nuse community-owned infrastructure for all critical components and that\nincludes package repositories.</p>\n<p>Publishing packages to the Google-hosted repository is a manual process that\ncan be done only by a team of Google employees called\n<a href=\"https://kubernetes.io/releases/release-managers/#build-admins\">Google Build Admins</a>.\n<a href=\"https://kubernetes.io/releases/release-managers/#release-managers\">The Kubernetes Release Managers team</a>\nis a very diverse team especially in terms of timezones that we work in.\nGiven this constraint, we have to do very careful planning for every release to\nensure that we have both Release Manager and Google Build Admin available to\ncarry out the release.</p>\n<p>Another problem is that we only have a single package repository. Because of\nthis, we were not able to publish packages for prerelease versions (alpha,\nbeta, and rc). This made testing Kubernetes prereleases harder for anyone who\nis interested to do so. The feedback that we receive from people testing these\nreleases is critical to ensure the best quality of releases, so we want to make\ntesting these releases as easy as possible. On top of that, having only one\nrepository limited us when it comes to publishing dependencies like <code>cri-tools</code>\nand <code>kubernetes-cni</code>.</p>\n<p>Regardless of all these issues, we're very thankful to Google and Google Build\nAdmins for their involvement, support, and help all these years!</p>\n<h2 id=\"how-the-new-package-repositories-work\">How the new package repositories work?</h2>\n<p>The new package repositories are hosted at <code>pkgs.k8s.io</code> for both Debian and\nRPM packages. At this time, this domain points to a CloudFront CDN backed by S3\nbucket that contains repositories and packages. However, we plan on onboarding\nadditional mirrors in the future, giving possibility for other companies to\nhelp us with serving packages.</p>\n<p>Packages are built and published via the <a href=\"http://openbuildservice.org\">OpenBuildService (OBS) platform</a>.\nAfter a long period of evaluating different solutions, we made a decision to\nuse OpenBuildService as a platform to manage our repositories and packages.\nFirst of all, OpenBuildService is an open source platform used by a large\nnumber of open source projects and companies, like openSUSE, VideoLAN,\nDell, Intel, and more. OpenBuildService has many features making it very\nflexible and easy to integrate with our existing release tooling. It also\nallows us to build packages in a similar way as for the Google-hosted\nrepository making the migration process as seamless as possible.</p>\n<p>SUSE sponsors the Kubernetes project with access to their reference\nOpenBuildService setup (<a href=\"http://build.opensuse.org\"><code>build.opensuse.org</code></a>) and\nwith technical support to integrate OBS with our release processes.</p>\n<p>We use SUSE's OBS instance for building and publishing packages. Upon building\na new release, our tooling automatically pushes needed artifacts and\npackage specifications to <code>build.opensuse.org</code>. That will trigger the build\nprocess that's going to build packages for all supported architectures (AMD64,\nARM64, PPC64LE, S390X). At the end, generated packages will be automatically\npushed to our community-owned S3 bucket making them available to all users.</p>\n<p>We want to take this opportunity to thank SUSE for allowing us to use\n<code>build.opensuse.org</code> and their generous support to make this integration\npossible!</p>\n<h2 id=\"what-are-significant-differences-between-the-google-hosted-and-kubernetes-package-repositories\">What are significant differences between the Google-hosted and Kubernetes package repositories?</h2>\n<p>There are three significant differences that you should be aware of:</p>\n<ul>\n<li>There's a dedicated package repository for each Kubernetes minor release.\nFor example, repository called <code>core:/stable:/v1.28</code> only hosts packages for\nstable Kubernetes v1.28 releases. This means you can install v1.28.0 from\nthis repository, but you can't install v1.27.0 or any other minor release\nother than v1.28. Upon upgrading to another minor version, you have to add a\nnew repository and optionally remove the old one</li>\n<li>There's a difference in what <code>cri-tools</code> and <code>kubernetes-cni</code> package\nversions are available in each Kubernetes repository\n<ul>\n<li>These two packages are dependencies for <code>kubelet</code> and <code>kubeadm</code></li>\n<li>Kubernetes repositories for v1.24 to v1.27 have same versions of these\npackages as the Google-hosted repository</li>\n<li>Kubernetes repositories for v1.28 and onwards are going to have published\nonly versions that are used by that Kubernetes minor release\n<ul>\n<li>Speaking of v1.28, only kubernetes-cni 1.2.0 and cri-tols v1.28 are going\nto be available in the repository for Kubernetes v1.28</li>\n<li>Similar for v1.29, we only plan on publishing cri-tools v1.29 and\nwhatever kubernetes-cni version is going to be used by Kubernetes v1.29</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>The revision part of the package version (the <code>-00</code> part in <code>1.28.0-00</code>) is\nnow autogenerated by the OpenBuildService platform and has a different format.\nThe revision is now in the format of <code>-x.y</code>, e.g. <code>1.28.0-1.1</code></li>\n</ul>\n<h2 id=\"does-this-in-any-way-affect-existing-google-hosted-repositories\">Does this in any way affect existing Google-hosted repositories?</h2>\n<p>The Google-hosted repository and all packages published to it will continue\nworking in the same way as before. There are no changes in how we build and\npublish packages to the Google-hosted repository, all newly-introduced changes\nare only affecting packages publish to the community-owned repositories.</p>\n<p>However, as mentioned at the beginning of this blog post, we plan to stop\npublishing packages to the Google-hosted repository in the future.</p>\n<h2 id=\"how-to-migrate\">How to migrate to the Kubernetes community-owned repositories?</h2>\n<h3 id=\"how-to-migrate-deb\">Debian, Ubuntu, and operating systems using <code>apt</code>/<code>apt-get</code></h3>\n<ol>\n<li>\n<p>Replace the <code>apt</code> repository definition so that <code>apt</code> points to the new\nrepository instead of the Google-hosted repository. Make sure to replace the\nKubernetes minor version in the command below with the minor version\nthat you're currently using:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#a2f\">echo</span> <span style=\"color:#b44\">&#34;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list\n</span></span></code></pre></div></li>\n<li>\n<p>Download the public signing key for the Kubernetes package repositories.\nThe same signing key is used for all repositories, so you can disregard the\nversion in the URL:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n</span></span></code></pre></div></li>\n<li>\n<p>Update the <code>apt</code> package index:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>sudo apt-get update\n</span></span></code></pre></div></li>\n</ol>\n<h3 id=\"how-to-migrate-rpm\">CentOS, Fedora, RHEL, and operating systems using <code>rpm</code>/<code>dnf</code></h3>\n<ol>\n<li>\n<p>Replace the <code>yum</code> repository definition so that <code>yum</code> points to the new\nrepository instead of the Google-hosted repository. Make sure to replace the\nKubernetes minor version in the command below with the minor version\nthat you're currently using:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">[kubernetes]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">name=Kubernetes\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">baseurl=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">enabled=1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">gpgcheck=1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">gpgkey=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/repodata/repomd.xml.key\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div></li>\n</ol>\n<h2 id=\"can-i-rollback-to-the-google-hosted-repository-after-migrating-to-the-kubernetes-repositories\">Can I rollback to the Google-hosted repository after migrating to the Kubernetes repositories?</h2>\n<p>In general, yes. Just do the same steps as when migrating, but use parameters\nfor the Google-hosted repository. You can find those parameters in a document\nlike <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm\">&quot;Installing kubeadm&quot;</a>.</p>\n<h2 id=\"why-isn-t-there-a-stable-list-of-domains-ips-why-can-t-i-restrict-package-downloads\">Why isn‚Äôt there a stable list of domains/IPs? Why can‚Äôt I restrict package downloads?</h2>\n<p>Our plan for <code>pkgs.k8s.io</code> is to make it work as a redirector to a set of\nbackends (package mirrors) based on user's location. The nature of this change\nmeans that a user downloading a package could be redirected to any mirror at\nany time. Given the architecture and our plans to onboard additional mirrors in\nthe near future, we can't provide a list of IP addresses or domains that you\ncan add to an allow list.</p>\n<p>Restrictive control mechanisms like man-in-the-middle proxies or network\npolicies that restrict access to a specific list of IPs/domains will break with\nthis change. For these scenarios, we encourage you to mirror the release\npackages to a local package repository that you have strict control over.</p>\n<h2 id=\"what-should-i-do-if-i-detect-some-abnormality-with-the-new-repositories\">What should I do if I detect some abnormality with the new repositories?</h2>\n<p>If you encounter any issue with new Kubernetes package repositories, please\nfile an issue in the\n<a href=\"https://github.com/kubernetes/release/issues/new/choose\"><code>kubernetes/release</code> repository</a>.</p>","PublishedAt":"2023-08-15 20:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/","SourceName":"Kubernetes"}},{"node":{"ID":4373,"Title":"Blog: Kubernetes v1.28: Planternetes","Description":"<p><strong>Authors</strong>: <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.28/release-team.md\">Kubernetes v1.28 Release Team</a></p>\n<p>Announcing the release of Kubernetes v1.28 Planternetes, the second release of 2023!</p>\n<p>This release consists of 45 enhancements. Of those enhancements, 19 are entering Alpha, 14 have graduated to Beta, and 12 have graduated to Stable.</p>\n<h2 id=\"release-theme-and-logo\">Release Theme And Logo</h2>\n<p><strong>Kubernetes v1.28: <em>Planternetes</em></strong></p>\n<p>The theme for Kubernetes v1.28 is <em>Planternetes</em>.</p>\n<figure class=\"release-logo\">\n<img src=\"https://kubernetes.io/images/blog/2023-08-15-kubernetes-1.28-blog/kubernetes-1.28.png\"\nalt=\"Kubernetes 1.28 Planternetes logo\"/>\n</figure>\n<p>Each Kubernetes release is the culmination of the hard work of thousands of individuals from our community. The people behind this release come from a wide range of backgrounds, some of us industry veterans, parents, others students and newcomers to open-source. We combine our unique experience to create a collective artifact with global impact.</p>\n<p>Much like a garden, our release has ever-changing growth, challenges and opportunities. This theme celebrates the meticulous care, intention and efforts to get the release to where we are today. Harmoniously together, we grow better.</p>\n<h1 id=\"what-s-new-major-themes\">What's New (Major Themes)</h1>\n<h2 id=\"changes-to-supported-skew-between-control-plane-and-node-versions\">Changes to supported skew between control plane and node versions</h2>\n<p>This enables testing and expanding the supported skew between core node and control plane components by one version from n-2 to n-3, so that node components (kubelet and kube-proxy) for the oldest supported minor version work with control plane components (kube-apiserver, kube-scheduler, kube-controller-manager, cloud-controller-manager) for the newest supported minor version.</p>\n<p>This is valuable for end users as control plane upgrade will be a little faster than node upgrade, which are almost always going to be the longer with running workloads.</p>\n<p>The Kubernetes yearly support period already makes annual upgrades possible. Users can upgrade to the latest patch versions to pick up security fixes and do 3 sequential minor version upgrades once a year to &quot;catch up&quot; to the latest supported minor version.</p>\n<p>However, since the tested/supported skew between nodes and control planes is currently limited to 2 versions, a 3-version upgrade would have to update nodes twice to stay within the supported skew.</p>\n<h2 id=\"generally-available-recovery-from-non-graceful-node-shutdown\">Generally available: recovery from non-graceful node shutdown</h2>\n<p>If a node shuts down down unexpectedly or ends up in a non-recoverable state (perhaps due to hardware failure or unresponsive OS), Kubernetes allows you to clean up afterwards and allow stateful workloads to restart on a different node. For Kubernetes v1.28, that's now a stable feature.</p>\n<p>This allows stateful workloads to failover to a different node successfully after the original node is shut down or in a non-recoverable state, such as the hardware failure or broken OS.</p>\n<p>Versions of Kubernetes earlier than v1.20 lacked handling for node shutdown on Linux, the kubelet integrates with systemd\nand implements graceful node shutdown (beta, and enabled by default). However, even an intentional\nshutdown might not get handled well that could be because:</p>\n<ul>\n<li>the node runs Windows</li>\n<li>the node runs Linux, but uses a different <code>init</code> (not <code>systemd</code>)</li>\n<li>the shutdown does not trigger the system inhibitor locks mechanism</li>\n<li>because of a node-level configuration error\n(such as not setting appropriate values for <code>shutdownGracePeriod</code> and <code>shutdownGracePeriodCriticalPods</code>).</li>\n</ul>\n<p>When a node shutdowns or fails, and that shutdown was not detected by the kubelet, the pods that are part\nof a StatefulSet will be stuck in terminating status on the shutdown node. If the stopped node restarts, the\nkubelet on that node can clean up (<code>DELETE</code>) the Pods that the Kubernetes API still sees as bound to that node.\nHowever, if the node stays stopped - or if the kubelet isn't able to start after a reboot - then Kubernetes may\nnot be able to create replacement Pods. When the kubelet on the shut-down node is not available to delete\nthe old pods, an associated StatefulSet cannot create a new pod (which would have the same name).</p>\n<p>There's also a problem with storage. If there are volumes used by the pods, existing VolumeAttachments will\nnot be disassociated from the original - and now shut down - node so the PersistentVolumes used by these\npods cannot be attached to a different, healthy node. As a result, an application running on an\naffected StatefulSet may not be able to function properly. If the original, shut down node does come up, then\ntheir pods will be deleted by its kubelet and new pods can be created on a different running node.\nIf the original node does not come up (common with an <a href=\"https://glossary.cncf.io/immutable-infrastructure/\">immutable infrastructure</a> design), those pods would be stuck in a <code>Terminating</code> status on the shut-down node forever.</p>\n<p>For more information on how to trigger cleanup after a non-graceful node shutdown,\nread <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown\">non-graceful node shutdown</a>.</p>\n<h2 id=\"improvements-to-customresourcedefinition-validation-rules\">Improvements to CustomResourceDefinition validation rules</h2>\n<p>The <a href=\"https://github.com/google/cel-go\">Common Expression Language (CEL)</a> can be used to validate\n<a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">custom resources</a>. The primary goal is to allow the majority of the validation use cases that might once have needed you, as a CustomResourceDefinition (CRD) author, to design and implement a webhook. Instead, and as a beta feature, you can add <em>validation expressions</em> directly into the schema of a CRD.</p>\n<p>CRDs need direct support for non-trivial validation. While admission webhooks do support CRDs validation, they significantly complicate the development and operability of CRDs.</p>\n<p>In 1.28, two optional fields <code>reason</code> and <code>fieldPath</code> were added to allow user to specify the failure reason and fieldPath when validation failed.</p>\n<p>For more information, read <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">validation rules</a> in the CRD documentation.</p>\n<h2 id=\"validatingadmissionpolicies-graduate-to-beta\">ValidatingAdmissionPolicies graduate to beta</h2>\n<p>Common Expression language for admission control is customizable, in-process validation of requests to the Kubernetes API server as an alternative to validating admission webhooks.</p>\n<p>This builds on the capabilities of the CRD Validation Rules feature that graduated to beta in 1.25 but with a focus on the policy enforcement capabilities of validating admission control.</p>\n<p>This will lower the infrastructure barrier to enforcing customizable policies as well as providing primitives that help the community establish and adhere to the best practices of both K8s and its extensions.</p>\n<p>To use <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/\">ValidatingAdmissionPolicies</a>, you need to enable both the <code>admissionregistration.k8s.io/v1beta1</code> API group and the <code>ValidatingAdmissionPolicy</code> feature gate in your cluster's control plane.</p>\n<h2 id=\"match-conditions-for-admission-webhooks\">Match conditions for admission webhooks</h2>\n<p>Kubernetes v1.27 lets you specify <em>match conditions</em> for admission webhooks,\nwhich lets you narrow the scope of when Kubernetes makes a remote HTTP call at admission time.\nThe <code>matchCondition</code> field for ValidatingWebhookConfiguration and MutatingWebhookConfiguration\nis a CEL expression that must evaluate to true for the admission request to be sent to the webhook.</p>\n<p>In Kubernetes v1.28, that field moved to beta, and it's enabled by default.</p>\n<p>To learn more, see <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchconditions\"><code>matchConditions</code></a> in the Kubernetes documentation.</p>\n<h2 id=\"beta-support-for-enabling-swap-space-on-linux\">Beta support for enabling swap space on Linux</h2>\n<p>This adds swap support to nodes in a controlled, predictable manner so that Kubernetes users can perform testing and provide data to continue building cluster capabilities on top of swap.</p>\n<p>There are two distinct types of users for swap, who may overlap:</p>\n<ul>\n<li>\n<p>Node administrators, who may want swap available for node-level performance tuning and stability/reducing noisy neighbor issues.</p>\n</li>\n<li>\n<p>Application developers, who have written applications that would benefit from using swap memory.</p>\n</li>\n</ul>\n<h2 id=\"mixed-version-proxy\">Mixed version proxy (alpha)</h2>\n<p>When a cluster has multiple API servers at mixed versions (such as during an upgrade/downgrade or when runtime-config changes and a rollout happens), not every apiserver can serve every resource at every version.</p>\n<p>For Kubernetes v1.28, you can enable the <em>mixed version proxy</em> within the API server's aggregation layer.\nThe mixed version proxy finds requests that the local API server doesn't recognize but another API server\ninside the control plan is able to support. Having found a suitable peer, the aggregation layer proxies\nthe request to a compatible API server; this is transparent from the client's perspective.</p>\n<p>When an upgrade or downgrade is performed on a cluster, for some period of time the API servers\nwithin the control plane may be at differing versions; when that happens, different subsets of the\nAPI servers are able to serve different sets of built-in resources (different groups, versions, and resources\nare all possible). This new alpha mechanism lets you hide that skew from clients.</p>\n<h2 id=\"source-code-reorganization-for-control-plane-components\">Source code reorganization for control plane components</h2>\n<p>Kubernetes contributors have begun to reorganize the code for the kube-apiserver to build on a new staging repository that consumes <a href=\"https://github.com/kubernetes/apiserver\">k/apiserver</a> but has a bigger, carefully chosen subset of the functionality of kube-apiserver such that it is reusable.</p>\n<p>This is a gradual reorganization; eventually there will be a new git repository with generic functionality abstracted from Kubernetes' API server.</p>\n<h2 id=\"cdi-device-plugin\">Support for CDI injection into containers (alpha)</h2>\n<p>CDI provides a standardized way of injecting complex devices into a container (i.e. devices that logically require more than just a single /dev node to be injected for them to work). This new feature enables plugin developers to utilize the CDIDevices field added to the CRI in 1.27 to pass CDI devices directly to CDI enabled runtimes (of which containerd and crio-o are in recent releases).</p>\n<h2 id=\"sidecar-init-containers\">API awareness of sidecar containers (alpha)</h2>\n<p>Kubernetes 1.28 introduces an alpha <code>restartPolicy</code> field for <a href=\"https://github.com/kubernetes/website/blob/main/content/en/docs/concepts/workloads/pods/init-containers.md\">init containers</a>,\nand uses that to indicate when an init container is also a <em>sidecar container</em>. The will start init containers with <code>restartPolicy: Always</code> in the order they are defined, along with other init containers. Instead of waiting for that sidecar container to complete before starting the main container(s) for the Pod, the kubelet only waits for\nthe sidecar init container to have started.</p>\n<p>The condition for startup completion will be that the startup probe succeeded (or if no startup probe is defined) and postStart handler is completed. This condition is represented with the field Started of ContainerStatus type. See the section &quot;Pod startup completed condition&quot; for considerations on picking this signal.</p>\n<p>For init containers, you can either omit the <code>restartPolicy</code> field, or set it to <code>Always</code>. Omitting the field\nmeans that you want a true init container that runs to completion before application startup.</p>\n<p>Sidecar containers do not block Pod completion: if all regular containers are complete, sidecar\ncontainers in that Pod will be terminated.</p>\n<p>For sidecar containers, the restart behavior is more complex than for init containers. In a Pod with\n<code>restartPolicy</code> set to <code>Never</code>, a sidecar container that fails during Pod startup will <strong>not</strong> be restarted\nand the whole Pod is treated as having failed. If the Pod's <code>restartPolicy</code> is <code>Always</code> or <code>OnFailure</code>,\na sidecar that fails to start will be retried.</p>\n<p>Once the sidecar container has started (process running, <code>postStart</code> was successful, and\nany configured startup probe is passing), and then there's a failure, that sidecar container will be\nrestarted even when the Pod's overall <code>restartPolicy</code> is <code>Never</code> or <code>OnFailure</code>.\nFurthermore, sidecar containers will be restarted (on failure or on normal exit)\n<em>even during Pod termination</em>.</p>\n<p>To learn more, read <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#api-for-sidecar-containers\">API for sidecar containers</a>.</p>\n<h2 id=\"automatic-retroactive-assignment-of-a-default-storageclass-graduates-to-stable\">Automatic, retroactive assignment of a default StorageClass graduates to stable</h2>\n<p>Kubernetes automatically sets a <code>storageClassName</code> for a PersistentVolumeClaim (PVC) if you don't provide\na value. The control plane also sets a StorageClass for any existing PVC that doesn't have a <code>storageClassName</code>\ndefined.\nPrevious versions of Kubernetes also had this behavior; for Kubernetes v1.28 is is automatic and always\nactive; the feature has graduated to stable (general availability).</p>\n<p>To learn more, read about <a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/\">StorageClass</a> in the Kubernetes\ndocumentation.</p>\n<h2 id=\"pod-replacement-policy\">Pod replacement policy for Jobs (alpha)</h2>\n<p>Kubernetes 1.28 adds a new field for the Job API that allows you to specify if you want the control\nplane to make new Pods as soon as the previous Pods begin termination (existing behavior),\nor only once the existing pods are fully terminated (new, optional behavior).</p>\n<p>Many common machine learning frameworks, such as Tensorflow and JAX, require unique pods per index.\nWith the older behaviour, if a pod that belongs to an <code>Indexed</code> Job enters a terminating state (due to preemption, eviction or other external factors), a replacement pod is created but then immediately fails to start due\nto the clash with the old pod that has not yet shut down.</p>\n<p>Having a replacement Pod appear before the previous one fully terminates can also cause problems\nin clusters with scarce resources or with tight budgets. These resources can be difficult to obtain so pods may only be able to find nodes once the existing pods have been terminated. If cluster autoscaler is enabled, early creation of replacement Pods might produce undesired scale-ups.</p>\n<p>To learn more, read <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#delayed-creation-of-replacement-pods\">Delayed creation of replacement pods</a>\nin the Job documentation.</p>\n<h2 id=\"job-per-index-retry-backoff\">Job retry backoff limit, per index (alpha)</h2>\n<p>This extends the Job API to support indexed jobs where the backoff limit is per index, and the Job can continue execution despite some of its indexes failing.</p>\n<p>Currently, the indexes of an indexed job share a single backoff limit. When the job reaches this shared backoff limit, the job controller marks the entire job as failed, and the resources are cleaned up, including indexes that have yet to run to completion.</p>\n<p>As a result, the existing implementation did not cover the situation where the workload is truly\n<a href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\">embarrassingly parallel</a>: each index is\nfully independent of other indexes.</p>\n<p>For instance, if indexed jobs were used as the basis for a suite of long-running integration tests, then each test run would only be able to find a single test failure.</p>\n<p>For more information, read <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#handling-pod-and-container-failures\">Handling Pod and container failures</a> in the Kubernetes documentation.</p>\n<h2 id=\"cri-container-and-pod-statistics-without-cadvisor\">CRI container and pod statistics without cAdvisor</h2>\n<p>This encompasses two related pieces of work (changes to the kubelet's <code>/metrics/cadvisor</code> endpoint and improvements to the replacement <em>summary</em> API).</p>\n<p>There are two main APIs that consumers use to gather stats about running containers and pods: summary API and <code>/metrics/cadvisor</code>. The Kubelet is responsible for implementing the summary API, and cadvisor is responsible for fulfilling <code>/metrics/cadvisor</code>.</p>\n<p>This enhances CRI implementations to be able to fulfill all the stats needs of Kubernetes. At a high level, there are two pieces of this:</p>\n<ul>\n<li>\n<p>It enhances the CRI API with enough metrics to supplement the pod and container fields in the summary API directly from CRI.</p>\n</li>\n<li>\n<p>It enhances the CRI implementations to broadcast the required metrics to fulfill the pod and container fields in the <code>/metrics/cadvisor</code> endpoint.</p>\n</li>\n</ul>\n<h2 id=\"feature-graduations-and-deprecations-in-kubernetes-v1-28\">Feature graduations and deprecations in Kubernetes v1.28</h2>\n<h3 id=\"graduations-to-stable\">Graduations to stable</h3>\n<p>This release includes a total of 12 enhancements promoted to Stable:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1440\"><code>kubectl events</code></a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3333\">Retroactive default StorageClass assignment</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2268\">Non-graceful node shutdown</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/606\">Support 3rd party device monitoring plugins</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3325\">Auth API to get self-user attributes</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1669\">Proxy Terminating Endpoints</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2595\">Expanded DNS Configuration</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3178\">Cleaning up IPTables Chain Ownership</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3453\">Minimizing iptables-restore input size</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3743\">Graduate the kubelet pod resources endpoint to GA</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2403\">Extend podresources API to report allocatable resources</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3685\">Move EndpointSlice Reconciler into Staging</a></li>\n</ul>\n<h3 id=\"deprecations-and-removals\">Deprecations and removals</h3>\n<p>Removals:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1488\">Removal of CSI Migration for GCE PD</a></li>\n</ul>\n<p>Deprecations:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/118303\">Ceph RBD in-tree plugin</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/118143\">Ceph FS in-tree plugin</a></li>\n</ul>\n<h2 id=\"release-notes\">Release Notes</h2>\n<p>The complete details of the Kubernetes v1.28 release are available in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md\">release notes</a>.</p>\n<h2 id=\"availability\">Availability</h2>\n<p>Kubernetes v1.28 is available for download on <a href=\"https://github.com/kubernetes/kubernetes/releases/tag/v1.28.0\">GitHub</a>. To get started with Kubernetes, you can run local Kubernetes clusters using <a href=\"https://minikube.sigs.k8s.io/docs/\">minikube</a>, <a href=\"https://kind.sigs.k8s.io/\">kind</a>, etc. You can also easily install v1.28 using <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h2 id=\"release-team\">Release Team</h2>\n<p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is comprised of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.</p>\n<p>We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.28 release for our community.</p>\n<p>Special thanks to our release lead, Grace Nguyen, for guiding us through a smooth and successful release cycle.</p>\n<h2 id=\"ecosystem-updates\">Ecosystem Updates</h2>\n<ul>\n<li>KubeCon + CloudNativeCon China 2023 will take place in Shanghai, China, from 26 ‚Äì 28 September 2023! You can find more information about the conference and registration on the <a href=\"https://www.lfasiallc.com/kubecon-cloudnativecon-open-source-summit-china/\">event site</a>.</li>\n<li>KubeCon + CloudNativeCon North America 2023 will take place in Chicago, Illinois, The United States of America, from 6 ‚Äì 9 November 2023! You can find more information about the conference and registration on the <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\">event site</a>.</li>\n</ul>\n<h2 id=\"project-velocity\">Project Velocity</h2>\n<p>The <a href=\"https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m\">CNCF K8s DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>\n<p>In the v1.28 release cycle, which <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.28\">ran for 14 weeks</a> (May 15 to August 15), we saw contributions from <a href=\"https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.27.0%20-%20now&amp;var-metric=contributions\">911 companies</a> and <a href=\"https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.27.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-repo_name=kubernetes%2Fkubernetes&amp;var-country_name=All&amp;var-companies=All\">1440 individuals</a>.</p>\n<h2 id=\"upcoming-release-webinar\">Upcoming Release Webinar</h2>\n<p>Join members of the Kubernetes v1.28 release team on Friday, September 14, 2023, at 10 a.m. PDT to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the <a href=\"https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v128-release/\">event page</a> on the CNCF Online Programs site.</p>\n<h2 id=\"get-involved\">Get Involved</h2>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests.</p>\n<p>Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below:</p>\n<ul>\n<li>\n<p>Find out more about contributing to Kubernetes at the <a href=\"https://www.kubernetes.dev/\">Kubernetes Contributors website</a>.</p>\n</li>\n<li>\n<p>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for the latest updates.</p>\n</li>\n<li>\n<p>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a>.</p>\n</li>\n<li>\n<p>Join the community on <a href=\"https://communityinviter.com/apps/kubernetes/community\">Slack</a>.</p>\n</li>\n<li>\n<p>Post questions (or answer questions) on <a href=\"https://serverfault.com/questions/tagged/kubernetes\">Server Fault</a>.</p>\n</li>\n<li>\n<p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">Share</a> your Kubernetes story.</p>\n</li>\n<li>\n<p>Read more about what‚Äôs happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a>.</p>\n</li>\n<li>\n<p>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a>.</p>\n</li>\n</ul>","PublishedAt":"2023-08-15 12:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/15/kubernetes-v1-28-release/","SourceName":"Kubernetes"}},{"node":{"ID":4358,"Title":"Blog: Spotlight on SIG ContribEx","Description":"<p><strong>Author</strong>: Fyka Ansari</p>\n<p>Welcome to the world of Kubernetes and its vibrant contributor\ncommunity! In this blog post, we'll be shining a spotlight on the\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-contributor-experience/README.md\">Special Interest Group for Contributor\nExperience</a>\n(SIG ContribEx), an essential component of the Kubernetes project.</p>\n<p>SIG ContribEx in Kubernetes is responsible for developing and\nmaintaining a healthy and productive community of contributors to the\nproject. This involves identifying and addressing bottlenecks that may\nhinder the project's growth and feature velocity, such as pull request\nlatency and the number of open pull requests and issues.</p>\n<p>SIG ContribEx works to improve the overall contributor experience by\ncreating and maintaining guidelines, tools, and processes that\nfacilitate collaboration and communication among contributors. They\nalso focus on community building and support, including outreach\nprograms and mentorship initiatives to onboard and retain new\ncontributors.</p>\n<p>Ultimately, the role of SIG ContribEx is to foster a welcoming and\ninclusive environment that encourages contribution and supports the\nlong-term sustainability of the Kubernetes project.</p>\n<p>In this blog post, <a href=\"https://twitter.com/1fyka\">Fyka Ansari</a> interviews\n<a href=\"https://twitter.com/kaslinfields\">Kaslin Fields</a>, a DevRel Engineer\nat Google, who is a chair of SIG ContribEx, and <a href=\"https://twitter.com/MadhavJivrajani\">Madhav\nJivrajani</a>, a Software Engineer\nat VMWare who serves as a SIG ContribEx Tech Lead. This interview\ncovers various aspects of SIG ContribEx, including current\ninitiatives, exciting developments, and how interested individuals can\nget involved and contribute to the group. It provides valuable\ninsights into the workings of SIG ContribEx and highlights the\nimportance of its role in the Kubernetes ecosystem.</p>\n<h3 id=\"introductions\">Introductions</h3>\n<p><strong>Fyka:</strong> Let's start by diving into your background and how you got\ninvolved in the Kubernetes ecosystem. Can you tell us more about that\njourney?</p>\n<p><strong>Kaslin:</strong> I first got involved in the Kubernetes ecosystem through\nmy mentor, Jonathan Rippy, who introduced me to containers during my\nearly days in tech. Eventually, I transitioned to a team working with\ncontainers, which sparked my interest in Kubernetes when it was\nannounced. While researching Kubernetes in that role, I eagerly sought\nopportunities to engage with the containers/Kubernetes community. It\nwas not until my subsequent job that I found a suitable role to\ncontribute consistently. I joined SIG ContribEx, specifically in the\nContributor Comms subproject, to both deepen my knowledge of\nKubernetes and support the community better.</p>\n<p><strong>Madhav:</strong> My journey with Kubernetes began when I was a student,\nsearching for interesting and exciting projects to work on. With my\npeers, I discovered open source and attended The New Contributor\nWorkshop organized by the Kubernetes community. The workshop not only\nprovided valuable insights into the community structure but also gave\nme a sense of warmth and welcome, which motivated me to join and\nremain involved. I realized that collaboration is at the heart of\nopen-source communities, and to get answers and support, I needed to\ncontribute and do my part. I started working on issues in ContribEx,\nparticularly focusing on GitHub automation, despite not fully\nunderstanding the task at first. I continued to contribute for various\ntechnical and non-technical aspects of the project, finding it to be\none of the most professionally rewarding experiences in my life.</p>\n<p><strong>Fyka:</strong> That's such an inspiration in itself! I'm sure beginners who\nare reading this got the ultimate motivation to take their first\nsteps. Embracing the Learning journey, seeking mentorship, and\nengaging with the Kubernetes community can pave the way for exciting\nopportunities in the tech industry. Your stories proved the importance\nof starting small and being proactive, just like Madhav said Don't be\nafraid to take on tasks, even if you're uncertain at first.</p>\n<h3 id=\"primary-goals-and-scope\">Primary goals and scope</h3>\n<p><strong>Fyka:</strong> Given your experience as a member of SIG ContribEx, could\nyou tell us a bit about the group's primary goals and initiatives? Its\ncurrent focus areas? What do you see as the scope of SIG ContribEx and\nthe impact it has on the Kubernetes community?</p>\n<p><strong>Kaslin:</strong> SIG ContribEx's primary goals are to simplify the\ncontributions of Kubernetes contributors and foster a welcoming\ncommunity. It collaborates with other Kubernetes SIGs, such as\nplanning the Contributor Summit at KubeCon, ensuring it meets the\nneeds of various groups. The group's impact is evident in projects\nlike updating org membership policies and managing critical platforms\nlike Zoom, YouTube, and Slack. Its scope encompasses making the\ncontributor experience smoother and supporting the overall Kubernetes\ncommunity.</p>\n<p><strong>Madhav:</strong> The Kubernetes project has vertical SIGs and cross-cutting\nSIGs, ContribEx is a deeply cross-cutting SIG, impacting virtually\nevery area of the Kubernetes community. Adding to Kaslin,\nsustainability in the Kubernetes project and community is critical now\nmore than ever, it plays a central role in addressing critical issues,\nsuch as maintainer succession, by facilitating cohorts for SIGs to\ntrain experienced community members to take on leadership\nroles. Excellent examples include SIG CLI and SIG Apps, leading to the\nonboarding of new reviewers. Additionally, SIG ContribEx is essential\nin managing GitHub automation tools, including bots and commands used\nby contributors for interacting with <a href=\"https://docs.prow.k8s.io/\">Prow</a>\nand other automation (label syncing, group and GitHub team management,\netc).</p>\n<h3 id=\"beginner-s-guide\">Beginner's guide!</h3>\n<p><strong>Fyka:</strong> I'll never forget talking to Kaslin when I joined the\ncommunity and needed help with contributing. Kaslin, your quick and\nclear answers were a huge help in getting me started. Can you both\ngive some tips for people new to contributing to Kubernetes? What\nmakes SIG ContribEx a great starting point? Why should beginners and\ncurrent contributors consider it? And what cool opportunities are\nthere for newbies to jump in?</p>\n<p><strong>Kaslin:</strong> If you want to contribute to Kubernetes for the first\ntime, it can be overwhelming to know where to start. A good option is\nto join SIG ContribEx as it offers great opportunities to know and\nserve the community. Within SIG ContribEx, various subprojects allow\nyou to explore different parts of the Kubernetes project while you\nlearn how contributions work. Once you know a bit more, it‚Äôs common\nfor you to move to other SIGs within the project, and we think that‚Äôs\nwonderful. While many newcomers look for &quot;good first issues&quot; to start\nwith, these opportunities can be scarce and get claimed\nquickly. Instead, the real benefit lies in attending meetings and\ngetting to know the community. As you learn more about the project and\nthe people involved, you'll be better equipped to offer your help, and\nthe community will be more inclined to seek your assistance when\nneeded. As a co-lead for the Contributor Comms subproject, I can\nconfidently say that it's an excellent place for beginners to get\ninvolved. We have supportive leads and particularly beginner-friendly\nprojects too.</p>\n<p><strong>Madhav:</strong> To begin, read the <a href=\"https://github.com/kubernetes/community/tree/master#readme\">SIG\nREADME</a> on\nGitHub, which provides an overview of the projects the SIG\nmanages. While attending meetings is beneficial for all SIGs, it's\nespecially recommended for SIG ContribEx, as each subproject gets\ndedicated slots for updates and areas that need help. If you can't\nattend in real-time due to time zone differences, you can catch the\nmeeting recordings or\n<a href=\"https://docs.google.com/document/d/1K3vjCZ9C3LwYrOJOhztQtFuDQCe-urv-ewx1bI8IPVQ/edit?usp=sharing\">Notes</a>\nlater.</p>\n<h3 id=\"skills-you-learn\">Skills you learn!</h3>\n<p><strong>Fyka:</strong> What skills do you look for when bringing in new\ncontributors to SIG ContribEx, from passion to expertise?\nAdditionally, what skills can contributors expect to develop while\nworking with SIG ContribEx?</p>\n<p><strong>Kaslin:</strong> Skills folks need to have or will acquire vary depending\non what area of ContribEx they work upon. Even within a subproject, a\nrange of skills can be useful and/or developed. For example, the tech\nlead role involves technical tasks and overseeing automation, while\nthe social media lead role requires excellent communication\nskills. Working with SIG ContribEx allows contributors to acquire\nvarious skills based on their chosen subproject. By participating in\nmeetings, listening, learning, and taking on tasks related to their\ninterests, they can develop and hone these skills. Some subprojects\nmay require more specialized skills, like program management for the\nmentoring project, but all contributors can benefit from offering\ntheir talents to help teach others and contribute to the community.</p>\n<h3 id=\"sub-projects-under-sig-contribex\">Sub-projects under SIG ContribEx</h3>\n<p><strong>Fyka:</strong> SIG ContribEx has several smaller projects. Can you tell me\nabout the aims of these projects and how they've impacted the\nKubernetes community?</p>\n<p><strong>Kaslin:</strong> Some SIGs have one or two subprojects and some have none\nat all, but in SIG ContribEx, we have ELEVEN!</p>\n<p>Here‚Äôs a list of them and their respective mission statements</p>\n<ol>\n<li><strong>Community</strong>: Manages the community repository, documentation,\nand operations.</li>\n<li><strong>Community management</strong>: Handles communication platforms and\npolicies for the community.</li>\n<li><strong>Contributor-comms</strong>: Focuses on promoting the success of\nKubernetes contributors through marketing.</li>\n<li><strong>Contributors-documentation</strong>: Writes and maintains documentation\nfor contributing to Kubernetes.</li>\n<li><strong>Devstats</strong>: Maintains and updates the <a href=\"https://k8s.devstats.cncf.io\">Kubernetes\nstatistics</a> website.</li>\n<li><strong>Elections</strong>: Oversees community elections and maintains related\ndocumentation and software.</li>\n<li><strong>Events</strong>: Organizes contributor-focused events like the\nContributor Summit.</li>\n<li><strong>Github management</strong>: Manages permissions, repositories, and\ngroups on GitHub.</li>\n<li><strong>Mentoring</strong>: Develop programs to help contributors progress in\ntheir contributions.</li>\n<li><strong>Sigs-GitHub-actions</strong>: Repository for GitHub actions related to\nall SIGs in Kubernetes.</li>\n<li><strong>Slack-infra</strong>: Creates and maintains tools and automation for\nKubernetes Slack.</li>\n</ol>\n<p><strong>Madhav:</strong> Also, Devstats is critical from a sustainability\nstandpoint!</p>\n<p><em>(If you are willing to learn more and get involved with any of these\nsub-projects, check out the</em> <a href=\"https://github.com/kubernetes/community/blob/master/sig-contributor-experience/README.md#subprojects\">SIG ContribEx\nREADME</a>)._</p>\n<h3 id=\"accomplishments\">Accomplishments</h3>\n<p><strong>Fyka:</strong> With that said, any SIG-related accomplishment that you‚Äôre\nproud of?</p>\n<p><strong>Kaslin:</strong> I'm proud of the accomplishments made by SIG ContribEx and\nits contributors in supporting the community. Some of the recent\nachievements include:</p>\n<ol>\n<li><em>Establishment of the elections subproject</em>: Kubernetes is a massive\nproject, and ensuring smooth leadership transitions is\ncrucial. The contributors in this subproject organize fair and\nconsistent elections, which helps keep the project running\neffectively.</li>\n<li><em>New issue triage proces</em>: With such a large open-source project\nlike Kubernetes, there's always a lot of work to be done. To\nensure things progress safely, we implemented new labels and\nupdated functionality for issue triage using our PROW tool. This\nreduces bottlenecks in the workflow and allows leaders to\naccomplish more.</li>\n<li><em>New org membership requirements</em>: Becoming an org member in\nKubernetes can be overwhelming for newcomers. We view org\nmembership as a significant milestone for contributors aiming to\ntake on leadership roles. We recently updated the rules to\nautomatically remove privileges from inactive members, making sure\nthat the right people have access to the necessary tools and\nresponsibilities.</li>\n</ol>\n<p>Overall, these accomplishments have greatly benefited our fellow\ncontributors and strengthened the Kubernetes community.</p>\n<h3 id=\"upcoming-initiatives\">Upcoming initiatives</h3>\n<p><strong>Fyka:</strong> Could you give us a sneak peek into what's next for the\ngroup? We're excited to hear about upcoming projects and initiatives\nfrom this dynamic team.</p>\n<p><strong>Madhav:</strong> We‚Äôd love for more groups to sign up for mentoring\ncohorts! We‚Äôre probably going to have to spend some time polishing the\nprocess around that.</p>\n<h3 id=\"final-thoughts\">Final thoughts</h3>\n<p><strong>Fyka:</strong> As we wrap up our conversation, would you like to share some\nfinal thoughts for those interested in contributing to SIG ContribEx\nor getting involved with Kubernetes?</p>\n<p><strong>Madhav</strong>: Kubernetes is meant to be overwhelming and difficult\ninitially! You‚Äôre coming into something that‚Äôs taken multiple people,\nfrom multiple countries, multiple years to build. Embrace that\ndiversity! Use the high entropy initially to collide around and gain\nas much knowledge about the project and community as possible before\nyou decide to settle in your niche.</p>\n<p><strong>Fyka:</strong> Thank You Madhav and Kaslin, it was an absolute pleasure\nchatting about SIG ContribEx and your experiences as a member. It's\nclear that the role of SIG ContribEx in Kubernetes is significant and\nessential, ensuring scalability, growth and productivity, and I hope\nthis interview inspires more people to get involved and contribute to\nKubernetes. I wish SIG ContribEx all the best, and can't wait to see\nwhat exciting things lie ahead!</p>\n<h2 id=\"what-next\">What next?</h2>\n<p>We love meeting new contributors and helping them in investigating\ndifferent Kubernetes project spaces. If you are interested in getting\nmore involved with SIG ContribEx, here are some resources for you to\nget started:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/community/tree/master/sig-contributor-experience#contributor-experience-special-interest-group\">GitHub</a></li>\n<li><a href=\"https://groups.google.com/g/kubernetes-sig-contribex\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fcontributor-experience\">Open Community\nIssues/PRs</a></li>\n<li><a href=\"https://slack.k8s.io/\">Slack</a></li>\n<li><a href=\"https://kubernetes.slack.com/messages/sig-contribex\">Slack channel\n#sig-contribex</a></li>\n<li>SIG Contribex also hosted a <a href=\"https://youtu.be/5Bs1bs6iFmY\">KubeCon\ntalk</a> about studying Kubernetes\nContributor experiences.</li>\n</ul>","PublishedAt":"2023-08-14 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/08/14/sig-contribex-spotlight-2023/","SourceName":"Kubernetes"}},{"node":{"ID":4184,"Title":"Blog: Spotlight on SIG CLI","Description":"<p><strong>Author</strong>: Arpit Agrawal</p>\n<p>In the world of Kubernetes, managing containerized applications at\nscale requires powerful and efficient tools. The command-line\ninterface (CLI) is an integral part of any developer or operator‚Äôs\ntoolkit, offering a convenient and flexible way to interact with a\nKubernetes cluster.</p>\n<p>SIG CLI plays a crucial role in improving the <a href=\"https://github.com/kubernetes/community/tree/master/sig-cli\">Kubernetes\nCLI</a>\nexperience by focusing on the development and enhancement of\n<code>kubectl</code>, the primary command-line tool for Kubernetes.</p>\n<p>In this SIG CLI Spotlight, Arpit Agrawal, SIG ContribEx-Comms team\nmember, talked with <a href=\"https://github.com/KnVerey\">Katrina Verey</a>, Tech\nLead &amp; Chair of SIG CLI,and <a href=\"https://github.com/soltysh\">Maciej\nSzulik</a>, SIG CLI Batch Lead, about SIG\nCLI, current projects, challenges and how anyone can get involved.</p>\n<p>So, whether you are a seasoned Kubernetes enthusiast or just getting\nstarted, understanding the significance of SIG CLI will undoubtedly\nenhance your Kubernetes journey.</p>\n<h2 id=\"introductions\">Introductions</h2>\n<p><strong>Arpit</strong>: Could you tell us a bit about yourself, your role, and how\nyou got involved in SIG CLI?</p>\n<p><strong>Maciej</strong>: I‚Äôm one of the technical leads for SIG-CLI. I was working\non Kubernetes in multiple areas since 2014, and in 2018 I got\nappointed a lead.</p>\n<p><strong>Katrina</strong>: I‚Äôve been working with Kubernetes as an end-user since\n2016, but it was only in late 2019 that I discovered how well SIG CLI\naligned with my experience from internal projects. I started regularly\nattending meetings and made a few small PRs, and by 2021 I was working\nmore deeply with the\n<a href=\"https://github.com/kubernetes-sigs/kustomize\">Kustomize</a> team\nspecifically. Later that year, I was appointed to my current roles as\nsubproject owner for Kustomize and KRM Functions, and as SIG CLI Tech\nLead and Chair.</p>\n<h2 id=\"about-sig-cli\">About SIG CLI</h2>\n<p><strong>Arpit</strong>: Thank you! Could you share with us the purpose and goals of SIG CLI?</p>\n<p><strong>Maciej</strong>: Our\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-cli/\">charter</a>\nhas the most detailed description, but in few words, we handle all CLI\ntooling that helps you manage your Kubernetes manifests and interact\nwith your Kubernetes clusters.</p>\n<p><strong>Arpit</strong>: I see. And how does SIG CLI work to promote best-practices\nfor CLI development and usage in the cloud native ecosystem?</p>\n<p><strong>Maciej</strong>: Within <code>kubectl</code>, we have several on-going efforts that\ntry to encourage new contributors to align existing commands to new\nstandards. We publish several libraries which hopefully make it easier\nto write CLIs that interact with Kubernetes APIs, such as cli-runtime\nand\n<a href=\"https://github.com/kubernetes-sigs/kustomize/tree/master/kyaml\">kyaml</a>.</p>\n<p><strong>Katrina</strong>: We also maintain some interoperability specifications for\nCLI tooling, such as the <a href=\"https://github.com/kubernetes-sigs/kustomize/blob/master/cmd/config/docs/api-conventions/functions-spec.md\">KRM Functions\nSpecification</a>\n(GA) and the new ApplySet\nSpecification\n(alpha).</p>\n<h2 id=\"current-projects-and-challenges\">Current projects and challenges</h2>\n<p><strong>Arpit</strong>: Going through the README file, it‚Äôs clear SIG CLI has a\nnumber of subprojects, could you highlight some important ones?</p>\n<p><strong>Maciej</strong>: The four most active subprojects that are, in my opinion,\nworthy of your time investment would be:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubectl\"><code>kubectl</code></a>: the canonical Kubernetes CLI.</li>\n<li><a href=\"https://github.com/kubernetes-sigs/kustomize\">Kustomize</a>: a\ntemplate-free customization tool for Kubernetes yaml manifest files.</li>\n<li><a href=\"https://kui.tools\">KUI</a> - a GUI interface to Kubernetes, think\n<code>kubectl</code> on steroids.</li>\n<li><a href=\"https://github.com/kubernetes-sigs/krew\"><code>krew</code></a>: a plugin manager for <code>kubectl</code>.</li>\n</ul>\n<p><strong>Arpit</strong>: Are there any upcoming initiatives or developments that SIG\nCLI is working on?</p>\n<p><strong>Maciej</strong>: There are always several initiatives we‚Äôre working on at\nany given point in time. It‚Äôs best to join <a href=\"https://github.com/kubernetes/community/tree/master/sig-cli/#meetings\">one of our\ncalls</a>\nto learn about the current ones.</p>\n<p><strong>Katrina</strong>: For major features, you can check out <a href=\"https://www.kubernetes.dev/resources/keps/\">our open\nKEPs</a>. For instance, in\n1.27 we introduced alphas for <a href=\"https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning/\">a new pruning mode in kubectl\napply</a>,\nand for kubectl create plugins. Exciting ideas that are currently\nunder discussion include an interactive mode for <code>kubectl</code> delete\n(<a href=\"https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning\">KEP\n3895</a>)\nand the <code>kuberc</code> user preferences file (<a href=\"https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning\">KEP\n3104</a>).</p>\n<p><strong>Arpit</strong>: Could you discuss any challenges that SIG CLI faces in its\nefforts to improve CLIs for cloud-native technologies? What are the\nfuture efforts to solve them?</p>\n<p><strong>Katrina</strong>: The biggest challenge we‚Äôre facing with every decision is\nbackwards compatibility and ensuring we don‚Äôt break existing users. It\nfrequently happens that fixing what's on the surface may seem\nstraightforward, but even fixing a bug could constitute a breaking\nchange for some users, which means we need to go through an extended\ndeprecation process to change it, or in some cases we can‚Äôt change it\nat all. Another challenge is the need to balance customization with\nusability in the flag sets we expose on our tools. For example, we get\nmany proposals for new flags that would certainly be useful to some\nusers, but not a large enough subset to justify the increased\ncomplexity having them in the tool entails for everyone. The <code>kuberc</code>\nproposal may help with some of these problems by giving individual\nusers the ability to set or override default values we can‚Äôt change,\nand even create custom subcommands via aliases</p>\n<p><strong>Arpit</strong>: With every new version release of Kubernetes, maintaining\nconsistency and integrity is surely challenging: how does the SIG CLI\nteam tackle it?</p>\n<p><strong>Maciej</strong>: This is mostly similar to the topic mentioned in the\nprevious question: every new change, especially to existing commands\ngoes through a lot of scrutiny to ensure we don‚Äôt break existing\nusers. At any point in time we have to keep a reasonable balance\nbetween features and not breaking users.</p>\n<h2 id=\"future-plans-and-contribution\">Future plans and contribution</h2>\n<p><strong>Arpit</strong>: How do you see the role of CLI tools in the cloud-native\necosystem evolving in the future?</p>\n<p><strong>Maciej</strong>: I think that CLI tools were and will always be an\nimportant piece of the ecosystem. Whether used by administrators on\nremote machines that don‚Äôt have GUI or in every CI/CD pipeline, they\nare irreplaceable.</p>\n<p><strong>Arpit</strong>: Kubernetes is a community-driven project. Any\nrecommendation for anyone looking into getting involved in SIG CLI\nwork? Where should they start? Are there any prerequisites?</p>\n<p><strong>Maciej</strong>: There are no prerequisites other than a little bit of free\ntime on your hands and willingness to learn something new :-)</p>\n<p><strong>Katrina</strong>: A working knowledge of <a href=\"https://go.dev/\">Go</a> often helps,\nbut we also have areas in need of non-code contributions, such as the\n<a href=\"https://github.com/kubernetes-sigs/kustomize/issues/4338\">Kustomize docs consolidation\nproject</a>.</p>","PublishedAt":"2023-07-20 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/07/20/sig-cli-spotlight-2023/","SourceName":"Kubernetes"}},{"node":{"ID":4089,"Title":"Blog: Confidential Kubernetes: Use Confidential Virtual Machines and Enclaves to improve your cluster security","Description":"<p><strong>Authors:</strong> Fabian Kammel (Edgeless Systems), Mikko Ylinen (Intel), Tobin Feldman-Fitzthum (IBM)</p>\n<p>In this blog post, we will introduce the concept of Confidential Computing (CC) to improve any computing environment's security and privacy properties. Further, we will show how\nthe Cloud-Native ecosystem, particularly Kubernetes, can benefit from the new compute paradigm.</p>\n<p>Confidential Computing is a concept that has been introduced previously in the cloud-native world. The\n<a href=\"https://confidentialcomputing.io/\">Confidential Computing Consortium</a> (CCC) is a project community in the Linux Foundation\nthat already worked on\n<a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/85/2019/12/CCC_Overview.pdf\">Defining and Enabling Confidential Computing</a>.\nIn the <a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/85/2023/01/CCC-A-Technical-Analysis-of-Confidential-Computing-v1.3_Updated_November_2022.pdf\">Whitepaper</a>,\nthey provide a great motivation for the use of Confidential Computing:</p>\n<blockquote>\n<p>Data exists in three states: in transit, at rest, and in use. ‚Ä¶Protecting sensitive data\nin all of its states is more critical than ever. Cryptography is now commonly deployed\nto provide both data confidentiality (stopping unauthorized viewing) and data integrity\n(preventing or detecting unauthorized changes). While techniques to protect data in transit\nand at rest are now commonly deployed, the third state - protecting data in use - is the new frontier.</p>\n</blockquote>\n<p>Confidential Computing aims to primarily solve the problem of <strong>protecting data in use</strong>\nby introducing a hardware-enforced Trusted Execution Environment (TEE).</p>\n<h2 id=\"trusted-execution-environments\">Trusted Execution Environments</h2>\n<p>For more than a decade, Trusted Execution Environments (TEEs) have been available in commercial\ncomputing hardware in the form of <a href=\"https://en.wikipedia.org/wiki/Hardware_security_module\">Hardware Security Modules</a>\n(HSMs) and <a href=\"https://www.iso.org/standard/50970.html\">Trusted Platform Modules</a> (TPMs). These\ntechnologies provide trusted environments for shielded computations. They can\nstore highly sensitive cryptographic keys and carry out critical cryptographic operations\nsuch as signing or encrypting data.</p>\n<p>TPMs are optimized for low cost, allowing them to be integrated into mainboards and act as a\nsystem's physical root of trust. To keep the cost low, TPMs are limited in scope, i.e., they\nprovide storage for only a few keys and are capable of just a small subset of cryptographic operations.</p>\n<p>In contrast, HSMs are optimized for high performance, providing secure storage for far\nmore keys and offering advanced physical attack detection mechanisms. Additionally, high-end HSMs\ncan be programmed so that arbitrary code can be compiled and executed. The downside\nis that they are very costly. A managed CloudHSM from AWS costs\n<a href=\"https://aws.amazon.com/cloudhsm/pricing/\">around $1.50 / hour</a> or ~$13,500 / year.</p>\n<p>In recent years, a new kind of TEE has gained popularity. Technologies like\n<a href=\"https://developer.amd.com/sev/\">AMD SEV</a>,\n<a href=\"https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html\">Intel SGX</a>,\nand <a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-trust-domain-extensions.html\">Intel TDX</a>\nprovide TEEs that are closely integrated with userspace. Rather than low-power or high-performance\ndevices that support specific use cases, these TEEs shield normal processes or virtual machines\nand can do so with relatively low overhead. These technologies each have different design goals,\nadvantages, and limitations, and they are available in different environments, including consumer\nlaptops, servers, and mobile devices.</p>\n<p>Additionally, we should mention\n<a href=\"https://www.arm.com/technologies/trustzone-for-cortex-a\">ARM TrustZone</a>, which is optimized\nfor embedded devices such as smartphones, tablets, and smart TVs, as well as\n<a href=\"https://aws.amazon.com/ec2/nitro/nitro-enclaves/\">AWS Nitro Enclaves</a>, which are only available\non <a href=\"https://aws.amazon.com/\">Amazon Web Services</a> and have a different threat model compared\nto the CPU-based solutions by Intel and AMD.</p>\n<p><a href=\"https://www.ibm.com/docs/en/linux-on-systems?topic=virtualization-secure-execution\">IBM Secure Execution for Linux</a>\nlets you run your Kubernetes cluster's nodes as KVM guests within a trusted execution environment on\nIBM Z series hardware. You can use this hardware-enhanced virtual machine isolation to\nprovide strong isolation between tenants in a cluster, with hardware attestation about the (virtual) node's integrity.</p>\n<h3 id=\"security-properties-and-feature-set\">Security properties and feature set</h3>\n<p>In the following sections, we will review the security properties and additional features\nthese new technologies bring to the table. Only some solutions will provide all properties;\nwe will discuss each technology in further detail in their respective section.</p>\n<p>The <strong>Confidentiality</strong> property ensures that information cannot be viewed while it is\nin use in the TEE. This provides us with the highly desired feature to secure\n<strong>data in use</strong>. Depending on the specific TEE used, both code and data may be protected\nfrom outside viewers. The differences in TEE architectures and how their use\nin a cloud native context are important considerations when designing end-to-end security\nfor sensitive workloads with a minimal <strong>Trusted Computing Base</strong> (TCB) in mind. CCC has recently\nworked on a <a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/85/2023/01/Common-Terminology-for-Confidential-Computing.pdf\">common vocabulary and supporting material</a>\nthat helps to explain where confidentiality boundaries are drawn with the different TEE\narchitectures and how that impacts the TCB size.</p>\n<p>Confidentiality is a great feature, but an attacker can still manipulate\nor inject arbitrary code and data for the TEE to execute and, therefore, easily leak critical\ninformation. <strong>Integrity</strong> guarantees a TEE owner that neither code nor data can be\ntampered with while running critical computations.</p>\n<p><strong>Availability</strong> is a basic property often discussed in the context of information\nsecurity. However, this property is outside the scope of most TEEs. Usually, they can be controlled\n(shut down, restarted, ‚Ä¶) by some higher level abstraction. This could be the CPU itself, the\nhypervisor, or the kernel. This is to preserve the overall system's availability,\nnot the TEE itself. When running in the cloud, availability is usually guaranteed by\nthe cloud provider in terms of Service Level Agreements (SLAs) and is not cryptographically enforceable.</p>\n<p>Confidentiality and Integrity by themselves are only helpful in some cases. For example,\nconsider a TEE running in a remote cloud. How would you know the TEE is genuine and running\nyour intended software? It could be an imposter stealing your data as soon as you send it over.\nThis fundamental problem is addressed by <strong>Attestability</strong>. Attestation allows us to verify\nthe identity, confidentiality, and integrity of TEEs based on cryptographic certificates issued\nfrom the hardware itself. This feature can also be made available to clients outside of the\nconfidential computing hardware in the form of remote attestation.</p>\n<p>TEEs can hold and process information that predates or outlives the trusted environment. That\ncould mean across restarts, different versions, or platform migrations. Therefore <strong>Recoverability</strong>\nis an important feature. Data and the state of a TEE need to be sealed before they are written\nto persistent storage to maintain confidentiality and integrity guarantees. The access to such\nsealed data needs to be well-defined. In most cases, the unsealing is bound to a TEE's identity.\nHence, making sure the recovery can only happen in the same confidential context.</p>\n<p>This does not have to limit the flexibility of the overall system.\n<a href=\"https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf\">AMD SEV-SNP's migration agent (MA)</a>\nallows users to migrate a confidential virtual machine to a different host system\nwhile keeping the security properties of the TEE intact.</p>\n<h2 id=\"feature-comparison\">Feature comparison</h2>\n<p>These sections of the article will dive a little bit deeper into the specific implementations,\ncompare supported features and analyze their security properties.</p>\n<h3 id=\"amd-sev\">AMD SEV</h3>\n<p>AMD's <a href=\"https://developer.amd.com/sev/\">Secure Encrypted Virtualization (SEV)</a> technologies\nare a set of features to enhance the security of virtual machines on AMD's server CPUs. SEV\ntransparently encrypts the memory of each VM with a unique key. SEV can also calculate a\nsignature of the memory contents, which can be sent to the VM's owner as an attestation that\nthe initial guest memory was not manipulated.</p>\n<p>The second generation of SEV, known as\n<a href=\"https://www.amd.com/system/files/TechDocs/Protecting%20VM%20Register%20State%20with%20SEV-ES.pdf\">Encrypted State</a>\nor SEV-ES, provides additional protection from the hypervisor by encrypting all\nCPU register contents when a context switch occurs.</p>\n<p>The third generation of SEV,\n<a href=\"https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf\">Secure Nested Paging</a>\nor SEV-SNP, is designed to prevent software-based integrity attacks and reduce the risk associated with\ncompromised memory integrity. The basic principle of SEV-SNP integrity is that if a VM can read\na private (encrypted) memory page, it must always read the value it last wrote.</p>\n<p>Additionally, by allowing the guest to obtain remote attestation statements dynamically,\nSNP enhances the remote attestation capabilities of SEV.</p>\n<p>AMD SEV has been implemented incrementally. New features and improvements have been added with\neach new CPU generation. The Linux community makes these features available as part of the KVM hypervisor\nand for host and guest kernels. The first SEV features were discussed and implemented in 2016 - see\n<a href=\"https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/kaplan\">AMD x86 Memory Encryption Technologies</a>\nfrom the 2016 Usenix Security Symposium. The latest big addition was\n<a href=\"https://www.phoronix.com/news/AMD-SEV-SNP-Arrives-Linux-5.19\">SEV-SNP guest support in Linux 5.19</a>.</p>\n<p><a href=\"https://azure.microsoft.com/en-us/updates/azureconfidentialvm/\">Confidential VMs based on AMD SEV-SNP</a>\nare available in Microsoft Azure since July 2022. Similarly, Google Cloud Platform (GCP) offers\n<a href=\"https://cloud.google.com/compute/confidential-vm/docs/about-cvm\">confidential VMs based on AMD SEV-ES</a>.</p>\n<h3 id=\"intel-sgx\">Intel SGX</h3>\n<p>Intel's\n<a href=\"https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html\">Software Guard Extensions</a>\nhas been available since 2015 and were introduced with the Skylake architecture.</p>\n<p>SGX is an instruction set that enables users to create a protected and isolated process called\nan <em>enclave</em>. It provides a reverse sandbox that protects enclaves from the operating system,\nfirmware, and any other privileged execution context.</p>\n<p>The enclave memory cannot be read or written from outside the enclave, regardless of\nthe current privilege level and CPU mode. The only way to call an enclave function is\nthrough a new instruction that performs several protection checks. Its memory is encrypted.\nTapping the memory or connecting the DRAM modules to another system will yield only encrypted\ndata. The memory encryption key randomly changes every power cycle. The key is stored\nwithin the CPU and is not accessible.</p>\n<p>Since the enclaves are process isolated, the operating system's libraries are not usable as is;\ntherefore, SGX enclave SDKs are required to compile programs for SGX. This also implies applications\nneed to be designed and implemented to consider the trusted/untrusted isolation boundaries.\nOn the other hand, applications get built with very minimal TCB.</p>\n<p>An emerging approach to easily transition to process-based confidential computing\nand avoid the need to build custom applications is to utilize library OSes. These OSes\nfacilitate running native, unmodified Linux applications inside SGX enclaves.\nA library OS intercepts all application requests to the host OS and processes them securely\nwithout the application knowing it's running a TEE.</p>\n<p>The 3rd generation Xeon CPUs (aka Ice Lake Server - &quot;ICX&quot;) and later generations did switch to using a technology called\n<a href=\"https://www.intel.com/content/www/us/en/developer/articles/news/runtime-encryption-of-memory-with-intel-tme-mk.html\">Total Memory Encryption - Multi-Key</a>\n(TME-MK) that uses AES-XTS, moving away from the\n<a href=\"https://eprint.iacr.org/2016/204.pdf\">Memory Encryption Engine</a>\nthat the consumer and Xeon E CPUs used. This increased the possible\n<a href=\"https://sgx101.gitbook.io/sgx101/sgx-bootstrap/enclave#enclave-page-cache-epc\">enclave page cache</a>\n(EPC) size (up to 512GB/CPU) and improved performance. More info\nabout SGX on multi-socket platforms can be found in the\n<a href=\"https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/supporting-intel-sgx-on-mulit-socket-platforms.pdf\">Whitepaper</a>.</p>\n<p>A <a href=\"https://ark.intel.com/content/www/us/en/ark/search/featurefilter.html?productType=873\">list of supported platforms</a>\nis available from Intel.</p>\n<p>SGX is available on\n<a href=\"https://azure.microsoft.com/de-de/updates/intel-sgx-based-confidential-computing-vms-now-available-on-azure-dedicated-hosts/\">Azure</a>,\n<a href=\"https://www.alibabacloud.com/help/en/elastic-compute-service/latest/build-an-sgx-encrypted-computing-environment\">Alibaba Cloud</a>,\n<a href=\"https://cloud.ibm.com/docs/bare-metal?topic=bare-metal-bm-server-provision-sgx\">IBM</a>, and many more.</p>\n<h3 id=\"intel-tdx\">Intel TDX</h3>\n<p>Where Intel SGX aims to protect the context of a single process,\n<a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-trust-domain-extensions.html\">Intel's Trusted Domain Extensions</a>\nprotect a full virtual machine and are, therefore, most closely comparable to AMD SEV.</p>\n<p>As with SEV-SNP, guest support for TDX was <a href=\"https://www.phoronix.com/news/Intel-TDX-For-Linux-5.19\">merged in Linux Kernel 5.19</a>.\nHowever, hardware support will land with <a href=\"https://en.wikipedia.org/wiki/Sapphire_Rapids\">Sapphire Rapids</a> during 2023:\n<a href=\"https://www.alibabacloud.com/help/en/elastic-compute-service/latest/build-a-tdx-confidential-computing-environment\">Alibaba Cloud provides</a>\ninvitational preview instances, and\n<a href=\"https://techcommunity.microsoft.com/t5/azure-confidential-computing/preview-introducing-dcesv5-and-ecesv5-series-confidential-vms/ba-p/3800718\">Azure has announced</a>\nits TDX preview opportunity.</p>\n<h2 id=\"overhead-analysis\">Overhead analysis</h2>\n<p>The benefits that Confidential Computing technologies provide via strong isolation and enhanced\nsecurity to customer data and workloads are not for free. Quantifying this impact is challenging and\ndepends on many factors: The TEE technology, the benchmark, the metrics, and the type of workload\nall have a huge impact on the expected performance overhead.</p>\n<p>Intel SGX-based TEEs are hard to benchmark, as <a href=\"https://arxiv.org/pdf/2205.06415.pdf\">shown</a>\n<a href=\"https://www.ibr.cs.tu-bs.de/users/mahhouk/papers/eurosec2021.pdf\">by</a>\n<a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3533737.3535098\">different papers</a>. The chosen SDK/library\nOS, the application itself, as well as the resource requirements (especially large memory requirements)\nhave a huge impact on performance. A single-digit percentage overhead can be expected if an application\nis well suited to run inside an enclave.</p>\n<p>Confidential virtual machines based on AMD SEV-SNP require no changes to the executed program\nand operating system and are a lot easier to benchmark. A\n<a href=\"https://community.amd.com/t5/business/microsoft-azure-confidential-computing-powered-by-3rd-gen-epyc/ba-p/497796\">benchmark from Azure and AMD</a>\nshows that SEV-SNP VM overhead is &lt;10%, sometimes as low as 2%.</p>\n<p>Although there is a performance overhead, it should be low enough to enable real-world workloads\nto run in these protected environments and improve the security and privacy of our data.</p>\n<h2 id=\"confidential-computing-compared-to-fhe-zkp-and-mpc\">Confidential Computing compared to FHE, ZKP, and MPC</h2>\n<p>Fully Homomorphic Encryption (FHE), Zero Knowledge Proof/Protocol (ZKP), and Multi-Party\nComputations (MPC) are all a form of encryption or cryptographic protocols that offer\nsimilar security guarantees to Confidential Computing but do not require hardware support.</p>\n<p>Fully (also partially and somewhat) homomorphic encryption allows one to perform\ncomputations, such as addition or multiplication, on encrypted data. This provides\nthe property of encryption in use but does not provide integrity protection or attestation\nlike confidential computing does. Therefore, these two technologies can <a href=\"https://confidentialcomputing.io/2023/03/29/confidential-computing-and-homomorphic-encryption/\">complement to each other</a>.</p>\n<p>Zero Knowledge Proofs or Protocols are a privacy-preserving technique (PPT) that\nallows one party to prove facts about their data without revealing anything else about\nthe data. ZKP can be used instead of or in addition to Confidential Computing to protect\nthe privacy of the involved parties and their data. Similarly, Multi-Party Computation\nenables multiple parties to work together on a computation, i.e., each party provides\ntheir data to the result without leaking it to any other parties.</p>\n<h2 id=\"use-cases-of-confidential-computing\">Use cases of Confidential Computing</h2>\n<p>The presented Confidential Computing platforms show that both the isolation of a single container\nprocess and, therefore, minimization of the trusted computing base and the isolation of a\n``\nfull virtual machine are possible. This has already enabled a lot of interesting and secure\nprojects to emerge:</p>\n<h3 id=\"confidential-containers\">Confidential Containers</h3>\n<p><a href=\"https://github.com/confidential-containers\">Confidential Containers</a> (CoCo) is a\nCNCF sandbox project that isolates Kubernetes pods inside of confidential virtual machines.</p>\n<p>CoCo can be installed on a Kubernetes cluster with an operator.\nThe operator will create a set of runtime classes that can be used to deploy\npods inside an enclave on several different platforms, including\nAMD SEV, Intel TDX, Secure Execution for IBM Z, and Intel SGX.</p>\n<p>CoCo is typically used with signed and/or encrypted container images\nwhich are pulled, verified, and decrypted inside the enclave.\nSecrets, such as image decryption keys, are conditionally provisioned\nto the enclave by a trusted Key Broker Service that validates the\nhardware evidence of the TEE prior to releasing any sensitive information.</p>\n<p>CoCo has several deployment models. Since the Kubernetes control plane\nis outside the TCB, CoCo is suitable for managed environments. CoCo can\nbe run in virtual environments that don't support nesting with the help of an\nAPI adaptor that starts pod VMs in the cloud. CoCo can also be run on\nbare metal, providing strong isolation even in multi-tenant environments.</p>\n<h3 id=\"managed-confidential-kubernetes\">Managed confidential Kubernetes</h3>\n<p><a href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-node-pool-aks\">Azure</a> and\n<a href=\"https://cloud.google.com/blog/products/identity-security/announcing-general-availability-of-confidential-gke-nodes\">GCP</a>\nboth support the use of confidential virtual machines as worker nodes for their managed Kubernetes offerings.</p>\n<p>Both services aim for better workload protection and security guarantees by enabling memory encryption\nfor container workloads. However, they don't seek to fully isolate the cluster or workloads against\nthe service provider or infrastructure. Specifically, they don't offer a dedicated confidential control\nplane or expose attestation capabilities for the confidential cluster/nodes.</p>\n<p>Azure also enables\n<a href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-nodes-aks-overview\">Confidential Containers</a>\nin their managed Kubernetes offering. They support the creation based on\n<a href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-containers-enclaves\">Intel SGX enclaves</a>\nand <a href=\"https://techcommunity.microsoft.com/t5/azure-confidential-computing/microsoft-introduces-preview-of-confidential-containers-on-azure/ba-p/3410394\">AMD SEV-based VMs</a>.</p>\n<h3 id=\"constellation\">Constellation</h3>\n<p><a href=\"https://github.com/edgelesssys/constellation\">Constellation</a> is a Kubernetes engine that aims to\nprovide the best possible data security. Constellation wraps your entire Kubernetes cluster into\na single confidential context that is shielded from the underlying cloud infrastructure. Everything\ninside is always encrypted, including at runtime in memory. It shields both the worker and control\nplane nodes. In addition, it already integrates with popular CNCF software such as Cilium for\nsecure networking and provides extended CSI drivers to write data securely.</p>\n<h3 id=\"occlum-and-gramine\">Occlum and Gramine</h3>\n<p><a href=\"https://occlum.io/\">Occlum</a> and <a href=\"https://gramineproject.io/\">Gramine</a> are examples of open source\nlibrary OS projects that can be used to run unmodified applications in SGX enclaves. They\nare member projects under the CCC, but similar projects and products maintained by companies\nalso exist. With these libOS projects, existing containerized applications can be\neasily converted into confidential computing enabled containers. Many curated prebuilt\ncontainers are also available.</p>\n<h2 id=\"where-are-we-today-vendors-limitations-and-foss-landscape\">Where are we today? Vendors, limitations, and FOSS landscape</h2>\n<p>As we hope you have seen from the previous sections, Confidential Computing is a powerful new concept\nto improve security, but we are still in the (early) adoption phase. New products are\nstarting to emerge to take advantage of the unique properties.</p>\n<p>Google and Microsoft are the first major cloud providers to have confidential offerings that\ncan run unmodified applications inside a protected boundary.\nStill, these offerings are limited to compute, while end-to-end solutions for confidential\ndatabases, cluster networking, and load balancers have to be self-managed.</p>\n<p>These technologies provide opportunities to bring even the most\nsensitive workloads into the cloud and enables them to leverage all the\ntools in the CNCF landscape.</p>\n<h2 id=\"call-to-action\">Call to action</h2>\n<p>If you are currently working on a high-security product that struggles to run in the\npublic cloud due to legal requirements or are looking to bring the privacy and security\nof your cloud-native project to the next level: Reach out to all the great projects\nwe have highlighted! Everyone is keen to improve the security of our ecosystem, and you can\nplay a vital role in that journey.</p>\n<ul>\n<li><a href=\"https://github.com/confidential-containers\">Confidential Containers</a></li>\n<li><a href=\"https://github.com/edgelesssys/constellation\">Constellation: Always Encrypted Kubernetes</a></li>\n<li><a href=\"https://occlum.io/\">Occlum</a></li>\n<li><a href=\"https://gramineproject.io/\">Gramine</a></li>\n<li>CCC also maintains a <a href=\"https://confidentialcomputing.io/projects/\">list of projects</a></li>\n</ul>","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/07/06/confidential-kubernetes/","SourceName":"Kubernetes"}},{"node":{"ID":4059,"Title":"Blog: Verifying Container Image Signatures Within CRI Runtimes","Description":"<p><strong>Author</strong>: Sascha Grunert</p>\n<p>The Kubernetes community has been signing their container image-based artifacts\nsince release v1.24. While the graduation of the <a href=\"https://github.com/kubernetes/enhancements/issues/3031\">corresponding enhancement</a>\nfrom <code>alpha</code> to <code>beta</code> in v1.26 introduced signatures for the binary artifacts,\nother projects followed the approach by providing image signatures for their\nreleases, too. This means that they either create the signatures within their\nown CI/CD pipelines, for example by using GitHub actions, or rely on the\nKubernetes <a href=\"https://github.com/kubernetes-sigs/promo-tools/blob/e2b96dd/docs/image-promotion.md\">image promotion</a> process to automatically sign the images by\nproposing pull requests to the <a href=\"https://github.com/kubernetes/k8s.io/tree/4b95cc2/k8s.gcr.io\">k/k8s.io</a> repository. A requirement for\nusing this process is that the project is part of the <code>kubernetes</code> or\n<code>kubernetes-sigs</code> GitHub organization, so that they can utilize the community\ninfrastructure for pushing images into staging buckets.</p>\n<p>Assuming that a project now produces signed container image artifacts, how can\none actually verify the signatures? It is possible to do it manually like\noutlined in the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures\">official Kubernetes documentation</a>. The problem with this\napproach is that it involves no automation at all and should be only done for\ntesting purposes. In production environments, tools like the <a href=\"https://docs.sigstore.dev/policy-controller/overview\">sigstore\npolicy-controller</a> can help with the automation. These tools\nprovide a higher level API by using <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources\">Custom Resource Definitions (CRD)</a> as\nwell as an integrated <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers\">admission controller and webhook</a> to verify\nthe signatures.</p>\n<p>The general usage flow for an admission controller based verification is:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/flow.svg\"\nalt=\"Create an instance of the policy and annotate the namespace to validate the signatures. Then create the pod. The controller evaluates the policy and if it passes, then it does the image pull if necessary. If the policy evaluation fails, then it will not admit the pod.\"/>\n</figure>\n<p>A key benefit of this architecture is simplicity: A single instance within the\ncluster validates the signatures before any image pull can happen in the\ncontainer runtime on the nodes, which gets initiated by the kubelet. This\nbenefit also brings along the issue of separation: The node which should pull\nthe container image is not necessarily the same node that performs the admission. This\nmeans that if the controller is compromised, then a cluster-wide policy\nenforcement can no longer be possible.</p>\n<p>One way to solve this issue is doing the policy evaluation directly within the\n<a href=\"https://kubernetes.io/docs/concepts/architecture/cri\">Container Runtime Interface (CRI)</a> compatible container runtime. The\nruntime is directly connected to the <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet\">kubelet</a> on a node and does all\nthe tasks like pulling images. <a href=\"https://github.com/cri-o/cri-o\">CRI-O</a> is one of those available runtimes\nand will feature full support for container image signature verification in v1.28.</p>\n<p>How does it work? CRI-O reads a file called <a href=\"https://github.com/containers/image/blob/b3e0ba2/docs/containers-policy.json.5.md#sigstoresigned\"><code>policy.json</code></a>, which\ncontains all the rules defined for container images. For example, you can define a\npolicy which only allows signed images <code>quay.io/crio/signed</code> for any tag or\ndigest like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;default&#34;</span>: [{ <span style=\"color:#008000;font-weight:bold\">&#34;type&#34;</span>: <span style=\"color:#b44\">&#34;reject&#34;</span> }],\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;transports&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;docker&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;quay.io/crio/signed&#34;</span>: [\n</span></span><span style=\"display:flex;\"><span> {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;type&#34;</span>: <span style=\"color:#b44\">&#34;sigstoreSigned&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;signedIdentity&#34;</span>: { <span style=\"color:#008000;font-weight:bold\">&#34;type&#34;</span>: <span style=\"color:#b44\">&#34;matchRepository&#34;</span> },\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;fulcio&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;oidcIssuer&#34;</span>: <span style=\"color:#b44\">&#34;https://github.com/login/oauth&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;subjectEmail&#34;</span>: <span style=\"color:#b44\">&#34;sgrunert@redhat.com&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;caData&#34;</span>: <span style=\"color:#b44\">&#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUI5ekNDQVh5Z0F3SUJBZ0lVQUxaTkFQRmR4SFB3amVEbG9Ed3lZQ2hBTy80d0NnWUlLb1pJemowRUF3TXcKS2pFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUkV3RHdZRFZRUURFd2h6YVdkemRHOXlaVEFlRncweQpNVEV3TURjeE16VTJOVGxhRncwek1URXdNRFV4TXpVMk5UaGFNQ294RlRBVEJnTlZCQW9UREhOcFozTjBiM0psCkxtUmxkakVSTUE4R0ExVUVBeE1JYzJsbmMzUnZjbVV3ZGpBUUJnY3Foa2pPUFFJQkJnVXJnUVFBSWdOaUFBVDcKWGVGVDRyYjNQUUd3UzRJYWp0TGszL09sbnBnYW5nYUJjbFlwc1lCcjVpKzR5bkIwN2NlYjNMUDBPSU9aZHhleApYNjljNWlWdXlKUlErSHowNXlpK1VGM3VCV0FsSHBpUzVzaDArSDJHSEU3U1hyazFFQzVtMVRyMTlMOWdnOTJqCll6QmhNQTRHQTFVZER3RUIvd1FFQXdJQkJqQVBCZ05WSFJNQkFmOEVCVEFEQVFIL01CMEdBMVVkRGdRV0JCUlkKd0I1ZmtVV2xacWw2ekpDaGt5TFFLc1hGK2pBZkJnTlZIU01FR0RBV2dCUll3QjVma1VXbFpxbDZ6SkNoa3lMUQpLc1hGK2pBS0JnZ3Foa2pPUFFRREF3TnBBREJtQWpFQWoxbkhlWFpwKzEzTldCTmErRURzRFA4RzFXV2cxdENNCldQL1dIUHFwYVZvMGpoc3dlTkZaZ1NzMGVFN3dZSTRxQWpFQTJXQjlvdDk4c0lrb0YzdlpZZGQzL1Z0V0I1YjkKVE5NZWE3SXgvc3RKNVRmY0xMZUFCTEU0Qk5KT3NRNHZuQkhKCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=&#34;</span>\n</span></span><span style=\"display:flex;\"><span> },\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;rekorPublicKeyData&#34;</span>: <span style=\"color:#b44\">&#34;LS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS0KTUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFMkcyWSsydGFiZFRWNUJjR2lCSXgwYTlmQUZ3cgprQmJtTFNHdGtzNEwzcVg2eVlZMHp1ZkJuaEM4VXIvaXk1NUdoV1AvOUEvYlkyTGhDMzBNOStSWXR3PT0KLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tCg==&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> ]\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>CRI-O has to be started to use that policy as the global source of truth:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crio --log-level debug --signature-policy ./policy.json\n</span></span></code></pre></div><p>CRI-O is now able to pull the image while verifying its signatures. This can be\ndone by using <a href=\"https://github.com/kubernetes-sigs/cri-tools\"><code>crictl</code> (cri-tools)</a>, for example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl -D pull quay.io/crio/signed\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] get image connection\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] PullImageRequest: &amp;PullImageRequest{Image:&amp;ImageSpec{Image:quay.io/crio/signed,Annotations:map[string]string{},},Auth:nil,SandboxConfig:nil,}\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] PullImageResponse: &amp;PullImageResponse{ImageRef:quay.io/crio/signed@sha256:18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a,}\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Image is up to date for quay.io/crio/signed@sha256:18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a\n</span></span></span></code></pre></div><p>The CRI-O debug logs will also indicate that the signature got successfully\nvalidated:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] IsRunningImageAllowed for image docker:quay.io/crio/signed:latest\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Using transport &#34;docker&#34; specific policy section quay.io/crio/signed\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Reading /var/lib/containers/sigstore/crio/signed@sha256=18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a/signature-1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Looking for sigstore attachments in quay.io/crio/signed:sha256-18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a.sig\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] GET https://quay.io/v2/crio/signed/manifests/sha256-18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a.sig\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Content-Type from manifest GET is &#34;application/vnd.oci.image.manifest.v1+json&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Found a sigstore attachment manifest with 1 layers\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Fetching sigstore attachment 1/1: sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Downloading /v2/crio/signed/blobs/sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] GET https://quay.io/v2/crio/signed/blobs/sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Requirement 0: allowed\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">DEBU[‚Ä¶] Overall: allowed\n</span></span></span></code></pre></div><p>All of the defined fields like <code>oidcIssuer</code> and <code>subjectEmail</code> in the policy\nhave to match, while <code>fulcio.caData</code> and <code>rekorPublicKeyData</code> are the public\nkeys from the upstream <a href=\"https://github.com/sigstore/fulcio\">fulcio (OIDC PKI)</a> and <a href=\"https://github.com/sigstore/rekor\">rekor\n(transparency log)</a> instances.</p>\n<p>This means that if you now invalidate the <code>subjectEmail</code> of the policy, for example to\n<code>wrong@mail.com</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> jq <span style=\"color:#b44\">&#39;.transports.docker.&#34;quay.io/crio/signed&#34;[0].fulcio.subjectEmail = &#34;wrong@mail.com&#34;&#39;</span> policy.json &gt; new-policy.json\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> mv new-policy.json policy.json\n</span></span></code></pre></div><p>Then remove the image, since it already exists locally:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl rmi quay.io/crio/signed\n</span></span></code></pre></div><p>Now when you pull the image, CRI-O complains that the required email is wrong:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl pull quay.io/crio/signed\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">FATA[‚Ä¶] pulling image: rpc error: code = Unknown desc = Source image rejected: Required email wrong@mail.com not found (got []string{&#34;sgrunert@redhat.com&#34;})\n</span></span></span></code></pre></div><p>It is also possible to test an unsigned image against the policy. For that you\nhave to modify the key <code>quay.io/crio/signed</code> to something like\n<code>quay.io/crio/unsigned</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sed -i <span style=\"color:#b44\">&#39;s;quay.io/crio/signed;quay.io/crio/unsigned;&#39;</span> policy.json\n</span></span></code></pre></div><p>If you now pull the container image, CRI-O will complain that no signature exists\nfor it:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo crictl pull quay.io/crio/unsigned\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">FATA[‚Ä¶] pulling image: rpc error: code = Unknown desc = SignatureValidationFailed: Source image rejected: A signature was required, but no signature exists\n</span></span></span></code></pre></div><p>It is important to mention that CRI-O will match the\n<code>.critical.identity.docker-reference</code> field within the signature to match with\nthe image repository. For example, if you verify the image\n<code>registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.3</code>, then the corresponding\n<code>docker-reference</code> should be <code>registry.k8s.io/kube-apiserver-amd64</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> cosign verify registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.3 <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span><span style=\"color:#888\"> --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> --certificate-oidc-issuer https://accounts.google.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> | jq -r &#39;.[0].critical.identity.&#34;docker-reference&#34;&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">registry.k8s.io/kubernetes/kube-apiserver-amd64\n</span></span></span></code></pre></div><p>The Kubernetes community introduced <code>registry.k8s.io</code> as proxy mirror for\nvarious registries. Before the release of <a href=\"https://github.com/kubernetes-sigs/promo-tools/releases/tag/v4.0.2\">kpromo v4.0.2</a>, images\nhad been signed with the actual mirror rather than <code>registry.k8s.io</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> cosign verify registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.2 <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span><span style=\"color:#888\"> --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> --certificate-oidc-issuer https://accounts.google.com \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> | jq -r &#39;.[0].critical.identity.&#34;docker-reference&#34;&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">asia-northeast2-docker.pkg.dev/k8s-artifacts-prod/images/kubernetes/kube-apiserver-amd64\n</span></span></span></code></pre></div><p>The change of the <code>docker-reference</code> to <code>registry.k8s.io</code> makes it easier for\nend users to validate the signatures, because they cannot know anything about the\nunderlying infrastructure being used. The feature to set the identity on image\nsigning has been added to <a href=\"https://github.com/sigstore/cosign/pull/2984\">cosign</a> via the flag <code>sign --sign-container-identity</code> as well and will be part of its upcoming release.</p>\n<p>The Kubernetes image pull error code <code>SignatureValidationFailed</code> got <a href=\"https://github.com/kubernetes/kubernetes/pull/117717\">recently added to\nKubernetes</a> and will be available from v1.28. This error code allows\nend-users to understand image pull failures directly from the kubectl CLI. For\nexample, if you run CRI-O together with Kubernetes using the policy which requires\n<code>quay.io/crio/unsigned</code> to be signed, then a pod definition like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>container<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>quay.io/crio/unsigned<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Will cause the <code>SignatureValidationFailed</code> error when applying the pod manifest:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl apply -f pod.yaml\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pod/pod created\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl get pods\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pod 0/1 SignatureValidationFailed 0 4s\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl describe pod pod | tail -n8\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Type Reason Age From Message\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ---- ------ ---- ---- -------\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Normal Scheduled 58s default-scheduler Successfully assigned default/pod to 127.0.0.1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Normal BackOff 22s (x2 over 55s) kubelet Back-off pulling image &#34;quay.io/crio/unsigned&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Warning Failed 22s (x2 over 55s) kubelet Error: ImagePullBackOff\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Normal Pulling 9s (x3 over 58s) kubelet Pulling image &#34;quay.io/crio/unsigned&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Warning Failed 6s (x3 over 55s) kubelet Failed to pull image &#34;quay.io/crio/unsigned&#34;: SignatureValidationFailed: Source image rejected: A signature was required, but no signature exists\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Warning Failed 6s (x3 over 55s) kubelet Error: SignatureValidationFailed\n</span></span></span></code></pre></div><p>This overall behavior provides a more Kubernetes native experience and does not\nrely on third party software to be installed in the cluster.</p>\n<p>There are still a few corner cases to consider: For example, what if you want to\nallow policies per namespace in the same way the policy-controller supports it?\nWell, there is an upcoming CRI-O feature in v1.28 for that! CRI-O will support\nthe <code>--signature-policy-dir</code> / <code>signature_policy_dir</code> option, which defines the\nroot path for pod namespace-separated signature policies. This means that CRI-O\nwill lookup that path and assemble a policy like <code>&lt;SIGNATURE_POLICY_DIR&gt;/&lt;NAMESPACE&gt;.json</code>,\nwhich will be used on image pull if existing. If no pod namespace is\nprovided on image pull (<a href=\"https://github.com/kubernetes/cri-api/blob/e5515a5/pkg/apis/runtime/v1/api.proto#L1448\">via the sandbox config</a>), or the\nconcatenated path is non-existent, then CRI-O's global policy will be used as\nfallback.</p>\n<p>Another corner case to consider is critical for the correct signature\nverification within container runtimes: The kubelet only invokes container image\npulls if the image does not already exist on disk. This means that an\nunrestricted policy from Kubernetes namespace A can allow pulling an image,\nwhile namespace B is not able to enforce the policy because it already exits on\nthe node. Finally, CRI-O has to verify the policy not only on image pull, but\nalso on container creation. This fact makes things even a bit more complicated,\nbecause the CRI does not really pass down the user specified image reference on\ncontainer creation, but an already resolved image ID, or digest. A <a href=\"https://github.com/kubernetes/kubernetes/pull/118652\">small\nchange to the CRI</a> can help with that.</p>\n<p>Now that everything happens within the container runtime, someone has to\nmaintain and define the policies to provide a good user experience around that\nfeature. The CRDs of the policy-controller are great, while we could imagine that\na daemon within the cluster can write the policies for CRI-O per namespace. This\nwould make any additional hook obsolete and moves the responsibility of\nverifying the image signature to the actual instance which pulls the image. <a href=\"https://groups.google.com/g/kubernetes-sig-node/c/kgpxqcsJ7Vc/m/7X7t_ElsAgAJ\">I\nevaluated</a> other possible paths toward a better container image\nsignature verification within plain Kubernetes, but I could not find a great fit\nfor a native API. This means that I believe that a CRD is the way to go, but\nusers still need an instance which actually serves it.</p>\n<p>Thank you for reading this blog post! If you're interested in more, providing\nfeedback or asking for help, then feel free to get in touch with me directly via\n<a href=\"https://kubernetes.slack.com/messages/crio\">Slack (#crio)</a> or the <a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">SIG Node mailing list</a>.</p>","PublishedAt":"2023-06-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/","SourceName":"Kubernetes"}},{"node":{"ID":3878,"Title":"Blog: dl.k8s.io to adopt a Content Delivery Network","Description":"<p><strong>Authors</strong>: Arnaud Meukam (VMware), Hannah Aubry (Fast Forward), Frederico\nMu√±oz (SAS Institute)</p>\n<p>We're happy to announce that dl.k8s.io, home of the official Kubernetes\nbinaries, will soon be powered by <a href=\"https://www.fastly.com\">Fastly</a>.</p>\n<p>Fastly is known for its high-performance content delivery network (CDN) designed\nto deliver content quickly and reliably around the world. With its powerful\nnetwork, Fastly will help us deliver official Kubernetes binaries to users\nfaster and more reliably than ever before.</p>\n<p>The decision to use Fastly was made after an extensive evaluation process in\nwhich we carefully evaluated several potential content delivery network\nproviders. Ultimately, we chose Fastly because of their commitment to the open\ninternet and proven track record of delivering fast and secure digital\nexperiences to some of the most known open source projects (through their <a href=\"https://www.fastly.com/fast-forward\">Fast\nForward</a> program).</p>\n<h2 id=\"what-you-need-to-know-about-this-change\">What you need to know about this change</h2>\n<ul>\n<li>On Monday, July 24th, the IP addresses and backend storage associated with the\ndl.k8s.io domain name will change.</li>\n<li>The change will not impact the vast majority of users since the domain\nname will remain the same.</li>\n<li>If you restrict access to specific IP ranges, access to the dl.k8s.io domain\ncould stop working.</li>\n</ul>\n<p>If you think you may be impacted or want to know more about this change,\nplease keep reading.</p>\n<h2 id=\"why-are-we-making-this-change\">Why are we making this change</h2>\n<p>The official Kubernetes binaries site, dl.k8s.io, is used by thousands of users\nall over the world, and currently serves <em>more than 5 petabytes of binaries each\nmonth</em>. This change will allow us to improve access to those resources by\nleveraging a world-wide CDN.</p>\n<h2 id=\"does-this-affect-dl-k8s-io-only-or-are-other-domains-also-affected\">Does this affect dl.k8s.io only, or are other domains also affected?</h2>\n<p>Only dl.k8s.io will be affected by this change.</p>\n<h2 id=\"my-company-specifies-the-domain-names-that-we-are-allowed-to-be-accessed-will-this-change-affect-the-domain-name\">My company specifies the domain names that we are allowed to be accessed. Will this change affect the domain name?</h2>\n<p>No, the domain name (<code>dl.k8s.io</code>) will remain the same: no change will be\nnecessary, and access to the Kubernetes release binaries site should not be\naffected.</p>\n<h2 id=\"my-company-uses-some-form-of-ip-filtering-will-this-change-affect-access-to-the-site\">My company uses some form of IP filtering. Will this change affect access to the site?</h2>\n<p>If IP-based filtering is in place, it‚Äôs possible that access to the site will be\naffected when the new IP addresses become active.</p>\n<h2 id=\"if-my-company-doesn-t-use-ip-addresses-to-restrict-network-traffic-do-we-need-to-do-anything\">If my company doesn‚Äôt use IP addresses to restrict network traffic, do we need to do anything?</h2>\n<p>No, the switch to the CDN should be transparent.</p>\n<h2 id=\"will-there-be-a-dual-running-period\">Will there be a dual running period?</h2>\n<p><strong>No, it is a cutover.</strong> You can, however, test your networks right now to check\nif they can route to the new public IP addresses from Fastly. You should add\nthe new IPs to your network's <code>allowlist</code> before July 24th. Once the transfer is\ncomplete, ensure your networks use the new IP addresses to connect to\nthe <code>dl.k8s.io</code> service.</p>\n<h2 id=\"what-are-the-new-ip-addresses\">What are the new IP addresses?</h2>\n<p>If you need to manage an allow list for downloads, you can get the ranges to\nmatch from the Fastly API, in JSON: <a href=\"https://api.fastly.com/public-ip-list\">public IP address\nranges</a>. You don't need any credentials\nto download that list of ranges.</p>\n<h2 id=\"what-next-steps-would-you-recommend\">What next steps would you recommend?</h2>\n<p>If you have IP-based filtering in place, we recommend the following course of\naction <strong>before July, 24th</strong>:</p>\n<ul>\n<li>Add the new IP addresses to your allowlist.</li>\n<li>Conduct tests with your networks/firewall to ensure your networks can route to\nthe new IP addresses.</li>\n</ul>\n<p>After the change is made, we recommend double-checking that HTTP calls are\naccessing dl.k8s.io with the new IP addresses.</p>\n<h2 id=\"what-should-i-do-if-i-detect-some-abnormality-after-the-cutover-date\">What should I do if I detect some abnormality after the cutover date?</h2>\n<p>If you encounter any weirdness during binaries download, please <a href=\"https://github.com/kubernetes/k8s.io/issues/new/choose\">open an\nissue</a>.</p>","PublishedAt":"2023-06-09 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/06/09/dl-adopt-cdn/","SourceName":"Kubernetes"}},{"node":{"ID":3746,"Title":"Blog: Using OCI artifacts to distribute security profiles for seccomp, SELinux and AppArmor","Description":"<p><strong>Author</strong>: Sascha Grunert</p>\n<p>The <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator\">Security Profiles Operator (SPO)</a> makes managing seccomp, SELinux and\nAppArmor profiles within Kubernetes easier than ever. It allows cluster\nadministrators to define the profiles in a predefined custom resource YAML,\nwhich then gets distributed by the SPO into the whole cluster. Modification and\nremoval of the security profiles are managed by the operator in the same way,\nbut that‚Äôs a small subset of its capabilities.</p>\n<p>Another core feature of the SPO is being able to stack seccomp profiles. This\nmeans that users can define a <code>baseProfileName</code> in the YAML specification, which\nthen gets automatically resolved by the operator and combines the syscall rules.\nIf a base profile has another <code>baseProfileName</code>, then the operator will\nrecursively resolve the profiles up to a certain depth. A common use case is to\ndefine base profiles for low level container runtimes (like <a href=\"https://github.com/opencontainers/runc\">runc</a> or\n<a href=\"https://github.com/containers/crun\">crun</a>) which then contain syscalls which are required in any case to run\nthe container. Alternatively, application developers can define seccomp base\nprofiles for their standard distribution containers and stack dedicated profiles\nfor the application logic on top. This way developers can focus on maintaining\nseccomp profiles which are way simpler and scoped to the application logic,\nwithout having a need to take the whole infrastructure setup into account.</p>\n<p>But how to maintain those base profiles? For example, the amount of required\nsyscalls for a runtime can change over its release cycle in the same way it can\nchange for the main application. Base profiles have to be available in the same\ncluster, otherwise the main seccomp profile will fail to deploy. This means that\nthey‚Äôre tightly coupled to the main application profiles, which acts against the\nmain idea of base profiles. Distributing and managing them as plain files feels\nlike an additional burden to solve.</p>\n<h2 id=\"oci-artifacts-to-the-rescue\">OCI artifacts to the rescue</h2>\n<p>The <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/v0.8.0\">v0.8.0</a> release of the Security Profiles Operator supports\nmanaging base profiles as OCI artifacts! Imagine OCI artifacts as lightweight\ncontainer images, storing files in layers in the same way images do, but without\na process to be executed. Those artifacts can be used to store security profiles\nlike regular container images in compatible registries. This means they can be\nversioned, namespaced and annotated similar to regular container images.</p>\n<p>To see how that works in action, specify a <code>baseProfileName</code> prefixed with\n<code>oci://</code> within a seccomp profile CRD, for example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>security-profiles-operator.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>SeccompProfile<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">defaultAction</span>:<span style=\"color:#bbb\"> </span>SCMP_ACT_ERRNO<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">baseProfileName</span>:<span style=\"color:#bbb\"> </span>oci://ghcr.io/security-profiles/runc:v1.1.5<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">syscalls</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">action</span>:<span style=\"color:#bbb\"> </span>SCMP_ACT_ALLOW<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">names</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- uname<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The operator will take care of pulling the content by using <a href=\"https://oras.land\">oras</a>, as\nwell as verifying the <a href=\"https://github.com/sigstore/cosign\">sigstore (cosign)</a> signatures of the artifact. If\nthe artifacts are not signed, then the SPO will reject them. The resulting\nprofile <code>test</code> will then contain all base syscalls from the remote <code>runc</code>\nprofile plus the additional allowed <code>uname</code> one. It is also possible to\nreference the base profile by its digest (SHA256) making the artifact to be\npulled more specific, for example by referencing\n<code>oci://ghcr.io/security-profiles/runc@sha256:380‚Ä¶</code>.</p>\n<p>The operator internally caches pulled artifacts up to 24 hours for 1000\nprofiles, meaning that they will be refreshed after that time period, if the\ncache is full or the operator daemon gets restarted.</p>\n<p>Because the overall resulting syscalls are hidden from the user (I only have the\n<code>baseProfileName</code> listed in the SeccompProfile, and not the syscalls themselves), I'll additionally\nannotate that SeccompProfile with the final <code>syscalls</code>.</p>\n<p>Here's how the SeccompProfile looks after I annotate it:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl describe seccompprofile <span style=\"color:#a2f\">test</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Name: test\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Namespace: security-profiles-operator\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Labels: spo.x-k8s.io/profile-id=SeccompProfile-test\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Annotations: syscalls:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> [{&#34;names&#34;:[&#34;arch_prctl&#34;,&#34;brk&#34;,&#34;capget&#34;,&#34;capset&#34;,&#34;chdir&#34;,&#34;clone&#34;,&#34;close&#34;,...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">API Version: security-profiles-operator.x-k8s.io/v1beta1\n</span></span></span></code></pre></div><p>The SPO maintainers provide all public base profiles as part of the <a href=\"https://github.com/orgs/security-profiles/packages\">‚ÄúSecurity\nProfiles‚Äù GitHub organization</a>.</p>\n<h2 id=\"managing-oci-security-profiles\">Managing OCI security profiles</h2>\n<p>Alright, now the official SPO provides a bunch of base profiles, but how can I\ndefine my own? Well, first of all we have to choose a working registry. There\nare a bunch of registries that already supports OCI artifacts:</p>\n<ul>\n<li><a href=\"https://github.com/distribution/distribution\">CNCF Distribution</a></li>\n<li><a href=\"https://aka.ms/acr\">Azure Container Registry</a></li>\n<li><a href=\"https://aws.amazon.com/ecr\">Amazon Elastic Container Registry</a></li>\n<li><a href=\"https://cloud.google.com/artifact-registry\">Google Artifact Registry</a></li>\n<li><a href=\"https://docs.github.com/en/packages/guides/about-github-container-registry\">GitHub Packages container registry</a></li>\n<li><a href=\"https://bundle.bar/docs/supported-clients/oras\">Bundle Bar</a></li>\n<li><a href=\"https://hub.docker.com\">Docker Hub</a></li>\n<li><a href=\"https://zotregistry.io\">Zot Registry</a></li>\n</ul>\n<p>The Security Profiles Operator ships a new command line interface called <code>spoc</code>,\nwhich is a little helper tool for managing OCI profiles among doing various other\nthings which are out of scope of this blog post. But, the command <code>spoc push</code>\ncan be used to push a security profile to a registry:</p>\n<pre tabindex=\"0\"><code>&gt; export USERNAME=my-user\n&gt; export PASSWORD=my-pass\n&gt; spoc push -f ./examples/baseprofile-crun.yaml ghcr.io/security-profiles/crun:v1.8.3\n16:35:43.899886 Pushing profile ./examples/baseprofile-crun.yaml to: ghcr.io/security-profiles/crun:v1.8.3\n16:35:43.899939 Creating file store in: /tmp/push-3618165827\n16:35:43.899947 Adding profile to store: ./examples/baseprofile-crun.yaml\n16:35:43.900061 Packing files\n16:35:43.900282 Verifying reference: ghcr.io/security-profiles/crun:v1.8.3\n16:35:43.900310 Using tag: v1.8.3\n16:35:43.900313 Creating repository for ghcr.io/security-profiles/crun\n16:35:43.900319 Using username and password\n16:35:43.900321 Copying profile to repository\n16:35:46.976108 Signing container image\nGenerating ephemeral keys...\nRetrieving signed certificate...\nNote that there may be personally identifiable information associated with this signed artifact.\nThis may include the email address associated with the account with which you authenticate.\nThis information will be used for signing this artifact and will be stored in public transparency logs and cannot be removed later.\nBy typing &#39;y&#39;, you attest that you grant (or have permission to grant) and agree to have this information stored permanently in transparency logs.\nYour browser will now be opened to:\nhttps://oauth2.sigstore.dev/auth/auth?access_type=‚Ä¶\nSuccessfully verified SCT...\ntlog entry created with index: 16520520\nPushing signature to: ghcr.io/security-profiles/crun\n</code></pre><p>You can see that the tool automatically signs the artifact and pushes the\n<code>./examples/baseprofile-crun.yaml</code> to the registry, which is then directly ready\nfor usage within the SPO. If username and password authentication is required,\neither use the <code>--username</code>, <code>-u</code> flag or export the <code>USERNAME</code> environment\nvariable. To set the password, export the <code>PASSWORD</code> environment variable.</p>\n<p>It is possible to add custom annotations to the security profile by using the\n<code>--annotations</code> / <code>-a</code> flag multiple times in <code>KEY:VALUE</code> format. Those have no\neffect for now, but at some later point additional features of the operator may\nrely them.</p>\n<p>The <code>spoc</code> client is also able to pull security profiles from OCI artifact\ncompatible registries. To do that, just run <code>spoc pull</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> spoc pull ghcr.io/security-profiles/runc:v1.1.5\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:29.795597 Pulling profile from: ghcr.io/security-profiles/runc:v1.1.5\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:29.795610 Verifying signature\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">Verification for ghcr.io/security-profiles/runc:v1.1.5 --\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">The following checks were performed on each of these signatures:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> - Existence of the claims in the transparency log was verified offline\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> - The code-signing certificate was verified using trusted certificate authority certificates\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">[{&#34;critical&#34;:{&#34;identity&#34;:{&#34;docker-reference&#34;:&#34;ghcr.io/security-profiles/runc&#34;},‚Ä¶}}]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:33.208695 Creating file store in: /tmp/pull-3199397214\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:33.208713 Verifying reference: ghcr.io/security-profiles/runc:v1.1.5\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:33.208718 Creating repository for ghcr.io/security-profiles/runc\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:33.208742 Using tag: v1.1.5\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:33.208743 Copying profile from repository\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:34.119652 Reading profile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:34.119677 Trying to unmarshal seccomp profile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:34.120114 Got SeccompProfile: runc-v1.1.5\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">16:32:34.120119 Saving profile in: /tmp/profile.yaml\n</span></span></span></code></pre></div><p>The profile can be now found in <code>/tmp/profile.yaml</code> or the specified output file\n<code>--output-file</code> / <code>-o</code>. We can specify an username and password in the same way\nas for <code>spoc push</code>.</p>\n<p><code>spoc</code> makes it easy to manage security profiles as OCI artifacts, which can be\nthen consumed directly by the operator itself.</p>\n<p>That was our compact journey through the latest possibilities of the Security\nProfiles Operator! If you're interested in more, providing feedback or asking\nfor help, then feel free to get in touch with us directly via <a href=\"https://kubernetes.slack.com/messages/security-profiles-operator\">Slack\n(#security-profiles-operator)</a> or <a href=\"https://groups.google.com/forum/#!forum/kubernetes-dev\">the mailing list</a>.</p>","PublishedAt":"2023-05-24 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/24/oci-security-profiles/","SourceName":"Kubernetes"}},{"node":{"ID":3747,"Title":"Blog: Having fun with seccomp profiles on the edge","Description":"<p><strong>Author</strong>: Sascha Grunert</p>\n<p>The <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator\">Security Profiles Operator (SPO)</a> is a feature-rich\n<a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator\">operator</a> for Kubernetes to make managing seccomp, SELinux and\nAppArmor profiles easier than ever. Recording those profiles from scratch is one\nof the key features of this operator, which usually involves the integration\ninto large CI/CD systems. Being able to test the recording capabilities of the\noperator in edge cases is one of the recent development efforts of the SPO and\nmakes it excitingly easy to play around with seccomp profiles.</p>\n<h2 id=\"recording-seccomp-profiles-with-spoc-record\">Recording seccomp profiles with <code>spoc record</code></h2>\n<p>The <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/v0.8.0\">v0.8.0</a> release of the Security Profiles Operator shipped a new\ncommand line interface called <code>spoc</code>, a little helper tool for recording and\nreplaying seccomp profiles among various other things that are out of scope of\nthis blog post.</p>\n<p>Recording a seccomp profile requires a binary to be executed, which can be a\nsimple golang application which just calls <a href=\"https://man7.org/linux/man-pages/man2/uname.2.html\"><code>uname(2)</code></a>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">package</span> main\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">import</span> (\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#b44\">&#34;syscall&#34;</span>\n</span></span><span style=\"display:flex;\"><span>)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">func</span> <span style=\"color:#00a000\">main</span>() {\n</span></span><span style=\"display:flex;\"><span> utsname <span style=\"color:#666\">:=</span> syscall.Utsname{}\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#a2f;font-weight:bold\">if</span> err <span style=\"color:#666\">:=</span> syscall.<span style=\"color:#00a000\">Uname</span>(<span style=\"color:#666\">&amp;</span>utsname); err <span style=\"color:#666\">!=</span> <span style=\"color:#a2f;font-weight:bold\">nil</span> {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#a2f\">panic</span>(err)\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>Building a binary from that code can be done by:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> go build -o main main.go\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> ldd ./main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> not a dynamic executable\n</span></span></span></code></pre></div><p>Now it's possible to download the latest binary of <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/download/v0.8.0/spoc.amd64\"><code>spoc</code> from\nGitHub</a> and run the application on Linux with it:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo ./spoc record ./main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.591945 Loading bpf module\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.591958 Using system btf file\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">libbpf: loading object &#39;recorder.bpf.o&#39; from buffer\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">libbpf: prog &#39;sys_enter&#39;: relo #3: patched insn #22 (ALU/ALU64) imm 16 -&gt; 16\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.610767 Getting bpf program sys_enter\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.610778 Attaching bpf tracepoint\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.611574 Getting syscalls map\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.611582 Getting pid_mntns map\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.613097 Module successfully loaded\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.613311 Processing events\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.613693 Running command with PID: 336007\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.613835 Received event: pid: 336007, mntns: 4026531841\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.613951 No container ID found for PID (pid=336007, mntns=4026531841, err=unable to find container ID in cgroup path)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.614856 Processing recorded data\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.614975 Found process mntns 4026531841 in bpf map\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.615110 Got syscalls: read, close, mmap, rt_sigaction, rt_sigprocmask, madvise, nanosleep, clone, uname, sigaltstack, arch_prctl, gettid, futex, sched_getaffinity, exit_group, openat\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.615195 Adding base syscalls: access, brk, capget, capset, chdir, chmod, chown, close_range, dup2, dup3, epoll_create1, epoll_ctl, epoll_pwait, execve, faccessat2, fchdir, fchmodat, fchown, fchownat, fcntl, fstat, fstatfs, getdents64, getegid, geteuid, getgid, getpid, getppid, getuid, ioctl, keyctl, lseek, mkdirat, mknodat, mount, mprotect, munmap, newfstatat, openat2, pipe2, pivot_root, prctl, pread64, pselect6, readlink, readlinkat, rt_sigreturn, sched_yield, seccomp, set_robust_list, set_tid_address, setgid, setgroups, sethostname, setns, setresgid, setresuid, setsid, setuid, statfs, statx, symlinkat, tgkill, umask, umount2, unlinkat, unshare, write\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.616293 Wrote seccomp profile to: /tmp/profile.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:08:25.616298 Unloading bpf module\n</span></span></span></code></pre></div><p>I have to execute <code>spoc</code> as root because it will internally run an <a href=\"https://ebpf.io\">ebpf</a>\nprogram by reusing the same code parts from the Security Profiles Operator\nitself. I can see that the bpf module got loaded successfully and <code>spoc</code>\nattached the required tracepoint to it. Then it will track the main application\nby using its <a href=\"https://man7.org/linux/man-pages/man7/mount_namespaces.7.html\">mount namespace</a> and process the recorded syscall data. The\nnature of ebpf programs is that they see the whole context of the Kernel, which\nmeans that <code>spoc</code> tracks all syscalls of the system, but does not interfere with\ntheir execution.</p>\n<p>The logs indicate that <code>spoc</code> found the syscalls <code>read</code>, <code>close</code>,\n<code>mmap</code> and so on, including <code>uname</code>. All other syscalls than <code>uname</code> are coming\nfrom the golang runtime and its garbage collection, which already adds overhead\nto a basic application like in our demo. I can also see from the log line\n<code>Adding base syscalls: ‚Ä¶</code> that <code>spoc</code> adds a bunch of base syscalls to the\nresulting profile. Those are used by the OCI runtime (like <a href=\"https://github.com/opencontainers/runc\">runc</a> or\n<a href=\"https://github.com/containers/crun\">crun</a>) in order to be able to run a container. This means that <code>spoc</code>\ncan be used to record seccomp profiles which then can be containerized directly.\nThis behavior can be disabled in <code>spoc</code> by using the <code>--no-base-syscalls</code>/<code>-n</code>\nor customized via the <code>--base-syscalls</code>/<code>-b</code> command line flags. This can be\nhelpful in cases where different OCI runtimes other than crun and runc are used,\nor if I just want to record the seccomp profile for the application and stack\nit with another <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/35ebdda/installation-usage.md#base-syscalls-for-a-container-runtime\">base profile</a>.</p>\n<p>The resulting profile is now available in <code>/tmp/profile.yaml</code>, but the default\nlocation can be changed using the <code>--output-file value</code>/<code>-o</code> flag:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> cat /tmp/profile.yaml\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>security-profiles-operator.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>SeccompProfile<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">creationTimestamp</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">null</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>main<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">architectures</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- SCMP_ARCH_X86_64<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">defaultAction</span>:<span style=\"color:#bbb\"> </span>SCMP_ACT_ERRNO<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">syscalls</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">action</span>:<span style=\"color:#bbb\"> </span>SCMP_ACT_ALLOW<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">names</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- access<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- arch_prctl<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- brk<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ‚Ä¶<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- uname<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ‚Ä¶<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">status</span>:<span style=\"color:#bbb\"> </span>{}<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The seccomp profile Custom Resource Definition (CRD) can be directly used\ntogether with the Security Profiles Operator for managing it within Kubernetes.\n<code>spoc</code> is also capable of producing raw seccomp profiles (as JSON), by using the\n<code>--type</code>/<code>-t</code> <code>raw-seccomp</code> flag:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo ./spoc record --type raw-seccomp ./main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">52.628827 Wrote seccomp profile to: /tmp/profile.json\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> jq . /tmp/profile.json\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;defaultAction&#34;</span>: <span style=\"color:#b44\">&#34;SCMP_ACT_ERRNO&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;architectures&#34;</span>: [<span style=\"color:#b44\">&#34;SCMP_ARCH_X86_64&#34;</span>],\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;syscalls&#34;</span>: [\n</span></span><span style=\"display:flex;\"><span> {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;names&#34;</span>: [<span style=\"color:#b44\">&#34;access&#34;</span>, <span style=\"color:#b44\">&#34;‚Ä¶&#34;</span>, <span style=\"color:#b44\">&#34;write&#34;</span>],\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;action&#34;</span>: <span style=\"color:#b44\">&#34;SCMP_ACT_ALLOW&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> ]\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>The utility <code>spoc record</code> allows us to record complex seccomp profiles directly\nfrom binary invocations in any Linux system which is capable of running the ebpf\ncode within the Kernel. But it can do more: How about modifying the seccomp\nprofile and then testing it by using <code>spoc run</code>.</p>\n<h2 id=\"running-seccomp-profiles-with-spoc-run\">Running seccomp profiles with <code>spoc run</code></h2>\n<p><code>spoc</code> is also able to run binaries with applied seccomp profiles, making it\neasy to test any modification to it. To do that, just run:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo ./spoc run ./main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.153263 Reading file /tmp/profile.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.153311 Assuming YAML profile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.154138 Setting up seccomp\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.154178 Load seccomp profile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.154189 Starting audit log enricher\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.154224 Enricher reading from file /var/log/audit/audit.log\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:29:58.155356 Running command with PID: 437880\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">&gt;</span>\n</span></span></code></pre></div><p>It looks like that the application exited successfully, which is anticipated\nbecause I did not modify the previously recorded profile yet. I can also\nspecify a custom location for the profile by using the <code>--profile</code>/<code>-p</code> flag,\nbut this was not necessary because I did not modify the default output location\nfrom the record. <code>spoc</code> will automatically determine if it's a raw (JSON) or CRD\n(YAML) based seccomp profile and then apply it to the process.</p>\n<p>The Security Profiles Operator supports a <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/35ebdda/installation-usage.md#using-the-log-enricher\">log enricher feature</a>,\nwhich provides additional seccomp related information by parsing the audit logs.\n<code>spoc run</code> uses the enricher in the same way to provide more data to the end\nusers when it comes to debugging seccomp profiles.</p>\n<p>Now I have to modify the profile to see anything valuable in the output. For\nexample, I could remove the allowed <code>uname</code> syscall:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> jq <span style=\"color:#b44\">&#39;del(.syscalls[0].names[] | select(. == &#34;uname&#34;))&#39;</span> /tmp/profile.json &gt; /tmp/no-uname-profile.json\n</span></span></code></pre></div><p>And then try to run it again with the new profile <code>/tmp/no-uname-profile.json</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo ./spoc run -p /tmp/no-uname-profile.json ./main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.707798 Reading file /tmp/no-uname-profile.json\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.707892 Setting up seccomp\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.707920 Load seccomp profile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.707982 Starting audit log enricher\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.707998 Enricher reading from file /var/log/audit/audit.log\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.709164 Running command with PID: 480512\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">panic: operation not permitted\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">goroutine 1 [running]:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">main.main()\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> /path/to/main.go:10 +0x85\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:39:12.713035 Unable to run: launch runner: wait for command: exit status 2\n</span></span></span></code></pre></div><p>Alright, that was expected! The applied seccomp profile blocks the <code>uname</code>\nsyscall, which results in an &quot;operation not permitted&quot; error. This error is\npretty generic and does not provide any hint on what got blocked by seccomp.\nIt is generally extremely difficult to predict how applications behave if single\nsyscalls are forbidden by seccomp. It could be possible that the application\nterminates like in our simple demo, but it could also lead to a strange\nmisbehavior and the application does not stop at all.</p>\n<p>If I now change the default seccomp action of the profile from <code>SCMP_ACT_ERRNO</code>\nto <code>SCMP_ACT_LOG</code> like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> jq <span style=\"color:#b44\">&#39;.defaultAction = &#34;SCMP_ACT_LOG&#34;&#39;</span> /tmp/no-uname-profile.json &gt; /tmp/no-uname-profile-log.json\n</span></span></code></pre></div><p>Then the log enricher will give us a hint that the <code>uname</code> syscall got blocked\nwhen using <code>spoc run</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo ./spoc run -p /tmp/no-uname-profile-log.json ./main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.470126 Reading file /tmp/no-uname-profile-log.json\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.470234 Setting up seccomp\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.470245 Load seccomp profile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.470302 Starting audit log enricher\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.470339 Enricher reading from file /var/log/audit/audit.log\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.470889 Running command with PID: 522268\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10:48:07.472007 Seccomp: uname (63)\n</span></span></span></code></pre></div><p>The application will not terminate any more, but seccomp will log the behavior\nto <code>/var/log/audit/audit.log</code> and <code>spoc</code> will parse the data to correlate it\ndirectly to our program. Generating the log messages to the audit subsystem\ncomes with a large performance overhead and should be handled with care in\nproduction systems. It also comes with a security risk when running untrusted\napps in audit mode in production environments.</p>\n<p>This demo should give you an impression how to debug seccomp profile issues with\napplications, probably by using our shiny new helper tool powered by the\nfeatures of the Security Profiles Operator. <code>spoc</code> is a flexible and portable\nbinary suitable for edge cases where resources are limited and even Kubernetes\nitself may not be available with its full capabilities.</p>\n<p>Thank you for reading this blog post! If you're interested in more, providing\nfeedback or asking for help, then feel free to get in touch with us directly via\n<a href=\"https://kubernetes.slack.com/messages/security-profiles-operator\">Slack (#security-profiles-operator)</a> or the <a href=\"https://groups.google.com/forum/#!forum/kubernetes-dev\">mailing list</a>.</p>","PublishedAt":"2023-05-18 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/18/seccomp-profiles-edge/","SourceName":"Kubernetes"}},{"node":{"ID":3676,"Title":"Blog: Kubernetes 1.27: KMS V2 Moves to Beta","Description":"<p><strong>Authors:</strong> Anish Ramasekar, Mo Khan, and Rita Zhang (Microsoft)</p>\n<p>With Kubernetes 1.27, we (SIG Auth) are moving Key Management Service (KMS) v2 API to beta.</p>\n<h2 id=\"what-is-kms\">What is KMS?</h2>\n<p>One of the first things to consider when securing a Kubernetes cluster is encrypting etcd data at\nrest. KMS provides an interface for a provider to utilize a key stored in an external key service to\nperform this encryption.</p>\n<p>KMS v1 has been a feature of Kubernetes since version 1.10, and is currently in beta as of version\nv1.12. KMS v2 was introduced as alpha in v1.25.</p>\n<div class=\"alert alert-primary\" role=\"alert\">\n<h4 class=\"alert-heading\">Note</h4>\nThe KMS v2 API and implementation changed in incompatible\nways in-between the alpha release in v1.25 and the beta release in v1.27. The design of KMS v2 has\nchanged since <a href=\"https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/\">the previous blog post</a>\nwas written and it is not compatible with the design in this blog post. Attempting to upgrade from\nold versions with the alpha feature enabled will result in data loss.\n</div>\n<h2 id=\"what-s-new-in-v2beta1\">What‚Äôs new in <code>v2beta1</code>?</h2>\n<p>The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd. The data is\nencrypted using a data encryption key (DEK). The DEKs are encrypted with a key encryption key (KEK)\nthat is stored and managed in a remote KMS. With KMS v1, a new DEK is generated for each encryption.\nWith KMS v2, a new DEK is only generated on server startup and when the KMS plugin informs the API\nserver that a KEK rotation has occurred.</p>\n<div class=\"alert alert-warning\" role=\"alert\">\n<h4 class=\"alert-heading\">Caution</h4>\n<p>If you are running virtual machine (VM) based nodes\nthat leverage VM state store with this feature, you must not use KMS v2.</p>\n<p>With KMS v2, the API server uses AES-GCM with a 12 byte nonce (8 byte atomic counter and 4 bytes\nrandom data) for encryption. The following issues could occur if the VM is saved and restored:</p>\n<ol>\n<li>The counter value may be lost or corrupted if the VM is saved in an inconsistent state or\nrestored improperly. This can lead to a situation where the same counter value is used twice,\nresulting in the same nonce being used for two different messages.</li>\n<li>If the VM is restored to a previous state, the counter value may be set back to its previous\nvalue, resulting in the same nonce being used again.</li>\n</ol>\n<p>Although both of these cases are partially mitigated by the 4 byte random nonce, this can compromise\nthe security of the encryption.</p>\n</div>\n<h3 id=\"sequence-diagram\">Sequence Diagram</h3>\n<h4 id=\"encrypt-request\">Encrypt Request</h4>\n<!-- source\n```mermaid\n%%{init:{\"theme\":\"neutral\", \"sequence\": {\"mirrorActors\":true},\n\"themeVariables\": {\n\"actorBkg\":\"royalblue\",\n\"actorTextColor\":\"white\"\n}}}%%\nsequenceDiagram\nparticipant user\nparticipant kube_api_server\nparticipant kms_plugin\nparticipant external_kms\nalt Generate DEK at startup\nNote over kube_api_server,external_kms: Refer to Generate Data Encryption Key (DEK) diagram for details\nend\nuser->>kube_api_server: create/update resource that's to be encrypted\nkube_api_server->>kube_api_server: encrypt resource with DEK\nkube_api_server->>etcd: store encrypted object\n```\n-->\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-encryption.svg\"\nalt=\"Sequence diagram for KMSv2 beta Encrypt\"/>\n</figure>\n<h4 id=\"decrypt-request\">Decrypt Request</h4>\n<!-- source\n```mermaid\n%%{init:{\"theme\":\"neutral\", \"sequence\": {\"mirrorActors\":true},\n\"themeVariables\": {\n\"actorBkg\":\"royalblue\",\n\"actorTextColor\":\"white\"\n}}}%%\nsequenceDiagram\nparticipant user\nparticipant kube_api_server\nparticipant kms_plugin\nparticipant external_kms\nparticipant etcd\nuser->>kube_api_server: get/list resource that's encrypted\nkube_api_server->>etcd: get encrypted resource\netcd->>kube_api_server: encrypted resource\nalt Encrypted DEK not in cache\nkube_api_server->>kms_plugin: decrypt request\nkms_plugin->>external_kms: decrypt DEK with remote KEK\nexternal_kms->>kms_plugin: decrypted DEK\nkms_plugin->>kube_api_server: return decrypted DEK\nkube_api_server->>kube_api_server: cache decrypted DEK\nend\nkube_api_server->>kube_api_server: decrypt resource with DEK\nkube_api_server->>user: return decrypted resource\n```\n-->\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-decryption.svg\"\nalt=\"Sequence diagram for KMSv2 beta Decrypt\"/>\n</figure>\n<h4 id=\"status-request\">Status Request</h4>\n<!-- source\n```mermaid\n%%{init:{\"theme\":\"neutral\", \"sequence\": {\"mirrorActors\":true},\n\"themeVariables\": {\n\"actorBkg\":\"royalblue\",\n\"actorTextColor\":\"white\"\n}}}%%\nsequenceDiagram\nparticipant kube_api_server\nparticipant kms_plugin\nparticipant external_kms\nalt Generate DEK at startup\nNote over kube_api_server,external_kms: Refer to Generate Data Encryption Key (DEK) diagram for details\nend\nloop every minute (or every 10s if error or unhealthy)\nkube_api_server->>kms_plugin: status request\nkms_plugin->>external_kms: validate remote KEK\nexternal_kms->>kms_plugin: KEK status\nkms_plugin->>kube_api_server: return status response <br/> {\"healthz\": \"ok\", key_id: \"<remote KEK ID>\", \"version\": \"v2beta1\"}\nalt KEK rotation detected (key_id changed), rotate DEK\nNote over kube_api_server,external_kms: Refer to Generate Data Encryption Key (DEK) diagram for details\nend\nend\n```\n-->\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-status.svg\"\nalt=\"Sequence diagram for KMSv2 beta Status\"/>\n</figure>\n<h4 id=\"generate-data-encryption-key-dek\">Generate Data Encryption Key (DEK)</h4>\n<!-- source\n```mermaid\n%%{init:{\"theme\":\"neutral\", \"sequence\": {\"mirrorActors\":true},\n\"themeVariables\": {\n\"actorBkg\":\"royalblue\",\n\"actorTextColor\":\"white\"\n}}}%%\nsequenceDiagram\nparticipant kube_api_server\nparticipant kms_plugin\nparticipant external_kms\nkube_api_server->>kube_api_server: generate DEK\nkube_api_server->>kms_plugin: encrypt request\nkms_plugin->>external_kms: encrypt DEK with remote KEK\nexternal_kms->>kms_plugin: encrypted DEK\nkms_plugin->>kube_api_server: return encrypt response <br/> {\"ciphertext\": \"<encrypted DEK>\", key_id: \"<remote KEK ID>\", \"annotations\": {}}\n```\n-->\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-generate-dek.svg\"\nalt=\"Sequence diagram for KMSv2 beta Generate DEK\"/>\n</figure>\n<h3 id=\"performance-improvements\">Performance Improvements</h3>\n<p>With KMS v2, we have made significant improvements to the performance of the KMS encryption\nprovider. In case of KMS v1, a new DEK is generated for every encryption. This means that for every\nwrite request, the API server makes a call to the KMS plugin to encrypt the DEK using the remote\nKEK. The API server also has to cache the DEKs to avoid making a call to the KMS plugin for every\nread request. When the API server restarts, it has to populate the cache by making a call to the KMS\nplugin for every DEK in the etcd store based on the cache size. This is a significant overhead for\nthe API server. With KMS v2, the API server generates a DEK at startup and caches it. The API server\nalso makes a call to the KMS plugin to encrypt the DEK using the remote KEK. This is a one-time call\nat startup and on KEK rotation. The API server then uses the cached DEK to encrypt the resources.\nThis reduces the number of calls to the KMS plugin and improves the overall latency of the API\nserver requests.</p>\n<p>We conducted a test that created 12k secrets and measured the time taken for the API server to\nencrypt the resources. The metric used was\n<a href=\"https://kubernetes.io/docs/reference/instrumentation/metrics/\"><code>apiserver_storage_transformation_duration_seconds</code></a>.\nFor KMS v1, the test was run on a managed Kubernetes v1.25 cluster with 2 nodes. There was no\nadditional load on the cluster during the test. For KMS v2, the test was run in the Kubernetes CI\nenvironment with the following <a href=\"https://github.com/kubernetes/kubernetes/blob/release-1.27/test/e2e/testing-manifests/auth/encrypt/kind.yaml\">cluster\nconfiguration</a>.</p>\n<table>\n<thead>\n<tr>\n<th>KMS Provider</th>\n<th>Time taken by 95 percentile</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>KMS v1</td>\n<td>160ms</td>\n</tr>\n<tr>\n<td>KMS v2</td>\n<td>80Œºs</td>\n</tr>\n</tbody>\n</table>\n<p>The results show that the KMS v2 encryption provider is three orders of magnitude faster than the\nKMS v1 encryption provider.</p>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>For Kubernetes v1.28, we expect the feature to stay in beta. In the coming releases we want to\ninvestigate:</p>\n<ul>\n<li>Cryptographic changes to remove the limitation on VM state store.</li>\n<li>Kubernetes REST API changes to enable a more robust story around key rotation.</li>\n<li>Handling undecryptable resources. Refer to the\n<a href=\"https://github.com/kubernetes/enhancements/pull/3927\">KEP</a> for details.</li>\n</ul>\n<p>You can learn more about KMS v2 by reading <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/\">Using a KMS provider for data\nencryption</a>. You can also follow along on the\n<a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/3299-kms-v2-improvements/#readme\">KEP</a>\nto track progress across the coming Kubernetes releases.</p>\n<h2 id=\"call-to-action\">Call to action</h2>\n<p>In this blog post, we have covered the improvements made to the KMS encryption provider in\nKubernetes v1.27. We have also discussed the new KMS v2 API and how it works. We would love to hear\nyour feedback on this feature. In particular, we would like feedback from Kubernetes KMS plugin\nimplementors as they go through the process of building their integrations with this new API. Please\nreach out to us on the <a href=\"https://kubernetes.slack.com/archives/C03035EH4VB\">#sig-auth-kms-dev</a>\nchannel on Kubernetes Slack.</p>\n<h2 id=\"how-to-get-involved\">How to get involved</h2>\n<p>If you are interested in getting involved in the development of this feature, share feedback, or\nparticipate in any other ongoing SIG Auth projects, please reach out on the\n<a href=\"https://kubernetes.slack.com/archives/C0EN96KUY\">#sig-auth</a> channel on Kubernetes Slack.</p>\n<p>You are also welcome to join the bi-weekly <a href=\"https://github.com/kubernetes/community/blob/master/sig-auth/README.md#meetings\">SIG Auth\nmeetings</a>, held\nevery-other Wednesday.</p>\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n<p>This feature has been an effort driven by contributors from several different companies. We would\nlike to extend a huge thank you to everyone that contributed their time and effort to help make this\npossible.</p>","PublishedAt":"2023-05-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/16/kms-v2-moves-to-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3665,"Title":"Blog: Kubernetes 1.27: updates on speeding up Pod startup","Description":"<p><strong>Authors</strong>: Paco Xu (DaoCloud), Sergey Kanzhelev (Google), Ruiwen Zhao (Google)</p>\n<p>How can Pod start-up be accelerated on nodes in large clusters? This is a common issue that\ncluster administrators may face.</p>\n<p>This blog post focuses on methods to speed up pod start-up from the kubelet side. It does not\ninvolve the creation time of pods by controller-manager through kube-apiserver, nor does it\ninclude scheduling time for pods or webhooks executed on it.</p>\n<p>We have mentioned some important factors here to consider from the kubelet's perspective, but\nthis is not an exhaustive list. As Kubernetes v1.27 is released, this blog highlights\nsignificant changes in v1.27 that aid in speeding up pod start-up.</p>\n<h2 id=\"parallel-container-image-pulls\">Parallel container image pulls</h2>\n<p>Pulling images always takes some time and what's worse is that image pulls are done serially by\ndefault. In other words, kubelet will send only one image pull request to the image service at\na time. Other image pull requests have to wait until the one being processed is complete.</p>\n<p>To enable parallel image pulls, set the <code>serializeImagePulls</code> field to false in the kubelet\nconfiguration. When <code>serializeImagePulls</code> is disabled, requests for image pulls are immediately\nsent to the image service and multiple images can be pulled concurrently.</p>\n<h3 id=\"maximum-parallel-image-pulls-will-help-secure-your-node-from-overloading-on-image-pulling\">Maximum parallel image pulls will help secure your node from overloading on image pulling</h3>\n<p>We introduced a new feature in kubelet that sets a limit on the number of parallel image\npulls at the node level. This limit restricts the maximum number of images that can be pulled\nsimultaneously. If there is an image pull request beyond this limit, it will be blocked until\none of the ongoing image pulls finishes. Before enabling this feature, please ensure that your\ncontainer runtime's image service can handle parallel image pulls effectively.</p>\n<p>To limit the number of simultaneous image pulls, you can configure the <code>maxParallelImagePulls</code>\nfield in kubelet. By setting <code>maxParallelImagePulls</code> to a value of <em>n</em>, only <em>n</em> images will\nbe pulled concurrently. Any additional image pulls beyond this limit will wait until at least\none ongoing pull is complete.</p>\n<p>You can find more details in the associated KEP: <a href=\"https://kep.k8s.io/3673\">Kubelet limit of Parallel Image Pulls</a>\n(KEP-3673).</p>\n<h2 id=\"raised-default-api-query-per-second-limits-for-kubelet\">Raised default API query-per-second limits for kubelet</h2>\n<p>To improve pod startup in scenarios with multiple pods on a node, particularly sudden scaling\nsituations, it is necessary for Kubelet to synchronize the pod status and prepare configmaps,\nsecrets, or volumes. This requires a large bandwidth to access kube-apiserver.</p>\n<p>In versions prior to v1.27, the default <code>kubeAPIQPS</code> was 5 and <code>kubeAPIBurst</code> was 10. However,\nthe kubelet in v1.27 has increased these defaults to 50 and 100 respectively for better performance during\npod startup. It's worth noting that this isn't the only reason why we've bumped up the API QPS\nlimits for Kubelet.</p>\n<ol>\n<li>It has a potential to be hugely throttled now (default QPS = 5)</li>\n<li>In large clusters they can generate significant load anyway as there are a lot of them</li>\n<li>They have a dedicated PriorityLevel and FlowSchema that we can easily control</li>\n</ol>\n<p>Previously, we often encountered <code>volume mount timeout</code> on kubelet in node with more than 50 pods\nduring pod start up. We suggest that cluster operators bump <code>kubeAPIQPS</code> to 20 and <code>kubeAPIBurst</code> to 40,\nespecially if using bare metal nodes.</p>\n<p>More detials can be found in the KEP <a href=\"https://kep.k8s.io/1040\">https://kep.k8s.io/1040</a> and the pull request <a href=\"https://github.com/kubernetes/kubernetes/pull/116121\">#116121</a>.</p>\n<h2 id=\"event-triggered-updates-to-container-status\">Event triggered updates to container status</h2>\n<p><code>Evented PLEG</code> (PLEG is short for &quot;Pod Lifecycle Event Generator&quot;) is set to be in beta for v1.27,\nKubernetes offers two ways for the kubelet to detect Pod lifecycle events, such as a the last\nprocess in a container shutting down.\nIn Kubernetes v1.27, the <em>event based</em> mechanism has graduated to beta but remains\ndisabled by default. If you do explicitly switch to event-based lifecycle change detection,\nthe kubelet is able to start Pods more quickly than with the default approach that relies on polling.\nThe default mechanism, polling for lifecycle changes, adds a noticeable overhead; this affects\nthe kubelet's ability to handle different tasks in parallel, and leads to poor performance and\nreliability issues. For these reasons, we recommend that you switch your nodes to use\nevent-based pod lifecycle change detection.</p>\n<p>Further details can be found in the KEP <a href=\"https://kep.k8s.io/3386\">https://kep.k8s.io/3386</a> and\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/switch-to-evented-pleg/\">Switching From Polling to CRI Event-based Updates to Container Status</a>.</p>\n<h2 id=\"raise-your-pod-resource-limit-if-needed\">Raise your pod resource limit if needed</h2>\n<p>During start-up, some pods may consume a considerable amount of CPU or memory. If the CPU limit is\nlow, this can significantly slow down the pod start-up process. To improve the memory management,\nKubernetes v1.22 introduced a feature gate called MemoryQoS to kubelet. This feature enables\nkubelet to set memory QoS at container, pod, and QoS levels for better protection and guaranteed\nquality of memory when running with cgroups v2. Although it has benefits, it is possible that\nenabling this feature gate may affect the start-up speed of the pod if the pod startup consumes\na large amount of memory.</p>\n<p>Kubelet configuration now includes <code>memoryThrottlingFactor</code>. This factor is multiplied by\nthe memory limit or node allocatable memory to set the cgroupv2 memory.high value for enforcing\nMemoryQoS. Decreasing this factor sets a lower high limit for container cgroups, increasing reclaim\npressure. Increasing this factor will put less reclaim pressure. The default value is 0.8 initially\nand will change to 0.9 in Kubernetes v1.27. This parameter adjustment can reduce the potential\nimpact of this feature on pod startup speed.</p>\n<p>Further details can be found in the KEP <a href=\"https://kep.k8s.io/2570\">https://kep.k8s.io/2570</a>.</p>\n<h2 id=\"what-s-more\">What's more?</h2>\n<p>In Kubernetes v1.26, a new histogram metric <code>pod_start_sli_duration_seconds</code> was added for Pod\nstartup latency SLI/SLO details. Additionally, the kubelet log will now display more information\nabout pod start-related timestamps, as shown below:</p>\n<blockquote>\n<p>Dec 30 15:33:13.375379 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:13.375359 8362 pod_startup_latency_tracker.go:102] &quot;Observed pod startup duration&quot; pod=&quot;kube-system/konnectivity-agent-gnc9k&quot; podStartSLOduration=-9.223372029479458e+09 pod.CreationTimestamp=&quot;2022-12-30 15:33:06 +0000 UTC&quot; firstStartedPulling=&quot;2022-12-30 15:33:09.258791695 +0000 UTC m=+13.029631711&quot; lastFinishedPulling=&quot;0001-01-01 00:00:00 +0000 UTC&quot; observedRunningTime=&quot;2022-12-30 15:33:13.375009262 +0000 UTC m=+17.145849275&quot; watchObservedRunningTime=&quot;2022-12-30 15:33:13.375317944 +0000 UTC m=+17.146157970&quot;</p>\n</blockquote>\n<p>The SELinux Relabeling with Mount Options feature moved to Beta in v1.27. This feature speeds up\ncontainer startup by mounting volumes with the correct SELinux label instead of changing each file\non the volumes recursively. Further details can be found in the KEP <a href=\"https://kep.k8s.io/1710\">https://kep.k8s.io/1710</a>.</p>\n<p>To identify the cause of slow pod startup, analyzing metrics and logs can be helpful. Other\nfactorsthat may impact pod startup include container runtime, disk speed, CPU and memory\nresources on the node.</p>\n<p>SIG Node is responsible for ensuring fast Pod startup times, while addressing issues in large\nclusters falls under the purview of SIG Scalability as well.</p>","PublishedAt":"2023-05-15 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/15/speed-up-pod-startup/","SourceName":"Kubernetes"}},{"node":{"ID":3645,"Title":"Blog: Kubernetes 1.27: In-place Resource Resize for Kubernetes Pods (alpha)","Description":"<p><strong>Author:</strong> Vinay Kulkarni (Kubescaler Labs)</p>\n<p>If you have deployed Kubernetes pods with CPU and/or memory resources\nspecified, you may have noticed that changing the resource values involves\nrestarting the pod. This has been a disruptive operation for running\nworkloads... until now.</p>\n<p>In Kubernetes v1.27, we have added a new alpha feature that allows users\nto resize CPU/memory resources allocated to pods without restarting the\ncontainers. To facilitate this, the <code>resources</code> field in a pod's containers\nnow allow mutation for <code>cpu</code> and <code>memory</code> resources. They can be changed\nsimply by patching the running pod spec.</p>\n<p>This also means that <code>resources</code> field in the pod spec can no longer be\nrelied upon as an indicator of the pod's actual resources. Monitoring tools\nand other such applications must now look at new fields in the pod's status.\nKubernetes queries the actual CPU and memory requests and limits enforced on\nthe running containers via a CRI (Container Runtime Interface) API call to the\nruntime, such as containerd, which is responsible for running the containers.\nThe response from container runtime is reflected in the pod's status.</p>\n<p>In addition, a new <code>restartPolicy</code> for resize has been added. It gives users\ncontrol over how their containers are handled when resources are resized.</p>\n<h2 id=\"what-s-new-in-v1-27\">What's new in v1.27?</h2>\n<p>Besides the addition of resize policy in the pod's spec, a new field named\n<code>allocatedResources</code> has been added to <code>containerStatuses</code> in the pod's status.\nThis field reflects the node resources allocated to the pod's containers.</p>\n<p>In addition, a new field called <code>resources</code> has been added to the container's\nstatus. This field reflects the actual resource requests and limits configured\non the running containers as reported by the container runtime.</p>\n<p>Lastly, a new field named <code>resize</code> has been added to the pod's status to show the\nstatus of the last requested resize. A value of <code>Proposed</code> is an acknowledgement\nof the requested resize and indicates that request was validated and recorded. A\nvalue of <code>InProgress</code> indicates that the node has accepted the resize request\nand is in the process of applying the resize request to the pod's containers.\nA value of <code>Deferred</code> means that the requested resize cannot be granted at this\ntime, and the node will keep retrying. The resize may be granted when other pods\nleave and free up node resources. A value of <code>Infeasible</code> is a signal that the\nnode cannot accommodate the requested resize. This can happen if the requested\nresize exceeds the maximum resources the node can ever allocate for a pod.</p>\n<h2 id=\"when-to-use-this-feature\">When to use this feature</h2>\n<p>Here are a few examples where this feature may be useful:</p>\n<ul>\n<li>Pod is running on node but with either too much or too little resources.</li>\n<li>Pods are not being scheduled do to lack of sufficient CPU or memory in a\ncluster that is underutilized by running pods that were overprovisioned.</li>\n<li>Evicting certain stateful pods that need more resources to schedule them\non bigger nodes is an expensive or disruptive operation when other lower\npriority pods in the node can be resized down or moved.</li>\n</ul>\n<h2 id=\"how-to-use-this-feature\">How to use this feature</h2>\n<p>In order to use this feature in v1.27, the <code>InPlacePodVerticalScaling</code>\nfeature gate must be enabled. A local cluster with this feature enabled\ncan be started as shown below:</p>\n<pre tabindex=\"0\"><code>root@vbuild:~/go/src/k8s.io/kubernetes# FEATURE_GATES=InPlacePodVerticalScaling=true ./hack/local-up-cluster.sh\ngo version go1.20.2 linux/arm64\n+++ [0320 13:52:02] Building go targets for linux/arm64\nk8s.io/kubernetes/cmd/kubectl (static)\nk8s.io/kubernetes/cmd/kube-apiserver (static)\nk8s.io/kubernetes/cmd/kube-controller-manager (static)\nk8s.io/kubernetes/cmd/cloud-controller-manager (non-static)\nk8s.io/kubernetes/cmd/kubelet (non-static)\n...\n...\nLogs:\n/tmp/etcd.log\n/tmp/kube-apiserver.log\n/tmp/kube-controller-manager.log\n/tmp/kube-proxy.log\n/tmp/kube-scheduler.log\n/tmp/kubelet.log\nTo start using your cluster, you can open up another terminal/tab and run:\nexport KUBECONFIG=/var/run/kubernetes/admin.kubeconfig\ncluster/kubectl.sh\nAlternatively, you can write to the default kubeconfig:\nexport KUBERNETES_PROVIDER=local\ncluster/kubectl.sh config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt\ncluster/kubectl.sh config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt\ncluster/kubectl.sh config set-context local --cluster=local --user=myself\ncluster/kubectl.sh config use-context local\ncluster/kubectl.sh\n</code></pre><p>Once the local cluster is up and running, Kubernetes users can schedule pods\nwith resources, and resize the pods via kubectl. An example of how to use this\nfeature is illustrated in the following demo video.</p>\n<div style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\">\n<iframe src=\"https://www.youtube.com/embed/1m2FOuB6Bh0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"In-place resize of pod CPU and memory resources\"></iframe>\n</div>\n<h2 id=\"example-use-cases\">Example Use Cases</h2>\n<h3 id=\"cloud-based-development-environment\">Cloud-based Development Environment</h3>\n<p>In this scenario, developers or development teams write their code locally\nbut build and test their code in Kubernetes pods with consistent configs\nthat reflect production use. Such pods need minimal resources when the\ndevelopers are writing code, but need significantly more CPU and memory\nwhen they build their code or run a battery of tests. This use case can\nleverage in-place pod resize feature (with a little help from eBPF) to\nquickly resize the pod's resources and avoid kernel OOM (out of memory)\nkiller from terminating their processes.</p>\n<p>This <a href=\"https://www.youtube.com/watch?v=jjfa1cVJLwc\">KubeCon North America 2022 conference talk</a>\nillustrates the use case.</p>\n<h3 id=\"java-processes-initialization-cpu-requirements\">Java processes initialization CPU requirements</h3>\n<p>Some Java applications may need significantly more CPU during initialization\nthan what is needed during normal process operation time. If such applications\nspecify CPU requests and limits suited for normal operation, they may suffer\nfrom very long startup times. Such pods can request higher CPU values at the\ntime of pod creation, and can be resized down to normal running needs once the\napplication has finished initializing.</p>\n<h2 id=\"known-issues\">Known Issues</h2>\n<p>This feature enters v1.27 at <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages\">alpha stage</a>.\nBelow are a few known issues users may encounter:</p>\n<ul>\n<li>containerd versions below v1.6.9 do not have the CRI support needed for full\nend-to-end operation of this feature. Attempts to resize pods will appear\nto be <em>stuck</em> in the <code>InProgress</code> state, and <code>resources</code> field in the pod's\nstatus are never updated even though the new resources may have been enacted\non the running containers.</li>\n<li>Pod resize may encounter a race condition with other pod updates, causing\ndelayed enactment of pod resize.</li>\n<li>Reflecting the resized container resources in pod's status may take a while.</li>\n<li>Static CPU management policy is not supported with this feature.</li>\n</ul>\n<h2 id=\"credits\">Credits</h2>\n<p>This feature is a result of the efforts of a very collaborative Kubernetes community.\nHere's a little shoutout to just a few of the many many people that contributed\ncountless hours of their time and helped make this happen.</p>\n<ul>\n<li><a href=\"https://github.com/thockin\">@thockin</a> for detail-oriented API design and air-tight code reviews.</li>\n<li><a href=\"https://github.com/derekwaynecarr\">@derekwaynecarr</a> for simplifying the design and thorough API and node reviews.</li>\n<li><a href=\"https://github.com/dchen1107\">@dchen1107</a> for bringing vast knowledge from Borg and helping us avoid pitfalls.</li>\n<li><a href=\"https://github.com/ruiwen-zhao\">@ruiwen-zhao</a> for adding containerd support that enabled full E2E implementation.</li>\n<li><a href=\"https://github.com/wangchen615\">@wangchen615</a> for implementing comprehensive E2E tests and driving scheduler fixes.</li>\n<li><a href=\"https://github.com/bobbypage\">@bobbypage</a> for invaluable help getting CI ready and quickly investigating issues, covering for me on my vacation.</li>\n<li><a href=\"https://github.com/Random-Liu\">@Random-Liu</a> for thorough kubelet reviews and identifying problematic race conditions.</li>\n<li><a href=\"https://github.com/Huang-Wei\">@Huang-Wei</a>, <a href=\"https://github.com/ahg-g\">@ahg-g</a>, <a href=\"https://github.com/alculquicondor\">@alculquicondor</a> for helping get scheduler changes done.</li>\n<li><a href=\"https://github.com/mikebrow\">@mikebrow</a> <a href=\"https://github.com/marosset\">@marosset</a> for reviews on short notice that helped CRI changes make it into v1.25.</li>\n<li><a href=\"https://github.com/endocrimes\">@endocrimes</a>, <a href=\"https://github.com/ehashman\">@ehashman</a> for helping ensure that the oft-overlooked tests are in good shape.</li>\n<li><a href=\"https://github.com/mrunalp\">@mrunalp</a> for reviewing cgroupv2 changes and ensuring clean handling of v1 vs v2.</li>\n<li><a href=\"https://github.com/liggitt\">@liggitt</a>, <a href=\"https://github.com/gjkim42\">@gjkim42</a> for tracking down, root-causing important missed issues post-merge.</li>\n<li><a href=\"https://github.com/SergeyKanzhelev\">@SergeyKanzhelev</a> for supporting and shepherding various issues during the home stretch.</li>\n<li><a href=\"https://github.com/pdgetrf\">@pdgetrf</a> for making the first prototype a reality.</li>\n<li><a href=\"https://github.com/dashpole\">@dashpole</a> for bringing me up to speed on 'the Kubernetes way' of doing things.</li>\n<li><a href=\"https://github.com/bsalamat\">@bsalamat</a>, <a href=\"https://github.com/kgolab\">@kgolab</a> for very thoughtful insights and suggestions in the early stages.</li>\n<li><a href=\"https://github.com/sftim\">@sftim</a>, <a href=\"https://github.com/tengqm\">@tengqm</a> for ensuring docs are easy to follow.</li>\n<li><a href=\"https://github.com/dims\">@dims</a> for being omnipresent and helping make merges happen at critical hours.</li>\n<li>Release teams for ensuring that the project stayed healthy.</li>\n</ul>\n<p>And a big thanks to my very supportive management <a href=\"https://www.linkedin.com/in/xiaoningding/\">Dr. Xiaoning Ding</a>\nand <a href=\"https://www.linkedin.com/in/ying-xiong-59a2482/\">Dr. Ying Xiong</a> for their patience and encouragement.</p>\n<h2 id=\"references\">References</h2>\n<h3 id=\"for-app-developers\">For app developers</h3>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/\">Resize CPU and Memory Resources assigned to Containers</a></p>\n</li>\n<li>\n<p><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/\">Assign Memory Resources to Containers and Pods</a></p>\n</li>\n<li>\n<p><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/\">Assign CPU Resources to Containers and Pods</a></p>\n</li>\n</ul>\n<h3 id=\"for-cluster-administrators\">For cluster administrators</h3>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\">Configure Default Memory Requests and Limits for a Namespace</a></p>\n</li>\n<li>\n<p><a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/\">Configure Default CPU Requests and Limits for a Namespace</a></p>\n</li>\n</ul>","PublishedAt":"2023-05-12 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":3633,"Title":"Blog: Kubernetes 1.27: Avoid Collisions Assigning Ports to NodePort Services","Description":"<p><strong>Author:</strong> Xu Zhenglun (Alibaba)</p>\n<p>In Kubernetes, a Service can be used to provide a unified traffic endpoint for\napplications running on a set of Pods. Clients can use the virtual IP address (or <em>VIP</em>) provided\nby the Service for access, and Kubernetes provides load balancing for traffic accessing\ndifferent back-end Pods, but a ClusterIP type of Service is limited to providing access to\nnodes within the cluster, while traffic from outside the cluster cannot be routed.\nOne way to solve this problem is to use a <code>type: NodePort</code> Service, which sets up a mapping\nto a specific port of all nodes in the cluster, thus redirecting traffic from the\noutside to the inside of the cluster.</p>\n<h2 id=\"how-kubernetes-allocates-node-ports-to-services\">How Kubernetes allocates node ports to Services?</h2>\n<p>When a <code>type: NodePort</code> Service is created, its corresponding port(s) are allocated in one\nof two ways:</p>\n<ul>\n<li>\n<p><strong>Dynamic</strong> : If the Service type is <code>NodePort</code> and you do not set a <code>nodePort</code>\nvalue explicitly in the <code>spec</code> for that Service, the Kubernetes control plane will\nautomatically allocate an unused port to it at creation time.</p>\n</li>\n<li>\n<p><strong>Static</strong> : In addition to the dynamic auto-assignment described above, you can also\nexplicitly assign a port that is within the nodeport port range configuration.</p>\n</li>\n</ul>\n<p>The value of <code>nodePort</code> that you manually assign must be unique across the whole cluster.\nAttempting to create a Service of <code>type: NodePort</code> where you explicitly specify a node port that\nwas already allocated results in an error.</p>\n<h2 id=\"why-do-you-need-to-reserve-ports-of-nodeport-service\">Why do you need to reserve ports of NodePort Service?</h2>\n<p>Sometimes, you may want to have a NodePort Service running on well-known ports\nso that other components and users inside o r outside the cluster can use them.</p>\n<p>In some complex cluster deployments with a mix of Kubernetes nodes and other servers on the same network,\nit may be necessary to use some pre-defined ports for communication. In particular, some fundamental\ncomponents cannot rely on the VIPs that back <code>type: LoadBalancer</code> Services\nbecause the virtual IP address mapping implementation for that cluster also relies on\nthese foundational components.</p>\n<p>Now suppose you need to expose a Minio object storage service on Kubernetes to clients\nrunning outside the Kubernetes cluster, and the agreed port is <code>30009</code>, we need to\ncreate a Service as follows:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>minio<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ports</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>api<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodePort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">30009</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">9000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">protocol</span>:<span style=\"color:#bbb\"> </span>TCP<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">targetPort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">9000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">app</span>:<span style=\"color:#bbb\"> </span>minio<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>NodePort<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>However, as mentioned before, if the port (30009) required for the <code>minio</code> Service is not reserved,\nand another <code>type: NodePort</code> (or possibly <code>type: LoadBalancer</code>) Service is created and dynamically\nallocated before or concurrently with the <code>minio</code> Service, TCP port 30009 might be allocated to that\nother Service; if so, creation of the <code>minio</code> Service will fail due to a node port collision.</p>\n<h2 id=\"how-can-you-avoid-nodeport-service-port-conflicts\">How can you avoid NodePort Service port conflicts?</h2>\n<p>Kubernetes 1.24 introduced changes for <code>type: ClusterIP</code> Services, dividing the CIDR range for cluster\nIP addresses into two blocks that use different allocation policies to <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#avoiding-collisions\">reduce the risk of conflicts</a>.\nIn Kubernetes 1.27, as an alpha feature, you can adopt a similar policy for <code>type: NodePort</code> Services.\nYou can enable a new <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a>\n<code>ServiceNodePortStaticSubrange</code>. Turning this on allows you to use a different port allocation strategy\nfor <code>type: NodePort</code> Services, and reduce the risk of collision.</p>\n<p>The port range for <code>NodePort</code> will be divided, based on the formula <code>min(max(16, nodeport-size / 32), 128)</code>.\nThe outcome of the formula will be a number between 16 and 128, with a step size that increases as the\nsize of the nodeport range increases. The outcome of the formula determine that the size of static port\nrange. When the port range is less than 16, the size of static port range will be set to 0,\nwhich means that all ports will be dynamically allocated.</p>\n<p>Dynamic port assignment will use the upper band by default, once this has been exhausted it will use the lower range.\nThis will allow users to use static allocations on the lower band with a low risk of collision.</p>\n<h2 id=\"examples\">Examples</h2>\n<h3 id=\"default-range-30000-32767\">default range: 30000-32767</h3>\n<table>\n<thead>\n<tr>\n<th>Range properties</th>\n<th>Values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>service-node-port-range</td>\n<td>30000-32767</td>\n</tr>\n<tr>\n<td>Band Offset</td>\n<td>‚ÄÇ <code>min(max(16, 2768/32), 128)</code> <br>= <code>min(max(16, 86), 128)</code> <br>= <code>min(86, 128)</code> <br>= 86</td>\n</tr>\n<tr>\n<td>Static band start</td>\n<td>30000</td>\n</tr>\n<tr>\n<td>Static band end</td>\n<td>30085</td>\n</tr>\n<tr>\n<td>Dynamic band start</td>\n<td>30086</td>\n</tr>\n<tr>\n<td>Dynamic band end</td>\n<td>32767</td>\n</tr>\n</tbody>\n</table>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 30000-32767\n\"Static\" : 86\n\"Dynamic\" : 2682\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h3 id=\"very-small-range-30000-30015\">very small range: 30000-30015</h3>\n<table>\n<thead>\n<tr>\n<th>Range properties</th>\n<th>Values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>service-node-port-range</td>\n<td>30000-30015</td>\n</tr>\n<tr>\n<td>Band Offset</td>\n<td>0</td>\n</tr>\n<tr>\n<td>Static band start</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Static band end</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Dynamic band start</td>\n<td>30000</td>\n</tr>\n<tr>\n<td>Dynamic band end</td>\n<td>30015</td>\n</tr>\n</tbody>\n</table>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 30000-30015\n\"Static\" : 0\n\"Dynamic\" : 16\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h3 id=\"small-lower-boundary-range-30000-30127\">small(lower boundary) range: 30000-30127</h3>\n<table>\n<thead>\n<tr>\n<th>Range properties</th>\n<th>Values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>service-node-port-range</td>\n<td>30000-30127</td>\n</tr>\n<tr>\n<td>Band Offset</td>\n<td>‚ÄÇ <code>min(max(16, 128/32), 128)</code> <br>= <code>min(max(16, 4), 128)</code> <br>= <code>min(16, 128)</code> <br>= 16</td>\n</tr>\n<tr>\n<td>Static band start</td>\n<td>30000</td>\n</tr>\n<tr>\n<td>Static band end</td>\n<td>30015</td>\n</tr>\n<tr>\n<td>Dynamic band start</td>\n<td>30016</td>\n</tr>\n<tr>\n<td>Dynamic band end</td>\n<td>30127</td>\n</tr>\n</tbody>\n</table>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 30000-30127\n\"Static\" : 16\n\"Dynamic\" : 112\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h3 id=\"large-upper-boundary-range-30000-34095\">large(upper boundary) range: 30000-34095</h3>\n<table>\n<thead>\n<tr>\n<th>Range properties</th>\n<th>Values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>service-node-port-range</td>\n<td>30000-34095</td>\n</tr>\n<tr>\n<td>Band Offset</td>\n<td>‚ÄÇ <code>min(max(16, 4096/32), 128)</code> <br>= <code>min(max(16, 128), 128)</code> <br>= <code>min(128, 128)</code> <br>= 128</td>\n</tr>\n<tr>\n<td>Static band start</td>\n<td>30000</td>\n</tr>\n<tr>\n<td>Static band end</td>\n<td>30127</td>\n</tr>\n<tr>\n<td>Dynamic band start</td>\n<td>30128</td>\n</tr>\n<tr>\n<td>Dynamic band end</td>\n<td>34095</td>\n</tr>\n</tbody>\n</table>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 30000-34095\n\"Static\" : 128\n\"Dynamic\" : 3968\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h3 id=\"very-large-range-30000-38191\">very large range: 30000-38191</h3>\n<table>\n<thead>\n<tr>\n<th>Range properties</th>\n<th>Values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>service-node-port-range</td>\n<td>30000-38191</td>\n</tr>\n<tr>\n<td>Band Offset</td>\n<td>‚ÄÇ <code>min(max(16, 8192/32), 128)</code> <br>= <code>min(max(16, 256), 128)</code> <br>= <code>min(256, 128)</code> <br>= 128</td>\n</tr>\n<tr>\n<td>Static band start</td>\n<td>30000</td>\n</tr>\n<tr>\n<td>Static band end</td>\n<td>30127</td>\n</tr>\n<tr>\n<td>Dynamic band start</td>\n<td>30128</td>\n</tr>\n<tr>\n<td>Dynamic band end</td>\n<td>38191</td>\n</tr>\n</tbody>\n</table>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 30000-38191\n\"Static\" : 128\n\"Dynamic\" : 8064\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>","PublishedAt":"2023-05-11 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/11/nodeport-dynamic-and-static-allocation/","SourceName":"Kubernetes"}},{"node":{"ID":3615,"Title":"Blog: Kubernetes 1.27: Safer, More Performant Pruning in kubectl apply","Description":"<p><strong>Authors:</strong> Katrina Verey (independent) and Justin Santa Barbara (Google)</p>\n<p>Declarative configuration management with the <code>kubectl apply</code> command is the gold standard approach\nto creating or modifying Kubernetes resources. However, one challenge it presents is the deletion\nof resources that are no longer needed. In Kubernetes version 1.5, the <code>--prune</code> flag was\nintroduced to address this issue, allowing kubectl apply to automatically clean up previously\napplied resources removed from the current configuration.</p>\n<p>Unfortunately, that existing implementation of <code>--prune</code> has design flaws that diminish its\nperformance and can result in unexpected behaviors. The main issue stems from the lack of explicit\nencoding of the previously applied set by the preceding <code>apply</code> operation, necessitating\nerror-prone dynamic discovery. Object leakage, inadvertent over-selection of resources, and limited\ncompatibility with custom resources are a few notable drawbacks of this implementation. Moreover,\nits coupling to client-side apply hinders user upgrades to the superior server-side apply\nmechanism.</p>\n<p>Version 1.27 of <code>kubectl</code> introduces an alpha version of a revamped pruning implementation that\naddresses these issues. This new implementation, based on a concept called <em>ApplySet</em>, promises\nbetter performance and safety.</p>\n<p>An <em>ApplySet</em> is a group of resources associated with a <em>parent</em> object on the cluster, as\nidentified and configured through standardized labels and annotations. Additional standardized\nmetadata allows for accurate identification of ApplySet <em>member</em> objects within the cluster,\nsimplifying operations like pruning.</p>\n<p>To leverage ApplySet-based pruning, set the <code>KUBECTL_APPLYSET=true</code> environment variable and include\nthe flags <code>--prune</code> and <code>--applyset</code> in your <code>kubectl apply</code> invocation:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#b8860b\">KUBECTL_APPLYSET</span><span style=\"color:#666\">=</span><span style=\"color:#a2f\">true</span> kubectl apply -f &lt;directory/&gt; --prune --applyset<span style=\"color:#666\">=</span>&lt;name&gt;\n</span></span></code></pre></div><p>By default, ApplySet uses a Secret as the parent object. However, you can also use\na ConfigMap with the format <code>--applyset=configmaps/&lt;name&gt;</code>. If your desired Secret or\nConfigMap object does not yet exist, <code>kubectl</code> will create it for you. Furthermore, custom\nresources can be enabled for use as ApplySet parent objects.</p>\n<p>The ApplySet implementation is based on a new low-level specification that can support higher-level\necosystem tools by improving their interoperability. The lightweight nature of this specification\nenables these tools to continue to use existing object grouping systems while opting in to\nApplySet's metadata conventions to prevent inadvertent changes by other tools (such as <code>kubectl</code>).</p>\n<p>ApplySet-based pruning offers a promising solution to the shortcomings of the previous <code>--prune</code>\nimplementation in <code>kubectl</code> and can help streamline your Kubernetes resource management. Please\ngive this new feature a try and share your experiences with the community‚ÄîApplySet is under active\ndevelopment, and your feedback is invaluable!</p>\n<h3 id=\"additional-resources\">Additional resources</h3>\n<ul>\n<li>For more information how to use ApplySet-based pruning, read\n<a href=\"https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/\">Declarative Management of Kubernetes Objects Using Configuration Files</a> in the Kubernetes documentation.</li>\n<li>For a deeper dive into the technical design of this feature or to learn how to implement the\nApplySet specification in your own tools, refer to <a href=\"https://git.k8s.io/enhancements/keps/sig-cli/3659-kubectl-apply-prune/README.md\">KEP¬†3659</a>:\n<em>ApplySet: <code>kubectl apply --prune</code> redesign and graduation strategy</em>.</li>\n</ul>\n<h3 id=\"how-do-i-get-involved\">How do I get involved?</h3>\n<p>If you want to get involved in ApplySet development, you can get in touch with the developers at\n<a href=\"https://git.k8s.io/community/sig-cli\">SIG CLI</a>. To provide feedback on the feature, please\n<a href=\"https://github.com/kubernetes/kubectl/issues/new?assignees=knverey,justinsb&amp;labels=kind%2Fbug&amp;template=bug-report.md\">file a bug</a>\nor <a href=\"https://github.com/kubernetes/kubectl/issues/new?assignees=knverey,justinsb&amp;labels=kind%2Fbug&amp;template=enhancement.md\">request an enhancement</a>\non the <code>kubernetes/kubectl</code> repository.</p>","PublishedAt":"2023-05-09 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning/","SourceName":"Kubernetes"}},{"node":{"ID":3607,"Title":"Blog: Kubernetes 1.27: Introducing An API For Volume Group Snapshots","Description":"<p><strong>Author:</strong> Xing Yang (VMware)</p>\n<p>Volume group snapshot is introduced as an Alpha feature in Kubernetes v1.27.\nThis feature introduces a Kubernetes API that allows users to take crash consistent\nsnapshots for multiple volumes together. It uses a label selector to group multiple\n<code>PersistentVolumeClaims</code> for snapshotting.\nThis new feature is only supported for <a href=\"https://kubernetes-csi.github.io/docs/\">CSI</a> volume drivers.</p>\n<h2 id=\"an-overview-of-volume-group-snapshots\">An overview of volume group snapshots</h2>\n<p>Some storage systems provide the ability to create a crash consistent snapshot of\nmultiple volumes. A group snapshot represents ‚Äúcopies‚Äù from multiple volumes that\nare taken at the same point-in-time. A group snapshot can be used either to rehydrate\nnew volumes (pre-populated with the snapshot data) or to restore existing volumes to\na previous state (represented by the snapshots).</p>\n<h2 id=\"why-add-volume-group-snapshots-to-kubernetes\">Why add volume group snapshots to Kubernetes?</h2>\n<p>The Kubernetes volume plugin system already provides a powerful abstraction that\nautomates the provisioning, attaching, mounting, resizing, and snapshotting of block\nand file storage.</p>\n<p>Underpinning all these features is the Kubernetes goal of workload portability:\nKubernetes aims to create an abstraction layer between distributed applications and\nunderlying clusters so that applications can be agnostic to the specifics of the\ncluster they run on and application deployment requires no cluster specific knowledge.</p>\n<p>There is already a <a href=\"https://kubernetes.io/docs/concepts/storage/volume-snapshots/\">VolumeSnapshot</a> API\nthat provides the ability to take a snapshot of a persistent volume to protect against\ndata loss or data corruption. However, there are other snapshotting functionalities\nnot covered by the VolumeSnapshot API.</p>\n<p>Some storage systems support consistent group snapshots that allow a snapshot to be\ntaken from multiple volumes at the same point-in-time to achieve write order consistency.\nThis can be useful for applications that contain multiple volumes. For example,\nan application may have data stored in one volume and logs stored in another volume.\nIf snapshots for the data volume and the logs volume are taken at different times,\nthe application will not be consistent and will not function properly if it is restored\nfrom those snapshots when a disaster strikes.</p>\n<p>It is true that you can quiesce the application first, take an individual snapshot from\neach volume that is part of the application one after the other, and then unquiesce the\napplication after all the individual snapshots are taken. This way, you would get\napplication consistent snapshots.</p>\n<p>However, sometimes it may not be possible to quiesce an application or the application\nquiesce can be too expensive so you want to do it less frequently. Taking individual\nsnapshots one after another may also take longer time compared to taking a consistent\ngroup snapshot. Some users may not want to do application quiesce very often for these\nreasons. For example, a user may want to run weekly backups with application quiesce\nand nightly backups without application quiesce but with consistent group support which\nprovides crash consistency across all volumes in the group.</p>\n<h2 id=\"kubernetes-volume-group-snapshots-api\">Kubernetes Volume Group Snapshots API</h2>\n<p>Kubernetes Volume Group Snapshots introduce <a href=\"https://github.com/kubernetes-csi/external-snapshotter/blob/master/client/apis/volumegroupsnapshot/v1alpha1/types.go\">three new API\nobjects</a>\nfor managing snapshots:</p>\n<dl>\n<dt><code>VolumeGroupSnapshot</code></dt>\n<dd>Created by a Kubernetes user (or perhaps by your own automation) to request\ncreation of a volume group snapshot for multiple persistent volume claims.\nIt contains information about the volume group snapshot operation such as the\ntimestamp when the volume group snapshot was taken and whether it is ready to use.\nThe creation and deletion of this object represents a desire to create or delete a\ncluster resource (a group snapshot).</dd>\n<dt><code>VolumeGroupSnapshotContent</code></dt>\n<dd>Created by the snapshot controller for a dynamically created VolumeGroupSnapshot.\nIt contains information about the volume group snapshot including the volume group\nsnapshot ID.\nThis object represents a provisioned resource on the cluster (a group snapshot).\nThe VolumeGroupSnapshotContent object binds to the VolumeGroupSnapshot for which it\nwas created with a one-to-one mapping.</dd>\n<dt><code>VolumeGroupSnapshotClass</code></dt>\n<dd>Created by cluster administrators to describe how volume group snapshots should be\ncreated. including the driver information, the deletion policy, etc.</dd>\n</dl>\n<p>These three API kinds are defined as CustomResourceDefinitions (CRDs).\nThese CRDs must be installed in a Kubernetes cluster for a CSI Driver to support\nvolume group snapshots.</p>\n<h2 id=\"how-do-i-use-kubernetes-volume-group-snapshots\">How do I use Kubernetes Volume Group Snapshots</h2>\n<p>Volume group snapshots are implemented in the\n<a href=\"https://github.com/kubernetes-csi/external-snapshotter\">external-snapshotter</a> repository. Implementing volume\ngroup snapshots meant adding or changing several components:</p>\n<ul>\n<li>Added new CustomResourceDefinitions for VolumeGroupSnapshot and two supporting APIs.</li>\n<li>Volume group snapshot controller logic is added to the common snapshot controller.</li>\n<li>Volume group snapshot validation webhook logic is added to the common snapshot validation webhook.</li>\n<li>Adding logic to make CSI calls into the snapshotter sidecar controller.</li>\n</ul>\n<p>The volume snapshot controller, CRDs, and validation webhook are deployed once per\ncluster, while the sidecar is bundled with each CSI driver.</p>\n<p>Therefore, it makes sense to deploy the volume snapshot controller, CRDs, and validation\nwebhook as a cluster addon. I strongly recommend that Kubernetes distributors\nbundle and deploy the volume snapshot controller, CRDs, and validation webhook as part\nof their Kubernetes cluster management process (independent of any CSI Driver).</p>\n<h3 id=\"creating-a-new-group-snapshot-with-kubernetes\">Creating a new group snapshot with Kubernetes</h3>\n<p>Once a VolumeGroupSnapshotClass object is defined and you have volumes you want to\nsnapshot together, you may request a new group snapshot by creating a VolumeGroupSnapshot\nobject.</p>\n<p>The source of the group snapshot specifies whether the underlying group snapshot\nshould be dynamically created or if a pre-existing VolumeGroupSnapshotContent\nshould be used.</p>\n<p>A pre-existing VolumeGroupSnapshotContent is created by a cluster administrator.\nIt contains the details of the real volume group snapshot on the storage system which\nis available for use by cluster users.</p>\n<p>One of the following members in the source of the group snapshot must be set.</p>\n<ul>\n<li><code>selector</code> - a label query over PersistentVolumeClaims that are to be grouped\ntogether for snapshotting. This labelSelector will be used to match the label\nadded to a PVC.</li>\n<li><code>volumeGroupSnapshotContentName</code> - specifies the name of a pre-existing\nVolumeGroupSnapshotContent object representing an existing volume group snapshot.</li>\n</ul>\n<p>In the following example, there are two PVCs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>NAME STATUS VOLUME CAPACITY ACCESSMODES AGE<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>pvc-0 Bound pvc-a42d7ea2-e3df-11ed-b5ea-0242ac120002 1Gi RWO 48s<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>pvc-1 Bound pvc-a42d81b8-e3df-11ed-b5ea-0242ac120002 1Gi RWO 48s<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Label the PVCs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>% kubectl label pvc pvc-0 group=myGroup<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>persistentvolumeclaim/pvc-0 labeled<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>% kubectl label pvc pvc-1 group=myGroup<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>persistentvolumeclaim/pvc-1 labeled<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>For dynamic provisioning, a selector must be set so that the snapshot controller can\nfind PVCs with the matching labels to be snapshotted together.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>groupsnapshot.storage.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeGroupSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new-group-snapshot-demo<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeGroupSnapshotClassName</span>:<span style=\"color:#bbb\"> </span>csi-groupSnapclass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">group</span>:<span style=\"color:#bbb\"> </span>myGroup<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>In the VolumeGroupSnapshot spec, a user can specify the VolumeGroupSnapshotClass which\nhas the information about which CSI driver should be used for creating the group snapshot.</p>\n<p>Two individual volume snapshots will be created as part of the volume group snapshot creation.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>snapshot-62abb5db7204ac6e4c1198629fec533f2a5d9d60ea1a25f594de0bf8866c7947-2023-04-26-2.20.4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>snapshot-2026811eb9f0787466171fe189c805a22cdb61a326235cd067dc3a1ac0104900-2023-04-26-2.20.4<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"how-to-use-group-snapshot-for-restore-in-kubernetes\">How to use group snapshot for restore in Kubernetes</h3>\n<p>At restore time, the user can request a new PersistentVolumeClaim to be created from\na VolumeSnapshot object that is part of a VolumeGroupSnapshot. This will trigger\nprovisioning of a new volume that is pre-populated with data from the specified\nsnapshot. The user should repeat this until all volumes are created from all the\nsnapshots that are part of a group snapshot.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pvc0-restore<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>csi-hostpath-sc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSource</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>snapshot-62abb5db7204ac6e4c1198629fec533f2a5d9d60ea1a25f594de0bf8866c7947-2023-04-26-2.20.4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"as-a-storage-vendor-how-do-i-add-support-for-group-snapshots-to-my-csi-driver\">As a storage vendor, how do I add support for group snapshots to my CSI driver?</h2>\n<p>To implement the volume group snapshot feature, a CSI driver <strong>must</strong>:</p>\n<ul>\n<li>Implement a new group controller service.</li>\n<li>Implement group controller RPCs: <code>CreateVolumeGroupSnapshot</code>, <code>DeleteVolumeGroupSnapshot</code>, and <code>GetVolumeGroupSnapshot</code>.</li>\n<li>Add group controller capability <code>CREATE_DELETE_GET_VOLUME_GROUP_SNAPSHOT</code>.</li>\n</ul>\n<p>See the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md\">CSI spec</a>\nand the <a href=\"https://kubernetes-csi.github.io/docs/\">Kubernetes-CSI Driver Developer Guide</a>\nfor more details.</p>\n<p>a CSI Volume Driver as possible, it provides a suggested mechanism to deploy a\ncontainerized CSI driver to simplify the process.</p>\n<p>As part of this recommended deployment process, the Kubernetes team provides a number of\nsidecar (helper) containers, including the\n<a href=\"https://kubernetes-csi.github.io/docs/external-snapshotter.html\">external-snapshotter sidecar container</a>\nwhich has been updated to support volume group snapshot.</p>\n<p>The external-snapshotter watches the Kubernetes API server for the\n<code>VolumeGroupSnapshotContent</code> object and triggers <code>CreateVolumeGroupSnapshot</code> and\n<code>DeleteVolumeGroupSnapshot</code> operations against a CSI endpoint.</p>\n<h2 id=\"what-are-the-limitations\">What are the limitations?</h2>\n<p>The alpha implementation of volume group snapshots for Kubernetes has the following\nlimitations:</p>\n<ul>\n<li>Does not support reverting an existing PVC to an earlier state represented by\na snapshot (only supports provisioning a new volume from a snapshot).</li>\n<li>No application consistency guarantees beyond any guarantees provided by the storage system\n(e.g. crash consistency). See this <a href=\"https://github.com/kubernetes/community/blob/master/wg-data-protection/data-protection-workflows-white-paper.md#quiesce-and-unquiesce-hooks\">doc</a>\nfor more discussions on application consistency.</li>\n</ul>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI\nGroup Snapshot implementation to Beta in either 1.28 or 1.29.\nSome of the features we are interested in supporting include volume replication,\nreplication group, volume placement, application quiescing, changed block tracking, and more.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/3476-volume-group-snapshot\">design spec</a>\nfor the volume group snapshot feature.</li>\n<li>The <a href=\"https://github.com/kubernetes-csi/external-snapshotter\">code repository</a> for volume group\nsnapshot APIs and controller.</li>\n<li>CSI <a href=\"https://kubernetes-csi.github.io/docs/\">documentation</a> on the group snapshot feature.</li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>This project, like all of Kubernetes, is the result of hard work by many contributors\nfrom diverse backgrounds working together. On behalf of SIG Storage, I would like to\noffer a huge thank you to the contributors who stepped up these last few quarters\nto help the project reach alpha:</p>\n<ul>\n<li>Alex Meade (<a href=\"https://github.com/ameade\">ameade</a>)</li>\n<li>Ben Swartzlander (<a href=\"https://github.com/bswartz\">bswartz</a>)</li>\n<li>Humble Devassy Chirammal (<a href=\"https://github.com/humblec\">humblec</a>)</li>\n<li>James Defelice (<a href=\"https://github.com/jdef\">jdef</a>)</li>\n<li>Jan ≈†afr√°nek (<a href=\"https://github.com/jsafrane\">jsafrane</a>)</li>\n<li>Jing Xu (<a href=\"https://github.com/jingxu97\">jingxu97</a>)</li>\n<li>Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>)</li>\n<li>Niels de Vos (<a href=\"https://github.com/nixpanic\">nixpanic</a>)</li>\n<li>Rakshith R (<a href=\"https://github.com/Rakshith-R\">Rakshith-R</a>)</li>\n<li>Raunak Shah (<a href=\"https://github.com/RaunakShah\">RaunakShah</a>)</li>\n<li>Saad Ali (<a href=\"https://github.com/saad-ali\">saad-ali</a>)</li>\n<li>Thomas Watson (<a href=\"https://github.com/rbo54\">rbo54</a>)</li>\n<li>Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>)</li>\n<li>Yati Padia (<a href=\"https://github.com/yati1998\">yati1998</a>)</li>\n</ul>\n<p>We also want to thank everyone else who has contributed to the project, including others\nwho helped review the <a href=\"https://github.com/kubernetes/enhancements/pull/1551\">KEP</a>\nand the <a href=\"https://github.com/container-storage-interface/spec/pull/519\">CSI spec PR</a>.</p>\n<p>For those interested in getting involved with the design and development of CSI or\nany part of the Kubernetes Storage system, join the\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group</a> (SIG).\nWe always welcome new contributors.</p>\n<p>We also hold regular <a href=\"https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#\">Data Protection Working Group meetings</a>.\nNew attendees are welcome to join our discussions.</p>","PublishedAt":"2023-05-08 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/08/kubernetes-1-27-volume-group-snapshot-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":3591,"Title":"Blog: Kubernetes 1.27: Quality-of-Service for Memory Resources (alpha)","Description":"<p><strong>Authors:</strong> Dixita Narang (Google)</p>\n<p>Kubernetes v1.27, released in April 2023, introduced changes to\nMemory QoS (alpha) to improve memory management capabilites in Linux nodes.</p>\n<p>Support for Memory QoS was initially added in Kubernetes v1.22, and later some\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos#reasons-for-changing-the-formula-of-memoryhigh-calculation-in-alpha-v127\">limitations</a>\naround the formula for calculating <code>memory.high</code> were identified. These limitations are\naddressed in Kubernetes v1.27.</p>\n<h2 id=\"background\">Background</h2>\n<p>Kubernetes allows you to optionally specify how much of each resources a container needs\nin the Pod specification. The most common resources to specify are CPU and Memory.</p>\n<p>For example, a Pod manifest that defines container resource requirements could look like:</p>\n<pre tabindex=\"0\"><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: example\nspec:\ncontainers:\n- name: nginx\nresources:\nrequests:\nmemory: &#34;64Mi&#34;\ncpu: &#34;250m&#34;\nlimits:\nmemory: &#34;64Mi&#34;\ncpu: &#34;500m&#34;\n</code></pre><ul>\n<li>\n<p><code>spec.containers[].resources.requests</code></p>\n<p>When you specify the resource request for containers in a Pod, the\n<a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler\">Kubernetes scheduler</a>\nuses this information to decide which node to place the Pod on. The scheduler\nensures that for each resource type, the sum of the resource requests of the\nscheduled containers is less than the total allocatable resources on the node.</p>\n</li>\n<li>\n<p><code>spec.containers[].resources.limits</code></p>\n<p>When you specify the resource limit for containers in a Pod, the kubelet enforces\nthose limits so that the running containers are not allowed to use more of those\nresources than the limits you set.</p>\n</li>\n</ul>\n<p>When the kubelet starts a container as a part of a Pod, kubelet passes the\ncontainer's requests and limits for CPU and memory to the container runtime.\nThe container runtime assigns both CPU request and CPU limit to a container.\nProvided the system has free CPU time, the containers are guaranteed to be\nallocated as much CPU as they request. Containers cannot use more CPU than\nthe configured limit i.e. containers CPU usage will be throttled if they\nuse more CPU than the specified limit within a given time slice.</p>\n<p>Prior to Memory QoS feature, the container runtime only used the memory\nlimit and discarded the memory <code>request</code> (requests were, and still are,\nalso used to influence <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/#scheduling\">scheduling</a>).\nIf a container uses more memory¬†than the configured limit,\nthe Linux Out Of Memory (OOM) killer will be invoked.</p>\n<p>Let's compare how the container runtime on Linux typically configures memory\nrequest and limit in cgroups, with and without Memory QoS feature:</p>\n<ul>\n<li>\n<p><strong>Memory request</strong></p>\n<p>The memory request is mainly used by kube-scheduler during (Kubernetes) Pod\nscheduling. In cgroups v1, there are no controls to specify the minimum amount\nof memory the cgroups must always retain. Hence, the container runtime did not\nuse the value of requested memory set in the Pod spec.</p>\n<p>cgroups v2 introduced a <code>memory.min</code> setting, used to specify the minimum\namount of memory that should remain available to the processes within\na given cgroup. If the memory usage of a cgroup is within its effective\nmin boundary, the cgroup‚Äôs memory won‚Äôt be reclaimed under any conditions.\nIf the kernel cannot maintain at least <code>memory.min</code> bytes of memory for the\nprocesses within the cgroup, the kernel invokes its OOM killer. In other words,\nthe kernel guarantees at least this much memory is available or terminates\nprocesses (which may be outside the cgroup) in order to make memory more available.\nMemory QoS maps <code>memory.min</code> to <code>spec.containers[].resources.requests.memory</code>\nto ensure the availability of memory for containers in Kubernetes Pods.</p>\n</li>\n<li>\n<p><strong>Memory limit</strong></p>\n<p>The <code>memory.limit</code> specifies the memory limit, beyond which if the container tries\nto allocate more memory, Linux kernel will terminate a process with an\nOOM (Out of Memory) kill. If the terminated process was the main (or only) process\ninside the container, the container may exit.</p>\n<p>In cgroups v1, <code>memory.limit_in_bytes</code> interface is used to set the memory usage limit.\nHowever, unlike CPU, it was not possible to apply memory throttling: as soon as a\ncontainer crossed the memory limit, it would be OOM killed.</p>\n<p>In cgroups v2, <code>memory.max</code> is analogous to <code>memory.limit_in_bytes</code> in cgroupv1.\nMemory QoS maps <code>memory.max</code> to <code>spec.containers[].resources.limits.memory</code> to\nspecify the hard limit for memory usage. If the memory consumption goes above this\nlevel, the kernel invokes its OOM Killer.</p>\n<p>cgroups v2 also added <code>memory.high</code> configuration . Memory QoS uses <code>memory.high</code>\nto set memory usage throttle limit. If the <code>memory.high</code> limit is breached,\nthe offending cgroups are throttled, and the kernel tries to reclaim memory\nwhich may avoid an OOM kill.</p>\n</li>\n</ul>\n<h2 id=\"how-it-works\">How it works</h2>\n<h3 id=\"cgroups-v2-memory-controller-interfaces-kubernetes-container-resources-mapping\">Cgroups v2 memory controller interfaces &amp; Kubernetes container resources mapping</h3>\n<p>Memory QoS uses the memory controller of cgroups v2 to guarantee memory resources in\nKubernetes. cgroupv2 interfaces that this feature uses are:</p>\n<ul>\n<li><code>memory.max</code></li>\n<li><code>memory.min</code></li>\n<li><code>memory.high</code>.</li>\n</ul>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/memory-qos-cal.svg\"\nalt=\"Memory QoS Levels\"/> <figcaption>\n<h4>Memory QoS Levels</h4>\n</figcaption>\n</figure>\n<p><code>memory.max</code> is mapped to <code>limits.memory</code> specified in the Pod spec. The kubelet and\nthe container runtime configure the limit in the respective cgroup. The kernel\nenforces the limit to prevent the container from using more than the configured\nresource limit. If a process in a container tries to consume more than the\nspecified limit, kernel terminates a process(es) with an out of\nmemory Out of Memory (OOM) error.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-max.svg\"\nalt=\"memory.max maps to limits.memory\"/> <figcaption>\n<h4>memory.max maps to limits.memory</h4>\n</figcaption>\n</figure>\n<p><code>memory.min</code> is mapped to <code>requests.memory</code>, which results in reservation of memory resources\nthat should never be reclaimed by the kernel. This is how Memory QoS ensures the availability of\nmemory for Kubernetes pods. If there's no unprotected reclaimable memory available, the OOM\nkiller is invoked to make more memory available.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-min.svg\"\nalt=\"memory.min maps to requests.memory\"/> <figcaption>\n<h4>memory.min maps to requests.memory</h4>\n</figcaption>\n</figure>\n<p>For memory protection, in addition to the original way of limiting memory usage, Memory QoS\nthrottles workload approaching its memory limit, ensuring that the system is not overwhelmed\nby sporadic increases in memory usage. A new field, <code>memoryThrottlingFactor</code>, is available in\nthe KubeletConfiguration when you enable MemoryQoS feature. It is set to 0.9 by default.\n<code>memory.high</code> is mapped to throttling limit calculated by using <code>memoryThrottlingFactor</code>,\n<code>requests.memory</code> and <code>limits.memory</code> as in the formula below, and rounding down the\nvalue to the nearest page size:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high.svg\"\nalt=\"memory.high formula\"/> <figcaption>\n<h4>memory.high formula</h4>\n</figcaption>\n</figure>\n<p><strong>Note</strong>: If a container has no memory limits specified, <code>limits.memory</code> is substituted for node allocatable memory.</p>\n<p><strong>Summary:</strong></p>\n<table>\n<tr>\n<th style=\"text-align:center\">File</th>\n<th style=\"text-align:center\">Description</th>\n</tr>\n<tr>\n<td>memory.max</td>\n<td><code>memory.max</code> specifies the maximum memory limit,\na container is allowed to use. If a process within the container\ntries to consume more memory than the configured limit,\nthe kernel terminates the process with an Out of Memory (OOM) error.\n<br>\n<br>\n<i>It is mapped to the container's memory limit specified in Pod manifest.</i>\n</td>\n</tr>\n<tr>\n<td>memory.min</td>\n<td><code>memory.min</code> specifies a minimum amount of memory\nthe cgroups must always retain, i.e., memory that should never be\nreclaimed by the system.\nIf there's no unprotected reclaimable memory available, OOM kill is invoked.\n<br>\n<br>\n<i>It is mapped to the container's memory request specified in the Pod manifest.</i>\n</td>\n</tr>\n<tr>\n<td>memory.high</td>\n<td><code>memory.high</code> specifies the memory usage throttle limit.\nThis is the main mechanism to control a cgroup's memory use. If\ncgroups memory use goes over the high boundary specified here,\nthe cgroups processes are throttled and put under heavy reclaim pressure.\n<br>\n<br>\n<i>Kubernetes uses a formula to calculate <code>memory.high</code>,\ndepending on container's memory request, memory limit or node allocatable memory\n(if container's memory limit is empty) and a throttling factor.\nPlease refer to the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos\">KEP</a>\nfor more details on the formula.</i>\n</td>\n</tr>\n</table>\n<p><strong>Note</strong> <code>memory.high</code> is set only on container level cgroups while <code>memory.min</code> is set on\ncontainer, pod, and node level cgroups.</p>\n<h3 id=\"memory-min-calculations-for-cgroups-heirarchy\"><code>memory.min</code> calculations for cgroups heirarchy</h3>\n<p>When container memory requests are made, kubelet passes <code>memory.min</code> to the back-end\nCRI runtime (such as containerd or CRI-O) via the <code>Unified</code> field in CRI during\ncontainer creation. The <code>memory.min</code> in container level cgroups will be set to:</p>\n<p>$memory.min = pod.spec.containers[i].resources.requests[memory]$<br>\n<sub>for every i<sup>th</sup> container in a pod</sub>\n<br>\n<br>\nSince the <code>memory.min</code> interface requires that the ancestor cgroups directories are all\nset, the pod and node cgroups directories need to be set correctly.</p>\n<p><code>memory.min</code> in pod level cgroup:<br>\n$memory.min = \\sum_{i=0}^{no. of pods}pod.spec.containers[i].resources.requests[memory]$<br>\n<sub>for every i<sup>th</sup> container in a pod</sub>\n<br>\n<br>\n<code>memory.min</code> in node level cgroup:<br>\n$memory.min = \\sum_{i}^{no. of nodes}\\sum_{j}^{no. of pods}pod[i].spec.containers[j].resources.requests[memory]$<br>\n<sub>for every j<sup>th</sup> container in every i<sup>th</sup> pod on a node</sub>\n<br>\n<br>\nKubelet will manage the cgroups hierarchy of the pod level and node level cgroups\ndirectly using the libcontainer library (from the runc project), while container\ncgroups limits are managed by the container runtime.</p>\n<h3 id=\"support-for-pod-qos-classes\">Support for Pod QoS classes</h3>\n<p>Based on user feedback for the Alpha feature in Kubernetes v1.22, some users would like\nto opt out of MemoryQoS on a per-pod basis to ensure there is no early memory throttling.\nTherefore, in Kubernetes v1.27 Memory QOS also supports memory.high to be set as per\nQuality of Service(QoS) for Pod classes. Following are the different cases for memory.high\nas per QOS classes:</p>\n<ol>\n<li>\n<p><strong>Guaranteed pods</strong> by their QoS definition require memory requests=memory limits and are\nnot overcommitted. Hence MemoryQoS feature is disabled on those pods by not setting\nmemory.high. This ensures that Guaranteed pods can fully use their memory requests up\nto their set limit, and not hit any throttling.</p>\n</li>\n<li>\n<p><strong>Burstable pods</strong> by their QoS definition require at least one container in the Pod with\nCPU or memory request or limit set.</p>\n<ul>\n<li>\n<p>When requests.memory and limits.memory are set, the formula is used as-is:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high-limit.svg\"\nalt=\"memory.high when requests and limits are set\"/> <figcaption>\n<h4>memory.high when requests and limits are set</h4>\n</figcaption>\n</figure>\n</li>\n<li>\n<p>When requests.memory is set and limits.memory is not set, limits.memory is substituted\nfor node allocatable memory in the formula:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high-no-limits.svg\"\nalt=\"memory.high when requests and limits are not set\"/> <figcaption>\n<h4>memory.high when requests and limits are not set</h4>\n</figcaption>\n</figure>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>BestEffort</strong> by their QoS definition do not require any memory or CPU limits or requests.\nFor this case, kubernetes sets requests.memory = 0 and substitute limits.memory for node allocatable\nmemory in the formula:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/container-memory-high-best-effort.svg\"\nalt=\"memory.high for BestEffort Pod\"/> <figcaption>\n<h4>memory.high for BestEffort Pod</h4>\n</figcaption>\n</figure>\n</li>\n</ol>\n<p><strong>Summary</strong>: Only Pods in Burstable and BestEffort QoS classes will set <code>memory.high</code>.\nGuaranteed QoS pods do not set <code>memory.high</code> as their memory is guaranteed.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>The prerequisites for enabling Memory QoS feature on your Linux node are:</p>\n<ol>\n<li>Verify the <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/#requirements\">requirements</a>\nrelated to <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups\">Kubernetes support for cgroups v2</a>\nare met.</li>\n<li>Ensure CRI Runtime supports Memory QoS. At the time of writing, only containerd\nand CRI-O provide support compatible with Memory QoS (alpha). This was implemented\nin the following PRs:\n<ul>\n<li>Containerd: <a href=\"https://github.com/containerd/containerd/pull/5627\">Feature: containerd-cri support LinuxContainerResources.Unified #5627</a>.</li>\n<li>CRI-O: <a href=\"https://github.com/cri-o/cri-o/pull/5207\">implement kube alpha features for 1.22 #5207</a>.</li>\n</ul>\n</li>\n</ol>\n<p>Memory QoS remains an alpha feature for Kubernetes v1.27. You can enable the feature by setting\n<code>MemoryQoS=true</code> in the kubelet configuration file:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeletConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">featureGates</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">MemoryQoS</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>Huge thank you to all the contributors who helped with the design, implementation,\nand review of this feature:</p>\n<ul>\n<li>Dixita Narang (<a href=\"https://github.com/ndixita\">ndixita</a>)</li>\n<li>Tim Xu (<a href=\"https://github.com/xiaoxubeii\">xiaoxubeii</a>)</li>\n<li>Paco Xu (<a href=\"https://github.com/pacoxu\">pacoxu</a>)</li>\n<li>David Porter(<a href=\"https://github.com/bobbypage\">bobbypage</a>)</li>\n<li>Mrunal Patel(<a href=\"https://github.com/mrunalp\">mrunalp</a>)</li>\n</ul>\n<p>For those interested in getting involved in future discussions on Memory QoS feature,\nyou can reach out SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnode\">Open Community Issues/PRs</a></li>\n</ul>","PublishedAt":"2023-05-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/05/qos-memory-resources/","SourceName":"Kubernetes"}},{"node":{"ID":3583,"Title":"Blog: Kubernetes 1.27: StatefulSet PVC Auto-Deletion (beta)","Description":"<p><strong>Author:</strong> Matthew Cary (Google)</p>\n<p>Kubernetes v1.27 graduated to beta a new policy mechanism for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\"><code>StatefulSets</code></a> that controls the lifetime of\ntheir <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\"><code>PersistentVolumeClaims</code></a> (PVCs). The new PVC\nretention policy lets users specify if the PVCs generated from the <code>StatefulSet</code> spec template should\nbe automatically deleted or retrained when the <code>StatefulSet</code> is deleted or replicas in the <code>StatefulSet</code>\nare scaled down.</p>\n<h2 id=\"what-problem-does-this-solve\">What problem does this solve?</h2>\n<p>A <code>StatefulSet</code> spec can include <code>Pod</code> and PVC templates. When a replica is first created, the\nKubernetes control plane creates a PVC for that replica if one does not already exist. The behavior\nbefore the PVC retention policy was that the control plane never cleaned up the PVCs created for\n<code>StatefulSets</code> - this was left up to the cluster administrator, or to some add-on automation that\nyou‚Äôd have to find, check suitability, and deploy. The common pattern for managing PVCs, either\nmanually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,\nwith explicit lifecycle. Workflows that use <code>StatefulSets</code> must determine on their own what PVCs are\ncreated by a <code>StatefulSet</code> and what their lifecycle should be.</p>\n<p>Before this new feature, when a StatefulSet-managed replica disappears, either because the\n<code>StatefulSet</code> is reducing its replica count, or because its <code>StatefulSet</code> is deleted, the PVC and its\nbacking volume remains and must be manually deleted. While this behavior is appropriate when the\ndata is critical, in many cases the persistent data in these PVCs is either temporary, or can be\nreconstructed from another source. In those cases, PVCs and their backing volumes remaining after\ntheir <code>StatefulSet</code> or replicas have been deleted are not necessary, incur cost, and require manual\ncleanup.</p>\n<h2 id=\"the-new-statefulset-pvc-retention-policy\">The new <code>StatefulSet</code> PVC retention policy</h2>\n<p>The new <code>StatefulSet</code> PVC retention policy is used to control if and when PVCs created from a\n<code>StatefulSet</code>‚Äôs <code>volumeClaimTemplate</code> are deleted. There are two contexts when this may occur.</p>\n<p>The first context is when the <code>StatefulSet</code> resource is deleted (which implies that all replicas are\nalso deleted). This is controlled by the <code>whenDeleted</code> policy. The second context, controlled by\n<code>whenScaled</code> is when the <code>StatefulSet</code> is scaled down, which removes some but not all of the replicas\nin a <code>StatefulSet</code>. In both cases the policy can either be <code>Retain</code>, where the corresponding PVCs are\nnot touched, or <code>Delete</code>, which means that PVCs are deleted. The deletion is done with a normal\n<a href=\"https://kubernetes.io/docs/concepts/architecture/garbage-collection/\">object deletion</a>, so that, for example, all\nretention policies for the underlying PV are respected.</p>\n<p>This policy forms a matrix with four cases. I‚Äôll walk through and give an example for each one.</p>\n<ul>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Retain</code>.</strong></p>\n<p>This matches the existing behavior for <code>StatefulSets</code>, where no PVCs are deleted. This is also\nthe default retention policy. It‚Äôs appropriate to use when data on <code>StatefulSet</code> volumes may be\nirreplaceable and should only be deleted manually.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Delete</code> and <code>whenScaled</code> is <code>Retain</code>.</strong></p>\n<p>In this case, PVCs are deleted only when the entire <code>StatefulSet</code> is deleted. If the\n<code>StatefulSet</code> is scaled down, PVCs are not touched, meaning they are available to be reattached\nif a scale-up occurs with any data from the previous replica. This might be used for a temporary\n<code>StatefulSet</code>, such as in a CI instance or ETL pipeline, where the data on the <code>StatefulSet</code> is\nneeded only during the lifetime of the <code>StatefulSet</code> lifetime, but while the task is running the\ndata is not easily reconstructible. Any retained state is needed for any replicas that scale\ndown and then up.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Delete</code>.</strong></p>\n<p>PVCs are deleted immediately when their replica is no longer needed. Note this does not include\nwhen a <code>Pod</code> is deleted and a new version rescheduled, for example when a node is drained and\n<code>Pods</code> need to migrate elsewhere. The PVC is deleted only when the replica is no longer needed\nas signified by a scale-down or <code>StatefulSet</code> deletion. This use case is for when data does not\nneed to live beyond the life of its replica. Perhaps the data is easily reconstructable and the\ncost savings of deleting unused PVCs is more important than quick scale-up, or perhaps that when\na new replica is created, any data from a previous replica is not usable and must be\nreconstructed anyway.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Retain</code> and <code>whenScaled</code> is <code>Delete</code>.</strong></p>\n<p>This is similar to the previous case, when there is little benefit to keeping PVCs for fast\nreuse during scale-up. An example of a situation where you might use this is an Elasticsearch\ncluster. Typically you would scale that workload up and down to match demand, whilst ensuring a\nminimum number of replicas (for example: 3). When scaling down, data is migrated away from\nremoved replicas and there is no benefit to retaining those PVCs. However, it can be useful to\nbring the entire Elasticsearch cluster down temporarily for maintenance. If you need to take the\nElasticsearch system offline, you can do this by temporarily deleting the <code>StatefulSet</code>, and\nthen bringing the Elasticsearch cluster back by recreating the <code>StatefulSet</code>. The PVCs holding\nthe Elasticsearch data will still exist and the new replicas will automatically use them.</p>\n</li>\n</ul>\n<p>Visit the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies\">documentation</a> to\nsee all the details.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>Try it out! The <code>StatefulSetAutoDeletePVC</code> feature gate is beta and enabled by default on\ncluster running Kubernetes 1.27. Create a <code>StatefulSet</code> using the new policy, test it out and tell\nus what you think!</p>\n<p>I'm very curious to see if this owner reference mechanism works well in practice. For example, I\nrealized there is no mechanism in Kubernetes for knowing who set a reference, so it‚Äôs possible that\nthe <code>StatefulSet</code> controller may fight with custom controllers that set their own\nreferences. Fortunately, maintaining the existing retention behavior does not involve any new owner\nreferences, so default behavior will be compatible.</p>\n<p>Please tag any issues you report with the label <code>sig/apps</code> and assign them to Matthew Cary\n(<a href=\"https://github.com/mattcary\">@mattcary</a> at GitHub).</p>\n<p>Enjoy!</p>","PublishedAt":"2023-05-04 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/04/kubernetes-1-27-statefulset-pvc-auto-deletion-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3571,"Title":"Blog: Kubernetes 1.27: HorizontalPodAutoscaler ContainerResource type metric moves to beta","Description":"<p><strong>Author:</strong> <a href=\"https://github.com/sanposhiho\">Kensei Nakada</a> (Mercari)</p>\n<p>Kubernetes 1.20 introduced the <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#container-resource-metrics\"><code>ContainerResource</code> type metric</a>\nin HorizontalPodAutoscaler (HPA).</p>\n<p>In Kubernetes 1.27, this feature moves to beta and the corresponding feature gate (<code>HPAContainerMetrics</code>) gets enabled by default.</p>\n<h2 id=\"what-is-the-containerresource-type-metric\">What is the ContainerResource type metric</h2>\n<p>The ContainerResource type metric allows us to configure the autoscaling based on resource usage of individual containers.</p>\n<p>In the following example, the HPA controller scales the target\nso that the average utilization of the cpu in the application container of all the pods is around 60%.\n(See <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details\">the algorithm details</a>\nto know how the desired replica number is calculated exactly)</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>ContainerResource<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">containerResource</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cpu<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">container</span>:<span style=\"color:#bbb\"> </span>application<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">target</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Utilization<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">averageUtilization</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">60</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"the-difference-from-the-resource-type-metric\">The difference from the Resource type metric</h2>\n<p>HPA already had a <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-resource-metrics\">Resource type metric</a>.</p>\n<p>You can define the target resource utilization like the following,\nand then HPA will scale up/down the replicas based on the current utilization.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Resource<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">resource</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cpu<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">target</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Utilization<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">averageUtilization</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">60</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>But, this Resource type metric refers to the average utilization of the <strong>Pods</strong>.</p>\n<p>In case a Pod has multiple containers, the utilization calculation would be:</p>\n<pre tabindex=\"0\"><code>sum{the resource usage of each container} / sum{the resource request of each container}\n</code></pre><p>The resource utilization of each container may not have a direct correlation or may grow at different rates as the load changes.</p>\n<p>For example:</p>\n<ul>\n<li>A sidecar container is only providing an auxiliary service such as log shipping.\nIf the application does not log very frequently or does not produce logs in its hotpath\nthen the usage of the log shipper will not grow.</li>\n<li>A sidecar container which provides authentication. Due to heavy caching\nthe usage will only increase slightly when the load on the main container increases.\nIn the current blended usage calculation approach this usually results in\nthe HPA not scaling up the deployment because the blended usage is still low.</li>\n<li>A sidecar may be injected without resources set which prevents scaling\nbased on utilization. In the current logic the HPA controller can only scale\non absolute resource usage of the pod when the resource requests are not set.</li>\n</ul>\n<p>And, in such case, if only one container's resource utilization goes high,\nthe Resource type metric may not suggest scaling up.</p>\n<p>So, for the accurate autoscaling, you may want to use the ContainerResource type metric for such Pods instead.</p>\n<h2 id=\"what-s-new-for-the-beta\">What's new for the beta?</h2>\n<p>For Kubernetes v1.27, the ContainerResource type metric is available by default as described at the beginning\nof this article.\n(You can still disable it by the <code>HPAContainerMetrics</code> feature gate.)</p>\n<p>Also, we've improved the observability of HPA controller by exposing some metrics from the kube-controller-manager:</p>\n<ul>\n<li><code>metric_computation_total</code>: Number of metric computations.</li>\n<li><code>metric_computation_duration_seconds</code>: The time that the HPA controller takes to calculate one metric.</li>\n<li><code>reconciliations_total</code>: Number of reconciliation of HPA controller.</li>\n<li><code>reconciliation_duration_seconds</code>: The time that the HPA controller takes to reconcile a HPA object once.</li>\n</ul>\n<p>These metrics have labels <code>action</code> (<code>scale_up</code>, <code>scale_down</code>, <code>none</code>) and <code>error</code> (<code>spec</code>, <code>internal</code>, <code>none</code>).\nAnd, in addition to them, the first two metrics have the <code>metric_type</code> label\nwhich corresponds to <code>.spec.metrics[*].type</code> for a HorizontalPodAutoscaler.</p>\n<p>All metrics are useful for general monitoring of HPA controller,\nyou can get deeper insight into which part has a problem, where it takes time, how much scaling tends to happen at which time on your cluster etc.</p>\n<p>Another minor stuff, we've changed the <code>SuccessfulRescale</code> event's messages\nso that everyone can check whether the events came from the resource metric or\nthe container resource metric (See <a href=\"https://github.com/kubernetes/kubernetes/pull/116045\">the related PR</a>).</p>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>This feature is managed by <a href=\"https://github.com/kubernetes/community/tree/master/sig-autoscaling\">SIG Autoscaling</a>.\nPlease join us and share your feedback. We look forward to hearing from you!</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#container-resource-metrics\">The official document of the ContainerResource type metric</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-autoscaling/1610-container-resource-autoscaling\">KEP-1610: Container Resource based Autoscaling</a></li>\n</ul>","PublishedAt":"2023-05-02 04:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/05/02/hpa-container-resource-metric/","SourceName":"Kubernetes"}},{"node":{"ID":3528,"Title":"Blog: Kubernetes 1.27: StatefulSet Start Ordinal Simplifies Migration","Description":"<p><strong>Author</strong>: Peter Schuurman (Google)</p>\n<p>Kubernetes v1.26 introduced a new, alpha-level feature for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a> that controls\nthe ordinal numbering of Pod replicas. As of Kubernetes v1.27, this feature is\nnow beta. Ordinals can start from arbitrary\nnon-negative numbers. This blog post will discuss how this feature can be\nused.</p>\n<h2 id=\"background\">Background</h2>\n<p>StatefulSets ordinals provide sequential identities for pod replicas. When using\n<a href=\"https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#orderedready-pod-management\"><code>OrderedReady</code> Pod management</a>\nPods are created from ordinal index <code>0</code> up to <code>N-1</code>.</p>\n<p>With Kubernetes today, orchestrating a StatefulSet migration across clusters is\nchallenging. Backup and restore solutions exist, but these require the\napplication to be scaled down to zero replicas prior to migration. In today's\nfully connected world, even planned application downtime may not allow you to\nmeet your business goals. You could use\n<a href=\"https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#cascading-delete\">Cascading Delete</a>\nor\n<a href=\"https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#on-delete\">On Delete</a>\nto migrate individual pods, however this is error prone and tedious to manage.\nYou lose the self-healing benefit of the StatefulSet controller when your Pods\nfail or are evicted.</p>\n<p>Kubernetes v1.26 enables a StatefulSet to be responsible for a range of ordinals\nwithin a range {0..N-1} (the ordinals 0, 1, ... up to N-1).\nWith it, you can scale down a range\n{0..k-1} in a source cluster, and scale up the complementary range {k..N-1}\nin a destination cluster, while maintaining application availability. This\nenables you to retain <em>at most one</em> semantics (meaning there is at most one Pod\nwith a given identity running in a StatefulSet) and\n<a href=\"https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#rolling-update\">Rolling Update</a>\nbehavior when orchestrating a migration across clusters.</p>\n<h2 id=\"why-would-i-want-to-use-this-feature\">Why would I want to use this feature?</h2>\n<p>Say you're running your StatefulSet in one cluster, and need to migrate it out\nto a different cluster. There are many reasons why you would need to do this:</p>\n<ul>\n<li><strong>Scalability</strong>: Your StatefulSet has scaled too large for your cluster, and\nhas started to disrupt the quality of service for other workloads in your\ncluster.</li>\n<li><strong>Isolation</strong>: You're running a StatefulSet in a cluster that is accessed\nby multiple users, and namespace isolation isn't sufficient.</li>\n<li><strong>Cluster Configuration</strong>: You want to move your StatefulSet to a different\ncluster to use some environment that is not available on your current\ncluster.</li>\n<li><strong>Control Plane Upgrades</strong>: You want to move your StatefulSet to a cluster\nrunning an upgraded control plane, and can't handle the risk or downtime of\nin-place control plane upgrades.</li>\n</ul>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>Enable the <code>StatefulSetStartOrdinal</code> feature gate on a cluster, and create a\nStatefulSet with a customized <code>.spec.ordinals.start</code>.</p>\n<h2 id=\"try-it-out\">Try it out</h2>\n<p>In this demo, I'll use the new mechanism to migrate a\nStatefulSet from one Kubernetes cluster to another. The\n<a href=\"https://github.com/bitnami/charts/tree/main/bitnami/redis-cluster\">redis-cluster</a>\nBitnami Helm chart will be used to install Redis.</p>\n<p>Tools Required:</p>\n<ul>\n<li><a href=\"https://github.com/mikefarah/yq\">yq</a></li>\n<li><a href=\"https://helm.sh/docs/helm/helm_install/\">helm</a></li>\n</ul>\n<h3 id=\"demo-pre-requisites\">Pre-requisites</h3>\n<p>To do this, I need two Kubernetes clusters that can both access common\nnetworking and storage; I've named my clusters <code>source</code> and <code>destination</code>.\nSpecifically, I need:</p>\n<ul>\n<li>The <code>StatefulSetStartOrdinal</code> feature gate enabled on both clusters.</li>\n<li>Client configuration for <code>kubectl</code> that lets me access both clusters as an\nadministrator.</li>\n<li>The same <code>StorageClass</code> installed on both clusters, and set as the default\nStorageClass for both clusters. This <code>StorageClass</code> should provision\nunderlying storage that is accessible from either or both clusters.</li>\n<li>A flat network topology that allows for pods to send and receive packets to\nand from Pods in either clusters. If you are creating clusters on a cloud\nprovider, this configuration may be called private cloud or private network.</li>\n</ul>\n<ol>\n<li>\n<p>Create a demo namespace on both clusters:</p>\n<pre tabindex=\"0\"><code>kubectl create ns kep-3335\n</code></pre></li>\n<li>\n<p>Deploy a Redis cluster with six replicas in the source cluster:</p>\n<pre tabindex=\"0\"><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install redis --namespace kep-3335 \\\nbitnami/redis-cluster \\\n--set persistence.size=1Gi \\\n--set cluster.nodes=6\n</code></pre></li>\n<li>\n<p>Check the replication status in the source cluster:</p>\n<pre tabindex=\"0\"><code>kubectl exec -it redis-redis-cluster-0 -- /bin/bash -c \\\n&#34;redis-cli -c -h redis-redis-cluster -a $(kubectl get secret redis-redis-cluster -o jsonpath=&#34;{.data.redis-password}&#34; | base64 -d) CLUSTER NODES;&#34;\n</code></pre><pre tabindex=\"0\"><code>2ce30362c188aabc06f3eee5d92892d95b1da5c3 10.104.0.14:6379@16379 myself,master - 0 1669764411000 3 connected 10923-16383\n7743661f60b6b17b5c71d083260419588b4f2451 10.104.0.16:6379@16379 slave 2ce30362c188aabc06f3eee5d92892d95b1da5c3 0 1669764410000 3 connected\n961f35e37c4eea507cfe12f96e3bfd694b9c21d4 10.104.0.18:6379@16379 slave a8765caed08f3e185cef22bd09edf409dc2bcc61 0 1669764411000 1 connected\n7136e37d8864db983f334b85d2b094be47c830e5 10.104.0.15:6379@16379 slave 2cff613d763b22c180cd40668da8e452edef3fc8 0 1669764412595 2 connected\na8765caed08f3e185cef22bd09edf409dc2bcc61 10.104.0.19:6379@16379 master - 0 1669764411592 1 connected 0-5460\n2cff613d763b22c180cd40668da8e452edef3fc8 10.104.0.17:6379@16379 master - 0 1669764410000 2 connected 5461-10922\n</code></pre></li>\n<li>\n<p>Deploy a Redis cluster with zero replicas in the destination cluster:</p>\n<pre tabindex=\"0\"><code>helm install redis --namespace kep-3335 \\\nbitnami/redis-cluster \\\n--set persistence.size=1Gi \\\n--set cluster.nodes=0 \\\n--set redis.extraEnvVars\\[0\\].name=REDIS_NODES,redis.extraEnvVars\\[0\\].value=&#34;redis-redis-cluster-headless.kep-3335.svc.cluster.local&#34; \\\n--set existingSecret=redis-redis-cluster\n</code></pre></li>\n<li>\n<p>Scale down the <code>redis-redis-cluster</code> StatefulSet in the source cluster by 1,\nto remove the replica <code>redis-redis-cluster-5</code>:</p>\n<pre tabindex=\"0\"><code>kubectl patch sts redis-redis-cluster -p &#39;{&#34;spec&#34;: {&#34;replicas&#34;: 5}}&#39;\n</code></pre></li>\n<li>\n<p>Migrate dependencies from the source cluster to the destination cluster:</p>\n<p>The following commands copy resources from <code>source</code> to <code>destionation</code>. Details\nthat are not relevant in <code>destination</code> cluster are removed (eg: <code>uid</code>,\n<code>resourceVersion</code>, <code>status</code>).</p>\n<p><strong>Steps for the source cluster</strong></p>\n<p>Note: If using a <code>StorageClass</code> with <code>reclaimPolicy: Delete</code> configured, you\nshould patch the PVs in <code>source</code> with <code>reclaimPolicy: Retain</code> prior to\ndeletion to retain the underlying storage used in <code>destination</code>. See\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/\">Change the Reclaim Policy of a PersistentVolume</a>\nfor more details.</p>\n<pre tabindex=\"0\"><code>kubectl get pvc redis-data-redis-redis-cluster-5 -o yaml | yq &#39;del(.metadata.uid, .metadata.resourceVersion, .metadata.annotations, .metadata.finalizers, .status)&#39; &gt; /tmp/pvc-redis-data-redis-redis-cluster-5.yaml\nkubectl get pv $(yq &#39;.spec.volumeName&#39; /tmp/pvc-redis-data-redis-redis-cluster-5.yaml) -o yaml | yq &#39;del(.metadata.uid, .metadata.resourceVersion, .metadata.annotations, .metadata.finalizers, .spec.claimRef, .status)&#39; &gt; /tmp/pv-redis-data-redis-redis-cluster-5.yaml\nkubectl get secret redis-redis-cluster -o yaml | yq &#39;del(.metadata.uid, .metadata.resourceVersion)&#39; &gt; /tmp/secret-redis-redis-cluster.yaml\n</code></pre><p><strong>Steps for the destination cluster</strong></p>\n<p>Note: For the PV/PVC, this procedure only works if the underlying storage system\nthat your PVs use can support being copied into <code>destination</code>. Storage\nthat is associated with a specific node or topology may not be supported.\nAdditionally, some storage systems may store addtional metadata about\nvolumes outside of a PV object, and may require a more specialized\nsequence to import a volume.</p>\n<pre tabindex=\"0\"><code>kubectl create -f /tmp/pv-redis-data-redis-redis-cluster-5.yaml\nkubectl create -f /tmp/pvc-redis-data-redis-redis-cluster-5.yaml\nkubectl create -f /tmp/secret-redis-redis-cluster.yaml\n</code></pre></li>\n<li>\n<p>Scale up the <code>redis-redis-cluster</code> StatefulSet in the destination cluster by\n1, with a start ordinal of 5:</p>\n<pre tabindex=\"0\"><code>kubectl patch sts redis-redis-cluster -p &#39;{&#34;spec&#34;: {&#34;ordinals&#34;: {&#34;start&#34;: 5}, &#34;replicas&#34;: 1}}&#39;\n</code></pre></li>\n<li>\n<p>Check the replication status in the destination cluster:</p>\n<pre tabindex=\"0\"><code>kubectl exec -it redis-redis-cluster-5 -- /bin/bash -c \\\n&#34;redis-cli -c -h redis-redis-cluster -a $(kubectl get secret redis-redis-cluster -o jsonpath=&#34;{.data.redis-password}&#34; | base64 -d) CLUSTER NODES;&#34;\n</code></pre><p>I should see that the new replica (labeled <code>myself</code>) has joined the Redis\ncluster (the IP address belongs to a different CIDR block than the\nreplicas in the source cluster).</p>\n<pre tabindex=\"0\"><code>2cff613d763b22c180cd40668da8e452edef3fc8 10.104.0.17:6379@16379 master - 0 1669766684000 2 connected 5461-10922\n7136e37d8864db983f334b85d2b094be47c830e5 10.108.0.22:6379@16379 myself,slave 2cff613d763b22c180cd40668da8e452edef3fc8 0 1669766685609 2 connected\n2ce30362c188aabc06f3eee5d92892d95b1da5c3 10.104.0.14:6379@16379 master - 0 1669766684000 3 connected 10923-16383\n961f35e37c4eea507cfe12f96e3bfd694b9c21d4 10.104.0.18:6379@16379 slave a8765caed08f3e185cef22bd09edf409dc2bcc61 0 1669766683600 1 connected\na8765caed08f3e185cef22bd09edf409dc2bcc61 10.104.0.19:6379@16379 master - 0 1669766685000 1 connected 0-5460\n7743661f60b6b17b5c71d083260419588b4f2451 10.104.0.16:6379@16379 slave 2ce30362c188aabc06f3eee5d92892d95b1da5c3 0 1669766686613 3 connected\n</code></pre></li>\n<li>\n<p>Repeat steps #5 to #7 for the remainder of the replicas, until the\nRedis StatefulSet in the source cluster is scaled to 0, and the Redis\nStatefulSet in the destination cluster is healthy with 6 total replicas.</p>\n</li>\n</ol>\n<h2 id=\"what-s-next\">What's Next?</h2>\n<p>This feature provides a building block for a StatefulSet to be split up across\nclusters, but does not prescribe the mechanism as to how the StatefulSet should\nbe migrated. Migration requires coordination of StatefulSet replicas, along with\norchestration of the storage and network layer. This is dependent on the storage\nand connectivity requirements of the application installed by the StatefulSet.\nAdditionally, many StatefulSets are managed by\n<a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">operators</a>, which adds another\nlayer of complexity to migration.</p>\n<p>If you're interested in building enhancements to make these processes easier,\nget involved with\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-multicluster\">SIG Multicluster</a>\nto contribute!</p>","PublishedAt":"2023-04-28 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/28/statefulset-start-ordinal/","SourceName":"Kubernetes"}},{"node":{"ID":3485,"Title":"Blog: Updates to the Auto-refreshing Official CVE Feed","Description":"<p><strong>Authors</strong>: Cailyn Edwards (Shopify), Mah√© Tardy (Isovalent), Pushkar Joglekar</p>\n<p>Since launching the <a href=\"https://kubernetes.io/docs/reference/issues-security/official-cve-feed/\">Auto-refreshing Official CVE feed</a> as an alpha\nfeature in the 1.25 release, we have made significant improvements and updates. We are excited to announce the release of the\nbeta version of the feed. This blog post will outline the feedback received, the changes made, and talk about how you can help\nas we prepare to make this a stable feature in a future Kubernetes Release.</p>\n<h2 id=\"feedback-from-end-users\">Feedback from end-users</h2>\n<p>SIG Security received some feedback from end-users:</p>\n<ul>\n<li>The JSON CVE Feed <a href=\"https://github.com/kubernetes/website/issues/36808\">did not comply</a>\nwith the <a href=\"https://www.jsonfeed.org/\">JSON Feed specification</a> as its name would suggest.</li>\n<li>The feed could also <a href=\"https://github.com/kubernetes/sig-security/issues/77\">support RSS</a>\nin addition to JSON Feed format.</li>\n<li>Some metadata could be <a href=\"https://github.com/kubernetes/sig-security/issues/72\">added</a> to indicate the freshness of\nthe feed overall, or <a href=\"https://github.com/kubernetes/sig-security/issues/63\">specific CVEs</a>. Another suggestion was\nto <a href=\"https://github.com/kubernetes/sig-security/issues/71\">indicate</a> which Prow job recently updated the feed. See\nmore ideas directly on the <a href=\"https://github.com/kubernetes/sig-security/issues/1\">the umbrella issue</a>.</li>\n<li>The feed Markdown table on the website <a href=\"https://github.com/kubernetes/sig-security/issues/73\">should be ordered</a>\nfrom the most recent to the least recently announced CVE.</li>\n</ul>\n<h2 id=\"summary-of-changes\">Summary of changes</h2>\n<p>In response, the SIG did a <a href=\"https://github.com/kubernetes/sig-security/pull/76\">rework of the script generating the JSON feed</a>\nto comply with the JSON Feed specification from generation and add a\n<code>last_updated</code> root field to indicate overall freshness. This redesign needed a\n<a href=\"https://github.com/kubernetes/website/pull/38579\">corresponding fix on the Kubernetes website side</a>\nfor the CVE feed page to continue to work with the new format.</p>\n<p>After that, <a href=\"https://github.com/kubernetes/website/pull/39513\">RSS feed support</a>\ncould be added transparently so that end-users can consume the feed in their\npreferred format.</p>\n<p>Overall, the redesign based on the JSON Feed specification, which this time broke\nbackward compatibility, will allow updates in the future to address the rest of\nthe issue while being more transparent and less disruptive to end-users.</p>\n<h3 id=\"updates\">Updates</h3>\n<table>\n<thead>\n<tr>\n<th><strong>Title</strong></th>\n<th><strong>Issue</strong></th>\n<th><strong>Status</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CVE Feed: JSON feed should pass jsonfeed spec validator</td>\n<td><a href=\"https://github.com/kubernetes/website/issues/36808\">kubernetes/webite#36808</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/sig-security/pull/76\">kubernetes/sig-security#76</a></td>\n</tr>\n<tr>\n<td>CVE Feed: Add lastUpdatedAt as a metadata field</td>\n<td><a href=\"https://github.com/kubernetes/sig-security/issues/72\">kubernetes/sig-security#72</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/sig-security/pull/76\">kubernetes/sig-security#76</a></td>\n</tr>\n<tr>\n<td>Support RSS feeds by generating data in Atom format</td>\n<td><a href=\"https://github.com/kubernetes/sig-security/issues/77\">kubernetes/sig-security#77</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/website/pull/39513\">kubernetes/website#39513</a></td>\n</tr>\n<tr>\n<td>CVE Feed: Sort Markdown Table from most recent to least recently announced CVE</td>\n<td><a href=\"https://github.com/kubernetes/sig-security/issues/73\">kubernetes/sig-security#73</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/sig-security/pull/76\">kubernetes/sig-security#76</a></td>\n</tr>\n<tr>\n<td>CVE Feed: Include a timestamp field for each CVE indicating when it was last updated</td>\n<td><a href=\"https://github.com/kubernetes/sig-security/issues/63\">kubernetes/sig-security#63</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/sig-security/pull/76\">kubernetes/sig-security#76</a></td>\n</tr>\n<tr>\n<td>CVE Feed: Add Prow job link as a metadata field</td>\n<td><a href=\"https://github.com/kubernetes/sig-security/issues/71\">kubernetes/sig-security#71</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/sig-security/pull/83\">kubernetes/sig-security#83</a></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>In preparation to <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages\">graduate</a> the feed\nto stable i.e. <code>General Availability</code> stage, SIG Security is still gathering feedback from end users who are using the updated beta feed.</p>\n<p>To help us continue to improve the feed in future Kubernetes Releases please share feedback by adding a comment to\nthis <a href=\"https://github.com/kubernetes/sig-security/issues/1\">tracking issue</a> or\nlet us know on <a href=\"https://kubernetes.slack.com/archives/C01CUSVMHPY\">#sig-security-tooling</a>\nKubernetes Slack channel, join <a href=\"https://slack.k8s.io\">Kubernetes Slack here</a>.</p>","PublishedAt":"2023-04-25 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/25/k8s-cve-feed-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3475,"Title":"Blog: Kubernetes 1.27: Server Side Field Validation and OpenAPI V3 move to GA","Description":"<p><strong>Author</strong>: Jeffrey Ying (Google), Antoine Pelisse (Google)</p>\n<p>Before Kubernetes v1.8 (!), typos, mis-indentations or minor errors in\nYAMLs could have catastrophic consequences (e.g. a typo like\nforgetting the trailing s in <code>replica: 1000</code> could cause an outage,\nbecause the value would be ignored and missing, forcing a reset of\nreplicas back to 1). This was solved back then by fetching the OpenAPI\nv2 in kubectl and using it to verify that fields were correct and\npresent before applying. Unfortunately, at that time, Custom Resource\nDefinitions didn‚Äôt exist, and the code was written under that\nassumption. When CRDs were later introduced, the lack of flexibility\nin the validation code forced some hard decisions in the way CRDs\nexposed their schema, leaving us in a cycle of bad validation causing\nbad OpenAPI and vice-versa. With the new OpenAPI v3 and Server Field\nValidation being GA in 1.27, we‚Äôve now solved both of these problems.</p>\n<p>Server Side Field Validation offers resource validation on create,\nupdate and patch requests to the apiserver and was added to Kubernetes\nin v1.25, beta in v1.26 and is now GA in v1.27. It provides all the\nfunctionality of kubectl validate on the server side.</p>\n<p><a href=\"https://swagger.io/specification/\">OpenAPI</a> is a standard, language\nagnostic interface for discovering the set of operations and types\nthat a Kubernetes cluster supports. OpenAPI V3 is the latest standard\nof the OpenAPI and is an improvement upon <a href=\"https://kubernetes.io/blog/2016/12/kubernetes-supports-openapi/\">OpenAPI\nV2</a>\nwhich has been supported since Kubernetes 1.5. OpenAPI V3 support was\nadded in Kubernetes in v1.23, moved to beta in v1.24 and is now GA in\nv1.27.</p>\n<h2 id=\"openapi-v3\">OpenAPI V3</h2>\n<h3 id=\"what-does-openapi-v3-offer-over-v2\">What does OpenAPI V3 offer over V2</h3>\n<h4 id=\"built-in-types\">Built-in types</h4>\n<p>Kubernetes offers certain annotations on fields that are not\nrepresentable in OpenAPI V2, or sometimes not represented in the\nOpenAPI v2 that Kubernetes generate. Most notably, the &quot;default&quot; field\nis published in OpenAPI V3 while omitted in OpenAPI V2. A single type\nthat can represent multiple types is also expressed correctly in\nOpenAPI V3 with the oneOf field. This includes proper representations\nfor IntOrString and Quantity.</p>\n<h4 id=\"custom-resource-definitions\">Custom Resource Definitions</h4>\n<p>In Kubernetes, Custom Resource Definitions use a structural OpenAPI V3\nschema that cannot be represented as OpenAPI V2 without a loss of\ncertain fields. Some of these include nullable, default, anyOf, oneOf,\nnot, etc. OpenAPI V3 is a completely lossless representation of the\nCustomResourceDefinition structural schema.</p>\n<h3 id=\"how-do-i-use-it\">How do I use it?</h3>\n<p>The OpenAPI V3 root discovery can be found at the <code>/openapi/v3</code>\nendpoint of a Kubernetes API server. OpenAPI V3 documents are grouped\nby group-version to reduce the size of the data transported, the\nseparate documents can be accessed at\n<code>/openapi/v3/apis/&lt;group&gt;/&lt;version&gt;</code> and <code>/openapi/v3/api/v1</code>\nrepresenting the legacy group version. Please refer to the <a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\">Kubernetes\nAPI Documentation</a> for more\ninformation around this endpoint.</p>\n<p>Various consumers of the OpenAPI have already been updated to consume\nv3, including the entirety of kubectl, and server side apply. An\nOpenAPI V3 Golang client is available in\n<a href=\"https://github.com/kubernetes/client-go/blob/release-1.27/openapi3/root.go\">client-go</a>.</p>\n<h2 id=\"server-side-field-validation\">Server Side Field Validation</h2>\n<p>The query parameter <code>fieldValidation</code> may be used to indicate the\nlevel of field validation the server should perform. If the parameter\nis not passed, server side field validation is in <code>Warn</code> mode by\ndefault.</p>\n<ul>\n<li>Strict: Strict field validation, errors on validation failure</li>\n<li>Warn: Field validation is performed, but errors are exposed as\nwarnings rather than failing the request</li>\n<li>Ignore: No server side field validation is performed</li>\n</ul>\n<p>kubectl will skip client side validation and will automatically use\nserver side field validation in <code>Strict</code> mode. Controllers by default\nuse server side field validation in <code>Warn</code> mode.</p>\n<p>With client side validation, we had to be extra lenient because some\nfields were missing from OpenAPI V2 and we didn‚Äôt want to reject\npossibly valid objects. This is all fixed in server side validation.\nAdditional documentation may be found\n<a href=\"https://kubernetes.io/docs/reference/using-api/api-concepts/#field-validation\">here</a></p>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>With Server Side Field Validation and OpenAPI V3 released as GA, we\nintroduce more accurate representations of Kubernetes resources. It is\nrecommended to use server side field validation over client side, but\nwith OpenAPI V3, clients are free to implement their own validation if\nnecessary (to ‚Äúshift things left‚Äù) and we guarantee a full lossless\nschema published by OpenAPI.</p>\n<p>Some existing efforts will further improve the information available\nthrough OpenAPI including <a href=\"https://kubernetes.io/docs/reference/using-api/cel/\">CEL validation and\nadmission</a>, along with OpenAPI\nannotations on built-in types.</p>\n<p>Many other tools can be built for authoring and transforming resources\nusing the type information found in the OpenAPI v3.</p>\n<h2 id=\"how-to-get-involved\">How to get involved?</h2>\n<p>These two features are driven by the SIG API Machinery community,\navailable on the slack channel #sig-api-machinery, through the\n<a href=\"https://groups.google.com/g/kubernetes-sig-api-machinery\">mailing\nlist</a> and we\nmeet every other Wednesday at 11:00 AM PT on Zoom.</p>\n<p>We offer a huge thanks to all the contributors who helped design,\nimplement, and review these two features.</p>\n<ul>\n<li>Alexander Zielenski</li>\n<li>Antoine Pelisse</li>\n<li>Daniel Smith</li>\n<li>David Eads</li>\n<li>Jeffrey Ying</li>\n<li>Jordan Liggitt</li>\n<li>Kevin Delgado</li>\n<li>Sean Sullivan</li>\n</ul>","PublishedAt":"2023-04-24 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/24/openapi-v3-field-validation-ga/","SourceName":"Kubernetes"}},{"node":{"ID":3467,"Title":"Blog: Kubernetes 1.27: Query Node Logs Using The Kubelet API","Description":"<p><strong>Author:</strong> Aravindh Puthiyaparambil (Red Hat)</p>\n<p>Kubernetes 1.27 introduced a new feature called <em>Node log query</em> that allows\nviewing logs of services running on the node.</p>\n<h2 id=\"what-problem-does-it-solve\">What problem does it solve?</h2>\n<p>Cluster administrators face issues when debugging malfunctioning services\nrunning on the node. They usually have to SSH or RDP into the node to view the\nlogs of the service to debug the issue. The <em>Node log query</em> feature helps with\nthis scenario by allowing the cluster administrator to view the logs using\n<em>kubectl</em>. This is especially useful with Windows nodes where you run into the\nissue of the node going to the ready state but containers not coming up due to\nCNI misconfigurations and other issues that are not easily identifiable by\nlooking at the Pod status.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>The kubelet already has a <em>/var/log/</em> viewer that is accessible via the node\nproxy endpoint. The feature supplements this endpoint with a shim that shells\nout to <code>journalctl</code>, on Linux nodes, and the <code>Get-WinEvent</code> cmdlet on Windows\nnodes. It then uses the existing filters provided by the commands to allow\nfiltering the logs. The kubelet also uses heuristics to retrieve the logs.\nIf the user is not aware if a given system services logs to a file or to the\nnative system logger, the heuristics first checks the native operating system\nlogger and if that is not available it attempts to retrieve the first logs\nfrom <code>/var/log/&lt;servicename&gt;</code> or <code>/var/log/&lt;servicename&gt;.log</code> or\n<code>/var/log/&lt;servicename&gt;/&lt;servicename&gt;.log</code>.</p>\n<p>On Linux we assume that service logs are available via journald, and that\n<code>journalctl</code> is installed. On Windows we assume that service logs are available\nin the application log provider. Also note that fetching node logs is only\navailable if you are authorized to do so (in RBAC, that's <strong>get</strong> and\n<strong>create</strong> access to <code>nodes/proxy</code>). The privileges that you need to fetch node\nlogs also allow elevation-of-privilege attacks, so be careful about how you\nmanage them.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>To use the feature, ensure that the <code>NodeLogQuery</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> is\nenabled for that node, and that the kubelet configuration options\n<code>enableSystemLogHandler</code> and <code>enableSystemLogQuery</code> are both set to true. You can\nthen query the logs from all your nodes or just a subset. Here is an example to\nretrieve the kubelet service logs from a node:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Fetch kubelet logs from a node named node-1.example</span>\n</span></span><span style=\"display:flex;\"><span>kubectl get --raw <span style=\"color:#b44\">&#34;/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&#34;</span>\n</span></span></code></pre></div><p>You can further filter the query to narrow down the results:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Fetch kubelet logs from a node named node-1.example that have the word &#34;error&#34;</span>\n</span></span><span style=\"display:flex;\"><span>kubectl get --raw <span style=\"color:#b44\">&#34;/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&amp;pattern=error&#34;</span>\n</span></span></code></pre></div><p>You can also fetch files from <code>/var/log/</code> on a Linux node:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl get --raw <span style=\"color:#b44\">&#34;/api/v1/nodes/&lt;insert-node-name-here&gt;/proxy/logs/?query=/&lt;insert-log-file-name-here&gt;&#34;</span>\n</span></span></code></pre></div><p>You can read the\n<a href=\"https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#log-query\">documentation</a>\nfor all the available options.</p>\n<h2 id=\"how-do-i-help\">How do I help?</h2>\n<p>Please use the feature and provide feedback by opening GitHub issues or\nreaching out to us on the\n<a href=\"https://kubernetes.slack.com/archives/C0SJ4AFB7\">#sig-windows</a> channel on the\nKubernetes Slack or the SIG Windows\n<a href=\"https://groups.google.com/g/kubernetes-sig-windows\">mailing list</a>.</p>","PublishedAt":"2023-04-21 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/21/node-log-query-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":3456,"Title":"Blog: Kubernetes 1.27: Single Pod Access Mode for PersistentVolumes Graduates to Beta","Description":"<p><strong>Author:</strong> Chris Henzie (Google)</p>\n<p>With the release of Kubernetes v1.27 the ReadWriteOncePod feature has graduated\nto beta. In this blog post, we'll take a closer look at this feature, what it\ndoes, and how it has evolved in the beta release.</p>\n<h2 id=\"what-is-readwriteoncepod\">What is ReadWriteOncePod?</h2>\n<p>ReadWriteOncePod is a new access mode for\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes\">PersistentVolumes</a> (PVs)\nand <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims\">PersistentVolumeClaims</a> (PVCs)\nintroduced in Kubernetes v1.22. This access mode enables you to restrict volume\naccess to a single pod in the cluster, ensuring that only one pod can write to\nthe volume at a time. This can be particularly useful for stateful workloads\nthat require single-writer access to storage.</p>\n<p>For more context on access modes and how ReadWriteOncePod works read\n<a href=\"https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/#what-are-access-modes-and-why-are-they-important\">What are access modes and why are they important?</a>\nin the <em>Introducing Single Pod Access Mode for PersistentVolumes</em> article from 2021.</p>\n<h2 id=\"changes-in-the-readwriteoncepod-beta\">Changes in the ReadWriteOncePod beta</h2>\n<p>The ReadWriteOncePod beta adds support for\n<a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/\">scheduler preemption</a>\nof pods using ReadWriteOncePod PVCs.</p>\n<p>Scheduler preemption allows higher-priority pods to preempt lower-priority pods,\nso that they can start running on the same node. With this release, pods using\nReadWriteOncePod PVCs can also be preempted if a higher-priority pod requires\nthe same PVC.</p>\n<h2 id=\"how-can-i-start-using-readwriteoncepod\">How can I start using ReadWriteOncePod?</h2>\n<p>With ReadWriteOncePod now in beta, it will be enabled by default in cluster\nversions v1.27 and beyond.</p>\n<p>Note that ReadWriteOncePod is\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\">only supported for CSI volumes</a>.\nBefore using this feature you will need to update the following\n<a href=\"https://kubernetes-csi.github.io/docs/sidecar-containers.html\">CSI sidecars</a>\nto these versions or greater:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0\">csi-provisioner:v3.0.0+</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0\">csi-attacher:v3.3.0+</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0\">csi-resizer:v1.3.0+</a></li>\n</ul>\n<p>To start using ReadWriteOncePod, create a PVC with the ReadWriteOncePod access mode:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>single-writer-only<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOncePod<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># Allow only a single pod to access single-writer-only.</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>If your storage plugin supports\n<a href=\"https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/\">dynamic provisioning</a>,\nnew PersistentVolumes will be created with the ReadWriteOncePod access mode applied.</p>\n<p>Read <a href=\"https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/#migrating-existing-persistentvolumes\">Migrating existing PersistentVolumes</a>\nfor details on migrating existing volumes to use ReadWriteOncePod.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>Please see the <a href=\"https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha\">alpha blog post</a>\nand <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md\">KEP-2485</a>\nfor more details on the ReadWriteOncePod access mode and motivations for CSI spec changes.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>The <a href=\"https://kubernetes.slack.com/messages/csi\">Kubernetes #csi Slack channel</a>\nand any of the standard\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact\">SIG Storage communication channels</a>\nare great mediums to reach out to the SIG Storage and the CSI teams.</p>\n<p>Special thanks to the following people whose thoughtful reviews and feedback helped shape this feature:</p>\n<ul>\n<li>Abdullah Gharaibeh (ahg-g)</li>\n<li>Aldo Culquicondor (alculquicondor)</li>\n<li>Antonio Ojea (aojea)</li>\n<li>David Eads (deads2k)</li>\n<li>Jan ≈†afr√°nek (jsafrane)</li>\n<li>Joe Betz (jpbetz)</li>\n<li>Kante Yin (kerthcet)</li>\n<li>Michelle Au (msau42)</li>\n<li>Tim Bannister (sftim)</li>\n<li>Xing Yang (xing-yang)</li>\n</ul>\n<p>If you‚Äôre interested in getting involved with the design and development of CSI\nor any part of the Kubernetes storage system, join the\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group</a> (SIG).\nWe‚Äôre rapidly growing and always welcome new contributors.</p>","PublishedAt":"2023-04-20 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/20/read-write-once-pod-access-mode-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3444,"Title":"Blog: Kubernetes 1.27: Efficient SELinux volume relabeling (Beta)","Description":"<p><strong>Author:</strong> Jan ≈†afr√°nek (Red Hat)</p>\n<h1 id=\"the-problem\">The problem</h1>\n<p>On Linux with Security-Enhanced Linux (SELinux) enabled, it's traditionally\nthe container runtime that applies SELinux labels to a Pod and all its volumes.\nKubernetes only passes the SELinux label from a Pod's <code>securityContext</code> fields\nto the container runtime.</p>\n<p>The container runtime then recursively changes SELinux label on all files that\nare visible to the Pod's containers. This can be time-consuming if there are\nmany files on the volume, especially when the volume is on a remote filesystem.</p>\n<div class=\"alert alert-info\" role=\"alert\">\n<h4 class=\"alert-heading\">Note</h4>\nIf a container uses <code>subPath</code> of a volume, only that <code>subPath</code> of the whole\nvolume is relabeled. This allows two pods that have two different SELinux labels\nto use the same volume, as long as they use different subpaths of it.\n</div>\n<p>If a Pod does not have any SELinux label assigned in Kubernetes API, the\ncontainer runtime assigns a unique random one, so a process that potentially\nescapes the container boundary cannot access data of any other container on the\nhost. The container runtime still recursively relabels all pod volumes with this\nrandom SELinux label.</p>\n<h1 id=\"improvement-using-mount-options\">Improvement using mount options</h1>\n<p>If a Pod and its volume meet <strong>all</strong> of the following conditions, Kubernetes will\n<em>mount</em> the volume directly with the right SELinux label. Such mount will happen\nin a constant time and the container runtime will not need to recursively\nrelabel any files on it.</p>\n<ol>\n<li>\n<p>The operating system must support SELinux.</p>\n<p>Without SELinux support detected, kubelet and the container runtime do not\ndo anything with regard to SELinux.</p>\n</li>\n<li>\n<p>The <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gates</a>\n<code>ReadWriteOncePod</code> and <code>SELinuxMountReadWriteOncePod</code> must be enabled.\nThese feature gates are Beta in Kubernetes 1.27 and Alpha in 1.25.</p>\n<p>With any of these feature gates disabled, SELinux labels will be always\napplied by the container runtime by a recursive walk through the volume\n(or its subPaths).</p>\n</li>\n<li>\n<p>The Pod must have at least <code>seLinuxOptions.level</code> assigned in its <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context\">Pod Security Context</a> or all Pod containers must have it set in their <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1\">Security Contexts</a>.\nKubernetes will read the default <code>user</code>, <code>role</code> and <code>type</code> from the operating\nsystem defaults (typically <code>system_u</code>, <code>system_r</code> and <code>container_t</code>).</p>\n<p>Without Kubernetes knowing at least the SELinux <code>level</code>, the container\nruntime will assign a random one <em>after</em> the volumes are mounted. The\ncontainer runtime will still relabel the volumes recursively in that case.</p>\n</li>\n<li>\n<p>The volume must be a Persistent Volume with\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\">Access Mode</a>\n<code>ReadWriteOncePod</code>.</p>\n<p>This is a limitation of the initial implementation. As described above,\ntwo Pods can have a different SELinux label and still use the same volume,\nas long as they use a different <code>subPath</code> of it. This use case is not\npossible when the volumes are <em>mounted</em> with the SELinux label, because the\nwhole volume is mounted and most filesystems don't support mounting a single\nvolume multiple times with multiple SELinux labels.</p>\n<p>If running two Pods with two different SELinux contexts and using\ndifferent <code>subPaths</code> of the same volume is necessary in your deployments,\nplease comment in the <a href=\"https://github.com/kubernetes/enhancements/issues/1710\">KEP</a>\nissue (or upvote any existing comment - it's best not to duplicate).\nSuch pods may not run when the feature is extended to cover all volume access modes.</p>\n</li>\n<li>\n<p>The volume plugin or the CSI driver responsible for the volume supports\nmounting with SELinux mount options.</p>\n<p>These in-tree volume plugins support mounting with SELinux mount options:\n<code>fc</code>, <code>iscsi</code>, and <code>rbd</code>.</p>\n<p>CSI drivers that support mounting with SELinux mount options must announce\nthat in their\n<a href=\"https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/\">CSIDriver</a>\ninstance by setting <code>seLinuxMount</code> field.</p>\n<p>Volumes managed by other volume plugins or CSI drivers that don't\nset <code>seLinuxMount: true</code> will be recursively relabelled by the container\nruntime.</p>\n</li>\n</ol>\n<h2 id=\"mounting-with-selinux-context\">Mounting with SELinux context</h2>\n<p>When all aforementioned conditions are met, kubelet will\npass <code>-o context=&lt;SELinux label&gt;</code> mount option to the volume plugin or CSI\ndriver. CSI driver vendors must ensure that this mount option is supported\nby their CSI driver and, if necessary, the CSI driver appends other mount\noptions that are needed for <code>-o context</code> to work.</p>\n<p>For example, NFS may need <code>-o context=&lt;SELinux label&gt;,nosharecache</code>, so each\nvolume mounted from the same NFS server can have a different SELinux label\nvalue. Similarly, CIFS may need <code>-o context=&lt;SELinux label&gt;,nosharesock</code>.</p>\n<p>It's up to the CSI driver vendor to test their CSI driver in a SELinux enabled\nenvironment before setting <code>seLinuxMount: true</code> in the CSIDriver instance.</p>\n<h1 id=\"how-can-i-learn-more\">How can I learn more?</h1>\n<p>SELinux in containers: see excellent\n<a href=\"https://opensource.com/business/13/11/selinux-policy-guide\">visual SELinux guide</a>\nby Daniel J Walsh. Note that the guide is older than Kubernetes, it describes\n<em>Multi-Category Security</em> (MCS) mode using virtual machines as an example,\nhowever, a similar concept is used for containers.</p>\n<p>See a series of blog posts for details how exactly SELinux is applied to\ncontainers by container runtimes:</p>\n<ul>\n<li><a href=\"https://www.redhat.com/en/blog/how-selinux-separates-containers-using-multi-level-security\">How SELinux separates containers using Multi-Level Security</a></li>\n<li><a href=\"https://www.redhat.com/en/blog/why-you-should-be-using-multi-category-security-your-linux-containers\">Why you should be using Multi-Category Security for your Linux containers</a></li>\n</ul>\n<p>Read the KEP: <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1710-selinux-relabeling\">Speed up SELinux volume relabeling using mounts</a></p>","PublishedAt":"2023-04-18 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/18/kubernetes-1-27-efficient-selinux-relabeling-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3419,"Title":"Blog: Kubernetes 1.27: More fine-grained pod topology spread policies reached beta","Description":"<p><strong>Authors:</strong> <a href=\"https://github.com/denkensk\">Alex Wang</a> (Shopee), <a href=\"https://github.com/kerthcet\">Kante Yin</a> (DaoCloud), <a href=\"https://github.com/sanposhiho\">Kensei Nakada</a> (Mercari)</p>\n<p>In Kubernetes v1.19, <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\">Pod topology spread constraints</a>\nwent to general availability (GA).</p>\n<p>As time passed, we - SIG Scheduling - received feedback from users,\nand, as a result, we're actively working on improving the Topology Spread feature via three KEPs.\nAll of these features have reached beta in Kubernetes v1.27 and are enabled by default.</p>\n<p>This blog post introduces each feature and the use case behind each of them.</p>\n<h2 id=\"kep-3022-min-domains-in-pod-topology-spread\">KEP-3022: min domains in Pod Topology Spread</h2>\n<p>Pod Topology Spread has the <code>maxSkew</code> parameter to define the degree to which Pods may be unevenly distributed.</p>\n<p>But, there wasn't a way to control the number of domains over which we should spread.\nSome users want to force spreading Pods over a minimum number of domains, and if there aren't enough already present, make the cluster-autoscaler provision them.</p>\n<p>Kubernetes v1.24 introduced the <code>minDomains</code> parameter for pod topology spread constraints,\nas an alpha feature.\nVia <code>minDomains</code> parameter, you can define the minimum number of domains.</p>\n<p>For example, assume there are 3 Nodes with the enough capacity,\nand a newly created ReplicaSet has the following <code>topologySpreadConstraints</code> in its Pod template.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">...</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">topologySpreadConstraints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>- <span style=\"color:#008000;font-weight:bold\">maxSkew</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">minDomains</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5</span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># requires 5 Nodes at least (because each Node has a unique hostname).</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">whenUnsatisfiable</span>:<span style=\"color:#bbb\"> </span>DoNotSchedule<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># minDomains is valid only when DoNotSchedule is used.</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">topologyKey</span>:<span style=\"color:#bbb\"> </span>kubernetes.io/hostname<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labelSelector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">foo</span>:<span style=\"color:#bbb\"> </span>bar<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>In this case, 3 Pods will be scheduled to those 3 Nodes,\nbut other 2 Pods from this replicaset will be unschedulable until more Nodes join the cluster.</p>\n<p>You can imagine that the cluster autoscaler provisions new Nodes based on these unschedulable Pods,\nand as a result, the replicas are finally spread over 5 Nodes.</p>\n<h2 id=\"kep-3094-take-taints-tolerations-into-consideration-when-calculating-podtopologyspread-skew\">KEP-3094: Take taints/tolerations into consideration when calculating podTopologySpread skew</h2>\n<p>Before this enhancement, when you deploy a pod with <code>podTopologySpread</code> configured, kube-scheduler would\ntake the Nodes that satisfy the Pod's nodeAffinity and nodeSelector into consideration\nin filtering and scoring, but would not care about whether the node taints are tolerated by the incoming pod or not.\nThis may lead to a node with untolerated taint as the only candidate for spreading, and as a result,\nthe pod will stuck in Pending if it doesn't tolerate the taint.</p>\n<p>To allow more fine-gained decisions about which Nodes to account for when calculating spreading skew,\nKubernetes 1.25 introduced two new fields within <code>topologySpreadConstraints</code> to define node inclusion policies:\n<code>nodeAffinityPolicy</code> and <code>nodeTaintPolicy</code>.</p>\n<p>A manifest that applies these policies looks like the following:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># Configure a topology spread constraint</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">topologySpreadConstraints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">maxSkew</span>:<span style=\"color:#bbb\"> </span>&lt;integer&gt;<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># ...</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodeAffinityPolicy</span>:<span style=\"color:#bbb\"> </span>[Honor|Ignore]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodeTaintsPolicy</span>:<span style=\"color:#bbb\"> </span>[Honor|Ignore]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># other Pod fields go here</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The <code>nodeAffinityPolicy</code> field indicates how Kubernetes treats a Pod's <code>nodeAffinity</code> or <code>nodeSelector</code> for\npod topology spreading.\nIf <code>Honor</code>, kube-scheduler filters out nodes not matching <code>nodeAffinity</code>/<code>nodeSelector</code> in the calculation of\nspreading skew.\nIf <code>Ignore</code>, all nodes will be included, regardless of whether they match the Pod's <code>nodeAffinity</code>/<code>nodeSelector</code>\nor not.</p>\n<p>For backwards compatibility, <code>nodeAffinityPolicy</code> defaults to <code>Honor</code>.</p>\n<p>The <code>nodeTaintsPolicy</code> field defines how Kubernetes considers node taints for pod topology spreading.\nIf <code>Honor</code>, only tainted nodes for which the incoming pod has a toleration, will be included in the calculation of spreading skew.\nIf <code>Ignore</code>, kube-scheduler will not consider the node taints at all in the calculation of spreading skew, so a node with\npod untolerated taint will also be included.</p>\n<p>For backwards compatibility, <code>nodeTaintsPolicy</code> defaults to <code>Ignore</code>.</p>\n<p>The feature was introduced in v1.25 as alpha. By default, it was disabled, so if you want to use this feature in v1.25,\nyou had to explictly enable the feature gate <code>NodeInclusionPolicyInPodTopologySpread</code>. In the following v1.26\nrelease, that associated feature graduated to beta and is enabled by default.</p>\n<h2 id=\"kep-3243-respect-pod-topology-spread-after-rolling-upgrades\">KEP-3243: Respect Pod topology spread after rolling upgrades</h2>\n<p>Pod Topology Spread uses the field <code>labelSelector</code> to identify the group of pods over which\nspreading will be calculated. When using topology spreading with Deployments, it is common\npractice to use the <code>labelSelector</code> of the Deployment as the <code>labelSelector</code> in the topology\nspread constraints. However, this implies that all pods of a Deployment are part of the spreading\ncalculation, regardless of whether they belong to different revisions. As a result, when a new revision\nis rolled out, spreading will apply across pods from both the old and new ReplicaSets, and so by the\ntime the new ReplicaSet is completely rolled out and the old one is rolled back, the actual spreading\nwe are left with may not match expectations because the deleted pods from the older ReplicaSet will cause\nskewed distribution for the remaining pods. To avoid this problem, in the past users needed to add a\nrevision label to Deployment and update it manually at each rolling upgrade (both the label on the\npod template and the <code>labelSelector</code> in the <code>topologySpreadConstraints</code>).</p>\n<p>To solve this problem with a simpler API, Kubernetes v1.25 introduced a new field named\n<code>matchLabelKeys</code> to <code>topologySpreadConstraints</code>. <code>matchLabelKeys</code> is a list of pod label keys to select\nthe pods over which spreading will be calculated. The keys are used to lookup values from the labels of\nthe Pod being scheduled, those key-value labels are ANDed with <code>labelSelector</code> to select the group of\nexisting pods over which spreading will be calculated for the incoming pod.</p>\n<p>With <code>matchLabelKeys</code>, you don't need to update the <code>pod.spec</code> between different revisions.\nThe controller or operator managing rollouts just needs to set different values to the same label key for different revisions.\nThe scheduler will assume the values automatically based on <code>matchLabelKeys</code>.\nFor example, if you are configuring a Deployment, you can use the label keyed with\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pod-template-hash-label\">pod-template-hash</a>,\nwhich is added automatically by the Deployment controller, to distinguish between different\nrevisions in a single Deployment.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">topologySpreadConstraints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">maxSkew</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">topologyKey</span>:<span style=\"color:#bbb\"> </span>kubernetes.io/hostname<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">whenUnsatisfiable</span>:<span style=\"color:#bbb\"> </span>DoNotSchedule<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labelSelector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">app</span>:<span style=\"color:#bbb\"> </span>foo<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabelKeys</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- pod-template-hash<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"getting-involved\">Getting involved</h2>\n<p>These features are managed by Kubernetes <a href=\"https://github.com/kubernetes/community/tree/master/sig-scheduling\">SIG Scheduling</a>.</p>\n<p>Please join us and share your feedback. We look forward to hearing from you!</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\">Pod Topology Spread Constraints</a> in the Kubernetes documentation</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/3022-min-domains-in-pod-topology-spread\">KEP-3022: min domains in Pod Topology Spread</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/3094-pod-topology-spread-considering-taints\">KEP-3094: Take taints/tolerations into consideration when calculating PodTopologySpread skew</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/3243-respect-pod-topology-spread-after-rolling-upgrades\">KEP-3243: Respect PodTopologySpread after rolling upgrades</a></li>\n</ul>","PublishedAt":"2023-04-17 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/17/fine-grained-pod-topology-spread-features-beta/","SourceName":"Kubernetes"}},{"node":{"ID":3386,"Title":"Blog: Kubernetes v1.27: Chill Vibes","Description":"<p><strong>Authors</strong>: <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.27/release-team.md\">Kubernetes v1.27 Release Team</a></p>\n<p>Announcing the release of Kubernetes v1.27, the first release of 2023!</p>\n<p>This release consist of 60 enhancements. 18 of those enhancements are entering Alpha, 29 are graduating to Beta, and 13 are graduating to Stable.</p>\n<h2 id=\"release-theme-and-logo\">Release theme and logo</h2>\n<p><strong>Kubernetes v1.27: Chill Vibes</strong></p>\n<p>The theme for Kubernetes v1.27 is <em>Chill Vibes</em>.</p>\n<figure class=\"release-logo\">\n<img src=\"https://kubernetes.io/images/blog/2023-04-11-kubernetes-1.27-blog/kubernetes-1.27.png\"\nalt=\"Kubernetes 1.27 Chill Vibes logo\"/>\n</figure>\n<p>It's a little silly, but there were some important shifts in this release that helped inspire the theme. Throughout a typical Kubernetes release cycle, there are several deadlines that features need to meet to remain included. If a feature misses any of these deadlines, there is an exception process they can go through. Handling these exceptions is a very normal part of the release. But v1.27 is the first release that anyone can remember where we didn't receive a single exception request after the enhancements freeze. Even as the release progressed, things remained much calmer than any of us are used to.</p>\n<p>There's a specific reason we were able to enjoy a more calm release this time around, and that's all the work that folks put in behind the scenes to improve how we manage the release. That's what this theme celebrates, people putting in the work to make things better for the community.</p>\n<p>Special thanks to <a href=\"https://www.instagram.com/artsyfie/\">Britnee Laverack</a> for creating the logo. Britnee also designed the logo for <a href=\"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#release-theme-and-logo\">Kubernetes 1.24: Stargazer</a>.</p>\n<h1 id=\"what-s-new-major-themes\">What's New (Major Themes)</h1>\n<h2 id=\"freeze-k8s-gcr-io-image-registry\">Freeze <code>k8s.gcr.io</code> image registry</h2>\n<p>Replacing the old image registry, <a href=\"https://cloud.google.com/container-registry/\">k8s.gcr.io</a> with <a href=\"https://github.com/kubernetes/registry.k8s.io\">registry.k8s.io</a> which has been generally available for several months. The Kubernetes project created and runs the <code>registry.k8s.io</code> image registry, which is fully controlled by the community.\nThis means that the old registry <code>k8s.gcr.io</code> will be frozen and no further images for Kubernetes and related sub-projects will be published to the old registry.</p>\n<p>What does this change mean for contributors?</p>\n<ul>\n<li>If you are a maintainer of a sub-project, you will need to update your manifests and Helm charts to use the new registry. For more information, checkout¬†this <a href=\"https://github.com/kubernetes-sigs/community-images\">project</a>.</li>\n</ul>\n<p>What does this change mean for end users?</p>\n<ul>\n<li>\n<p>Kubernetes <code>v1.27</code> release will not be published to the <code>k8s.gcr.io</code> registry.</p>\n</li>\n<li>\n<p>Patch releases for <code>v1.24</code>, <code>v1.25</code>, and <code>v1.26</code> will no longer be published to the old registry after April.</p>\n</li>\n<li>\n<p>Starting in v1.25, the default image registry has been set to <code>registry.k8s.io</code>. This value is overridable in kubeadm and kubelet but setting it to <code>k8s.gcr.io</code> will fail for new releases after April as they won‚Äôt be present in the old registry.</p>\n</li>\n<li>\n<p>If you want to increase the reliability of your cluster and remove dependency on the community-owned registry or you are running Kubernetes in networks where external traffic is restricted, you should consider hosting local image registry mirrors. Some cloud vendors may offer hosted solutions for this.</p>\n</li>\n</ul>\n<h2 id=\"seccompdefault-graduates-to-stable\"><code>SeccompDefault</code> graduates to stable</h2>\n<p>To use seccomp profile defaulting, you must run the kubelet with the <code>--seccomp-default</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet\">command line flag</a> enabled for each node where you want to use it.\nIf enabled, the kubelet will use the <code>RuntimeDefault</code> seccomp profile by default, which is defined by the container runtime, instead of using the <code>Unconfined</code> (seccomp disabled) mode. The default profiles aim to provide a strong set of security defaults while preserving the functionality of the workload. It is possible that the default profiles differ between container runtimes and their release versions.</p>\n<p>You can find detailed information about a possible upgrade and downgrade strategy in the related Kubernetes Enhancement Proposal (KEP): <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2413-seccomp-by-default\">Enable seccomp by default</a>.</p>\n<h2 id=\"mutable-scheduling-directives-for-jobs-graduates-to-ga\">Mutable scheduling directives for Jobs graduates to GA</h2>\n<p>This was introduced in v1.22 and started as a beta level, now it's stable. In most cases a parallel job will want the pods to run with constraints, like all in the same zone, or all either on GPU model x or y but not a mix of both. The <code>suspend</code> field is the first step towards achieving those semantics. <code>suspend</code> allows a custom queue controller to decide when a job should start. However, once a job is unsuspended, a custom queue controller has no influence on where the pods of a job will actually land.</p>\n<p>This feature allows updating a Job's scheduling directives before it starts, which gives custom queue controllers\nthe ability to influence pod placement while at the same time offloading actual pod-to-node assignment to\nkube-scheduler. This is allowed only for suspended Jobs that have never been unsuspended before.\nThe fields in a Job's pod template that can be updated are node affinity, node selector, tolerations, labels\n,annotations, and <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/\">scheduling gates</a>.\nFind more details in the KEP:\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/2926-job-mutable-scheduling-directives\">Allow updating scheduling directives of jobs</a>.</p>\n<h2 id=\"downwardapihugepages-graduates-to-stable\">DownwardAPIHugePages graduates to stable</h2>\n<p>In Kubernetes v1.20, support for <code>requests.hugepages-&lt;pagesize&gt;</code> and <code>limits.hugepages-&lt;pagesize&gt;</code> was added\nto the <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/\">downward API</a> to be consistent with other resources like cpu, memory, and ephemeral storage.\nThis feature graduates to stable in this release. You can find more details in the KEP:\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2053-downward-api-hugepages\">Downward API HugePages</a>.</p>\n<h2 id=\"pod-scheduling-readiness-goes-to-beta\">Pod Scheduling Readiness goes to beta</h2>\n<p>Upon creation, Pods are ready for scheduling. Kubernetes scheduler does its due diligence to find nodes to place all pending Pods. However, in a real-world case, some Pods may stay in a <em>missing-essential-resources</em> state for a long period. These Pods actually churn the scheduler (and downstream integrators like Cluster Autoscaler) in an unnecessary manner.</p>\n<p>By specifying/removing a Pod's <code>.spec.schedulingGates</code>, you can control when a Pod is ready to be considered for scheduling.</p>\n<p>The <code>schedulingGates</code> field contains a list of strings, and each string literal is perceived as a criteria that must be satisfied before a Pod is considered schedulable. This field can be initialized only when a Pod is created (either by the client, or mutated during admission). After creation, each schedulingGate can be removed in an arbitrary order, but addition of a new scheduling gate is disallowed.</p>\n<h2 id=\"node-log-access-via-kubernetes-api\">Node log access via Kubernetes API</h2>\n<p>This feature helps cluster administrators debug issues with services running on nodes by allowing them to query service logs. To use this feature, ensure that the <code>NodeLogQuery</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> is enabled on that node, and that the kubelet configuration options <code>enableSystemLogHandler</code> and <code>enableSystemLogQuery</code> are both set to true.\nOn Linux, we assume that service logs are available via journald. On Windows, we assume that service logs are available in the application log provider. You can also fetch logs from the <code>/var/log/</code> and <code>C:\\var\\log</code> directories on Linux and Windows, respectively.</p>\n<p>A cluster administrator can try out this alpha feature across all nodes of their cluster, or on a subset of them.</p>\n<h2 id=\"readwriteoncepod-persistentvolume-access-mode-goes-to-beta\">ReadWriteOncePod PersistentVolume access mode goes to beta</h2>\n<p>Kuberentes <code>v1.22</code> introduced a new access mode <code>ReadWriteOncePod</code> for <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes\">PersistentVolumes</a> (PVs) and <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims\">PersistentVolumeClaims</a> (PVCs). This access mode enables you to restrict volume access to a single pod in the cluster, ensuring that only one pod can write to the volume at a time. This can be particularly useful for stateful workloads that require single-writer access to storage.</p>\n<p>The ReadWriteOncePod beta adds support for <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/\">scheduler preemption</a>\nof pods that use ReadWriteOncePod PVCs.\nScheduler preemption allows higher-priority pods to preempt lower-priority pods. For example when a pod (A) with a <code>ReadWriteOncePod</code> PVC is scheduled, if another pod (B) is found using the same PVC and pod (A) has higher priority, the scheduler will return an <code>Unschedulable</code> status and attempt to preempt pod (B).\nFor more context, see the KEP: <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode\">ReadWriteOncePod PersistentVolume AccessMode</a>.</p>\n<h2 id=\"respect-podtopologyspread-after-rolling-upgrades\">Respect PodTopologySpread after rolling upgrades</h2>\n<p><code>matchLabelKeys</code> is a list of pod label keys used to select the pods over which spreading will be calculated. The keys are used to lookup values from the pod labels. Those key-value labels are ANDed with <code>labelSelector</code> to select the group of existing pods over which spreading will be calculated for the incoming pod. Keys that don't exist in the pod labels will be ignored. A null or empty list means only match against the <code>labelSelector</code>.</p>\n<p>With <code>matchLabelKeys</code>, users don't need to update the <code>pod.spec</code> between different revisions. The controller/operator just needs to set different values to the same <code>label</code> key for different revisions. The scheduler will assume the values automatically based on <code>matchLabelKeys</code>. For example, if users use Deployment, they can use the label keyed with <code>pod-template-hash</code>, which is added automatically by the Deployment controller, to distinguish between different revisions in a single Deployment.</p>\n<h2 id=\"faster-selinux-volume-relabeling-using-mounts\">Faster SELinux volume relabeling using mounts</h2>\n<p>In this release, how SELinux labels are applied to volumes used by Pods is graduating to beta. This feature speeds up container startup by mounting volumes with the correct SELinux label instead of changing each file on the volumes recursively. Linux kernel with SELinux support allows the first mount of a volume to set SELinux label on the whole volume using <code>-o context=</code> mount option. This way, all files will have assigned the given label in a constant time, without recursively walking through the whole volumes.</p>\n<p>The <code>context</code> mount option cannot be applied to bind mounts or re-mounts of already mounted volumes.\nFor CSI storage, a CSI driver does the first mount of a volume, and so it must be the CSI driver that actually\napplies this mount option. We added a new field <code>SELinuxMount</code> to CSIDriver objects, so that drivers can\nannounce whether they support the <code>-o context</code> mount option.</p>\n<p>If Kubernetes knows the SELinux label of a Pod <strong>and</strong> the CSI driver responsible for a pod's volume\nannounces <code>SELinuxMount: true</code> <strong>and</strong> the volume has Access Mode <code>ReadWriteOncePod</code>, then it\nwill ask the CSI driver to mount the volume with mount option <code>context=</code> <strong>and</strong> it will tell the container\nruntime not to relabel content of the volume (because all files already have the right label).\nGet more information on this from the KEP: <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1710-selinux-relabeling\">Speed up SELinux volume relabeling using mounts</a>.</p>\n<h2 id=\"robust-volumemanager-reconstruction-goes-to-beta\">Robust VolumeManager reconstruction goes to beta</h2>\n<p>This is a volume manager refactoring that allows the kubelet to populate additional information about how\nexisting volumes are mounted during the kubelet startup. In general, this makes volume cleanup more robust.\nIf you enable the <code>NewVolumeManagerReconstruction</code> feature gate on a node, you'll get enhanced discovery of mounted volumes during kubelet startup.</p>\n<p>Before Kubernetes v1.25, the kubelet used different default behavior for discovering mounted volumes during the kubelet startup. If you disable this feature gate (it's enabled by default), you select the legacy discovery behavior.</p>\n<p>In Kubernetes v1.25 and v1.26, this behavior toggle was part of the <code>SELinuxMountReadWriteOncePod</code> feature gate.</p>\n<h2 id=\"mutable-pod-scheduling-directives-goes-to-beta\">Mutable Pod Scheduling Directives goes to beta</h2>\n<p>This allows mutating a pod that is blocked on a scheduling readiness gate with a more constrained node affinity/selector. It gives the ability to mutate a pods scheduling directives before it is allowed to be scheduled and gives an external resource controller the ability to influence pod placement while at the same time offload actual pod-to-node assignment to kube-scheduler.</p>\n<p>This opens the door for a new pattern of adding scheduling features to Kubernetes. Specifically, building lightweight schedulers that implement features not supported by kube-scheduler, while relying on the existing kube-scheduler to support all upstream features and handle the pod-to-node binding. This pattern should be the preferred one if the custom feature doesn't require implementing a schedule plugin, which entails re-building and maintaining a custom kube-scheduler binary.</p>\n<h2 id=\"feature-graduations-and-deprecations-in-kubernetes-v1-27\">Feature graduations and deprecations in Kubernetes v1.27</h2>\n<h3 id=\"graduations-to-stable\">Graduations to stable</h3>\n<p>This release includes a total of 9 enhancements promoted to Stable:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2227\">Default container annotation that to be used by kubectl</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3140\">TimeZone support in CronJob</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1748\">Expose metrics about resource requests and limits that represent the pod model</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2885\">Server Side Unknown Field Validation</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/693\">Node Topology Manager</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2727\">Add gRPC probe to Pod.Spec.Container.{Liveness,Readiness,Startup} Probe</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2238\">Add configurable grace period to probes</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2896\">OpenAPI v3</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3744\">Stay on supported Go versions</a></li>\n</ul>\n<h3 id=\"deprecations-and-removals\">Deprecations and removals</h3>\n<p>This release saw several removals:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/108445\">Removal of <code>storage.k8s.io/v1beta1</code> from CSIStorageCapacity</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/114947\">Removal of support for deprecated seccomp annotations</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/112797\">Removal of <code>--master-service-namespace</code> command line argument</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/113534\">Removal of the <code>ControllerManagerLeaderMigration</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/111411\">Removal of <code>--enable-taint-manager</code> command line argument</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/113710\">Removal of <code>--pod-eviction-timeout</code> command line argument</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/110410\">Removal of the <code>CSI Migration</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/111258\">Removal of <code>CSIInlineVolume</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/111402\">Removal of <code>EphemeralContainers</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/111513\">Removal of <code>LocalStorageCapacityIsolation</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/110868\">Removal of <code>NetworkPolicyEndPort</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/110896\">Removal of <code>StatefulSetMinReadySeconds</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/111229\">Removal of <code>IdentifyPodOS</code> feature gate</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/111194\">Removal of <code>DaemonSetUpdateSurge</code> feature gate</a></li>\n</ul>\n<h2 id=\"release-notes\">Release notes</h2>\n<p>The complete details of the Kubernetes v1.27 release are available in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md\">release notes</a>.</p>\n<h2 id=\"availability\">Availability</h2>\n<p>Kubernetes v1.27 is available for download on <a href=\"https://github.com/kubernetes/kubernetes/releases/tag/v1.27.0\">GitHub</a>. To get started with Kubernetes, you can run local Kubernetes clusters using <a href=\"https://minikube.sigs.k8s.io/docs/\">minikube</a>, <a href=\"https://kind.sigs.k8s.io/\">kind</a>, etc. You can also easily install v1.27 using <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h2 id=\"release-team\">Release team</h2>\n<p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires people with specialised skills from all corners of our community, from the code itself to its documentation and project management.</p>\n<p>Special thanks to our Release Lead Xander Grzywinski for guiding us through a smooth and successful release cycle and to all members of the release team for supporting one another and working so hard to produce the v1.27 release for the community.</p>\n<h2 id=\"ecosystem-updates\">Ecosystem updates</h2>\n<ul>\n<li>KubeCon + CloudNativeCon Europe 2023 will take place in Amsterdam, The Netherlands, from 17 ‚Äì 21 April 2023! You can find more information about the conference and registration on the <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/\">event site</a>.</li>\n<li>cdCon + GitOpsCon will be held in Vancouver, Canada, on May 8th and 9th, 2023! More information about the conference and registration can be found on the <a href=\"https://events.linuxfoundation.org/cdcon-gitopscon/\">event site</a>.</li>\n</ul>\n<h2 id=\"project-velocity\">Project velocity</h2>\n<p>The <a href=\"https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m\">CNCF K8s DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>\n<p>In the v1.27 release cycle, which <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.27\">ran for 14 weeks</a> (January 9 to April 11), we saw contributions from <a href=\"https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.26.0%20-%20now&amp;var-metric=contributions\">1020 companies</a> and <a href=\"https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.26.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-repo_name=kubernetes%2Fkubernetes&amp;var-country_name=All&amp;var-companies=All\">1603 individuals</a>.</p>\n<h2 id=\"upcoming-release-webinar\">Upcoming release webinar</h2>\n<p>Join members of the Kubernetes v1.27 release team on Friday, April 14, 2023, at 10 a.m. PDT to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the <a href=\"https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v127-release/\">event page</a> on the CNCF Online Programs site.</p>\n<h2 id=\"get-involved\">Get Involved</h2>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests.</p>\n<p>Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below:</p>\n<ul>\n<li>\n<p>Find out more about contributing to Kubernetes at the <a href=\"https://www.kubernetes.dev/\">Kubernetes Contributors website</a>.</p>\n</li>\n<li>\n<p>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for the latest updates.</p>\n</li>\n<li>\n<p>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a>.</p>\n</li>\n<li>\n<p>Join the community on <a href=\"https://communityinviter.com/apps/kubernetes/community\">Slack</a>.</p>\n</li>\n<li>\n<p>Post questions (or answer questions) on <a href=\"https://serverfault.com/questions/tagged/kubernetes\">Server Fault</a>.</p>\n</li>\n<li>\n<p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">Share</a> your Kubernetes story.</p>\n</li>\n<li>\n<p>Read more about what‚Äôs happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a>.</p>\n</li>\n<li>\n<p>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a>.</p>\n</li>\n</ul>","PublishedAt":"2023-04-11 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/11/kubernetes-v1-27-release/","SourceName":"Kubernetes"}},{"node":{"ID":3354,"Title":"Blog: Keeping Kubernetes Secure with Updated Go Versions","Description":"<p><strong>Author</strong>: <a href=\"https://github.com/liggitt\">Jordan Liggitt</a> (Google)</p>\n<h3 id=\"the-problem\">The problem</h3>\n<p>Since v1.19 (released in 2020), the Kubernetes project provides 12-14 months of patch releases for each minor version.\nThis enables users to qualify and adopt Kubernetes versions in an annual upgrade cycle and receive security fixes for a year.</p>\n<p>The <a href=\"https://github.com/golang/go/wiki/Go-Release-Cycle#release-maintenance\">Go project</a> releases new minor versions twice a year,\nand provides security fixes for the last two minor versions, resulting in about a year of support for each Go version.\nEven though each new Kubernetes minor version is built with a supported Go version when it is first released,\nthat Go version falls out of support before the Kubernetes minor version does,\nand the lengthened Kubernetes patch support since v1.19 only widened that gap.</p>\n<p>At the time this was written, just over half of all <a href=\"https://go.dev/doc/devel/release\">Go patch releases</a> (88/171) have contained fixes for issues with possible security implications.\nEven though many of these issues were not relevant to Kubernetes, some were, so it remained important to use supported Go versions that received those fixes.</p>\n<p>An obvious solution would be to simply update Kubernetes release branches to new minor versions of Go.\nHowever, Kubernetes avoids <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-release/cherry-picks.md#what-kind-of-prs-are-good-for-cherry-picks\">destabilizing changes in patch releases</a>,\nand historically, this prevented updating existing release branches to new minor versions of Go, due to changes that were considered prohibitively complex, risky, or breaking to include in a patch release.\nExamples include:</p>\n<ul>\n<li>Go 1.6: enabling http/2 by default</li>\n<li>Go 1.14: EINTR handling issues</li>\n<li>Go 1.17: dropping x509 CN support, ParseIP changes</li>\n<li>Go 1.18: disabling x509 SHA-1 certificate support by default</li>\n<li>Go 1.19: dropping current-dir LookPath behavior</li>\n</ul>\n<p>Some of these changes could be easily mitigated in Kubernetes code,\nsome could only be opted out of via a user-specified <code>GODEBUG</code> envvar,\nand others required invasive code changes or could not be avoided at all.\nBecause of this inconsistency, Kubernetes release branches have typically remained on a single Go minor version,\nand risked being unable to pick up relevant Go security fixes for the last several months of each Kubernetes minor version's support lifetime.</p>\n<p>When a relevant Go security fix was only available in newer Kubernetes minor versions,\nusers would have to upgrade away from older Kubernetes minor versions before their 12-14 month support period ended, just to pick up those fixes.\nIf a user was not prepared to do that upgrade, it could result in vulnerable Kubernetes clusters.\nEven if a user could accommodate the unexpected upgrade, the uncertainty made Kubernetes' annual support less reliable for planning.</p>\n<h3 id=\"the-solution\">The solution</h3>\n<p>We're happy to announce that the gap between supported Kubernetes versions and supported Go versions has been resolved as of January 2023.</p>\n<p>We worked closely with the Go team over the past year to address the difficulties adopting new Go versions.\nThis prompted a <a href=\"https://github.com/golang/go/discussions/55090\">discussion</a>, <a href=\"https://github.com/golang/go/issues/56986\">proposal</a>,\n<a href=\"https://www.youtube.com/watch?v=v24wrd3RwGo\">talk at GopherCon</a>, and a <a href=\"https://go.dev/design/56986-godebug\">design</a> for improving backward compatibility in Go,\nensuring new Go versions can maintain compatible runtime behavior with previous Go versions for a minimum of two years (four Go releases).\nThis allows projects like Kubernetes to update release branches to supported Go versions without exposing users to behavior changes.</p>\n<p>The proposed improvements are on track to be <a href=\"https://tip.golang.org/doc/godebug\">included in Go 1.21</a>, and the Go team already delivered targeted compatibility improvements in a Go 1.19 patch release in late 2022.\nThose changes enabled Kubernetes 1.23+ to update to Go 1.19 in January of 2023, while avoiding any user-facing configuration or behavior changes.\nAll supported Kubernetes release branches now use supported Go versions, and can pick up new Go patch releases with available security fixes.</p>\n<p>Going forward, Kubernetes maintainers remain committed to making Kubernetes patch releases as safe and non-disruptive as possible,\nso there are several requirements a new Go minor version must meet before existing Kubernetes release branches will update to use it:</p>\n<ol>\n<li>The new Go version must be available for at least 3 months.\nThis gives time for adoption by the Go community, and for reports of issues or regressions.</li>\n<li>The new Go version must be used in a new Kubernetes minor release for at least 1 month.\nThis ensures all Kubernetes release-blocking tests pass on the new Go version,\nand gives time for feedback from the Kubernetes community on release candidates and early adoption of the new minor release.</li>\n<li>There must be no regressions from the previous Go version known to impact Kubernetes.</li>\n<li>Runtime behavior must be preserved by default, without requiring any action by Kubernetes users / administrators.</li>\n<li>Kubernetes libraries like <code>k8s.io/client-go</code> must remain compatible with the original Go version used for each minor release,\nso consumers won't <em>have</em> to update Go versions to pick up a library patch release (though they are encouraged to build with supported Go versions,\nwhich is made even easier with the <a href=\"https://go.dev/design/56986-godebug\">compatibility improvements</a> planned in Go 1.21).</li>\n</ol>\n<p>The goal of all of this work is to unobtrusively make Kubernetes patch releases safer and more secure,\nand to make Kubernetes minor versions safe to use for the entire duration of their support lifetime.</p>\n<p>Many thanks to the Go team, especially Russ Cox, for helping drive these improvements in ways that will benefit all Go users, not just Kubernetes.</p>","PublishedAt":"2023-04-06 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/04/06/keeping-kubernetes-secure-with-updated-go-versions/","SourceName":"Kubernetes"}},{"node":{"ID":3294,"Title":"Blog: Kubernetes Validating Admission Policies: A Practical Example","Description":"<p><strong>Authors</strong>: Craig Box (ARMO), Ben Hirschberg (ARMO)</p>\n<p>Admission control is an important part of the Kubernetes control plane, with several internal\nfeatures depending on the ability to approve or change an API object as it is submitted to the\nserver. It is also useful for an administrator to be able to define business logic, or policies,\nregarding what objects can be admitted into a cluster. To better support that use case, <a href=\"https://kubernetes.io/blog/2017/06/kubernetes-1-7-security-hardening-stateful-application-extensibility-updates/\">Kubernetes\nintroduced external admission control in\nv1.7</a>.</p>\n<p>In addition to countless custom, internal implementations, many open source projects and commercial\nsolutions implement admission controllers with user-specified policy, including\n<a href=\"https://github.com/kyverno/kyverno\">Kyverno</a> and Open Policy Agent‚Äôs\n<a href=\"https://github.com/open-policy-agent/gatekeeper\">Gatekeeper</a>.</p>\n<p>While admission controllers for policy have seen adoption, there are blockers for their widespread\nuse. Webhook infrastructure must be maintained as a production service, with all that entails. The\nfailure case of an admission control webhook must either be closed, reducing the availability of the\ncluster; or open, negating the use of the feature for policy enforcement. The network hop and\nevaluation time makes admission control a notable component of latency when dealing with, for\nexample, pods being spun up to respond to a network request in a &quot;serverless&quot; environment.</p>\n<h2 id=\"validating-admission-policies-and-the-common-expression-language\">Validating admission policies and the Common Expression Language</h2>\n<p>Version 1.26 of Kubernetes introduced, in alpha, a compromise solution. <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/\">Validating admission\npolicies</a> are a declarative,\nin-process alternative to admission webhooks. They use the <a href=\"https://github.com/google/cel-spec\">Common Expression\nLanguage</a> (CEL) to declare validation rules.</p>\n<p>CEL was developed by Google for security and policy use cases, based on learnings from the Firebase\nreal-time database. Its design allows it to be safely embedded into applications and executed in\nmicroseconds, with limited compute and memory impact. <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">Validation rules for\nCRDs</a>\nintroduced CEL to the Kubernetes ecosystem in v1.23, and at the time it was noted that the language\nwould suit a more generic implementation of validation by admission control.</p>\n<h2 id=\"giving-cel-a-roll-a-practical-example\">Giving CEL a roll - a practical example</h2>\n<p><a href=\"https://github.com/kubescape/kubescape\">Kubescape</a> is a CNCF project which has become one of the\nmost popular ways for users to improve the security posture of a Kubernetes cluster and validate its\ncompliance. Its <a href=\"https://github.com/kubescape/regolibrary\">controls</a> ‚Äî groups of tests against API\nobjects ‚Äî are built in <a href=\"https://www.openpolicyagent.org/docs/latest/policy-language/\">Rego</a>, the\npolicy language of Open Policy Agent.</p>\n<p>Rego has a reputation for complexity, based largely on the fact that it is a declarative query\nlanguage (like SQL). It <a href=\"https://github.com/kubernetes/enhancements/blob/499e28/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md#alternatives\">was\nconsidered</a>\nfor use in Kubernetes, but it does not offer the same sandbox constraints as CEL.</p>\n<p>A common feature request for the project is to be able to implement policies based on Kubescape‚Äôs\nfindings and output. For example, after scanning pods for <a href=\"https://hub.armosec.io/docs/c-0020\">known paths to cloud credential\nfiles</a>, users would like the ability to enforce policy that\nthese pods should not be admitted at all. The Kubescape team thought this would be the perfect\nopportunity to try and port our existing controls to CEL and apply them as admission policies.</p>\n<h3 id=\"show-me-the-policy\">Show me the policy</h3>\n<p>It did not take us long to convert many of our controls and build a <a href=\"https://github.com/kubescape/cel-admission-library\">library of validating admission\npolicies</a>. Let‚Äôs look at one as an example.</p>\n<p>Kubescape‚Äôs <a href=\"https://hub.armosec.io/docs/c-0017\">control C-0017</a> covers the requirement for\ncontainers to have an immutable (read-only) root filesystem. This is a best practice according to\nthe <a href=\"https://kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#immutable-container-filesystems\">NSA Kubernetes hardening\nguidelines</a>,\nbut is not currently required as a part of any of the <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">pod security\nstandards</a>.</p>\n<p>Here's how we implemented it in CEL:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingAdmissionPolicy<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;kubescape-c-0017-deny-resources-with-mutable-container-filesystem&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">failurePolicy</span>:<span style=\"color:#bbb\"> </span>Fail<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchConstraints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceRules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CREATE&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;UPDATE&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;pods&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;apps&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CREATE&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;UPDATE&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;deployments&#34;</span>,<span style=\"color:#b44\">&#34;replicasets&#34;</span>,<span style=\"color:#b44\">&#34;daemonsets&#34;</span>,<span style=\"color:#b44\">&#34;statefulsets&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;batch&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CREATE&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;UPDATE&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;jobs&#34;</span>,<span style=\"color:#b44\">&#34;cronjobs&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">expression</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;object.kind != &#39;Pod&#39; || object.spec.containers.all(container, has(container.securityContext) &amp;&amp; has(container.securityContext.readOnlyRootFilesystem) &amp;&amp; container.securityContext.readOnlyRootFilesystem == true)&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;Pods having containers with mutable filesystem not allowed! (see more at https://hub.armosec.io/docs/c-0017)&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">expression</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;[&#39;Deployment&#39;,&#39;ReplicaSet&#39;,&#39;DaemonSet&#39;,&#39;StatefulSet&#39;,&#39;Job&#39;].all(kind, object.kind != kind) || object.spec.template.spec.containers.all(container, has(container.securityContext) &amp;&amp; has(container.securityContext.readOnlyRootFilesystem) &amp;&amp; container.securityContext.readOnlyRootFilesystem == true)&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;Workloads having containers with mutable filesystem not allowed! (see more at https://hub.armosec.io/docs/c-0017)&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">expression</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;object.kind != &#39;CronJob&#39; || object.spec.jobTemplate.spec.template.spec.containers.all(container, has(container.securityContext) &amp;&amp; has(container.securityContext.readOnlyRootFilesystem) &amp;&amp; container.securityContext.readOnlyRootFilesystem == true)&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;CronJob having containers with mutable filesystem not allowed! (see more at https://hub.armosec.io/docs/c-0017)&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Match constraints are provided for three possible API groups: the <code>core/v1</code> group for Pods, the\n<code>apps/v1</code> workload controllers, and the <code>batch/v1</code> job controllers.</p>\n<div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> <code>matchConstraints</code> will convert the API object to the matched version for you. If, for\nexample, an API request was for <code>apps/v1beta1</code> and you match <code>apps/v1</code> in matchConstraints, the API\nrequest will be converted from <code>apps/v1beta1</code> to <code>apps/v1</code> and then validated. This has the useful\nproperty of making validation rules secure against the introduction of new versions of APIs, which\nwould otherwise allow API requests to sneak past the validation rule by using the newly introduced\nversion.\n</div>\n<p>The <code>validations</code> include the CEL rules for the objects. There are three different expressions,\ncatering for the fact that a Pod <code>spec</code> can be at the root of the object (a <a href=\"https://kubernetes.io/docs/concepts/configuration/overview/#naked-pods-vs-replicasets-deployments-and-jobs\">naked\npod</a>),\nunder <code>template</code> (a workload controller or a Job), or under <code>jobTemplate</code> (a CronJob).</p>\n<p>In the event that any <code>spec</code> does not have <code>readOnlyRootFilesystem</code> set to true, the object will not\nbe admitted.</p>\n<div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> In our initial release, we have grouped the three expressions into the same policy\nobject. This means they can be enabled and disabled atomically, and thus there is no chance that a\nuser will accidentally leave a compliance gap by enabling policy for one API group and not the\nothers. Breaking them into separate policies would allow us access to improvements targeted for the\n1.27 release, including type checking. We are talking to SIG API Machinery about how to best address\nthis before the APIs reach <code>v1</code>.\n</div>\n<h3 id=\"using-the-cel-library-in-your-cluster\">Using the CEL library in your cluster</h3>\n<p>Policies are provided as Kubernetes objects, which are then bound to certain resources by a\n<a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\">selector</a>.</p>\n<p><a href=\"https://minikube.sigs.k8s.io/docs/\">Minikube</a> is a quick and easy way to install and configure a\nKubernetes cluster for testing. To install Kubernetes v1.26 with the <code>ValidatingAdmissionPolicy</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> enabled:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>minikube start --kubernetes-version<span style=\"color:#666\">=</span>1.26.1 --extra-config<span style=\"color:#666\">=</span>apiserver.runtime-config<span style=\"color:#666\">=</span>admissionregistration.k8s.io/v1alpha1 --feature-gates<span style=\"color:#666\">=</span><span style=\"color:#b44\">&#39;ValidatingAdmissionPolicy=true&#39;</span>\n</span></span></code></pre></div><p>To install the policies in your cluster:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Install configuration CRD</span>\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f https://github.com/kubescape/cel-admission-library/releases/latest/download/policy-configuration-definition.yaml\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Install basic configuration</span>\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f https://github.com/kubescape/cel-admission-library/releases/latest/download/basic-control-configuration.yaml\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Install policies</span>\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f https://github.com/kubescape/cel-admission-library/releases/latest/download/kubescape-validating-admission-policies.yaml\n</span></span></code></pre></div><p>To apply policies to objects, create a <code>ValidatingAdmissionPolicyBinding</code> resource. Let‚Äôs apply the\nabove Kubescape C-0017 control to any namespace with the label <code>policy=enforced</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Create a binding</span>\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOT\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: admissionregistration.k8s.io/v1alpha1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ValidatingAdmissionPolicyBinding\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: c0017-binding\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> policyName: kubescape-c-0017-deny-mutable-container-filesystem\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> matchResources:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> namespaceSelector:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> matchLabels:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> policy: enforced\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOT</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Create a namespace for running the example</span>\n</span></span><span style=\"display:flex;\"><span>kubectl create namespace policy-example\n</span></span><span style=\"display:flex;\"><span>kubectl label namespace policy-example <span style=\"color:#b44\">&#39;policy=enforced&#39;</span>\n</span></span></code></pre></div><p>Now, if you attempt to create an object without specifying a <code>readOnlyRootFilesystem</code>, it will not\nbe created.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># The next line should fail</span>\n</span></span><span style=\"display:flex;\"><span>kubectl -n policy-example run nginx --image<span style=\"color:#666\">=</span>nginx --restart<span style=\"color:#666\">=</span>Never\n</span></span></code></pre></div><p>The output shows our error:</p>\n<pre tabindex=\"0\"><code>The pods &#34;nginx&#34; is invalid: : ValidatingAdmissionPolicy &#39;kubescape-c-0017-deny-mutable-container-filesystem&#39; with binding &#39;c0017-binding&#39; denied request: Pods having containers with mutable filesystem not allowed! (see more at https://hub.armosec.io/docs/c-0017)\n</code></pre><h3 id=\"configuration\">Configuration</h3>\n<p>Policy objects can include configuration, which is provided in a different object. Many of the\nKubescape controls require a configuration: which labels to require, which capabilities to allow or\ndeny, which registries to allow containers to be deployed from, etc. Default values for those\ncontrols are defined in <a href=\"https://github.com/kubescape/cel-admission-library/blob/main/configuration/basic-control-configuration.yaml\">the ControlConfiguration\nobject</a>.</p>\n<p>To use this configuration object, or your own object in the same format, add a <code>paramRef.name</code> value\nto your binding object:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingAdmissionPolicyBinding<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>c0001-binding<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">policyName</span>:<span style=\"color:#bbb\"> </span>kubescape-c-0001-deny-forbidden-container-registries<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">paramRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>basic-control-configuration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchResources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespaceSelector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">policy</span>:<span style=\"color:#bbb\"> </span>enforced<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"summary\">Summary</h2>\n<p>Converting our controls to CEL was simple, in most cases. We cannot port the whole Kubescape\nlibrary, as some controls check for things outside a Kubernetes cluster, and some require data that\nis not available in the admission request object. Overall, we are happy to contribute this library\nto the Kubernetes community and will continue to develop it for Kubescape and Kubernetes users\nalike. We hope it becomes useful, either as something you use yourself, or as examples for you to\nwrite your own policies.</p>\n<p>As for the validating admission policy feature itself, we are very excited to see this native\nfunctionality introduced to Kubernetes. We look forward to watching it move to Beta and then GA,\nhopefully by the end of the year. It is important to note this feature is currently in Alpha, which\nmeans this is the perfect opportunity to play around with it in environments like Minikube and give\na test drive. However, it is not yet considered production-ready and stable, and will not be enabled\non most managed Kubernetes environments. We will not recommend Kubescape users use these policies in\nproduction until the underlying functionality becomes stable. Keep an eye on <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/3488-cel-admission-control/README.md\">the\nKEP</a>,\nand of course this blog, for an eventual release announcement.</p>","PublishedAt":"2023-03-30 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/03/30/kubescape-validating-admission-policy-library/","SourceName":"Kubernetes"}},{"node":{"ID":3216,"Title":"Blog: Kubernetes Removals and Major Changes In v1.27","Description":"<p><strong>Author</strong>: Harshita Sao</p>\n<p>As Kubernetes develops and matures, features may be deprecated, removed, or replaced with better ones for the project's overall health. Based on the information available at this point in the v1.27 release process, which is still ongoing and can introduce additional changes, this article identifies and describes some of the planned changes for the Kubernetes v1.27 release.</p>\n<h2 id=\"a-note-about-the-k8s-gcr-io-redirect-to-registry-k8s-io\">A note about the k8s.gcr.io redirect to registry.k8s.io</h2>\n<p>To host its container images, the Kubernetes project uses a community-owned image registry called registry.k8s.io. <strong>On March 20th, all traffic from the out-of-date <a href=\"https://cloud.google.com/container-registry/\">k8s.gcr.io</a> registry will be redirected to <a href=\"https://github.com/kubernetes/registry.k8s.io\">registry.k8s.io</a></strong>. The deprecated k8s.gcr.io registry will eventually be phased out.</p>\n<h3 id=\"what-does-this-change-mean\">What does this change mean?</h3>\n<ul>\n<li>\n<p>If you are a subproject maintainer, you must update your manifests and Helm charts to use the new registry.</p>\n</li>\n<li>\n<p>The v1.27 Kubernetes release will not be published to the old registry.</p>\n</li>\n<li>\n<p>From April, patch releases for v1.24, v1.25, and v1.26 will no longer be published to the old registry.</p>\n</li>\n</ul>\n<p>We have a <a href=\"https://kubernetes.io/blog/2023/03/10/image-registry-redirect/\">blog post</a> with all the information about this change and what to do if it impacts you.</p>\n<h2 id=\"the-kubernetes-api-removal-and-deprecation-process\">The Kubernetes API Removal and Deprecation process</h2>\n<p>The Kubernetes project has a well-documented <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">deprecation policy</a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API has been marked for removal in a future Kubernetes release, it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</p>\n<ul>\n<li>\n<p>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</p>\n</li>\n<li>\n<p>Beta or pre-release API versions must be supported for 3 releases after the deprecation.</p>\n</li>\n<li>\n<p>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</p>\n</li>\n</ul>\n<p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.</p>\n<h2 id=\"api-removals-and-other-changes-for-kubernetes-v1-27\">API removals, and other changes for Kubernetes v1.27</h2>\n<h3 id=\"removal-of-storage-k8s-io-v1beta1-from-csistoragecapacity\">Removal of <code>storage.k8s.io/v1beta1</code> from <code>CSIStorageCapacity</code></h3>\n<p>The <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1/\">CSIStorageCapacity</a> API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances the scheduling of pods that use CSI volumes with late binding. The <code>storage.k8s.io/v1beta1</code> API version of CSIStorageCapacity was deprecated in v1.24, and it will no longer be served in v1.27.</p>\n<p>Migrate manifests and API clients to use the <code>storage.k8s.io/v1</code> API version, available since v1.24. All existing persisted objects are accessible via the new API.</p>\n<p>Refer to the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking\">Storage Capacity Constraints for Pod Scheduling KEP</a> for more information.</p>\n<p>Kubernetes v1.27 is not removing any other APIs; however several other aspects are going\nto be removed. Read on for details.</p>\n<h3 id=\"support-for-deprecated-seccomp-annotations\">Support for deprecated seccomp annotations</h3>\n<p>In Kubernetes v1.19, the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/135-seccomp\">seccomp</a> (secure computing mode) support graduated to General Availability (GA). This feature can be used to increase the workload security by restricting the system calls for a Pod (applies to all containers) or single containers.</p>\n<p>The support for the alpha seccomp annotations <code>seccomp.security.alpha.kubernetes.io/pod</code> and <code>container.seccomp.security.alpha.kubernetes.io</code> were deprecated since v1.19, now have been completely removed. The seccomp fields are no longer auto-populated when pods with seccomp annotations are created. Pods should use the corresponding pod or container <code>securityContext.seccompProfile</code> field instead.</p>\n<h3 id=\"removal-of-several-feature-gates-for-volume-expansion\">Removal of several feature gates for volume expansion</h3>\n<p>The following feature gates for <a href=\"https://github.com/kubernetes/enhancements/issues/284\">volume expansion</a> GA features will be removed and must no longer be referenced in¬†<code>--feature-gates</code>¬†flags:</p>\n<dl>\n<dt><code>ExpandCSIVolumes</code></dt>\n<dd>Enable expanding of CSI volumes.</dd>\n<dt><code>ExpandInUsePersistentVolumes</code></dt>\n<dd>Enable expanding in-use PVCs.</dd>\n<dt><code>ExpandPersistentVolumes</code></dt>\n<dd>Enable expanding of persistent volumes.</dd>\n</dl>\n<h3 id=\"removal-of-master-service-namespace-command-line-argument\">Removal of <code>--master-service-namespace</code> command line argument</h3>\n<p>The kube-apiserver accepts a deprecated command line argument, <code>--master-service-namespace</code>, that specified where to create the Service named <code>kubernetes</code>\nto represent the API server.\nKubernetes v1.27 will remove that argument, which has been deprecated since the v1.26 release.</p>\n<h3 id=\"removal-of-the-controllermanagerleadermigration-feature-gate\">Removal of the <code>ControllerManagerLeaderMigration</code> feature gate</h3>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/2436\">Leader Migration</a> provides a mechanism in which HA clusters can safely migrate &quot;cloud-specific&quot; controllers between the <code>kube-controller-manager</code> and the <code>cloud-controller-manager</code> via a shared resource lock between the two components while upgrading the replicated control plane.</p>\n<p>The <code>ControllerManagerLeaderMigration</code> feature, GA since v1.24, is unconditionally enabled and for the v1.27 release the feature gate option will be removed. If you're setting this feature gate explicitly, you'll need to remove that from command line arguments or configuration files.</p>\n<h3 id=\"removal-of-enable-taint-manager-command-line-argument\">Removal of <code>--enable-taint-manager</code> command line argument</h3>\n<p>The kube-controller-manager command line argument <code>--enable-taint-manager</code> is deprecated, and will be removed in Kubernetes v1.27. The feature that it supports, <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions\">taint based eviction</a>,\nis already enabled by default and will continue to be implicitly enabled when the flag is removed.</p>\n<h3 id=\"removal-of-pod-eviction-timeout-command-line-argument\">Removal of <code>--pod-eviction-timeout</code> command line argument</h3>\n<p>The deprecated command line argument <code>--pod-eviction-timeout</code> will be removed from the\nkube-controller-manager.</p>\n<h3 id=\"removal-of-the-csi-migration-feature-gate\">Removal of the <code>CSI Migration</code> feature gate</h3>\n<p>The <a href=\"https://github.com/kubernetes/enhancements/issues/625\">CSI migration</a> programme allows\nmoving from in-tree volume plugins to out-of-tree CSI drivers. CSI migration is generally available since Kubernetes v1.16, and the associated <code>CSIMigration</code> feature gate will be removed in v1.27.</p>\n<h3 id=\"removal-of-csiinlinevolume-feature-gate\">Removal of <code>CSIInlineVolume</code> feature gate</h3>\n<p>The <a href=\"https://github.com/kubernetes/kubernetes/pull/111258\">CSI Ephemeral Volume</a> feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This feature graduated to GA in v1.25. Hence, the feature gate <code>CSIInlineVolume</code> will be removed in the v1.27 release.</p>\n<h3 id=\"removal-of-ephemeralcontainers-feature-gate\">Removal of <code>EphemeralContainers</code> feature gate</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/\">Ephemeral containers</a> graduated to GA in v1.25. These are containers with a temporary duration that executes within namespaces of an existing pod. Ephemeral containers are typically initiated by a user in order to observe the state of other pods and containers for troubleshooting and debugging purposes. For Kubernetes v1.27, API support for ephemeral containers is unconditionally enabled; the <code>EphemeralContainers</code> feature gate will be removed.</p>\n<h3 id=\"removal-of-localstoragecapacityisolation-feature-gate\">Removal of <code>LocalStorageCapacityIsolation</code> feature gate</h3>\n<p>The <a href=\"https://github.com/kubernetes/kubernetes/pull/111513\">Local Ephemeral Storage Capacity Isolation</a> feature moved to GA in v1.25. The feature provides support for capacity isolation of local ephemeral storage between pods, such as <code>emptyDir</code> volumes, so that a pod can be hard limited in its consumption of shared resources. The kubelet will evicting Pods if consumption of local ephemeral storage exceeds the configured limit. The feature gate, <code>LocalStorageCapacityIsolation</code>, will be removed in the v1.27 release.</p>\n<h3 id=\"removal-of-networkpolicyendport-feature-gate\">Removal of <code>NetworkPolicyEndPort</code> feature gate</h3>\n<p>The v1.25 release of Kubernetes promoted <code>endPort</code> in NetworkPolicy to GA. NetworkPolicy providers that support the <code>endPort</code> field that can be used to specify a range of ports to apply a NetworkPolicy. Previously, each NetworkPolicy could only target a single port. So the feature gate <code>NetworkPolicyEndPort</code> will be removed in this release.</p>\n<p>Please be aware that <code>endPort</code> field must be supported by the Network Policy provider. If your provider does not support <code>endPort</code>, and this field is specified in a Network Policy, the Network Policy will be created covering only the port field (single port).</p>\n<h3 id=\"removal-of-statefulsetminreadyseconds-feature-gate\">Removal of <code>StatefulSetMinReadySeconds</code> feature gate</h3>\n<p>For a pod that is part of a StatefulSet, Kubernetes can mark the Pod ready only if Pod is available (and passing checks) for at least the period you specify in <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds\"><code>minReadySeconds</code></a>. The feature became generally available in Kubernetes v1.25, and the <code>StatefulSetMinReadySeconds</code> feature gate will be locked to true and removed in the v1.27 release.</p>\n<h3 id=\"removal-of-identifypodos-feature-gate\">Removal of <code>IdentifyPodOS</code> feature gate</h3>\n<p>You can specify the operating system for a Pod, and the feature support for that is stable since the v1.25 release. The <code>IdentifyPodOS</code> feature gate will be removed for Kubernetes v1.27.</p>\n<h3 id=\"removal-of-daemonsetupdatesurge-feature-gate\">Removal of <code>DaemonSetUpdateSurge</code> feature gate</h3>\n<p>The v1.25 release of Kubernetes also stabilised surge support for DaemonSet pods, implemented in order to minimize DaemonSet downtime during rollouts. The <code>DaemonSetUpdateSurge</code> feature gate will be removed in Kubernetes v1.27.</p>\n<h2 id=\"looking-ahead\">Looking ahead</h2>\n<p>The official list of <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-29\">API removals</a> planned for Kubernetes v1.29 includes:</p>\n<ul>\n<li>The <code>flowcontrol.apiserver.k8s.io/v1beta2</code> API version of FlowSchema and PriorityLevelConfiguration will no longer be served in v1.29.</li>\n</ul>\n<h2 id=\"want-to-know-more\">Want to know more?</h2>\n<p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p>\n<ul>\n<li>\n<p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation\">Kubernetes v1.23</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation\">Kubernetes v1.24</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation\">Kubernetes v1.25</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#deprecation\">Kubernetes v1.26</a></p>\n</li>\n</ul>\n<p>We will formally announce the deprecations that come with <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md#deprecation\">Kubernetes v1.27</a> as part of the CHANGELOG for that release.</p>\n<p>For information on the process of deprecation and removal, check out the official Kubernetes <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api\">deprecation policy</a> document.</p>","PublishedAt":"2023-03-17 14:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/03/17/upcoming-changes-in-kubernetes-v1-27/","SourceName":"Kubernetes"}},{"node":{"ID":3141,"Title":"Blog: k8s.gcr.io Redirect to registry.k8s.io - What You Need to Know","Description":"<p><strong>Authors</strong>: Bob Killen (Google), Davanum Srinivas (AWS), Chris Short (AWS), Frederico Mu√±oz (SAS\nInstitute), Tim Bannister (The Scale Factory), Ricky Sadowski (AWS), Grace Nguyen (Expo), Mahamed\nAli (Rackspace Technology), Mars Toktonaliev (independent), Laura Santamaria (Dell), Kat Cosgrove\n(Dell)</p>\n<p>On Monday, March 20th, the k8s.gcr.io registry <a href=\"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/\">will be redirected to the community owned\nregistry</a>,\n<strong>registry.k8s.io</strong> .</p>\n<h2 id=\"tl-dr-what-you-need-to-know-about-this-change\">TL;DR: What you need to know about this change</h2>\n<ul>\n<li>On Monday, March 20th, traffic from the older k8s.gcr.io registry will be redirected to\nregistry.k8s.io with the eventual goal of sunsetting k8s.gcr.io.</li>\n<li>If you run in a restricted environment, and apply strict domain name or IP address access policies\nlimited to k8s.gcr.io, <strong>the image pulls will not function</strong> after k8s.gcr.io starts redirecting\nto the new registry.¬†</li>\n<li>A small subset of non-standard clients do not handle HTTP redirects by image registries, and will\nneed to be pointed directly at registry.k8s.io.</li>\n<li>The redirect is a stopgap to assist users in making the switch. The deprecated k8s.gcr.io registry\nwill be phased out at some point. <strong>Please update your manifests as soon as possible to point to\nregistry.k8s.io</strong>.</li>\n<li>If you host your own image registry, you can copy images you need there as well to reduce traffic\nto community owned registries.</li>\n</ul>\n<p>If you think you may be impacted, or would like to know more about this change, please keep reading.</p>\n<h2 id=\"why-did-kubernetes-change-to-a-different-image-registry\">Why did Kubernetes change to a different image registry?</h2>\n<p>k8s.gcr.io is hosted on a custom <a href=\"https://cloud.google.com/container-registry\">Google Container Registry\n(GCR)</a> domain that was set up solely for the Kubernetes\nproject. This has worked well since the inception of the project, and we thank Google for providing\nthese resources, but today, there are other cloud providers and vendors that would like to host\nimages to provide a better experience for the people on their platforms. In addition to Google‚Äôs\n<a href=\"https://www.cncf.io/google-cloud-recommits-3m-to-kubernetes/\">renewed commitment to donate $3\nmillion</a> to support the project's\ninfrastructure last year, Amazon Web Services announced a matching donation <a href=\"https://youtu.be/PPdimejomWo?t=236\">during their Kubecon NA\n2022 keynote in Detroit</a>. This will provide a better experience\nfor users (closer servers = faster downloads) and will reduce the egress bandwidth and costs from\nGCR at the same time.</p>\n<p>For more details on this change, check out <a href=\"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/\">registry.k8s.io: faster, cheaper and Generally Available\n(GA)</a>.</p>\n<h2 id=\"why-is-a-redirect-being-put-in-place\">Why is a redirect being put in place?</h2>\n<p>The project switched to <a href=\"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/\">registry.k8s.io last year with the 1.25\nrelease</a>; however, most of\nthe image pull traffic is still directed at the old endpoint k8s.gcr.io. This has not been\nsustainable for us as a project, as it is not utilizing the resources that have been donated to the\nproject from other providers, and we are in the danger of running out of funds due to the cost of\nserving this traffic.</p>\n<p>A redirect will enable the project to take advantage of these new resources, significantly reducing\nour egress bandwidth costs. We only expect this change to impact a small subset of users running in\nrestricted environments or using very old clients that do not respect redirects properly.</p>\n<h2 id=\"what-images-will-be-impacted\">What images will be impacted?</h2>\n<p><strong>ALL</strong> images on k8s.gcr.io will be impacted by this change. k8s.gcr.io hosts many images beyond\nKubernetes releases. A large number of Kubernetes subprojects host their images there as well. Some\nexamples include the <code>dns/k8s-dns-node-cache</code>, <code>ingress-nginx/controller</code>, and\n<code>node-problem-detector/node-problem-detector</code> images.</p>\n<h2 id=\"what-will-happen-to-k8s-gcr-io\">What will happen to k8s.gcr.io?</h2>\n<p>Separate from the the redirect, k8s.gcr.io will be frozen <a href=\"https://kubernetes.io/blog/2023/02/06/k8s-gcr-io-freeze-announcement/\">and will not be updated with new images\nafter April 3rd, 2023</a>. <code>k8s.gcr.io</code>\nwill not get any new releases, patches, or security updates. It will continue to remain available to\nhelp people migrate, but it <strong>WILL</strong> be phased out entirely in the future.</p>\n<h2 id=\"i-run-in-a-restricted-environment-what-should-i-do\">I run in a restricted environment. What should I do?</h2>\n<p>For impacted users that run in a restricted environment, the best option is to copy over the\nrequired images to a private registry or configure a pull-through cache in their registry.</p>\n<p>There are several tools to copy images between registries;\n<a href=\"https://github.com/google/go-containerregistry/blob/main/cmd/crane/doc/crane_copy.md\">crane</a> is one\nof those tools, and images can be copied to a private registry by using <code>crane copy SRC DST</code>. There\nare also vendor-specific tools, like e.g. Google‚Äôs\n<a href=\"https://cloud.google.com/container-registry/docs/migrate-external-containers#copy\">gcrane</a>, that\nperform a similar function but are streamlined for their platform.</p>\n<h2 id=\"how-can-i-check-registry-k8s-io-is-accessible-from-my-cluster\">How can I check registry.k8s.io is accessible from my cluster?</h2>\n<p>To test connectivity to registry.k8s.io and being able to pull images from there, here is a sample\ncommand that can be executed in the namespace of your choosing:</p>\n<pre tabindex=\"0\"><code>kubectl run hello-world --tty --rm -i --image=registry.k8s.io/busybox:latest sh\n</code></pre><p>When you run the command above, here‚Äôs what to expect when things work correctly:</p>\n<pre tabindex=\"0\"><code>$ kubectl run hello-world --tty --rm -i --image=registry.k8s.io/busybox:latest sh\nIf you don&#39;t see a command prompt, try pressing enter.\n/ # exit\nSession ended, resume using &#39;kubectl attach hello-world -c hello-world -i -t&#39; command when the pod is running\npod &#34;hello-world&#34; deleted\n</code></pre><h2 id=\"what-kind-of-errors-will-i-see-if-i-m-impacted\">What kind of errors will I see if I‚Äôm impacted?</h2>\n<p>Errors may depend on what kind of container runtime you are using, and what endpoint you are routed\nto, but it should present such as <code>ErrImagePull</code>, <code>ImagePullBackOff</code>, or a container failing to be\ncreated with the warning <code>FailedCreatePodSandBox</code>.</p>\n<p>Below is an example error message showing a proxied deployment failing to pull due to an unknown\ncertificate:</p>\n<pre tabindex=\"0\"><code>FailedCreatePodSandBox: Failed to create pod sandbox: rpc error: code = Unknown desc = Error response from daemon: Head ‚Äúhttps://us-west1-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.8‚Äù: x509: certificate signed by unknown authority\n</code></pre><h2 id=\"how-can-i-find-which-images-are-using-the-legacy-registry-and-fix-them\">How can I find which images are using the legacy registry, and fix them?</h2>\n<p><strong>Option 1</strong>: See the one line kubectl command in our <a href=\"https://kubernetes.io/blog/2023/02/06/k8s-gcr-io-freeze-announcement/#what-s-next\">earlier blog\npost</a>:</p>\n<pre tabindex=\"0\"><code>kubectl get pods --all-namespaces -o jsonpath=&#34;{.items[*].spec.containers[*].image}&#34; |\\\ntr -s &#39;[[:space:]]&#39; &#39;\\n&#39; |\\\nsort |\\\nuniq -c\n</code></pre><p><strong>Option 2</strong>: A <code>kubectl</code> <a href=\"https://krew.sigs.k8s.io/\">krew</a> plugin has been developed called\n<a href=\"https://github.com/kubernetes-sigs/community-images#kubectl-community-images\"><code>community-images</code></a>,\nthat will scan and report any images using the k8s.gcr.io endpoint.</p>\n<p>If you have krew installed, you can install it with:</p>\n<pre tabindex=\"0\"><code>kubectl krew install community-images\n</code></pre><p>and generate a report with:</p>\n<pre tabindex=\"0\"><code>kubectl community-images\n</code></pre><p>For alternate methods of install and example output, check out the repo:\n<a href=\"https://github.com/kubernetes-sigs/community-image\">kubernetes-sigs/community-images</a>.</p>\n<p><strong>Option 3</strong>: If you do not have access to a cluster directly, or manage many clusters - the best\nway is to run a search over your manifests and charts for <em>&quot;k8s.gcr.io&quot;</em>.</p>\n<p><strong>Option 4</strong>: If you wish to prevent k8s.gcr.io based images from running in your cluster, example\npolicies for <a href=\"https://open-policy-agent.github.io/gatekeeper-library/website/\">Gatekeeper</a> and\n<a href=\"https://kyverno.io/\">Kyverno</a> are available in the <a href=\"https://github.com/aws/aws-eks-best-practices/tree/master/policies/k8s-registry-deprecation\">AWS EKS Best Practices\nrepository</a>\nthat will block them from being pulled. You can use these third-party policies with any Kubernetes\ncluster.</p>\n<p><strong>Option 5</strong>: As a <strong>LAST</strong> possible option, you can use a <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks\">Mutating\nAdmission Webhook</a>\nto change the image address dynamically. This should only be\nconsidered a stopgap till your manifests have been updated. You can\nfind a (third party) Mutating Webhook and Kyverno policy in\n<a href=\"https://github.com/abstractinfrastructure/k8s-gcr-quickfix\">k8s-gcr-quickfix</a>.</p>\n<h2 id=\"i-still-have-questions-where-should-i-go\">I still have questions, where should I go?</h2>\n<p>For more information on registry.k8s.io and why it was developed, see <a href=\"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/\">registry.k8s.io: faster,\ncheaper and Generally Available</a>.</p>\n<p>If you would like to know more about the image freeze and the last images that will be available\nthere, see the blog post: <a href=\"https://kubernetes.io/blog/2023/02/06/k8s-gcr-io-freeze-announcement/\">k8s.gcr.io Image Registry Will Be Frozen From the 3rd of April\n2023</a>.</p>\n<p>Information on the architecture of registry.k8s.io and its <a href=\"https://github.com/kubernetes/registry.k8s.io/blob/8408d0501a88b3d2531ff54b14eeb0e3c900a4f3/cmd/archeio/docs/request-handling.md\">request handling decision\ntree</a>\ncan be found in the <a href=\"https://github.com/kubernetes/registry.k8s.io\">kubernetes/registry.k8s.io\nrepo</a>.</p>\n<p>If you believe you have encountered a bug with the new registry or the redirect, please open an\nissue in the <a href=\"https://github.com/kubernetes/registry.k8s.io/issues/new/choose\">kubernetes/registry.k8s.io\nrepo</a>. <strong>Please check if there is an issue already\nopen similar to what you are seeing before you create a new issue</strong>.</p>","PublishedAt":"2023-03-10 17:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/03/10/image-registry-redirect/","SourceName":"Kubernetes"}},{"node":{"ID":3136,"Title":"Blog: Forensic container analysis","Description":"<p><strong>Authors:</strong> Adrian Reber (Red Hat)</p>\n<p>In my previous article, <a href=\"https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/\">Forensic container checkpointing in\nKubernetes</a>, I introduced checkpointing in Kubernetes\nand how it has to be setup and how it can be used. The name of the\nfeature is Forensic container checkpointing, but I did not go into\nany details how to do the actual analysis of the checkpoint created by\nKubernetes. In this article I want to provide details how the\ncheckpoint can be analyzed.</p>\n<p>Checkpointing is still an alpha feature in Kubernetes and this article\nwants to provide a preview how the feature might work in the future.</p>\n<h2 id=\"preparation\">Preparation</h2>\n<p>Details about how to configure Kubernetes and the underlying CRI implementation\nto enable checkpointing support can be found in my <a href=\"https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/\">Forensic container\ncheckpointing in Kubernetes</a> article.</p>\n<p>As an example I prepared a container image (<code>quay.io/adrianreber/counter:blog</code>)\nwhich I want to checkpoint and then analyze in this article. This container allows\nme to create files in the container and also store information in memory which\nI later want to find in the checkpoint.</p>\n<p>To run that container I need a pod, and for this example I am using the following Pod manifest:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>counters<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>counter<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>quay.io/adrianreber/counter:blog<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>This results in a container called <code>counter</code> running in a pod called <code>counters</code>.</p>\n<p>Once the container is running I am performing following actions with that\ncontainer:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get pod counters --template <span style=\"color:#b44\">&#39;{{.status.podIP}}&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10.88.0.25\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> curl 10.88.0.25:8088/create?test-file\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> curl 10.88.0.25:8088/secret?RANDOM_1432_KEY\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> curl 10.88.0.25:8088\n</span></span></code></pre></div><p>The first access creates a file called <code>test-file</code> with the content <code>test-file</code>\nin the container and the second access stores my secret information\n(<code>RANDOM_1432_KEY</code>) somewhere in the container's memory. The last access just\nadds an additional line to the internal log file.</p>\n<p>The last step before I can analyze the checkpoint it to tell Kubernetes to create\nthe checkpoint. As described in the previous article this requires access to the\n<em>kubelet</em> only <code>checkpoint</code> API endpoint.</p>\n<p>For a container named <em>counter</em> in a pod named <em>counters</em> in a namespace named\n<em>default</em> the <em>kubelet</em> API endpoint is reachable at:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># run this on the node where that Pod is executing</span>\n</span></span><span style=\"display:flex;\"><span>curl -X POST <span style=\"color:#b44\">&#34;https://localhost:10250/checkpoint/default/counters/counter&#34;</span>\n</span></span></code></pre></div><p>For completeness the following <code>curl</code> command-line options are necessary to\nhave <code>curl</code> accept the <em>kubelet</em>'s self signed certificate and authorize the\nuse of the <em>kubelet</em> <code>checkpoint</code> API:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>--insecure --cert /var/run/kubernetes/client-admin.crt --key /var/run/kubernetes/client-admin.key\n</span></span></code></pre></div><p>Once the checkpointing has finished the checkpoint should be available at\n<code>/var/lib/kubelet/checkpoints/checkpoint-&lt;pod-name&gt;_&lt;namespace-name&gt;-&lt;container-name&gt;-&lt;timestamp&gt;.tar</code></p>\n<p>In the following steps of this article I will use the name <code>checkpoint.tar</code>\nwhen analyzing the checkpoint archive.</p>\n<h2 id=\"checkpoint-archive-analysis-using-checkpointctl\">Checkpoint archive analysis using <code>checkpointctl</code></h2>\n<p>To get some initial information about the checkpointed container I am using the\ntool <a href=\"https://github.com/checkpoint-restore/checkpointctl\">checkpointctl</a> like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> checkpointctl show checkpoint.tar --print-stats\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">+-----------+----------------------------------+--------------+---------+---------------------+--------+------------+------------+-------------------+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">| CONTAINER | IMAGE | ID | RUNTIME | CREATED | ENGINE | IP | CHKPT SIZE | ROOT FS DIFF SIZE |\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">+-----------+----------------------------------+--------------+---------+---------------------+--------+------------+------------+-------------------+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">| counter | quay.io/adrianreber/counter:blog | 059a219a22e5 | runc | 2023-03-02T06:06:49 | CRI-O | 10.88.0.23 | 8.6 MiB | 3.0 KiB |\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">+-----------+----------------------------------+--------------+---------+---------------------+--------+------------+------------+-------------------+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">CRIU dump statistics\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">+---------------+-------------+--------------+---------------+---------------+---------------+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">| FREEZING TIME | FROZEN TIME | MEMDUMP TIME | MEMWRITE TIME | PAGES SCANNED | PAGES WRITTEN |\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">+---------------+-------------+--------------+---------------+---------------+---------------+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">| 100809 us | 119627 us | 11602 us | 7379 us | 7800 | 2198 |\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">+---------------+-------------+--------------+---------------+---------------+---------------+\n</span></span></span></code></pre></div><p>This gives me already some information about the checkpoint in that checkpoint\narchive. I can see the name of the container, information about the container\nruntime and container engine. It also lists the size of the checkpoint (<code>CHKPT SIZE</code>). This is mainly the size of the memory pages included in the checkpoint,\nbut there is also information about the size of all changed files in the\ncontainer (<code>ROOT FS DIFF SIZE</code>).</p>\n<p>The additional parameter <code>--print-stats</code> decodes information in the checkpoint\narchive and displays them in the second table (<em>CRIU dump statistics</em>). This\ninformation is collected during checkpoint creation and gives an overview how much\ntime CRIU needed to checkpoint the processes in the container and how many\nmemory pages were analyzed and written during checkpoint creation.</p>\n<h2 id=\"digging-deeper\">Digging deeper</h2>\n<p>With the help of <code>checkpointctl</code> I am able to get some high level information\nabout the checkpoint archive. To be able to analyze the checkpoint archive\nfurther I have to extract it. The checkpoint archive is a <em>tar</em> archive and can\nbe extracted with the help of <code>tar xf checkpoint.tar</code>.</p>\n<p>Extracting the checkpoint archive will result in following files and directories:</p>\n<ul>\n<li><code>bind.mounts</code> - this file contains information about bind mounts and is needed\nduring restore to mount all external files and directories at the right location</li>\n<li><code>checkpoint/</code> - this directory contains the actual checkpoint as created by\nCRIU</li>\n<li><code>config.dump</code> and <code>spec.dump</code> - these files contain metadata about the container\nwhich is needed during restore</li>\n<li><code>dump.log</code> - this file contains the debug output of CRIU created during\ncheckpointing</li>\n<li><code>stats-dump</code> - this file contains the data which is used by <code>checkpointctl</code>\nto display dump statistics (<code>--print-stats</code>)</li>\n<li><code>rootfs-diff.tar</code> - this file contains all changed files on the container's\nfile-system</li>\n</ul>\n<h3 id=\"file-system-changes-rootfs-diff-tar\">File-system changes - <code>rootfs-diff.tar</code></h3>\n<p>The first step to analyze the container's checkpoint further is to look at\nthe files that have changed in my container. This can be done by looking at the\nfile <code>rootfs-diff.tar</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> tar xvf rootfs-diff.tar\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">home/counter/logfile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">home/counter/test-file\n</span></span></span></code></pre></div><p>Now the files that changed in the container can be studied:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> cat home/counter/logfile\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10.88.0.1 - - [02/Mar/2023 06:07:29] &#34;GET /create?test-file HTTP/1.1&#34; 200 -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10.88.0.1 - - [02/Mar/2023 06:07:40] &#34;GET /secret?RANDOM_1432_KEY HTTP/1.1&#34; 200 -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">10.88.0.1 - - [02/Mar/2023 06:07:43] &#34;GET / HTTP/1.1&#34; 200 -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span>¬†cat home/counter/test-file\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">test-file¬†\n</span></span></span></code></pre></div><p>Compared to the container image (<code>quay.io/adrianreber/counter:blog</code>) this\ncontainer is based on, I can see that the file <code>logfile</code> contains information\nabout all access to the service the container provides and the file <code>test-file</code>\nwas created just as expected.</p>\n<p>With the help of <code>rootfs-diff.tar</code> it is possible to inspect all files that\nwere created or changed compared to the base image of the container.</p>\n<h3 id=\"analyzing-the-checkpointed-processes-checkpoint\">Analyzing the checkpointed processes - <code>checkpoint/</code></h3>\n<p>The directory <code>checkpoint/</code> contains data created by CRIU while checkpointing\nthe processes in the container. The content in the directory <code>checkpoint/</code>\nconsists of different <a href=\"https://criu.org/Images\">image files</a> which can be analyzed with the\nhelp of the tool <a href=\"https://criu.org/CRIT\">CRIT</a> which is distributed as part of CRIU.</p>\n<p>First lets get an overview of the processes inside of the container:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> crit show checkpoint/pstree.img | jq .entries<span style=\"color:#666\">[]</span>.pid\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">7\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">8\n</span></span></span></code></pre></div><p>This output means that I have three processes inside of the container's PID\nnamespace with the PIDs: 1, 7, 8</p>\n<p>This is only the view from the inside of the container's PID namespace. During\nrestore exactly these PIDs will be recreated. From the outside of the\ncontainer's PID namespace the PIDs will change after restore.</p>\n<p>The next step is to get some additional information about these three processes:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> crit show checkpoint/core-1.img | jq .entries<span style=\"color:#666\">[</span>0<span style=\"color:#666\">]</span>.tc.comm\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&#34;bash&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> crit show checkpoint/core-7.img | jq .entries<span style=\"color:#666\">[</span>0<span style=\"color:#666\">]</span>.tc.comm\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&#34;counter.py&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> crit show checkpoint/core-8.img | jq .entries<span style=\"color:#666\">[</span>0<span style=\"color:#666\">]</span>.tc.comm\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&#34;tee&#34;\n</span></span></span></code></pre></div><p>This means the three processes in my container are <code>bash</code>, <code>counter.py</code> (a Python\ninterpreter) and <code>tee</code>. For details about the parent child relations of these processes there\nis more data to be analyzed in <code>checkpoint/pstree.img</code>.</p>\n<p>Let's compare the so far collected information to the still running container:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> crictl inspect --output go-template --template <span style=\"color:#b44\">&#34;{{(index .info.pid)}}&#34;</span> 059a219a22e56\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">722520\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> ps auxf | grep -A <span style=\"color:#666\">2</span> <span style=\"color:#666\">722520</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">fedora 722520 \\_ bash -c /home/counter/counter.py 2&gt;&amp;1 | tee /home/counter/logfile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">fedora 722541 \\_ /usr/bin/python3 /home/counter/counter.py\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">fedora 722542 \\_ /usr/bin/coreutils --coreutils-prog-shebang=tee /usr/bin/tee /home/counter/logfile\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span>¬†cat /proc/722520/comm\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">bash\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span>¬†cat /proc/722541/comm\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">counter.py\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> cat /proc/722542/comm\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">tee\n</span></span></span></code></pre></div><p>In this output I am first retrieving the PID of the first process in the\ncontainer and then I am looking for that PID and child processes on the system\nwhere the container is running. I am seeing three processes and the first one is\n&quot;bash&quot; which is PID 1 inside of the containers PID namespace. Then I am looking\nat <code>/proc/&lt;PID&gt;/comm</code> and I can find the exact same value\nas in the checkpoint image.</p>\n<p>Important to remember is that the checkpoint will contain the view from within the\ncontainer's PID namespace because that information is important to restore the\nprocesses.</p>\n<p>One last example of what <code>crit</code> can tell us about the container is the information\nabout the UTS namespace:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> crit show checkpoint/utsns-12.img\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">{\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> &#34;magic&#34;: &#34;UTSNS&#34;,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> &#34;entries&#34;: [\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> {\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> &#34;nodename&#34;: &#34;counters&#34;,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> &#34;domainname&#34;: &#34;(none)&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> }\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">}\n</span></span></span></code></pre></div><p>This tells me that the hostname inside of the UTS namespace is <code>counters</code>.</p>\n<p>For every resource CRIU collected during checkpointing the <code>checkpoint/</code>\ndirectory contains corresponding image files which can be analyzed with the help\nof <code>crit</code>.</p>\n<h4 id=\"looking-at-the-memory-pages\">Looking at the memory pages</h4>\n<p>In addition to the information from CRIU that can be decoded with the help\nof CRIT, there are also files containing the raw memory pages written by\nCRIU to disk:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> ls checkpoint/pages-*\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">checkpoint/pages-1.img checkpoint/pages-2.img checkpoint/pages-3.img\n</span></span></span></code></pre></div><p>When I initially used the container I stored a random key (<code>RANDOM_1432_KEY</code>)\nsomewhere in the memory. Let see if I can find it:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> grep -ao RANDOM_1432_KEY checkpoint/pages-*\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">checkpoint/pages-2.img:RANDOM_1432_KEY\n</span></span></span></code></pre></div><p>And indeed, there is my data. This way I can easily look at the content\nof all memory pages of the processes in the container, but it is also\nimportant to remember that anyone that can access the checkpoint\narchive has access to all information that was stored in the memory of the\ncontainer's processes.</p>\n<h4 id=\"using-gdb-for-further-analysis\">Using gdb for further analysis</h4>\n<p>Another possibility to look at the checkpoint images is <code>gdb</code>. The CRIU repository\ncontains the script <a href=\"https://github.com/checkpoint-restore/criu/tree/criu-dev/coredump\">coredump</a> which can convert a checkpoint\ninto a coredump file:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> /home/criu/coredump/coredump-python3\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> ls -al core*\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">core.1 core.7 core.8\n</span></span></span></code></pre></div><p>Running the <code>coredump-python3</code> script will convert the checkpoint images into\none coredump file for each process in the container. Using <code>gdb</code> I can also look\nat the details of the processes:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span>¬†echo info registers | gdb --core checkpoint/core.1 -q\n</span></span><span style=\"display:flex;\"><span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">[New LWP 1]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\">Core was generated by `bash -c /home/counter/counter.py 2&gt;&amp;1 | tee /home/counter/logfile&#39;.\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#000080;font-weight:bold\">#</span><span style=\"color:#666\">0</span> 0x00007fefba110198 in ?? <span style=\"color:#666\">()</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">(gdb)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rax 0x3d 61\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rbx 0x8 8\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rcx 0x7fefba11019a 140667595587994\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rdx 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rsi 0x7fffed9c1110 140737179816208\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rdi 0xffffffff 4294967295\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rbp 0x1 0x1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rsp 0x7fffed9c10e8 0x7fffed9c10e8\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r8 0x1 1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r9 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r10 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r11 0x246 582\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r12 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r13 0x7fffed9c1170 140737179816304\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r14 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">r15 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">rip 0x7fefba110198 0x7fefba110198\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">eflags 0x246 [ PF ZF IF ]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">cs 0x33 51\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">ss 0x2b 43\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">ds 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">es 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">fs 0x0 0\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">gs 0x0 0\n</span></span></span></code></pre></div><p>In this example I can see the value of all registers as they were during\ncheckpointing and I can also see the complete command-line of my container's PID\n1 process: <code>bash -c /home/counter/counter.py 2&gt;&amp;1 | tee /home/counter/logfile</code></p>\n<h2 id=\"summary\">Summary</h2>\n<p>With the help of container checkpointing, it is possible to create a\ncheckpoint of a running container without stopping the container and without the\ncontainer knowing that it was checkpointed. The result of checkpointing a\ncontainer in Kubernetes is a checkpoint archive; using different tools like\n<code>checkpointctl</code>, <code>tar</code>, <code>crit</code> and <code>gdb</code> the checkpoint can be analyzed. Even\nwith simple tools like <code>grep</code> it is possible to find information in the\ncheckpoint archive.</p>\n<p>The different examples I have shown in this article how to analyze a checkpoint\nare just the starting point. Depending on your requirements it is possible to\nlook at certain things in much more detail, but this article should give you an\nintroduction how to start the analysis of your checkpoint.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>You can reach SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-security\">#sig-security</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n</ul>","PublishedAt":"2023-03-10 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/03/10/forensic-container-analysis/","SourceName":"Kubernetes"}},{"node":{"ID":3064,"Title":"Blog: Introducing KWOK: Kubernetes WithOut Kubelet","Description":"<p><strong>Author:</strong> Shiming Zhang (DaoCloud), Wei Huang (Apple), Yibo Zhuang (Apple)</p>\n<img style=\"float: right; display: inline-block; margin-left: 2em; max-width: 15em;\" src=\"https://kubernetes.io/blog/2023/03/01/introducing-kwok/kwok.svg\" alt=\"KWOK logo\" />\n<p>Have you ever wondered how to set up a cluster of thousands of nodes just in seconds, how to simulate real nodes with a low resource footprint, and how to test your Kubernetes controller at scale without spending much on infrastructure?</p>\n<p>If you answered &quot;yes&quot; to any of these questions, then you might be interested in KWOK, a toolkit that enables you to create a cluster of thousands of nodes in seconds.</p>\n<h2 id=\"what-is-kwok\">What is KWOK?</h2>\n<p>KWOK stands for Kubernetes WithOut Kubelet. So far, it provides two tools:</p>\n<dl>\n<dt><code>kwok</code></dt>\n<dd><code>kwok</code> is the cornerstone of this project, responsible for simulating the lifecycle of fake nodes, pods, and other Kubernetes API resources.</dd>\n<dt><code>kwokctl</code></dt>\n<dd><code>kwokctl</code> is a CLI tool designed to streamline the creation and management of clusters, with nodes simulated by <code>kwok</code>.</dd>\n</dl>\n<h2 id=\"why-use-kwok\">Why use KWOK?</h2>\n<p>KWOK has several advantages:</p>\n<ul>\n<li><strong>Speed</strong>: You can create and delete clusters and nodes almost instantly, without waiting for boot or provisioning.</li>\n<li><strong>Compatibility</strong>: KWOK works with any tools or clients that are compliant with Kubernetes APIs, such as kubectl, helm, kui, etc.</li>\n<li><strong>Portability</strong>: KWOK has no specific hardware or software requirements. You can run it using pre-built images, once Docker or Nerdctl is installed. Alternatively, binaries are also available for all platforms and can be easily installed.</li>\n<li><strong>Flexibility</strong>: You can configure different node types, labels, taints, capacities, conditions, etc., and you can configure different pod behaviors, status, etc. to test different scenarios and edge cases.</li>\n<li><strong>Performance</strong>: You can simulate thousands of nodes on your laptop without significant consumption of CPU or memory resources.</li>\n</ul>\n<h2 id=\"what-are-the-use-cases\">What are the use cases?</h2>\n<p>KWOK can be used for various purposes:</p>\n<ul>\n<li><strong>Learning</strong>: You can use KWOK to learn about Kubernetes concepts and features without worrying about resource waste or other consequences.</li>\n<li><strong>Development</strong>: You can use KWOK to develop new features or tools for Kubernetes without accessing to a real cluster or requiring other components.</li>\n<li><strong>Testing</strong>:\n<ul>\n<li>You can measure how well your application or controller scales with different numbers of nodes and(or) pods.</li>\n<li>You can generate high loads on your cluster by creating many pods or services with different resource requests or limits.</li>\n<li>You can simulate node failures or network partitions by changing node conditions or randomly deleting nodes.</li>\n<li>You can test how your controller interacts with other components or features of Kubernetes by enabling different feature gates or API versions.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"what-are-the-limitations\">What are the limitations?</h2>\n<p>KWOK is not intended to replace others completely. It has some limitations that you should be aware of:</p>\n<ul>\n<li><strong>Functionality</strong>: KWOK is not a kubelet and may exhibit different behaviors in areas such as pod lifecycle management, volume mounting, and device plugins. Its primary function is to simulate updates of node and pod status.</li>\n<li><strong>Accuracy</strong>: It's important to note that KWOK doesn't accurately reflect the performance or behavior of real nodes under various workloads or environments. Instead, it approximates some behaviors using simple formulas.</li>\n<li><strong>Security</strong>: KWOK does not enforce any security policies or mechanisms on simulated nodes. It assumes that all requests from the kube-apiserver are authorized and valid.</li>\n</ul>\n<h2 id=\"getting-started\">Getting started</h2>\n<p>If you are interested in trying out KWOK, please check its <a href=\"https://kwok.sigs.k8s.io/\">documents</a> for more details.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/03/01/introducing-kwok/manage-clusters.svg\"\nalt=\"Animation of a terminal showing kwokctl in use\"/> <figcaption>\n<p>Using kwokctl to manage simulated clusters</p>\n</figcaption>\n</figure>\n<h2 id=\"getting-involved\">Getting Involved</h2>\n<p>If you're interested in participating in future discussions or development related to KWOK, there are several ways to get involved:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/kwok/\">#kwok</a> for general usage discussion, <a href=\"https://kubernetes.slack.com/messages/kwok-dev/\">#kwok-dev</a> for development discussion. (visit <a href=\"https://slack.k8s.io/\">slack.k8s.io</a> for a workspace invitation)</li>\n<li>Open Issues/PRs/Discussions in <a href=\"https://sigs.k8s.io/kwok/\">sigs.k8s.io/kwok</a></li>\n</ul>\n<p>We welcome feedback and contributions from anyone who wants to join us in this exciting project.</p>","PublishedAt":"2023-03-01 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/03/01/introducing-kwok/","SourceName":"Kubernetes"}},{"node":{"ID":2960,"Title":"Blog: Free Katacoda Kubernetes Tutorials Are Shutting Down","Description":"<p><strong>Author</strong>: Natali Vlatko, SIG Docs Co-Chair for Kubernetes</p>\n<p><a href=\"https://katacoda.com/kubernetes\">Katacoda</a>, the popular learning platform from O‚ÄôReilly that has been helping people learn all about\nJava, Docker, Kubernetes, Python, Go, C++, and more, <a href=\"https://www.oreilly.com/online-learning/leveraging-katacoda-technology.html\">shut down for public use in June 2022</a>.\nHowever, tutorials specifically for Kubernetes, linked from the Kubernetes website for our project‚Äôs\nusers and contributors, remained available and active after this change. Unfortunately, this will no\nlonger be the case, and Katacoda tutorials for learning Kubernetes will cease working after March 31st, 2023.</p>\n<p>The Kubernetes Project wishes to thank O'Reilly Media for the many years it has supported the community\nvia the Katacoda learning platform. You can read more about <a href=\"https://www.oreilly.com/online-learning/leveraging-katacoda-technology.html\">the decision to shutter katacoda.com</a>\non O'Reilly's own site. With this change, we‚Äôll be focusing on the work needed to remove links to\ntheir various tutorials. We have a general issue tracking this topic at <a href=\"https://github.com/kubernetes/website/issues/33936\">#33936</a> and <a href=\"https://github.com/kubernetes/website/discussions/38878\">GitHub discussion</a>. We‚Äôre also\ninterested in researching what other learning platforms could be beneficial for the Kubernetes community,\nreplacing Katacoda with a link to a platform or service that has a similar user experience. However,\nthis research will take time, so we‚Äôre actively looking for volunteers to help with this work.\nIf a replacement is found, it will need to be supported by Kubernetes leadership, specifically,\nSIG Contributor Experience, SIG Docs, and the Kubernetes Steering Committee.</p>\n<p>The Katacoda shutdown affects 25 tutorial pages, their localizations, as well as the Katacoda\nScenario repository: <a href=\"https://github.com/katacoda-scenarios/kubernetes-bootcamp-scenarios\">github.com/katacoda-scenarios/kubernetes-bootcamp-scenarios</a>. We recommend\nthat any links, guides, or documentation you have that points to the Katacoda learning platform be\nupdated immediately to reflect this change. While we have yet to find a replacement learning solution,\nthe Kubernetes website contains a lot of helpful documentation to support your continued learning and growth.\nYou can find all of our available documentation tutorials for Kubernetes at <a href=\"https://k8s.io/docs/tutorials/\">https://k8s.io/docs/tutorials/</a>.</p>\n<p>If you have any questions regarding the Katacoda shutdown, or subsequent link removal from Kubernetes\ntutorial pages, please feel free to comment on the <a href=\"https://github.com/kubernetes/website/issues/33936\">general issue tracking the shutdown</a>,\nor visit the #sig-docs channel on the Kubernetes Slack.</p>","PublishedAt":"2023-02-14 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/02/14/kubernetes-katacoda-tutorials-stop-from-2023-03-31/","SourceName":"Kubernetes"}},{"node":{"ID":2909,"Title":"Blog: k8s.gcr.io image registry will be frozen from the 3rd of April 2023","Description":"<p><strong>Authors</strong>: Mahamed Ali (Rackspace Technology)</p>\n<p>The Kubernetes project runs a community-owned image registry called <code>registry.k8s.io</code> to host its container images. On the 3rd of April 2023, the old registry <code>k8s.gcr.io</code> will be frozen and no further images for Kubernetes and related subprojects will be pushed to the old registry.</p>\n<p>This registry <code>registry.k8s.io</code> replaced the old one and has been generally available for several months. We have published a <a href=\"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/\">blog post</a> about its benefits to the community and the Kubernetes project. This post also announced that future versions of Kubernetes will not be available in the old registry. Now that time has come.</p>\n<p>What does this change mean for contributors:</p>\n<ul>\n<li>If you are a maintainer of a subproject, you will need to update your manifests and Helm charts to use the new registry.</li>\n</ul>\n<p>What does this change mean for end users:</p>\n<ul>\n<li>1.27 Kubernetes release will not be published to the old registry.</li>\n<li>Patch releases for 1.24, 1.25, and 1.26 will no longer be published to the old registry from April. Please read the timelines below for details of the final patch releases in the old registry.</li>\n<li>Starting in 1.25, the default image registry has been set to <code>registry.k8s.io</code>. This value is overridable in <code>kubeadm</code> and <code>kubelet</code> but setting it to <code>k8s.gcr.io</code> will fail for new releases after April as they won‚Äôt be present in the old registry.</li>\n<li>If you want to increase the reliability of your cluster and remove dependency on the community-owned registry or you are running Kubernetes in networks where external traffic is restricted, you should consider hosting local image registry mirrors. Some cloud vendors may offer hosted solutions for this.</li>\n</ul>\n<h2 id=\"timeline-of-the-changes\">Timeline of the Changes:</h2>\n<ul>\n<li><code>k8s.gcr.io</code> will be frozen on the 3rd of April 2023</li>\n<li>1.27 is expected to be released on the 12th of April 2023</li>\n<li>The last 1.23 release on <code>k8s.gcr.io</code> will be 1.23.18 (1.23 goes EoL before the freeze)</li>\n<li>The last 1.24 release on <code>k8s.gcr.io</code> will be 1.24.12</li>\n<li>The last 1.25 release on <code>k8s.gcr.io</code> will be 1.25.8</li>\n<li>The last 1.26 release on <code>k8s.gcr.io</code> will be 1.26.3</li>\n</ul>\n<h2 id=\"what-s-next\">What's next</h2>\n<p>Please make sure your cluster does not have dependencies on old image registry. For example, you can run this command to list the images used by pods:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl get pods --all-namespaces -o <span style=\"color:#b8860b\">jsonpath</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">&#34;{.items[*].spec.containers[*].image}&#34;</span> |<span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span>tr -s <span style=\"color:#b44\">&#39;[[:space:]]&#39;</span> <span style=\"color:#b44\">&#39;\\n&#39;</span> |<span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span>sort |<span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span>uniq -c\n</span></span></code></pre></div><p>There may be other dependencies on the old image registry. Make sure you review any potential dependencies to keep your cluster healthy and up to date.</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p><strong>Change is hard</strong>, and evolving our image-serving platform is needed to ensure a sustainable future for the project. We strive to make things better for everyone using Kubernetes. Many contributors from all corners of our community have been working long and hard to ensure we are making the best decisions possible, executing plans, and doing our best to communicate those plans.</p>\n<p>Thanks to Aaron Crickenberger, Arnaud Meukam, Benjamin Elder, Caleb Woodbine, Davanum Srinivas, Mahamed Ali, and Tim Hockin from SIG K8s Infra, Brian McQueen, and Sergey Kanzhelev from SIG Node, Lubomir Ivanov from SIG Cluster Lifecycle, Adolfo Garc√≠a Veytia, Jeremy Rickard, Sascha Grunert, and Stephen Augustus from SIG Release, Bob Killen and Kaslin Fields from SIG Contribex, Tim Allclair from the Security Response Committee. Also a big thank you to our friends acting as liaisons with our cloud provider partners: Jay Pipes from Amazon and Jon Johnson Jr. from Google.</p>","PublishedAt":"2023-02-06 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/02/06/k8s-gcr-io-freeze-announcement/","SourceName":"Kubernetes"}},{"node":{"ID":2897,"Title":"Blog: Spotlight on SIG Instrumentation","Description":"<p><strong>Author:</strong> Imran Noor Mohamed (Delivery Hero)</p>\n<p>Observability requires the right data at the right time for the right consumer (human or piece of software) to make the right decision. In the context of Kubernetes, having best practices for cluster observability across all Kubernetes components is crucial.</p>\n<p>SIG Instrumentation helps to address this issue by providing best practices and tools that all other SIGs use to instrument Kubernetes components-like the <em>Api server</em>, <em>scheduler</em>, <em>kubelet</em> and <em>kube-controller-manager</em>.</p>\n<p>In this SIG Instrumentation spotlight, <a href=\"https://www.linkedin.com/in/imrannoormohamed/\">Imran Noor Mohamed</a>, SIG ContribEx-Comms tech lead talked with <a href=\"https://twitter.com/ehashdn\">Elana Hashman</a>, and <a href=\"https://www.linkedin.com/in/hankang\">Han Kang</a>, chairs of SIG Instrumentation, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.</p>\n<h2 id=\"about-sig-instrumentation\">About SIG Instrumentation</h2>\n<p><strong>Imran (INM)</strong>: Hello, thank you for the opportunity of learning more about SIG Instrumentation. Could you tell us a bit about yourself, your role, and how you got involved in SIG Instrumentation?</p>\n<p><strong>Han (HK)</strong>: I started in SIG Instrumentation in 2018, and became a chair in 2020. I primarily got involved with SIG instrumentation due to a number of upstream issues with metrics which ended up affecting GKE in bad ways. As a result, we ended up launching an initiative to stabilize our metrics and make metrics a proper API.</p>\n<p><strong>Elana (EH)</strong>: I also joined SIG Instrumentation in 2018 and became a chair at the same time as Han. I was working as a site reliability engineer (SRE) on bare metal Kubernetes clusters and was working to build out our observability stack. I encountered some issues with label joins where Kubernetes metrics didn‚Äôt match kube-state-metrics (<a href=\"https://github.com/kubernetes/kube-state-metrics\">KSM</a>) and started participating in SIG meetings to improve things. I helped test performance improvements to kube-state-metrics and ultimately coauthored a KEP for overhauling metrics in the 1.14 release to improve usability.</p>\n<p><strong>Imran (INM)</strong>: Interesting! Does that mean SIG Instrumentation involves a lot of plumbing?</p>\n<p><strong>Han (HK)</strong>: I wouldn‚Äôt say it involves a ton of plumbing, though it does touch basically every code base. We have our own dedicated directories for our metrics, logs, and tracing frameworks which we tend to work out of primarily. We do have to interact with other SIGs in order to propagate our changes which makes us more of a horizontal SIG.</p>\n<p><strong>Imran (INM)</strong>: Speaking about interaction and coordination with other SIG could you describe how the SIGs is organized?</p>\n<p><strong>Elana (EH)</strong>: In SIG Instrumentation, we have two chairs, Han and myself, as well as two tech leads, David Ashpole and Damien Grisonnet. We all work together as the SIG‚Äôs leads in order to run meetings, triage issues and PRs, review and approve KEPs, plan for each release, present at KubeCon and community meetings, and write our annual report. Within the SIG we also have a number of important subprojects, each of which is stewarded by its subproject owners. For example, Marek Siarkowicz is a subproject owner of <a href=\"https://github.com/kubernetes-sigs/metrics-server\">metrics-server</a>.</p>\n<p>Because we‚Äôre a horizontal SIG, some of our projects have a wide scope and require coordination from a dedicated group of contributors. For example, in order to guide the Kubernetes migration to structured logging, we chartered the <a href=\"https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md\">Structured Logging</a> Working Group (WG), organized by Marek and Patrick Ohly. The WG doesn‚Äôt own any code, but helps with various components such as the <em>kubelet</em>, <em>scheduler</em>, etc. in migrating their code to use structured logs.</p>\n<p><strong>Imran (INM)</strong>: Walking through the <a href=\"https://github.com/kubernetes/community/blob/master/sig-instrumentation/charter.md\">charter</a> alone it‚Äôs clear that SIG Instrumentation has a lot of sub-projects. Could you highlight some important ones?</p>\n<p><strong>Han (HK)</strong>: We have many different sub-projects and we are in dire need of people who can come and help shepherd them. Our most important projects in-tree (that is, within the kubernetes/kubernetes repo) are metrics, tracing, and, structured logging. Our most important projects out-of-tree are (a) KSM (kube-state-metrics) and (b) metrics-server.</p>\n<p><strong>Elana (EH)</strong>: Echoing this, we would love to bring on more maintainers for kube-state-metrics and metrics-server. Our friends at WG Structured Logging are also looking for contributors. Other subprojects include klog, prometheus-adapter, and a new subproject that we just launched for collecting high-fidelity, scalable utilization metrics called <a href=\"https://github.com/kubernetes-sigs/usage-metrics-collector\">usage-metrics-collector</a>. All are seeking new contributors!</p>\n<h2 id=\"current-status-and-ongoing-challenges\">Current status and ongoing challenges</h2>\n<p><strong>Imran (INM)</strong>: For release <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.26\">1.26</a> we can see that there are a relevant number of metrics, logs, and tracing <a href=\"https://www.k8s.dev/resources/keps/\">KEPs</a> in the pipeline. Would you like to point out important things for last release (maybe alpha &amp; stable milestone candidates?)</p>\n<p><strong>Han (HK)</strong>: We can now generate <a href=\"https://kubernetes.io/docs/reference/instrumentation/metrics/\">documentation</a> for every single metric in the main Kubernetes code base! We have a pretty fancy static analysis pipeline that enables this functionality. We‚Äôve also added feature metrics so that you can look at your metrics to determine which features are enabled in your cluster at a given time. Lastly, we added a component-sli endpoint, which should make it easy for people to create availability SLOs for <em>control-plane</em> components.</p>\n<p><strong>Elana (EH)</strong>: We‚Äôve also been working on tracing KEPs for both the <em>API server</em> and <em>kubelet</em>, though neither graduated in 1.26. I‚Äôm also really excited about the work Han is doing with WG Reliability to extend and improve our metrics stability framework.</p>\n<p><strong>Imran (INM)</strong>: What do you think are the Kubernetes-specific challenges tackled by the SIG Instrumentation? What are the future efforts to solve them?</p>\n<p><strong>Han (HK)</strong>: SIG instrumentation suffered a bit in the past from being a horizontal SIG. We did not have an obvious location to put our code and did not have a good mechanism to audit metrics that people would randomly add. We‚Äôve fixed this over the years and now we have dedicated spots for our code and a reliable mechanism for auditing new metrics. We also now offer stability guarantees for metrics. We hope to have full-blown tracing up and down the kubernetes stack, and metric support via exemplars.</p>\n<p><strong>Elana (EH)</strong>: I think SIG Instrumentation is a really interesting SIG because it poses different kinds of opportunities to get involved than in other SIGs. You don‚Äôt have to be a software developer to contribute to our SIG! All of our components and subprojects are focused on better understanding Kubernetes and its performance in production, which allowed me to get involved as one of the few SIG Chairs working as an SRE at that time. I like that we provide opportunities for newcomers to contribute through using, testing, and providing feedback on our subprojects, which is a lower barrier to entry. Because many of these projects are out-of-tree, I think one of our challenges is to figure out what‚Äôs in scope for core Kubernetes SIGs instrumentation subprojects, what‚Äôs missing, and then fill in the gaps.</p>\n<h2 id=\"community-and-contribution\">Community and contribution</h2>\n<p><strong>Imran (INM)</strong>: Kubernetes values community over products. Any recommendation for anyone looking into getting involved in SIG Instrumentation work? Where should they start (new contributor-friendly areas within SIG?)</p>\n<p><strong>Han(HK) and Elana (EH)</strong>: Come to our bi-weekly triage <a href=\"https://github.com/kubernetes/community/tree/master/sig-instrumentation#meetings\">meetings</a>! They aren‚Äôt recorded and are a great place to ask questions and learn about our ongoing work. We strive to be a friendly community and one of the easiest SIGs to get started with. You can check out our latest KubeCon NA 2022 <a href=\"https://youtu.be/JIzrlWtAA8Y\">SIG Instrumentation Deep Dive</a> to get more insight into our work. We also invite you to join our Slack channel #sig-instrumentation and feel free to reach out to any of our SIG leads or subproject owners directly.</p>\n<p>Thank you so much for your time and insights into the workings of SIG Instrumentation!</p>","PublishedAt":"2023-02-03 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/02/03/sig-instrumentation-spotlight-2023/","SourceName":"Kubernetes"}},{"node":{"ID":2803,"Title":"Blog: Consider All Microservices Vulnerable ‚Äî And Monitor Their Behavior","Description":"<p><strong>Author:</strong>\nDavid Hadas (IBM Research Labs)</p>\n<p><em>This post warns Devops from a false sense of security. Following security best practices when developing and configuring microservices do not result in non-vulnerable microservices. The post shows that although all deployed microservices are vulnerable, there is much that can be done to ensure microservices are not exploited. It explains how analyzing the behavior of clients and services from a security standpoint, named here <strong>&quot;Security-Behavior Analysis&quot;</strong>, can protect the deployed vulnerable microservices. It points to <a href=\"http://knative.dev/security-guard\">Guard</a>, an open source project offering security-behavior monitoring and control of Kubernetes microservices presumed vulnerable.</em></p>\n<p>As cyber attacks continue to intensify in sophistication, organizations deploying cloud services continue to grow their cyber investments aiming to produce safe and non-vulnerable services. However, the year-by-year growth in cyber investments does not result in a parallel reduction in cyber incidents. Instead, the number of cyber incidents continues to grow annually. Evidently, organizations are doomed to fail in this struggle - no matter how much effort is made to detect and remove cyber weaknesses from deployed services, it seems offenders always have the upper hand.</p>\n<p>Considering the current spread of offensive tools, sophistication of offensive players, and ever-growing cyber financial gains to offenders, any cyber strategy that relies on constructing a non-vulnerable, weakness-free service in 2023 is clearly too na√Øve. It seems the only viable strategy is to:</p>\n<p>‚û• <strong>Admit that your services are vulnerable!</strong></p>\n<p>In other words, consciously accept that you will never create completely invulnerable services. If your opponents find even a single weakness as an entry-point, you lose! Admitting that in spite of your best efforts, all your services are still vulnerable is an important first step. Next, this post discusses what you can do about it...</p>\n<h2 id=\"how-to-protect-microservices-from-being-exploited\">How to protect microservices from being exploited</h2>\n<p>Being vulnerable does not necessarily mean that your service will be exploited. Though your services are vulnerable in some ways unknown to you, offenders still need to identify these vulnerabilities and then exploit them. If offenders fail to exploit your service vulnerabilities, you win! In other words, having a vulnerability that can‚Äôt be exploited, represents a risk that can‚Äôt be realized.</p>\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/Example.png\"\nalt=\"Image of an example of offender gaining foothold in a service\"/> <figcaption>\n<p>Figure 1. An Offender gaining foothold in a vulnerable service</p>\n</figcaption>\n</figure>\n<p>The above diagram shows an example in which the offender does not yet have a foothold in the service; that is, it is assumed that your service does not run code controlled by the offender on day 1. In our example the service has vulnerabilities in the API exposed to clients. To gain an initial foothold the offender uses a malicious client to try and exploit one of the service API vulnerabilities. The malicious client sends an exploit that triggers some unplanned behavior of the service.</p>\n<p>More specifically, let‚Äôs assume the service is vulnerable to an SQL injection. The developer failed to sanitize the user input properly, thereby allowing clients to send values that would change the intended behavior. In our example, if a client sends a query string with key ‚Äúusername‚Äù and value of <em>‚Äútom or 1=1‚Äù</em>, the client will receive the data of all users. Exploiting this vulnerability requires the client to send an irregular string as the value. Note that benign users will not be sending a string with spaces or with the equal sign character as a username, instead they will normally send legal usernames which for example may be defined as a short sequence of characters a-z. No legal username can trigger service unplanned behavior.</p>\n<p>In this simple example, one can already identify several opportunities to detect and block an attempt to exploit the vulnerability (un)intentionally left behind by the developer, making the vulnerability unexploitable. First, the malicious client behavior differs from the behavior of benign clients, as it sends irregular requests. If such a change in behavior is detected and blocked, the exploit will never reach the service. Second, the service behavior in response to the exploit differs from the service behavior in response to a regular request. Such behavior may include making subsequent irregular calls to other services such as a data store, taking irregular time to respond, and/or responding to the malicious client with an irregular response (for example, containing much more data than normally sent in case of benign clients making regular requests). Service behavioral changes, if detected, will also allow blocking the exploit in different stages of the exploitation attempt.</p>\n<p>More generally:</p>\n<ul>\n<li>\n<p>Monitoring the behavior of clients can help detect and block exploits against service API vulnerabilities. In fact, deploying efficient client behavior monitoring makes many vulnerabilities unexploitable and others very hard to achieve. To succeed, the offender needs to create an exploit undetectable from regular requests.</p>\n</li>\n<li>\n<p>Monitoring the behavior of services can help detect services as they are being exploited regardless of the attack vector used. Efficient service behavior monitoring limits what an attacker may be able to achieve as the offender needs to ensure the service behavior is undetectable from regular service behavior.</p>\n</li>\n</ul>\n<p>Combining both approaches may add a protection layer to the deployed vulnerable services, drastically decreasing the probability for anyone to successfully exploit any of the deployed vulnerable services. Next, let us identify four use cases where you need to use security-behavior monitoring.</p>\n<h2 id=\"use-cases\">Use cases</h2>\n<p>One can identify the following four different stages in the life of any service from a security standpoint. In each stage, security-behavior monitoring is required to meet different challenges:</p>\n<table>\n<thead>\n<tr>\n<th>Service State</th>\n<th>Use case</th>\n<th>What do you need in order to cope with this use case?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Normal</strong></td>\n<td><strong>No known vulnerabilities:</strong> The service owner is normally not aware of any known vulnerabilities in the service image or configuration. Yet, it is reasonable to assume that the service has weaknesses.</td>\n<td><strong>Provide generic protection against any unknown, zero-day, service vulnerabilities</strong> - Detect/block irregular patterns sent as part of incoming client requests that may be used as exploits.</td>\n</tr>\n<tr>\n<td><strong>Vulnerable</strong></td>\n<td><strong>An applicable CVE is published:</strong> The service owner is required to release a new non-vulnerable revision of the service. Research shows that in practice this process of removing a known vulnerability may take many weeks to accomplish (2 months on average).</td>\n<td><strong>Add protection based on the CVE analysis</strong> - Detect/block incoming requests that include specific patterns that may be used to exploit the discovered vulnerability. Continue to offer services, although the service has a known vulnerability.</td>\n</tr>\n<tr>\n<td><strong>Exploitable</strong></td>\n<td><strong>A known exploit is published:</strong> The service owner needs a way to filter incoming requests that contain the known exploit.</td>\n<td><strong>Add protection based on a known exploit signature</strong> - Detect/block incoming client requests that carry signatures identifying the exploit. Continue to offer services, although the presence of an exploit.</td>\n</tr>\n<tr>\n<td><strong>Misused</strong></td>\n<td><strong>An offender misuses pods backing the service:</strong> The offender can follow an attack pattern enabling him/her to misuse pods. The service owner needs to restart any compromised pods while using non compromised pods to continue offering the service. Note that once a pod is restarted, the offender needs to repeat the attack pattern before he/she may again misuse it.</td>\n<td><strong>Identify and restart instances of the component that is being misused</strong> - At any given time, some backing pods may be compromised and misused, while others behave as designed. Detect/remove the misused pods while allowing other pods to continue servicing client requests.</td>\n</tr>\n</tbody>\n</table>\n<p>Fortunately, microservice architecture is well suited to security-behavior monitoring as discussed next.</p>\n<h2 id=\"microservices-vs-monoliths\">Security-Behavior of microservices versus monoliths</h2>\n<p>Kubernetes is often used to support workloads designed with microservice architecture. By design, microservices aim to follow the UNIX philosophy of &quot;Do One Thing And Do It Well&quot;. Each microservice has a bounded context and a clear interface. In other words, you can expect the microservice clients to send relatively regular requests and the microservice to present a relatively regular behavior as a response to these requests. Consequently, a microservice architecture is an excellent candidate for security-behavior monitoring.</p>\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/Microservices.png\"\nalt=\"Image showing why microservices are well suited for security-behavior monitoring\"/> <figcaption>\n<p>Figure 2. Microservices are well suited for security-behavior monitoring</p>\n</figcaption>\n</figure>\n<p>The diagram above clarifies how dividing a monolithic service to a set of microservices improves our ability to perform security-behavior monitoring and control. In a monolithic service approach, different client requests are intertwined, resulting in a diminished ability to identify irregular client behaviors. Without prior knowledge, an observer of the intertwined client requests will find it hard to distinguish between types of requests and their related characteristics. Further, internal client requests are not exposed to the observer. Lastly, the aggregated behavior of the monolithic service is a compound of the many different internal behaviors of its components, making it hard to identify irregular service behavior.</p>\n<p>In a microservice environment, each microservice is expected by design to offer a more well-defined service and serve better defined type of requests. This makes it easier for an observer to identify irregular client behavior and irregular service behavior. Further, a microservice design exposes the internal requests and internal services which offer more security-behavior data to identify irregularities by an observer. Overall, this makes the microservice design pattern better suited for security-behavior monitoring and control.</p>\n<h2 id=\"security-behavior-monitoring-on-kubernetes\">Security-Behavior monitoring on Kubernetes</h2>\n<p>Kubernetes deployments seeking to add Security-Behavior may use <a href=\"http://knative.dev/security-guard\">Guard</a>, developed under the CNCF project Knative. Guard is integrated into the full Knative automation suite that runs on top of Kubernetes. Alternatively, <strong>you can deploy Guard as a standalone tool</strong> to protect any HTTP-based workload on Kubernetes.</p>\n<p>See:</p>\n<ul>\n<li><a href=\"https://github.com/knative-sandbox/security-guard\">Guard</a> on Github, for using Guard as a standalone tool.</li>\n<li>The Knative automation suite - Read about Knative, in the blog post <a href=\"https://davidhadas.wordpress.com/2022/08/29/knative-an-opinionated-kubernetes\">Opinionated Kubernetes</a> which describes how Knative simplifies and unifies the way web services are deployed on Kubernetes.</li>\n<li>You may contact Guard maintainers on the <a href=\"https://kubernetes.slack.com/archives/C019LFTGNQ3\">SIG Security</a> Slack channel or on the Knative community <a href=\"https://knative.slack.com/archives/CBYV1E0TG\">security</a> Slack channel. The Knative community channel will move soon to the <a href=\"https://communityinviter.com/apps/cloud-native/cncf\">CNCF Slack</a> under the name <code>#knative-security</code>.</li>\n</ul>\n<p>The goal of this post is to invite the Kubernetes community to action and introduce Security-Behavior monitoring and control to help secure Kubernetes based deployments. Hopefully, the community as a follow up will:</p>\n<ol>\n<li>Analyze the cyber challenges presented for different Kubernetes use cases</li>\n<li>Add appropriate security documentation for users on how to introduce Security-Behavior monitoring and control.</li>\n<li>Consider how to integrate with tools that can help users monitor and control their vulnerable services.</li>\n</ol>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>You are welcome to get involved and join the effort to develop security behavior monitoring\nand control for Kubernetes; to share feedback and contribute to code or documentation;\nand to make or suggest improvements of any kind.</p>","PublishedAt":"2023-01-20 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/01/20/security-behavior-analysis/","SourceName":"Kubernetes"}},{"node":{"ID":2750,"Title":"Blog: Protect Your Mission-Critical Pods From Eviction With PriorityClass","Description":"<p><strong>Author:</strong> Sunny Bhambhani (InfraCloud Technologies)</p>\n<p>Kubernetes has been widely adopted, and many organizations use it as their de-facto orchestration engine for running workloads that need to be created and deleted frequently.</p>\n<p>Therefore, proper scheduling of the pods is key to ensuring that application pods are up and running within the Kubernetes cluster without any issues. This article delves into the use cases around resource management by leveraging the <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass\">PriorityClass</a> object to protect mission-critical or high-priority pods from getting evicted and making sure that the application pods are up, running, and serving traffic.</p>\n<h2 id=\"resource-management-in-kubernetes\">Resource management in Kubernetes</h2>\n<p>The control plane consists of multiple components, out of which the scheduler (usually the built-in <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/\">kube-scheduler</a>) is one of the components which is responsible for assigning a node to a pod.</p>\n<p>Whenever a pod is created, it enters a &quot;pending&quot; state, after which the scheduler determines which node is best suited for the placement of the new pod.</p>\n<p>In the background, the scheduler runs as an infinite loop looking for pods without a <code>nodeName</code> set that are <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/\">ready for scheduling</a>. For each Pod that needs scheduling, the scheduler tries to decide which node should run that Pod.</p>\n<p>If the scheduler cannot find any node, the pod remains in the pending state, which is not ideal.</p>\n<div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> To name a few, <code>nodeSelector</code> , <code>taints and tolerations</code> , <code>nodeAffinity</code> , the rank of nodes based on available resources (for example, CPU and memory), and several other criteria are used to determine the pod's placement.\n</div>\n<p>The below diagram, from point number 1 through 4, explains the request flow:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/kube-scheduler.svg\"\nalt=\"A diagram showing the scheduling of three Pods that a client has directly created.\"/> <figcaption>\n<h4>Scheduling in Kubernetes</h4>\n</figcaption>\n</figure>\n<h2 id=\"typical-use-cases\">Typical use cases</h2>\n<p>Below are some real-life scenarios where control over the scheduling and eviction of pods may be required.</p>\n<ol>\n<li>\n<p>Let's say the pod you plan to deploy is critical, and you have some resource constraints. An example would be the DaemonSet of an infrastructure component like Grafana Loki. The Loki pods must run before other pods can on every node. In such cases, you could ensure resource availability by manually identifying and deleting the pods that are not required or by adding a new node to the cluster. Both these approaches are unsuitable since the former would be tedious to execute, and the latter could involve an expenditure of time and money.</p>\n</li>\n<li>\n<p>Another use case could be a single cluster that holds the pods for the below environments with associated priorities:</p>\n<ul>\n<li>Production (<code>prod</code>): top priority</li>\n<li>Preproduction (<code>preprod</code>): intermediate priority</li>\n<li>Development (<code>dev</code>): least priority</li>\n</ul>\n</li>\n</ol>\n<p>In the event of high resource consumption in the cluster, there is competition for CPU and memory resources on the nodes. While cluster-level autoscaling <em>may</em> add more nodes, it takes time. In the interim, if there are no further nodes to scale the cluster, some Pods could remain in a Pending state, or the service could be degraded as they compete for resources. If the kubelet does evict a Pod from the node, that eviction would be random because the kubelet doesn‚Äôt have any special information about which Pods to evict and which to keep.</p>\n<ol start=\"3\">\n<li>A third example could be a microservice backed by a queuing application or a database running into a resource crunch and the queue or database getting evicted. In such a case, all the other services would be rendered useless until the database can serve traffic again.</li>\n</ol>\n<p>There can also be other scenarios where you want to control the order of scheduling or order of eviction of pods.</p>\n<h2 id=\"priorityclasses-in-kubernetes\">PriorityClasses in Kubernetes</h2>\n<p>PriorityClass is a cluster-wide API object in Kubernetes and part of the <code>scheduling.k8s.io/v1</code> API group. It contains a mapping of the PriorityClass name (defined in <code>.metadata.name</code>) and an integer value (defined in <code>.value</code>). This represents the value that the scheduler uses to determine Pod's relative priority.</p>\n<p>Additionally, when you create a cluster using kubeadm or a managed Kubernetes service (for example, Azure Kubernetes Service), Kubernetes uses PriorityClasses to safeguard the pods that are hosted on the control plane nodes. This ensures that critical cluster components such as CoreDNS and kube-proxy can run even if resources are constrained.</p>\n<p>This availability of pods is achieved through the use of a special PriorityClass that ensures the pods are up and running and that the overall cluster is not affected.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get priorityclass\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME VALUE GLOBAL-DEFAULT AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">system-cluster-critical 2000000000 false 82m\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">system-node-critical 2000001000 false 82m\n</span></span></span></code></pre></div><p>The diagram below shows exactly how it works with the help of an example, which will be detailed in the upcoming section.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/decision-tree.svg\"\nalt=\"A flow chart that illustrates how the kube-scheduler prioritizes new Pods and potentially preempts existing Pods\"/> <figcaption>\n<h4>Pod scheduling and preemption</h4>\n</figcaption>\n</figure>\n<h3 id=\"pod-priority-and-preemption\">Pod priority and preemption</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption\">Pod preemption</a> is a Kubernetes feature that allows the cluster to preempt pods (removing an existing Pod in favor of a new Pod) on the basis of priority. <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority\">Pod priority</a> indicates the importance of a pod relative to other pods while scheduling. If there aren't enough resources to run all the current pods, the scheduler tries to evict lower-priority pods over high-priority ones.</p>\n<p>Also, when a healthy cluster experiences a node failure, typically, lower-priority pods get preempted to create room for higher-priority pods on the available node. This happens even if the cluster can bring up a new node automatically since pod creation is usually much faster than bringing up a new node.</p>\n<h3 id=\"priorityclass-requirements\">PriorityClass requirements</h3>\n<p>Before you set up PriorityClasses, there are a few things to consider.</p>\n<ol>\n<li>Decide which PriorityClasses are needed. For instance, based on environment, type of pods, type of applications, etc.</li>\n<li>The default PriorityClass resource for your cluster. The pods without a <code>priorityClassName</code> will be treated as priority 0.</li>\n<li>Use a consistent naming convention for all PriorityClasses.</li>\n<li>Make sure that the pods for your workloads are running with the right PriorityClass.</li>\n</ol>\n<h2 id=\"priorityclass-hands-on-example\">PriorityClass hands-on example</h2>\n<p>Let‚Äôs say there are 3 application pods: one for prod, one for preprod, and one for development. Below are three sample YAML manifest files for each of those.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#080;font-style:italic\"># development</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>dev-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">env</span>:<span style=\"color:#bbb\"> </span>dev<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>dev-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;256Mi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;0.2&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">limits</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;.5Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;0.5&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#080;font-style:italic\"># preproduction</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>preprod-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">env</span>:<span style=\"color:#bbb\"> </span>preprod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>preprod-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;1.5Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;1.5&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">limits</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#080;font-style:italic\"># production</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>prod-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">env</span>:<span style=\"color:#bbb\"> </span>prod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>prod-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">limits</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>You can create these pods with the <code>kubectl create -f &lt;FILE.yaml&gt;</code> command, and then check their status\nusing the <code>kubectl get pods</code> command. You can see if they are up and look ready to serve traffic:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get pods --show-labels\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE LABELS\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">dev-nginx 1/1 Running 0 55s env=dev\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">preprod-nginx 1/1 Running 0 55s env=preprod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">prod-nginx 0/1 Pending 0 55s env=prod\n</span></span></span></code></pre></div><p>Bad news. The pod for the Production environment is still Pending and isn't serving any traffic.</p>\n<p>Let's see why this is happening:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get events\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">5s Warning FailedScheduling pod/prod-nginx 0/2 nodes are available: 1 Insufficient cpu, 2 Insufficient memory.\n</span></span></span></code></pre></div><p>In this example, there is only one worker node, and that node has a resource crunch.</p>\n<p>Now, let's look at how PriorityClass can help in this situation since prod should be given higher priority than the other environments.</p>\n<h2 id=\"priorityclass-api\">PriorityClass API</h2>\n<p>Before creating PriorityClasses based on these requirements, let's see what a basic manifest for a PriorityClass looks like and outline some prerequisites:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>scheduling.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PriorityClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>PRIORITYCLASS_NAME<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">0</span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># any integer value between -1000000000 to 1000000000 </span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">description</span>:<span style=\"color:#bbb\"> </span>&gt;-<span style=\"color:#b44;font-style:italic\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> </span><span style=\"color:#bbb\"> </span>(Optional) description goes here!<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">globalDefault</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">false</span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># or true. Only one PriorityClass can be the global default.</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Below are some prerequisites for PriorityClasses:</p>\n<ul>\n<li>The name of a PriorityClass must be a valid DNS subdomain name.</li>\n<li>When you make your own PriorityClass, the name should not start with <code>system-</code>, as those names are\nreserved by Kubernetes itself (for example, they are used for two built-in PriorityClasses).</li>\n<li>Its absolute value should be between -1000000000 to 1000000000 (1 billion).</li>\n<li>Larger numbers are reserved by PriorityClasses such as <code>system-cluster-critical</code>\n(this Pod is critically important to the cluster) and <code>system-node-critical</code> (the node\ncritically relies on this Pod).\n<code>system-node-critical</code> is a higher priority than <code>system-cluster-critical</code>, because a\ncluster-critical Pod can only work well if the node where it is running has all its node-level\ncritical requirements met.</li>\n<li>There are two optional fields:\n<ul>\n<li><code>globalDefault</code>: When true, this PriorityClass is used for pods where a <code>priorityClassName</code> is not specified.\nOnly one PriorityClass with <code>globalDefault</code> set to true can exist in a cluster.<br>\nIf there is no PriorityClass defined with globalDefault set to true, all the pods with no priorityClassName defined will be treated with 0 priority (i.e. the least priority).</li>\n<li><code>description</code>: A string with a meaningful value so that people know when to use this PriorityClass.</li>\n</ul>\n</li>\n</ul>\n<div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> Adding a PriorityClass with <code>globalDefault</code> set to <code>true</code> does not mean it will apply the same to the existing pods that are already running. This will be applicable only to the pods that came into existence after the PriorityClass was created.\n</div>\n<h3 id=\"priorityclass-in-action\">PriorityClass in action</h3>\n<p>Here's an example. Next, create some environment-specific PriorityClasses:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>scheduling.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PriorityClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>dev-pc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1000000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">globalDefault</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">false</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">description</span>:<span style=\"color:#bbb\"> </span>&gt;-<span style=\"color:#b44;font-style:italic\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> </span><span style=\"color:#bbb\"> </span>(Optional) This priority class should only be used for all development pods.<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>scheduling.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PriorityClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>preprod-pc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">2000000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">globalDefault</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">false</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">description</span>:<span style=\"color:#bbb\"> </span>&gt;-<span style=\"color:#b44;font-style:italic\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> </span><span style=\"color:#bbb\"> </span>(Optional) This priority class should only be used for all preprod pods.<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>scheduling.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PriorityClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>prod-pc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">4000000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">globalDefault</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">false</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">description</span>:<span style=\"color:#bbb\"> </span>&gt;-<span style=\"color:#b44;font-style:italic\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> </span><span style=\"color:#bbb\"> </span>(Optional) This priority class should only be used for all prod pods.<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Use <code>kubectl create -f &lt;FILE.YAML&gt;</code> command to create a pc and <code>kubectl get pc</code> to check its status.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get pc\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME VALUE GLOBAL-DEFAULT AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">dev-pc 1000000 false 3m13s\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">preprod-pc 2000000 false 2m3s\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">prod-pc 4000000 false 7s\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">system-cluster-critical 2000000000 false 82m\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">system-node-critical 2000001000 false 82m\n</span></span></span></code></pre></div><p>The new PriorityClasses are in place now. A small change is needed in the pod manifest or pod template (in a ReplicaSet or Deployment). In other words, you need to specify the priority class name at <code>.spec.priorityClassName</code> (which is a string value).</p>\n<p>First update the previous production pod manifest file to have a PriorityClass assigned, then delete the Production pod and recreate it. You can't edit the priority class for a Pod that already exists.</p>\n<p>In my cluster, when I tried this, here's what happened.\nFirst, that change seems successful; the status of pods has been updated:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get pods --show-labels\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE LABELS\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">dev-nginx 1/1 Terminating 0 55s env=dev\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">preprod-nginx 1/1 Running 0 55s env=preprod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">prod-nginx 0/1 Pending 0 55s env=prod\n</span></span></span></code></pre></div><p>The dev-nginx pod is getting terminated. Once that is successfully terminated and there are enough resources for the prod pod, the control plane can schedule the prod pod:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">Warning FailedScheduling pod/prod-nginx 0/2 nodes are available: 1 Insufficient cpu, 2 Insufficient memory.\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Preempted pod/dev-nginx by default/prod-nginx on node node01\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Killing pod/dev-nginx Stopping container dev-nginx\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Scheduled pod/prod-nginx Successfully assigned default/prod-nginx to node01\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Pulling pod/prod-nginx Pulling image &#34;nginx&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Pulled pod/prod-nginx Successfully pulled image &#34;nginx&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Created pod/prod-nginx Created container prod-nginx\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Normal Started pod/prod-nginx Started container prod-nginx\n</span></span></span></code></pre></div><h2 id=\"enforcement\">Enforcement</h2>\n<p>When you set up PriorityClasses, they exist just how you defined them. However, people\n(and tools) that make changes to your cluster are free to set any PriorityClass, or to not\nset any PriorityClass at all.\nHowever, you can use other Kubernetes features to make sure that the priorities you wanted\nare actually applied.</p>\n<p>As an alpha feature, you can define a <a href=\"https://kubernetes.io/blog/2022/12/20/validating-admission-policies-alpha/\">ValidatingAdmissionPolicy</a> and a ValidatingAdmissionPolicyBinding so that, for example,\nPods that go into the <code>prod</code> namespace must use the <code>prod-pc</code> PriorityClass.\nWith another ValidatingAdmissionPolicyBinding you ensure that the <code>preprod</code> namespace\nuses the <code>preprod-pc</code> PriorityClass, and so on.\nIn <em>any</em> cluster, you can enforce similar controls using external projects such as\n<a href=\"https://kyverno.io/\">Kyverno</a> or <a href=\"https://open-policy-agent.github.io/gatekeeper/\">Gatekeeper</a>,\nthrough validating admission webhooks.</p>\n<p>However you do it, Kubernetes gives you options to make sure that the PriorityClasses are\nused how you wanted them to be, or perhaps just to\n<a href=\"https://open-policy-agent.github.io/gatekeeper/website/docs/violations/#warn-enforcement-action\">warn</a>\nusers when they pick an unsuitable option.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>The above example and its events show you what this feature of Kubernetes brings to the table, along with several scenarios where you can use this feature. To reiterate, this helps ensure that mission-critical pods are up and available to serve the traffic and, in the case of a resource crunch, determines cluster behavior.</p>\n<p>It gives you some power to decide the order of scheduling and order of <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption\">preemption</a> for Pods. Therefore, you need to define the PriorityClasses sensibly.\nFor example, if you have a cluster autoscaler to add nodes on demand,\nmake sure to run it with the <code>system-cluster-critical</code> PriorityClass. You don't want to\nget in a situation where the autoscaler has been preempted and there are no new nodes\ncoming online.</p>\n<p>If you have any queries or feedback, feel free to reach out to me on <a href=\"http://www.linkedin.com/in/sunnybhambhani\">LinkedIn</a>.</p>","PublishedAt":"2023-01-12 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/01/12/protect-mission-critical-pods-priorityclass/","SourceName":"Kubernetes"}},{"node":{"ID":2698,"Title":"Blog: Kubernetes 1.26: Eviction policy for unhealthy pods guarded by PodDisruptionBudgets","Description":"<p><strong>Authors:</strong> Filip K≈ôepinsk√Ω (Red Hat), Morten Torkildsen (Google), Ravi Gudimetla (Apple)</p>\n<p>Ensuring the disruptions to your applications do not affect its availability isn't a simple\ntask. Last month's release of Kubernetes v1.26 lets you specify an <em>unhealthy pod eviction policy</em>\nfor <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets\">PodDisruptionBudgets</a> (PDBs)\nto help you maintain that availability during node management operations.\nIn this article, we will dive deeper into what modifications were introduced for PDBs to\ngive application owners greater flexibility in managing disruptions.</p>\n<h2 id=\"what-problems-does-this-solve\">What problems does this solve?</h2>\n<p>API-initiated eviction of pods respects PodDisruptionBudgets (PDBs). This means that a requested <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/#pod-disruption\">voluntary disruption</a>\nvia an eviction to a Pod, should not disrupt a guarded application and <code>.status.currentHealthy</code> of a PDB should not fall\nbelow <code>.status.desiredHealthy</code>. Running pods that are <a href=\"https://kubernetes.io/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod\">Unhealthy</a>\ndo not count towards the PDB status, but eviction of these is only possible in case the application\nis not disrupted. This helps disrupted or not yet started application to achieve availability\nas soon as possible without additional downtime that would be caused by evictions.</p>\n<p>Unfortunately, this poses a problem for cluster administrators that would like to drain nodes\nwithout any manual interventions. Misbehaving applications with pods in <code>CrashLoopBackOff</code>\nstate (due to a bug or misconfiguration) or pods that are simply failing to become ready\nmake this task much harder. Any eviction request will fail due to violation of a PDB,\nwhen all pods of an application are unhealthy. Draining of a node cannot make any progress\nin that case.</p>\n<p>On the other hand there are users that depend on the existing behavior, in order to:</p>\n<ul>\n<li>prevent data-loss that would be caused by deleting pods that are guarding an underlying resource or storage</li>\n<li>achieve the best availability possible for their application</li>\n</ul>\n<p>Kubernetes 1.26 introduced a new experimental field to the PodDisruptionBudget API: <code>.spec.unhealthyPodEvictionPolicy</code>.\nWhen enabled, this field lets you support both of those requirements.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>API-initiated eviction is the process that triggers graceful pod termination.\nThe process can be initiated either by calling the API directly,\nby using a <code>kubectl drain</code> command, or other actors in the cluster.\nDuring this process every pod removal is consulted with appropriate PDBs,\nto ensure that a sufficient number of pods is always running in the cluster.</p>\n<p>The following policies allow PDB authors to have a greater control how the process deals with unhealthy pods.</p>\n<p>There are two policies <code>IfHealthyBudget</code> and <code>AlwaysAllow</code> to choose from.</p>\n<p>The former, <code>IfHealthyBudget</code>, follows the existing behavior to achieve the best availability\nthat you get by default. Unhealthy pods can be disrupted only if their application\nhas a minimum available <code>.status.desiredHealthy</code> number of pods.</p>\n<p>By setting the <code>spec.unhealthyPodEvictionPolicy</code> field of your PDB to <code>AlwaysAllow</code>,\nyou are choosing the best effort availability for your application.\nWith this policy it is always possible to evict unhealthy pods.\nThis will make it easier to maintain and upgrade your clusters.</p>\n<p>We think that <code>AlwaysAllow</code> will often be a better choice, but for some critical workloads you may\nstill prefer to protect even unhealthy Pods from node drains or other forms of API-initiated\neviction.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>This is an alpha feature, which means you have to enable the <code>PDBUnhealthyPodEvictionPolicy</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a>,\nwith the command line argument <code>--feature-gates=PDBUnhealthyPodEvictionPolicy=true</code>\nto the kube-apiserver.</p>\n<p>Here's an example. Assume that you've enabled the feature gate in your cluster, and that you\nalready defined a Deployment that runs a plain webserver. You labelled the Pods for that\nDeployment with <code>app: nginx</code>.\nYou want to limit avoidable disruption, and you know that best effort availability is\nsufficient for this app.\nYou decide to allow evictions even if those webserver pods are unhealthy.\nYou create a PDB to guard this application, with the <code>AlwaysAllow</code> policy for evicting\nunhealthy pods:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>policy/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PodDisruptionBudget<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>nginx-pdb<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">app</span>:<span style=\"color:#bbb\"> </span>nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">maxUnavailable</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">unhealthyPodEvictionPolicy</span>:<span style=\"color:#bbb\"> </span>AlwaysAllow<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>Read the KEP: <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3017-pod-healthy-policy-for-pdb\">Unhealthy Pod Eviction Policy for PDBs</a></li>\n<li>Read the documentation: <a href=\"https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy\">Unhealthy Pod Eviction Policy</a> for PodDisruptionBudgets</li>\n<li>Review the Kubernetes documentation for <a href=\"docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets\">PodDisruptionBudgets</a>, <a href=\"docs/tasks/administer-cluster/safely-drain-node/\">draining of Nodes</a> and <a href=\"docs/concepts/scheduling-eviction/api-eviction/\">evictions</a></li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>If you have any feedback, please reach out to us in the <a href=\"https://kubernetes.slack.com/archives/C18NZM5K9\">#sig-apps</a> channel on Slack (visit <a href=\"https://slack.k8s.io/\">https://slack.k8s.io/</a> for an invitation if you need one), or on the SIG Apps mailing list: <a href=\"mailto:kubernetes-sig-apps@googlegroups.com\">kubernetes-sig-apps@googlegroups.com</a></p>","PublishedAt":"2023-01-06 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/01/06/unhealthy-pod-eviction-policy-for-pdbs/","SourceName":"Kubernetes"}},{"node":{"ID":2692,"Title":"Blog: Kubernetes 1.26: Retroactive Default StorageClass","Description":"<p><strong>Author:</strong> Roman Bedn√°≈ô (Red Hat)</p>\n<p>The v1.25 release of Kubernetes introduced an alpha feature to change how a default StorageClass was assigned to a PersistentVolumeClaim (PVC).\nWith the feature enabled, you no longer need to create a default StorageClass first and PVC second to assign the class. Additionally, any PVCs without a StorageClass assigned can be updated later.\nThis feature was graduated to beta in Kubernetes 1.26.</p>\n<p>You can read <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retroactive-default-storageclass-assignment\">retroactive default StorageClass assignment</a> in the Kubernetes documentation for more details about how to use that,\nor you can read on to learn about why the Kubernetes project is making this change.</p>\n<h2 id=\"why-did-storageclass-assignment-need-improvements\">Why did StorageClass assignment need improvements</h2>\n<p>Users might already be familiar with a similar feature that assigns default StorageClasses to <strong>new</strong> PVCs at the time of creation. This is currently handled by the <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass\">admission controller</a>.</p>\n<p>But what if there wasn't a default StorageClass defined at the time of PVC creation?\nUsers would end up with a PVC that would never be assigned a class.\nAs a result, no storage would be provisioned, and the PVC would be somewhat &quot;stuck&quot; at this point.\nGenerally, two main scenarios could result in &quot;stuck&quot; PVCs and cause problems later down the road.\nLet's take a closer look at each of them.</p>\n<h3 id=\"changing-default-storageclass\">Changing default StorageClass</h3>\n<p>With the alpha feature enabled, there were two options admins had when they wanted to change the default StorageClass:</p>\n<ol>\n<li>\n<p>Creating a new StorageClass as default before removing the old one associated with the PVC.\nThis would result in having two defaults for a short period.\nAt this point, if a user were to create a PersistentVolumeClaim with storageClassName set to <code>null</code> (implying default StorageClass), the newest default StorageClass would be chosen and assigned to this PVC.</p>\n</li>\n<li>\n<p>Removing the old default first and creating a new default StorageClass.\nThis would result in having no default for a short time.\nSubsequently, if a user were to create a PersistentVolumeClaim with storageClassName set to <code>null</code> (implying default StorageClass), the PVC would be in <code>Pending</code> state forever.\nThe user would have to fix this by deleting the PVC and recreating it once the default StorageClass was available.</p>\n</li>\n</ol>\n<h3 id=\"resource-ordering-during-cluster-installation\">Resource ordering during cluster installation</h3>\n<p>If a cluster installation tool needed to create resources that required storage, for example, an image registry, it was difficult to get the ordering right.\nThis is because any Pods that required storage would rely on the presence of a default StorageClass and would fail to be created if it wasn't defined.</p>\n<h2 id=\"what-changed\">What changed</h2>\n<p>We've changed the PersistentVolume (PV) controller to assign a default StorageClass to any unbound PersistentVolumeClaim that has the storageClassName set to <code>null</code>.\nWe've also modified the PersistentVolumeClaim admission within the API server to allow the change of values from an unset value to an actual StorageClass name.</p>\n<h3 id=\"null-vs-empty-string\">Null <code>storageClassName</code> versus <code>storageClassName: &quot;&quot;</code> - does it matter?</h3>\n<p>Before this feature was introduced, those values were equal in terms of behavior. Any PersistentVolumeClaim with the storageClassName set to <code>null</code> or <code>&quot;&quot;</code> would bind to an existing PersistentVolume resource with storageClassName also set to <code>null</code> or <code>&quot;&quot;</code>.</p>\n<p>With this new feature enabled we wanted to maintain this behavior but also be able to update the StorageClass name.\nWith these constraints in mind, the feature changes the semantics of <code>null</code>. If a default StorageClass is present, <code>null</code> would translate to &quot;Give me a default&quot; and <code>&quot;&quot;</code> would mean &quot;Give me PersistentVolume that also has <code>&quot;&quot;</code> StorageClass name.&quot; In the absence of a StorageClass, the behavior would remain unchanged.</p>\n<p>Summarizing the above, we've changed the semantics of <code>null</code> so that its behavior depends on the presence or absence of a definition of default StorageClass.</p>\n<p>The tables below show all these cases to better describe when PVC binds and when its StorageClass gets updated.</p>\n<table>\n<caption>PVC binding behavior with Retroactive default StorageClass</caption>\n<thead>\n<tr>\n<th colspan=\"2\"></th>\n<th>PVC <tt>storageClassName</tt> = <code>\"\"</code></th>\n<th>PVC <tt>storageClassName</tt> = <code>null</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan=\"2\">Without default class</td>\n<td>PV <tt>storageClassName</tt> = <code>\"\"</code></td>\n<td>binds</td>\n<td>binds</td>\n</tr>\n<tr>\n<td>PV without <tt>storageClassName</tt></td>\n<td>binds</td>\n<td>binds</td>\n</tr>\n<tr>\n<td rowspan=\"2\">With default class</td>\n<td>PV <tt>storageClassName</tt> = <code>\"\"</code></td>\n<td>binds</td>\n<td>class updates</td>\n</tr>\n<tr>\n<td>PV without <tt>storageClassName</tt></td>\n<td>binds</td>\n<td>class updates</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"how-to-use-it\">How to use it</h2>\n<p>If you want to test the feature whilst it's alpha, you need to enable the relevant feature gate in the kube-controller-manager and the kube-apiserver. Use the <code>--feature-gates</code> command line argument:</p>\n<pre tabindex=\"0\"><code>--feature-gates=&#34;...,RetroactiveDefaultStorageClass=true&#34;\n</code></pre><h3 id=\"test-drive\">Test drive</h3>\n<p>If you would like to see the feature in action and verify it works fine in your cluster here's what you can try:</p>\n<ol>\n<li>Define a basic PersistentVolumeClaim:</li>\n</ol>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pvc-1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><ol start=\"2\">\n<li>Create the PersistentVolumeClaim when there is no default StorageClass. The PVC won't provision or bind (unless there is an existing, suitable PV already present) and will remain in <code>Pending</code> state.</li>\n</ol>\n<pre tabindex=\"0\"><code>$ kc get pvc\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\npvc-1 Pending\n</code></pre><ol start=\"3\">\n<li>Configure one StorageClass as default.</li>\n</ol>\n<pre tabindex=\"0\"><code>$ kc patch sc -p &#39;{&#34;metadata&#34;:{&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;true&#34;}}}&#39;\nstorageclass.storage.k8s.io/my-storageclass patched\n</code></pre><ol start=\"4\">\n<li>Verify that PersistentVolumeClaims is now provisioned correctly and was updated retroactively with new default StorageClass.</li>\n</ol>\n<pre tabindex=\"0\"><code>$ kc get pvc\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\npvc-1 Bound pvc-06a964ca-f997-4780-8627-b5c3bf5a87d8 1Gi RWO my-storageclass 87m\n</code></pre><h3 id=\"new-metrics\">New metrics</h3>\n<p>To help you see that the feature is working as expected we also introduced a new <code>retroactive_storageclass_total</code> metric to show how many times that the PV controller attempted to update PersistentVolumeClaim, and <code>retroactive_storageclass_errors_total</code> to show how many of those attempts failed.</p>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>We always welcome new contributors so if you would like to get involved you can join our <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special-Interest-Group</a> (SIG).</p>\n<p>If you would like to share feedback, you can do so on our <a href=\"https://app.slack.com/client/T09NY5SBT/C09QZFCE5\">public Slack channel</a>.</p>\n<p>Special thanks to all the contributors that provided great reviews, shared valuable insight and helped implement this feature (alphabetical order):</p>\n<ul>\n<li>Deep Debroy (<a href=\"https://github.com/ddebroy\">ddebroy</a>)</li>\n<li>Divya Mohan (<a href=\"https://github.com/divya-mohan0209\">divya-mohan0209</a>)</li>\n<li>Jan ≈†afr√°nek (<a href=\"https://github.com/jsafrane/\">jsafrane</a>)</li>\n<li>Joe Betz (<a href=\"https://github.com/jpbetz\">jpbetz</a>)</li>\n<li>Jordan Liggitt (<a href=\"https://github.com/liggitt\">liggitt</a>)</li>\n<li>Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>)</li>\n<li>Seokho Son (<a href=\"https://github.com/seokho-son\">seokho-son</a>)</li>\n<li>Shannon Kularathna (<a href=\"https://github.com/shannonxtreme\">shannonxtreme</a>)</li>\n<li>Tim Bannister (<a href=\"https://github.com/sftim\">sftim</a>)</li>\n<li>Tim Hockin (<a href=\"https://github.com/thockin\">thockin</a>)</li>\n<li>Wojciech Tyczynski (<a href=\"https://github.com/wojtek-t\">wojtek-t</a>)</li>\n<li>Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>)</li>\n</ul>","PublishedAt":"2023-01-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/01/05/retroactive-default-storage-class/","SourceName":"Kubernetes"}},{"node":{"ID":2674,"Title":"Blog: Kubernetes v1.26: Alpha support for cross-namespace storage data sources","Description":"<p><strong>Author:</strong> Takafumi Takahashi (Hitachi Vantara)</p>\n<p>Kubernetes v1.26, released last month, introduced an alpha feature that\nlets you specify a data source for a PersistentVolumeClaim, even where the source\ndata belong to a different namespace.\nWith the new feature enabled, you specify a namespace in the <code>dataSourceRef</code> field of\na new PersistentVolumeClaim. Once Kubernetes checks that access is OK, the new\nPersistentVolume can populate its data from the storage source specified in that other\nnamespace.\nBefore Kubernetes v1.26, provided your cluster had the <code>AnyVolumeDataSource</code> feature enabled,\nyou could already provision new volumes from a data source in the <strong>same</strong>\nnamespace.\nHowever, that only worked for the data source in the same namespace,\ntherefore users couldn't provision a PersistentVolume with a claim\nin one namespace from a data source in other namespace.\nTo solve this problem, Kubernetes v1.26 added a new alpha <code>namespace</code> field\nto <code>dataSourceRef</code> field in PersistentVolumeClaim the API.</p>\n<h2 id=\"how-it-works\">How it works</h2>\n<p>Once the csi-provisioner finds that a data source is specified with a <code>dataSourceRef</code> that\nhas a non-empty namespace name,\nit checks all reference grants within the namespace that's specified by the<code>.spec.dataSourceRef.namespace</code>\nfield of the PersistentVolumeClaim, in order to see if access to the data source is allowed.\nIf any ReferenceGrant allows access, the csi-provisioner provisions a volume from the data source.</p>\n<h2 id=\"trying-it-out\">Trying it out</h2>\n<p>The following things are required to use cross namespace volume provisioning:</p>\n<ul>\n<li>Enable the <code>AnyVolumeDataSource</code> and <code>CrossNamespaceVolumeDataSource</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gates</a> for the kube-apiserver and kube-controller-manager</li>\n<li>Install a CRD for the specific <code>VolumeSnapShot</code> controller</li>\n<li>Install the CSI Provisioner controller and enable the <code>CrossNamespaceVolumeDataSource</code> feature gate</li>\n<li>Install the CSI driver</li>\n<li>Install a CRD for ReferenceGrants</li>\n</ul>\n<h2 id=\"putting-it-all-together\">Putting it all together</h2>\n<p>To see how this works, you can install the sample and try it out.\nThis sample do to create PVC in dev namespace from VolumeSnapshot in prod namespace.\nThat is a simple example. For real world use, you might want to use a more complex approach.</p>\n<h3 id=\"example-assumptions\">Assumptions for this example</h3>\n<ul>\n<li>Your Kubernetes cluster was deployed with <code>AnyVolumeDataSource</code> and <code>CrossNamespaceVolumeDataSource</code> feature gates enabled</li>\n<li>There are two namespaces, dev and prod</li>\n<li>CSI driver is being deployed</li>\n<li>There is an existing VolumeSnapshot named <code>new-snapshot-demo</code> in the <em>prod</em> namespace</li>\n<li>The ReferenceGrant CRD (from the Gateway API project) is already deployed</li>\n</ul>\n<h3 id=\"grant-referencegrants-read-permission-to-the-csi-provisioner\">Grant ReferenceGrants read permission to the CSI Provisioner</h3>\n<p>Access to ReferenceGrants is only needed when the CSI driver\nhas the <code>CrossNamespaceVolumeDataSource</code> controller capability.\nFor this example, the external-provisioner needs <strong>get</strong>, <strong>list</strong>, and <strong>watch</strong>\npermissions for <code>referencegrants</code> (API group <code>gateway.networking.k8s.io</code>).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;gateway.networking.k8s.io&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;referencegrants&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">verbs</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;get&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;list&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;watch&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"enable-the-crossnamespacevolumedatasource-feature-gate-for-the-csi-provisioner\">Enable the CrossNamespaceVolumeDataSource feature gate for the CSI Provisioner</h3>\n<p>Add <code>--feature-gates=CrossNamespaceVolumeDataSource=true</code> to the csi-provisioner command line.\nFor example, use this manifest snippet to redefine the container:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">args</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- -v=5<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- --csi-address=/csi/csi.sock<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- --feature-gates=Topology=true<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- --feature-gates=CrossNamespaceVolumeDataSource=true<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>csi-provisioner:latest<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">imagePullPolicy</span>:<span style=\"color:#bbb\"> </span>IfNotPresent<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>csi-provisioner<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"create-a-referencegrant\">Create a ReferenceGrant</h3>\n<p>Here's a manifest for an example ReferenceGrant.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>gateway.networking.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ReferenceGrant<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>allow-prod-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>prod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">from</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">group</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>dev<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">to</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">group</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new-snapshot-demo<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"create-a-persistentvolumeclaim-by-using-cross-namespace-data-source\">Create a PersistentVolumeClaim by using cross namespace data source</h3>\n<p>Kubernetes creates a PersistentVolumeClaim on dev and the CSI driver populates\nthe PersistentVolume used on dev from snapshots on prod.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>dev<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSourceRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new-snapshot-demo<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>prod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>The enhancement proposal,\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/3294-provision-volumes-from-cross-namespace-snapshots\">Provision volumes from cross-namespace snapshots</a>, includes lots of detail about the history and technical implementation of this feature.</p>\n<p>Please get involved by joining the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group (SIG)</a>\nto help us enhance this feature.\nThere are a lot of good ideas already and we'd be thrilled to have more!</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>It takes a wonderful group to make wonderful software.\nSpecial thanks to the following people for the insightful reviews,\nthorough consideration and valuable contribution to the CrossNamespaceVolumeDataSouce feature:</p>\n<ul>\n<li>Michelle Au (msau42)</li>\n<li>Xing Yang (xing-yang)</li>\n<li>Masaki Kimura (mkimuram)</li>\n<li>Tim Hockin (thockin)</li>\n<li>Ben Swartzlander (bswartz)</li>\n<li>Rob Scott (robscott)</li>\n<li>John Griffith (j-griffith)</li>\n<li>Michael Henriksen (mhenriks)</li>\n<li>Mustafa Elbehery (Elbehery)</li>\n</ul>\n<p>It‚Äôs been a joy to work with y'all on this.</p>","PublishedAt":"2023-01-02 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/01/02/cross-namespace-data-sources-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":2669,"Title":"Blog: Kubernetes v1.26: Advancements in Kubernetes Traffic Engineering","Description":"<p><strong>Authors:</strong> Andrew Sy Kim (Google)</p>\n<p>Kubernetes v1.26 includes significant advancements in network traffic engineering with the graduation of\ntwo features (Service internal traffic policy support, and EndpointSlice terminating conditions) to GA,\nand a third feature (Proxy terminating endpoints) to beta. The combination of these enhancements aims\nto address short-comings in traffic engineering that people face today, and unlock new capabilities for the future.</p>\n<h2 id=\"traffic-loss-from-load-balancers-during-rolling-updates\">Traffic Loss from Load Balancers During Rolling Updates</h2>\n<p>Prior to Kubernetes v1.26, clusters could experience <a href=\"https://github.com/kubernetes/kubernetes/issues/85643\">loss of traffic</a>\nfrom Service load balancers during rolling updates when setting the <code>externalTrafficPolicy</code> field to <code>Local</code>.\nThere are a lot of moving parts at play here so a quick overview of how Kubernetes manages load balancers might help!</p>\n<p>In Kubernetes, you can create a Service with <code>type: LoadBalancer</code> to expose an application externally with a load balancer.\nThe load balancer implementation varies between clusters and platforms, but the Service provides a generic abstraction\nrepresenting the load balancer that is consistent across all Kubernetes installations.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>my-service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">app.kubernetes.io/name</span>:<span style=\"color:#bbb\"> </span>my-app<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ports</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">protocol</span>:<span style=\"color:#bbb\"> </span>TCP<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">80</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">targetPort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">9376</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>LoadBalancer<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Under the hood, Kubernetes allocates a NodePort for the Service, which is then used by kube-proxy to provide a\nnetwork data path from the NodePort to the Pod. A controller will then add all available Nodes in the cluster\nto the load balancer‚Äôs backend pool, using the designated NodePort for the Service as the backend target port.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-service-load-balancer.png\"\nalt=\"Figure 1: Overview of Service load balancers\"/> <figcaption>\n<p>Figure 1: Overview of Service load balancers</p>\n</figcaption>\n</figure>\n<p>Oftentimes it is beneficial to set <code>externalTrafficPolicy: Local</code> for Services, to avoid extra hops between\nNodes that are not running healthy Pods backing that Service. When using <code>externalTrafficPolicy: Local</code>,\nan additional NodePort is allocated for health checking purposes, such that Nodes that do not contain healthy\nPods are excluded from the backend pool for a load balancer.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-lb-healthy.png\"\nalt=\"Figure 2: Load balancer traffic to a healthy Node, when externalTrafficPolicy is Local\"/> <figcaption>\n<p>Figure 2: Load balancer traffic to a healthy Node, when externalTrafficPolicy is Local</p>\n</figcaption>\n</figure>\n<p>One such scenario where traffic can be lost is when a Node loses all Pods for a Service,\nbut the external load balancer has not probed the health check NodePort yet. The likelihood of this situation\nis largely dependent on the health checking interval configured on the load balancer. The larger the interval,\nthe more likely this will happen, since the load balancer will continue to send traffic to a node\neven after kube-proxy has removed forwarding rules for that Service. This also occurrs when Pods start terminating\nduring rolling updates. Since Kubernetes does not consider terminating Pods as ‚ÄúReady‚Äù, traffic can be loss\nwhen there are only terminating Pods on any given Node during a rolling update.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-lb-without-proxy-terminating-endpoints.png\"\nalt=\"Figure 3: Load balancer traffic to terminating endpoints, when externalTrafficPolicy is Local\"/> <figcaption>\n<p>Figure 3: Load balancer traffic to terminating endpoints, when externalTrafficPolicy is Local</p>\n</figcaption>\n</figure>\n<p>Starting in Kubernetes v1.26, kube-proxy enables the <code>ProxyTerminatingEndpoints</code> feature by default, which\nadds automatic failover and routing to terminating endpoints in scenarios where the traffic would otherwise\nbe dropped. More specifically, when there is a rolling update and a Node only contains terminating Pods,\nkube-proxy will route traffic to the terminating Pods based on their readiness. In addition, kube-proxy will\nactively fail the health check NodePort if there are only terminating Pods available. By doing so,\nkube-proxy alerts the external load balancer that new connections should not be sent to that Node but will\ngracefully handle requests for existing connections.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/traffic-engineering-lb-with-proxy-terminating-endpoints.png\"\nalt=\"Figure 4: Load Balancer traffic to terminating endpoints with ProxyTerminatingEndpoints enabled, when externalTrafficPolicy is Local\"/> <figcaption>\n<p>Figure 4: Load Balancer traffic to terminating endpoints with ProxyTerminatingEndpoints enabled, when externalTrafficPolicy is Local</p>\n</figcaption>\n</figure>\n<h3 id=\"endpointslice-conditions\">EndpointSlice Conditions</h3>\n<p>In order to support this new capability in kube-proxy, the EndpointSlice API introduced new conditions for endpoints:\n<code>serving</code> and <code>terminating</code>.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/endpointslice-overview.png\"\nalt=\"Figure 5: Overview of EndpointSlice conditions\"/> <figcaption>\n<p>Figure 5: Overview of EndpointSlice conditions</p>\n</figcaption>\n</figure>\n<p>The <code>serving</code> condition is semantically identical to <code>ready</code>, except that it can be <code>true</code> or <code>false</code>\nwhile a Pod is terminating, unlike <code>ready</code> which will always be <code>false</code> for terminating Pods for compatibility reasons.\nThe <code>terminating</code> condition is true for Pods undergoing termination (non-empty deletionTimestamp), false otherwise.</p>\n<p>The addition of these two conditions enables consumers of this API to understand Pod states that were previously not possible.\nFor example, we can now track &quot;ready&quot; and &quot;not ready&quot; Pods that are also terminating.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/endpointslice-with-terminating-pod.png\"\nalt=\"Figure 6: EndpointSlice conditions with a terminating Pod\"/> <figcaption>\n<p>Figure 6: EndpointSlice conditions with a terminating Pod</p>\n</figcaption>\n</figure>\n<p>Consumers of the EndpointSlice API, such as Kube-proxy and Ingress Controllers, can now use these conditions to coordinate connection draining\nevents, by continuing to forward traffic for existing connections but rerouting new connections to other non-terminating endpoints.</p>\n<h2 id=\"optimizing-internal-node-local-traffic\">Optimizing Internal Node-Local Traffic</h2>\n<p>Similar to how Services can set <code>externalTrafficPolicy: Local</code> to avoid extra hops for externally sourced traffic, Kubernetes\nnow supports <code>internalTrafficPolicy: Local</code>, to enable the same optimization for traffic originating within the cluster, specifically\nfor traffic using the Service Cluster IP as the destination address. This feature graduated to Beta in Kubernetes v1.24 and is graduating to GA in v1.26.</p>\n<p>Services default the <code>internalTrafficPolicy</code> field to <code>Cluster</code>, where traffic is randomly distributed to all endpoints.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/service-internal-traffic-policy-cluster.png\"\nalt=\"Figure 7: Service routing when internalTrafficPolicy is Cluster\"/> <figcaption>\n<p>Figure 7: Service routing when internalTrafficPolicy is Cluster</p>\n</figcaption>\n</figure>\n<p>When <code>internalTrafficPolicy</code> is set to <code>Local</code>, kube-proxy will forward internal traffic for a Service only if there is an available endpoint\nthat is local to the same Node.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/service-internal-traffic-policy-local.png\"\nalt=\"Figure 8: Service routing when internalTrafficPolicy is Local\"/> <figcaption>\n<p>Figure 8: Service routing when internalTrafficPolicy is Local</p>\n</figcaption>\n</figure>\n<div class=\"alert alert-warning caution callout\" role=\"alert\">\n<strong>Caution:</strong> When using <code>internalTrafficPoliy: Local</code>, traffic will be dropped by kube-proxy when no local endpoints are available.\n</div>\n<h2 id=\"getting-involved\">Getting Involved</h2>\n<p>If you're interested in future discussions on Kubernetes traffic engineering, you can get involved in SIG Network through the following ways:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-network\">#sig-network</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-network\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnetwork\">Open Community Issues/PRs</a></li>\n<li><a href=\"https://github.com/kubernetes/community/tree/master/sig-network#meetings\">Biweekly meetings</a></li>\n</ul>","PublishedAt":"2022-12-30 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/","SourceName":"Kubernetes"}},{"node":{"ID":2664,"Title":"Blog: Kubernetes 1.26: Job Tracking, to Support Massively Parallel Batch Workloads, Is Generally Available","Description":"<p><strong>Authors:</strong> Aldo Culquicondor (Google)</p>\n<p>The Kubernetes 1.26 release includes a stable implementation of the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Job</a>\ncontroller that can reliably track a large amount of Jobs with high levels of\nparallelism. <a href=\"https://github.com/kubernetes/community/tree/master/sig-apps\">SIG Apps</a>\nand <a href=\"https://github.com/kubernetes/community/tree/master/wg-batch\">WG Batch</a>\nhave worked on this foundational improvement since Kubernetes 1.22. After\nmultiple iterations and scale verifications, this is now the default\nimplementation of the Job controller.</p>\n<p>Paired with the Indexed <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode\">completion mode</a>,\nthe Job controller can handle massively parallel batch Jobs, supporting up to\n100k concurrent Pods.</p>\n<p>The new implementation also made possible the development of <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy\">Pod failure policy</a>,\nwhich is in beta in the 1.26 release.</p>\n<h2 id=\"how-do-i-use-this-feature\">How do I use this feature?</h2>\n<p>To use Job tracking with finalizers, upgrade to Kubernetes 1.25 or newer and\ncreate new Jobs. You can also use this feature in v1.23 and v1.24, if you have the\nability to enable the <code>JobTrackingWithFinalizers</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a>.</p>\n<p>If your cluster runs Kubernetes 1.26, Job tracking with finalizers is a stable\nfeature. For v1.25, it's behind that feature gate, and your cluster administrators may have\nexplicitly disabled it - for example, if you have a policy of not using\nbeta features.</p>\n<p>Jobs created before the upgrade will still be tracked using the legacy behavior.\nThis is to avoid retroactively adding finalizers to running Pods, which might\nintroduce race conditions.</p>\n<p>For maximum performance on large Jobs, the Kubernetes project recommends\nusing the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode\">Indexed completion mode</a>.\nIn this mode, the control plane is able to track Job progress with less API\ncalls.</p>\n<p>If you are a developer of operator(s) for batch, <a href=\"https://en.wikipedia.org/wiki/High-performance_computing\">HPC</a>,\n<a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\">AI</a>, <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">ML</a>\nor related workloads, we encourage you to use the Job API to delegate accurate\nprogress tracking to Kubernetes. If there is something missing in the Job API\nthat forces you to manage plain Pods, the <a href=\"https://github.com/kubernetes/community/tree/master/wg-batch\">Working Group Batch</a>\nwelcomes your feedback and contributions.</p>\n<h3 id=\"deprecation-notices\">Deprecation notices</h3>\n<p>During the development of the feature, the control plane added the annotation\n<a href=\"https://kubernetes.io/docs/reference/labels-annotations-taints/#batch-kubernetes-io-job-tracking\"><code>batch.kubernetes.io/job-tracking</code></a>\nto the Jobs that were created when the feature was enabled.\nThis allowed a safe transition for older Jobs, but it was never meant to stay.</p>\n<p>In the 1.26 release, we deprecated the annotation <code>batch.kubernetes.io/job-tracking</code>\nand the control plane will stop adding it in Kubernetes 1.27.\nAlong with that change, we will remove the legacy Job tracking implementation.\nAs a result, the Job controller will track all Jobs using finalizers and it will\nignore Pods that don't have the aforementioned finalizer.</p>\n<p>Before you upgrade your cluster to 1.27, we recommend that you verify that there\nare no running Jobs that don't have the annotation, or you wait for those jobs\nto complete.\nOtherwise, you might observe the control plane recreating some Pods.\nWe expect that this shouldn't affect any users, as the feature is enabled by\ndefault since Kubernetes 1.25, giving enough buffer for old jobs to complete.</p>\n<h2 id=\"what-problem-does-the-new-implementation-solve\">What problem does the new implementation solve?</h2>\n<p>Generally, Kubernetes workload controllers, such as ReplicaSet or StatefulSet,\nrely on the existence of Pods or other objects in the API to determine the\nstatus of the workload and whether replacements are needed.\nFor example, if a Pod that belonged to a ReplicaSet terminates or ceases to\nexist, the ReplicaSet controller needs to create a replacement Pod to satisfy\nthe desired number of replicas (<code>.spec.replicas</code>).</p>\n<p>Since its inception, the Job controller also relied on the existence of Pods in\nthe API to track Job status. A Job has <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode\">completion</a>\nand <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#handling-pod-and-container-failures\">failure handling</a>\npolicies, requiring the end state of a finished Pod to determine whether to\ncreate a replacement Pod or mark the Job as completed or failed. As a result,\nthe Job controller depended on Pods, even terminated ones, to remain in the API\nin order to keep track of the status.</p>\n<p>This dependency made the tracking of Job status unreliable, because Pods can be\ndeleted from the API for a number of reasons, including:</p>\n<ul>\n<li>The garbage collector removing orphan Pods when a Node goes down.</li>\n<li>The garbage collector removing terminated Pods when they reach a threshold.</li>\n<li>The Kubernetes scheduler preempting a Pod to accomodate higher priority Pods.</li>\n<li>The taint manager evicting a Pod that doesn't tolerate a <code>NoExecute</code> taint.</li>\n<li>External controllers, not included as part of Kubernetes, or humans deleting\nPods.</li>\n</ul>\n<h3 id=\"the-new-implementation\">The new implementation</h3>\n<p>When a controller needs to take an action on objects before they are removed, it\nshould add a <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/\">finalizer</a>\nto the objects that it manages.\nA finalizer prevents the objects from being deleted from the API until the\nfinalizers are removed. Once the controller is done with the cleanup and\naccounting for the deleted object, it can remove the finalizer from the object and the\ncontrol plane removes the object from the API.</p>\n<p>This is what the new Job controller is doing: adding a finalizer during Pod\ncreation, and removing the finalizer after the Pod has terminated and has been\naccounted for in the Job status. However, it wasn't that simple.</p>\n<p>The main challenge is that there are at least two objects involved: the Pod\nand the Job. While the finalizer lives in the Pod object, the accounting lives\nin the Job object. There is no mechanism to atomically remove the finalizer in\nthe Pod and update the counters in the Job status. Additionally, there could be\nmore than one terminated Pod at a given time.</p>\n<p>To solve this problem, we implemented a three staged approach, each translating\nto an API call.</p>\n<ol>\n<li>For each terminated Pod, add the unique ID (UID) of the Pod into short-lived\nlists stored in the <code>.status</code> of the owning Job\n(<a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/job-v1/#JobStatus\">.status.uncountedTerminatedPods</a>).</li>\n<li>Remove the finalizer from the Pods(s).</li>\n<li>Atomically do the following operations:\n<ul>\n<li>remove UIDs from the short-lived lists</li>\n<li>increment the overall <code>succeeded</code> and <code>failed</code> counters in the <code>status</code> of\nthe Job.</li>\n</ul>\n</li>\n</ol>\n<p>Additional complications come from the fact that the Job controller might\nreceive the results of the API changes in steps 1 and 2 out of order. We solved\nthis by adding an in-memory cache for removed finalizers.</p>\n<p>Still, we faced some issues during the beta stage, leaving some pods stuck\nwith finalizers in some conditions (<a href=\"https://github.com/kubernetes/kubernetes/issues/108645\">#108645</a>,\n<a href=\"https://github.com/kubernetes/kubernetes/issues/109485\">#109485</a>, and\n<a href=\"https://github.com/kubernetes/kubernetes/pull/111646\">#111646</a>). As a result,\nwe decided to switch that feature gate to be disabled by default for the 1.23\nand 1.24 releases.</p>\n<p>Once resolved, we re-enabled the feature for the 1.25 release. Since then, we\nhave received reports from our customers running tens of thousands of Pods at a\ntime in their clusters through the Job API. Seeing this success, we decided to\ngraduate the feature to stable in 1.26, as part of our long term commitment to\nmake the Job API the best way to run large batch Jobs in a Kubernetes cluster.</p>\n<p>To learn more about the feature, you can read the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2307-job-tracking-without-lingering-pods\">KEP</a>.</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>As with any Kubernetes feature, multiple people contributed to getting this\ndone, from testing and filing bugs to reviewing code.</p>\n<p>On behalf of SIG Apps, I would like to especially thank Jordan Liggitt (Google)\nfor helping me debug and brainstorm solutions for more than one race condition\nand Maciej Szulik (Red Hat) for his thorough reviews.</p>","PublishedAt":"2022-12-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/29/scalable-job-tracking-ga/","SourceName":"Kubernetes"}},{"node":{"ID":2655,"Title":"Blog: Kubernetes v1.26: CPUManager goes GA","Description":"<p><strong>Author:</strong>\nFrancesco Romani (Red Hat)</p>\n<p>The CPU Manager is a part of the kubelet, the Kubernetes node agent, which enables the user to allocate exclusive CPUs to containers.\nSince Kubernetes v1.10, where it <a href=\"https://kubernetes.io/blog/2018/07/24/feature-highlight-cpu-manager/\">graduated to Beta</a>, the CPU Manager proved itself reliable and\nfulfilled its role of allocating exclusive CPUs to containers, so adoption has steadily grown making it a staple component of performance-critical\nand low-latency setups. Over time, most changes were about bugfixes or internal refactoring, with the following noteworthy user-visible changes:</p>\n<ul>\n<li><a href=\"https://github.com/Kubernetes/Kubernetes/pull/83592\">support explicit reservation of CPUs</a>: it was already possible to request to reserve a given\nnumber of CPUs for system resources, including the kubelet itself, which will not be used for exclusive CPU allocation. Now it is possible to also\nexplicitly select which CPUs to reserve instead of letting the kubelet pick them up automatically.</li>\n<li><a href=\"https://github.com/Kubernetes/Kubernetes/pull/97415\">report the exclusively allocated CPUs</a> to containers, much like is already done for devices,\nusing the kubelet-local <a href=\"https://kubernetes.io/docs/concepts/extend-Kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources\">PodResources API</a>.</li>\n<li><a href=\"https://github.com/Kubernetes/Kubernetes/pull/101771\">optimize the usage of system resources</a>, eliminating unnecessary sysfs changes.</li>\n</ul>\n<p>The CPU Manager reached the point on which it &quot;just works&quot;, so in Kubernetes v1.26 it has graduated to generally available (GA).</p>\n<h2 id=\"cpu-managed-customization\">Customization options for CPU Manager</h2>\n<p>The CPU Manager supports two operation modes, configured using its <em>policies</em>. With the <code>none</code> policy, the CPU Manager allocates CPUs to containers\nwithout any specific constraint except the (optional) quota set in the Pod spec.\nWith the <code>static</code> policy, then provided that the pod is in the Guaranteed QoS class and every container in that Pod requests an integer amount of vCPU cores,\nthen the CPU Manager allocates CPUs exclusively. Exclusive assignment means that other containers (whether from the same Pod, or from a different Pod) do not\nget scheduled onto that CPU.</p>\n<p>This simple operational model served the user base pretty well, but as the CPU Manager matured more and more, users started to look at more elaborate use\ncases and how to better support them.</p>\n<p>Rather than add more policies, the community realized that pretty much all the novel use cases are some variation of the behavior enabled by the <code>static</code>\nCPU Manager policy. Hence, it was decided to add <a href=\"https://github.com/Kubernetes/enhancements/tree/master/keps/sig-node/2625-cpumanager-policies-thread-placement#proposed-change\">options to tune the behavior of the static policy</a>.\nThe options have a varying degree of maturity, like any other Kubernetes feature, and in order to be accepted, each new option provides a backward\ncompatible behavior when disabled, and to document how to interact with each other, should they interact at all.</p>\n<p>This enabled the Kubernetes project to graduate to GA the CPU Manager core component and core CPU allocation algorithms to GA,\nwhile also enabling a new age of experimentation in this area.\nIn Kubernetes v1.26, the CPU Manager supports <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies.md#static-policy-options\">three different policy options</a>:</p>\n<dl>\n<dt><code>full-pcpus-only</code></dt>\n<dd>restrict the CPU Manager core allocation algorithm to full physical cores only, reducing noisy neighbor issues from hardware technologies that allow sharing cores.</dd>\n<dt><code>distribute-cpus-across-numa</code></dt>\n<dd>drive the CPU Manager to evenly distribute CPUs across NUMA nodes, for cases where more than one NUMA node is required to satisfy the allocation.</dd>\n<dt><code>align-by-socket</code></dt>\n<dd>change how the CPU Manager allocates CPUs to a container: consider CPUs to be aligned at the socket boundary, instead of NUMA node boundary.</dd>\n</dl>\n<h2 id=\"further-development\">Further development</h2>\n<p>After graduating the main CPU Manager feature, each existing policy option will follow their graduation process, independent from CPU Manager and from each other option.\nThere is room for new options to be added, but there's also a growing demand for even more flexibility than what the CPU Manager, and its policy options, currently grant.</p>\n<p>Conversations are in progress in the community about splitting the CPU Manager and the other resource managers currently part of the kubelet executable\ninto pluggable, independent kubelet plugins. If you are interested in this effort, please join the conversation on SIG Node communication channels (Slack, mailing list, weekly meeting).</p>\n<h2 id=\"further-reading\">Further reading</h2>\n<p>Please check out the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/\">Control CPU Management Policies on the Node</a>\ntask page to learn more about the CPU Manager, and how it fits in relation to the other node-level resource managers.</p>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>This feature is driven by the <a href=\"https://github.com/Kubernetes/community/blob/master/sig-node/README.md\">SIG Node</a> community.\nPlease join us to connect with the community and share your ideas and feedback around the above feature and\nbeyond. We look forward to hearing from you!</p>","PublishedAt":"2022-12-27 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/27/cpumanager-ga/","SourceName":"Kubernetes"}},{"node":{"ID":2648,"Title":"Blog: Kubernetes 1.26: Pod Scheduling Readiness","Description":"<p><strong>Author:</strong> Wei Huang (Apple), Abdullah Gharaibeh (Google)</p>\n<p>Kubernetes 1.26 introduced a new Pod feature: <em>scheduling gates</em>. In Kubernetes, scheduling gates\nare keys that tell the scheduler when a Pod is ready to be considered for scheduling.</p>\n<h2 id=\"what-problem-does-it-solve\">What problem does it solve?</h2>\n<p>When a Pod is created, the scheduler will continuously attempt to find a node that fits it. This\ninfinite loop continues until the scheduler either finds a node for the Pod, or the Pod gets deleted.</p>\n<p>Pods that remain unschedulable for long periods of time (e.g., ones that are blocked on some external event)\nwaste scheduling cycles. A scheduling cycle may take ‚âÖ20ms or more depending on the complexity of\nthe Pod's scheduling constraints. Therefore, at scale, those wasted cycles significantly impact the\nscheduler's performance. See the arrows in the &quot;scheduler&quot; box below.</p>\n<figure>\n<div class=\"mermaid\">\ngraph LR;\npod((New Pod))-->queue\nsubgraph Scheduler\nqueue(scheduler queue)\nsched_cycle[/scheduling cycle/]\nschedulable{schedulable?}\nqueue==>|Pop out|sched_cycle\nsched_cycle==>schedulable\nschedulable==>|No|queue\nsubgraph note [Cycles wasted on keep rescheduling 'unready' Pods]\nend\nend\nclassDef plain fill:#ddd,stroke:#fff,stroke-width:1px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:1px,color:#fff;\nclassDef Scheduler fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nclassDef note fill:#edf2ae,stroke:#fff,stroke-width:1px;\nclass queue,sched_cycle,schedulable k8s;\nclass pod plain;\nclass note note;\nclass Scheduler Scheduler;\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<p>Scheduling gates helps address this problem. It allows declaring that newly created Pods are not\nready for scheduling. When scheduling gates are present on a Pod, the scheduler ignores the Pod\nand therefore saves unnecessary scheduling attempts. Those Pods will also be ignored by Cluster\nAutoscaler if you have it installed in the cluster.</p>\n<p>Clearing the gates is the responsibility of external controllers with knowledge of when the Pod\nshould be considered for scheduling (e.g., a quota manager).</p>\n<figure>\n<div class=\"mermaid\">\ngraph LR;\npod((New Pod))-->queue\nsubgraph Scheduler\nqueue(scheduler queue)\nsched_cycle[/scheduling cycle/]\nschedulable{schedulable?}\npopout{Pop out?}\nqueue==>|PreEnqueue check|popout\npopout-->|Yes|sched_cycle\npopout==>|No|queue\nsched_cycle-->schedulable\nschedulable-->|No|queue\nsubgraph note [A knob to gate Pod's scheduling]\nend\nend\nclassDef plain fill:#ddd,stroke:#fff,stroke-width:1px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:1px,color:#fff;\nclassDef Scheduler fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nclassDef note fill:#edf2ae,stroke:#fff,stroke-width:1px;\nclassDef popout fill:#f96,stroke:#fff,stroke-width:1px;\nclass queue,sched_cycle,schedulable k8s;\nclass pod plain;\nclass note note;\nclass popout popout;\nclass Scheduler Scheduler;\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>Scheduling gates in general works very similar to Finalizers. Pods with a non-empty\n<code>spec.schedulingGates</code> field will show as status <code>SchedulingGated</code> and be blocked from\nscheduling. Note that more than one gate can be added, but they all should be added upon Pod\ncreation (e.g., you can add them as part of the spec or via a mutating webhook).</p>\n<pre tabindex=\"0\"><code>NAME READY STATUS RESTARTS AGE\ntest-pod 0/1 SchedulingGated 0 10s\n</code></pre><p>To clear the gates, you update the Pod by removing all of the items from the Pod's <code>schedulingGates</code>\nfield. The gates do not need to be removed all at once, but only when all the gates are removed the\nscheduler will start to consider the Pod for scheduling.</p>\n<p>Under the hood, scheduling gates are implemented as a PreEnqueue scheduler plugin, a new scheduler\nframework extension point that is invoked at the beginning of each scheduling cycle.</p>\n<h2 id=\"use-cases\">Use Cases</h2>\n<p>An important use case this feature enables is dynamic quota management. Kubernetes supports\n<a href=\"https://kubernetes.io/docs/concepts/policy/resource-quotas/\">ResourceQuota</a>, however the API Server enforces quota at\nthe time you attempt Pod creation. For example, if a new Pod exceeds the CPU quota, it gets rejected.\nThe API Server doesn't queue the Pod; therefore, whoever created the Pod needs to continuously attempt\nto recreate it again. This either means a delay between resources becoming available and the Pod\nactually running, or it means load on the API server and Scheduler due to constant attempts.</p>\n<p>Scheduling gates allows an external quota manager to address the above limitation of ResourceQuota.\nSpecifically, the manager could add a <code>example.com/quota-check</code> scheduling gate to all Pods created in the\ncluster (using a mutating webhook). The manager would then remove the gate when there is quota to\nstart the Pod.</p>\n<h2 id=\"whats-next\">Whats next?</h2>\n<p>To use this feature, the <code>PodSchedulingReadiness</code> feature gate must be enabled in the API Server\nand scheduler. You're more than welcome to test it out and tell us (SIG Scheduling) what you think!</p>\n<h2 id=\"additional-resources\">Additional resources</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/\">Pod Scheduling Readiness</a>\nin the Kubernetes documentation</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/3521-pod-scheduling-readiness/README.md\">Kubernetes Enhancement Proposal</a></li>\n</ul>","PublishedAt":"2022-12-26 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/26/pod-scheduling-readiness-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":2636,"Title":"Blog: Kubernetes 1.26: Support for Passing Pod fsGroup to CSI Drivers At Mount Time","Description":"<p><strong>Authors:</strong> Fabio Bertinatto (Red Hat), Hemant Kumar (Red Hat)</p>\n<p>Delegation of <code>fsGroup</code> to CSI drivers was first introduced as alpha in Kubernetes 1.22,\nand graduated to beta in Kubernetes 1.25.\nFor Kubernetes 1.26, we are happy to announce that this feature has graduated to\nGeneral Availability (GA).</p>\n<p>In this release, if you specify a <code>fsGroup</code> in the\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\">security context</a>,\nfor a (Linux) Pod, all processes in the pod's containers are part of the additional group\nthat you specified.</p>\n<p>In previous Kubernetes releases, the kubelet would <em>always</em> apply the\n<code>fsGroup</code> ownership and permission changes to files in the volume according to the policy\nyou specified in the Pod's <code>.spec.securityContext.fsGroupChangePolicy</code> field.</p>\n<p>Starting with Kubernetes 1.26, CSI drivers have the option to apply the <code>fsGroup</code> settings during\nvolume mount time, which frees the kubelet from changing the permissions of files and directories\nin those volumes.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>CSI drivers that support this feature should advertise the\n<a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetcapabilities\"><code>VOLUME_MOUNT_GROUP</code></a> node capability.</p>\n<p>After recognizing this information, the kubelet passes the <code>fsGroup</code> information to\nthe CSI driver during pod startup. This is done through the\n<a href=\"https://github.com/container-storage-interface/spec/blob/v1.7.0/spec.md#nodestagevolume\"><code>NodeStageVolumeRequest</code></a> and\n<a href=\"https://github.com/container-storage-interface/spec/blob/v1.7.0/spec.md#nodepublishvolume\"><code>NodePublishVolumeRequest</code></a>\nCSI calls.</p>\n<p>Consequently, the CSI driver is expected to apply the <code>fsGroup</code> to the files in the volume using a\n<em>mount option</em>. As an example, <a href=\"https://github.com/kubernetes-sigs/azurefile-csi-driver\">Azure File CSIDriver</a> utilizes the <code>gid</code> mount option to map\nthe <code>fsGroup</code> information to all the files in the volume.</p>\n<p>It should be noted that in the example above the kubelet refrains from directly\napplying the permission changes into the files and directories in that volume files.\nAdditionally, two policy definitions no longer have an effect: neither\n<code>.spec.fsGroupPolicy</code> for the CSIDriver object, nor\n<code>.spec.securityContext.fsGroupChangePolicy</code> for the Pod.</p>\n<p>For more details about the inner workings of this feature, check out the\n<a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2317-fsgroup-on-mount/\">enhancement proposal</a>\nand the <a href=\"https://kubernetes-csi.github.io/docs/support-fsgroup.html\">CSI Driver <code>fsGroup</code> Support</a>\nin the CSI developer documentation.</p>\n<h2 id=\"why-is-it-important\">Why is it important?</h2>\n<p>Without this feature, applying the fsGroup information to files is not possible in certain storage environments.</p>\n<p>For instance, Azure File does not support a concept of POSIX-style ownership and permissions\nof files. The CSI driver is only able to set the file permissions at the volume level.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>This feature should be mostly transparent to users. If you maintain a CSI driver that should\nsupport this feature, read\n<a href=\"https://kubernetes-csi.github.io/docs/support-fsgroup.html\">CSI Driver <code>fsGroup</code> Support</a>\nfor more information on how to support this feature in your CSI driver.</p>\n<p>Existing CSI drivers that do not support this feature will continue to work as usual:\nthey will not receive any <code>fsGroup</code> information from the kubelet. In addition to that,\nthe kubelet will continue to perform the ownership and permissions changes to files\nfor those volumes, according to the policies specified in <code>.spec.fsGroupPolicy</code> for the\nCSIDriver and <code>.spec.securityContext.fsGroupChangePolicy</code> for the relevant Pod.</p>","PublishedAt":"2022-12-23 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/23/kubernetes-12-06-fsgroup-on-mount/","SourceName":"Kubernetes"}},{"node":{"ID":2628,"Title":"Blog: Kubernetes v1.26: GA Support for Kubelet Credential Providers","Description":"<p><strong>Authors:</strong> Andrew Sy Kim (Google), Dixita Narang (Google)</p>\n<p>Kubernetes v1.26 introduced generally available (GA) support for <a href=\"https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/\"><em>kubelet credential\nprovider plugins</em></a>,\noffering an extensible plugin framework to dynamically fetch credentials\nfor any container image registry.</p>\n<h2 id=\"background\">Background</h2>\n<p>Kubernetes supports the ability to dynamically fetch credentials for a container registry service.\nPrior to Kubernetes v1.20, this capability was compiled into the kubelet and only available for\nAmazon Elastic Container Registry, Azure Container Registry, and Google Cloud Container Registry.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-in-tree.png\"\nalt=\"Figure 1: Kubelet built-in credential provider support for Amazon Elastic Container Registry, Azure Container Registry, and Google Cloud Container Registry.\"/> <figcaption>\n<p>Figure 1: Kubelet built-in credential provider support for Amazon Elastic Container Registry, Azure Container Registry, and Google Cloud Container Registry.</p>\n</figcaption>\n</figure>\n<p>Kubernetes v1.20 introduced alpha support for kubelet credential providers plugins,\nwhich provides a mechanism for the kubelet to dynamically authenticate and pull images\nfor arbitrary container registries - whether these are public registries, managed services,\nor even a self-hosted registry.\nIn Kubernetes v1.26, this feature is now GA</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-plugin.png\"\nalt=\"Figure 2: Kubelet credential provider overview\"/> <figcaption>\n<p>Figure 2: Kubelet credential provider overview</p>\n</figcaption>\n</figure>\n<h2 id=\"why-is-it-important\">Why is it important?</h2>\n<p>Prior to Kubernetes v1.20, if you wanted to dynamically fetch credentials for image registries\nother than ACR (Azure Container Registry), ECR (Elastic Container Registry), or GCR\n(Google Container Registry), you needed to modify the kubelet code.\nThe new plugin mechanism can be used in any cluster, and lets you authenticate to new registries without\nany changes to Kubernetes itself. Any cloud provider or vendor can publish a plugin that lets you authenticate with their image registry.</p>\n<h2 id=\"how-it-works\">How it works</h2>\n<p>The kubelet and the exec plugin binary communicate through stdio (stdin, stdout, and stderr) by sending and receiving\njson-serialized api-versioned types. If the exec plugin is enabled and the kubelet requires authentication information for an image\nthat matches against a plugin, the kubelet will execute the plugin binary, passing the <code>CredentialProviderRequest</code> API via stdin. Then\nthe exec plugin communicates with the container registry to dynamically fetch the credentials and returns the credentials in an\nencoded response of the <code>CredentialProviderResponse</code> API to the kubelet via stdout.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-how-it-works.png\"\nalt=\"Figure 3: Kubelet credential provider plugin flow\"/> <figcaption>\n<p>Figure 3: Kubelet credential provider plugin flow</p>\n</figcaption>\n</figure>\n<p>On receiving credentials from the kubelet, the plugin can also indicate how long credentials can be cached for, to prevent unnecessary\nexecution of the plugin by the kubelet for subsequent image pull requests to the same registry. In cases where the cache duration\nis not specified by the plugin, a default cache duration can be specified by the kubelet (more details below).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;apiVersion&#34;</span>: <span style=\"color:#b44\">&#34;kubelet.k8s.io/v1&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;kind&#34;</span>: <span style=\"color:#b44\">&#34;CredentialProviderResponse&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;auth&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;cacheDuration&#34;</span>: <span style=\"color:#b44\">&#34;6h&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;private-registry.io/my-app&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;username&#34;</span>: <span style=\"color:#b44\">&#34;exampleuser&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;password&#34;</span>: <span style=\"color:#b44\">&#34;token12345&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>In addition, the plugin can specify the scope in which cached credentials are valid for. This is specified through the <code>cacheKeyType</code> field\nin <code>CredentialProviderResponse</code>. When the value is <code>Image</code>, the kubelet will only use cached credentials for future image pulls that exactly\nmatch the image of the first request. When the value is <code>Registry</code>, the kubelet will use cached credentials for any subsequent image pulls\ndestined for the same registry host but using different paths (for example, <code>gcr.io/foo/bar</code> and <code>gcr.io/bar/foo</code> refer to different images\nfrom the same registry). Lastly, when the value is <code>Global</code>, the kubelet will use returned credentials for all images that match against\nthe plugin, including images that can map to different registry hosts (for example, gcr.io vs k8s.gcr.io). The <code>cacheKeyType</code> field is required by plugin\nimplementations.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;apiVersion&#34;</span>: <span style=\"color:#b44\">&#34;kubelet.k8s.io/v1&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;kind&#34;</span>: <span style=\"color:#b44\">&#34;CredentialProviderResponse&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;auth&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;cacheKeyType&#34;</span>: <span style=\"color:#b44\">&#34;Registry&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;private-registry.io/my-app&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;username&#34;</span>: <span style=\"color:#b44\">&#34;exampleuser&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;password&#34;</span>: <span style=\"color:#b44\">&#34;token12345&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><h2 id=\"using-kubelet-credential-providers\">Using kubelet credential providers</h2>\n<p>You can configure credential providers by installing the exec plugin(s) into\na local directory accessible by the kubelet on every node. Then you set two command line arguments for the kubelet:</p>\n<ul>\n<li><code>--image-credential-provider-config</code>: the path to the credential provider plugin config file.</li>\n<li><code>--image-credential-provider-bin-dir</code>: the path to the directory where credential provider plugin binaries are located.</li>\n</ul>\n<p>The configuration file passed into <code>--image-credential-provider-config</code> is read by the kubelet to determine which exec plugins should be invoked for a container image used by a Pod.\nNote that the name of each <em>provider</em> must match the name of the binary located in the local directory specified in <code>--image-credential-provider-bin-dir</code>, otherwise the kubelet\ncannot locate the path of the plugin to invoke.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>CredentialProviderConfig<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">providers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>auth-provider-gcp<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>credentialprovider.kubelet.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchImages</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#b44\">&#34;container.cloud.google.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#b44\">&#34;gcr.io&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#b44\">&#34;*.gcr.io&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#b44\">&#34;*.pkg.dev&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">args</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- get-credentials<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- --v=3<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">defaultCacheDuration</span>:<span style=\"color:#bbb\"> </span>1m<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Below is an overview of how the Kubernetes project is using kubelet credential providers for end-to-end testing.</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/kubelet-credential-providers-enabling.png\"\nalt=\"Figure 4: Kubelet credential provider configuration used for Kubernetes e2e testing\"/> <figcaption>\n<p>Figure 4: Kubelet credential provider configuration used for Kubernetes e2e testing</p>\n</figcaption>\n</figure>\n<p>For more configuration details, see <a href=\"https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/\">Kubelet Credential Providers</a>.</p>\n<h2 id=\"getting-involved\">Getting Involved</h2>\n<p>Come join SIG Node if you want to report bugs or have feature requests for the Kubelet Credential Provider. You can reach us through the following ways:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnode\">Open Community Issues/PRs</a></li>\n<li><a href=\"https://github.com/kubernetes/community/tree/master/sig-node#meetings\">Biweekly meetings</a></li>\n</ul>","PublishedAt":"2022-12-22 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/","SourceName":"Kubernetes"}},{"node":{"ID":2601,"Title":"Blog: Kubernetes 1.26: Introducing Validating Admission Policies","Description":"<p><strong>Authors:</strong> Joe Betz (Google), Cici Huang (Google)</p>\n<p>In Kubernetes 1.26, the 1st alpha release of validating admission policies is\navailable!</p>\n<p>Validating admission policies use the <a href=\"https://github.com/google/cel-spec\">Common Expression\nLanguage</a> (CEL) to offer a declarative,\nin-process alternative to <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks\">validating admission\nwebhooks</a>.</p>\n<p>CEL was first introduced to Kubernetes for the <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">Validation rules for\nCustomResourceDefinitions</a>.\nThis enhancement expands the use of CEL in Kubernetes to support a far wider\nrange of admission use cases.</p>\n<p>Admission webhooks can be burdensome to develop and operate. Webhook developers\nmust implement and maintain a webhook binary to handle admission requests. Also,\nadmission webhooks are complex to operate. Each webhook must be deployed,\nmonitored and have a well defined upgrade and rollback plan. To make matters\nworse, if a webhook times out or becomes unavailable, the Kubernetes control\nplane can become unavailable. This enhancement avoids much of this complexity of\nadmission webhooks by embedding CEL expressions into Kubernetes resources\ninstead of calling out to a remote webhook binary.</p>\n<p>For example, to set a limit on how many replicas a Deployment can have.\nStart by defining a validation policy:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingAdmissionPolicy<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-policy.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchConstraints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceRules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;apps&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CREATE&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;UPDATE&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;deployments&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">expression</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;object.spec.replicas &lt;= 5&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The <code>expression</code> field contains the CEL expression that is used to validate\nadmission requests. <code>matchConstraints</code> declares what types of requests this\n<code>ValidatingAdmissionPolicy</code> is may validate.</p>\n<p>Next bind the policy to the appropriate resources:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingAdmissionPolicyBinding<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-binding-test.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">policy</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-policy.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchResources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespaceSelector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">key</span>:<span style=\"color:#bbb\"> </span>environment,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operator</span>:<span style=\"color:#bbb\"> </span>In,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">values</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;test&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>This <code>ValidatingAdmissionPolicyBinding</code> resource binds the above policy only to\nnamespaces where the <code>environment</code> label is set to <code>test</code>. Once this binding\nis created, the kube-apiserver will begin enforcing this admission policy.</p>\n<p>To emphasize how much simpler this approach is than admission webhooks, if this example\nwere instead implemented with a webhook, an entire binary would need to be\ndeveloped and maintained just to perform a <code>&lt;=</code> check. In our review of a wide\nrange of admission webhooks used in production, the vast majority performed\nrelatively simple checks, all of which can easily be expressed using CEL.</p>\n<p>Validation admission policies are highly configurable, enabling policy authors\nto define policies that can be parameterized and scoped to resources as needed\nby cluster administrators.</p>\n<p>For example, the above admission policy can be modified to make it configurable:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingAdmissionPolicy<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-policy.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">paramKind</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>rules.example.com/v1<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># You also need a CustomResourceDefinition for this API</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ReplicaLimit<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchConstraints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceRules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;apps&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CREATE&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;UPDATE&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;deployments&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">expression</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;object.spec.replicas &lt;= params.maxReplicas&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Here, <code>paramKind</code> defines the resources used to configure the policy and the\n<code>expression</code> uses the <code>params</code> variable to access the parameter resource.</p>\n<p>This allows multiple bindings to be defined, each configured differently. For\nexample:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingAdmissionPolicyBinding<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-binding-production.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">policy</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-policy.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">paramsRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-params-production.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchResources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespaceSelector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">key</span>:<span style=\"color:#bbb\"> </span>environment,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operator</span>:<span style=\"color:#bbb\"> </span>In,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">values</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;production&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>rules.example.com/v1<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># defined via a CustomResourceDefinition</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ReplicaLimit<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;demo-params-production.example.com&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">maxReplicas</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1000</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>This binding and parameter resource pair limit deployments in namespaces with the\n<code>environment</code> label set to <code>production</code> to a max of 1000 replicas.</p>\n<p>You can then use a separate binding and parameter pair to set a different limit\nfor namespaces in the <code>test</code> environment.</p>\n<p>I hope this has given you a glimpse of what is possible with validating\nadmission policies! There are many features that we have not yet touched on.</p>\n<p>To learn more, read\n<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/\">Validating Admission Policy</a>.</p>\n<p>We are working hard to add more features to admission policies and make the\nenhancement easier to use. Try it out, send us your feedback and help us build\na simpler alternative to admission webhooks!</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>If you want to get involved in development of admission policies, discuss enhancement\nroadmaps, or report a bug, you can get in touch with developers at\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-api-machinery\">SIG API Machinery</a>.</p>","PublishedAt":"2022-12-20 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/20/validating-admission-policies-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":2591,"Title":"Blog: Kubernetes 1.26: Device Manager graduates to GA","Description":"<p><strong>Author:</strong> Swati Sehgal (Red Hat)</p>\n<p>The Device Plugin framework was introduced in the Kubernetes v1.8 release as a vendor\nindependent framework to enable discovery, advertisement and allocation of external\ndevices without modifying core Kubernetes. The feature graduated to Beta in v1.10.\nWith the recent release of Kubernetes v1.26, Device Manager is now generally\navailable (GA).</p>\n<p>Within the kubelet, the Device Manager facilitates communication with device plugins\nusing gRPC through Unix sockets. Device Manager and Device plugins both act as gRPC\nservers and clients by serving and connecting to the exposed gRPC services respectively.\nDevice plugins serve a gRPC service that kubelet connects to for device discovery,\nadvertisement (as extended resources) and allocation. Device Manager connects to\nthe <code>Registration</code> gRPC service served by kubelet to register itself with kubelet.</p>\n<p>Please refer to the documentation for an <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#example-pod\">example</a> on how a pod can request a device exposed to the cluster by a device plugin.</p>\n<p>Here are some example implementations of device plugins:</p>\n<ul>\n<li><a href=\"https://github.com/RadeonOpenCompute/k8s-device-plugin\">AMD GPU device plugin</a></li>\n<li><a href=\"https://github.com/intel/intel-device-plugins-for-kubernetes\">Collection of Intel device plugins for Kubernetes</a></li>\n<li><a href=\"https://github.com/NVIDIA/k8s-device-plugin\">NVIDIA device plugin for Kubernetes</a></li>\n<li><a href=\"https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin\">SRIOV network device plugin for Kubernetes</a></li>\n</ul>\n<h2 id=\"noteworthy-developments-since-device-plugin-framework-introduction\">Noteworthy developments since Device Plugin framework introduction</h2>\n<h3 id=\"kubelet-apis-moved-to-kubelet-staging-repo\">Kubelet APIs moved to kubelet staging repo</h3>\n<p>External facing <code>deviceplugin</code> API packages moved from <code>k8s.io/kubernetes/pkg/kubelet/apis/</code>\nto <code>k8s.io/kubelet/pkg/apis/</code> in v1.17. Refer to <a href=\"https://github.com/kubernetes/kubernetes/pull/83551\">Move external facing kubelet apis to staging</a> for more details on the rationale behind this change.</p>\n<h3 id=\"device-plugin-api-updates\">Device Plugin API updates</h3>\n<p>Additional gRPC endpoints introduced:</p>\n<ol>\n<li><code>GetDevicePluginOptions</code> is used by device plugins to communicate\noptions to the <code>DeviceManager</code> in order to indicate if <code>PreStartContainer</code>,\n<code>GetPreferredAllocation</code> or other future optional calls are supported and\ncan be called before making devices available to the container.</li>\n<li><code>GetPreferredAllocation</code> allows a device plugin to forward allocation\npreferrence to the <code>DeviceManager</code> so it can incorporate this information\ninto its allocation decisions. The <code>DeviceManager</code> will call out to a\nplugin at pod admission time asking for a preferred device allocation\nof a given size from a list of available devices to make a more informed\ndecision. E.g. Specifying inter-device constraints to indicate preferrence\non best-connected set of devices when allocating devices to a container.</li>\n<li><code>PreStartContainer</code> is called before each container start if indicated by\ndevice plugins during registration phase. It allows Device Plugins to run device\nspecific operations on the Devices requested. E.g. reconfiguring or\nreprogramming FPGAs before the container starts running.</li>\n</ol>\n<p>Pull Requests that introduced these changes are here:</p>\n<ol>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/58282\">Invoke preStart RPC call before container start, if desired by plugin</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/92665\">Add GetPreferredAllocation() call to the v1beta1 device plugin API</a></li>\n</ol>\n<p>With introduction of the above endpoints the interaction between Device Manager in\nkubelet and Device Manager can be shown as below:</p>\n<figure class=\"diagram-large\">\n<img src=\"https://kubernetes.io/blog/2022/12/19/devicemanager-ga/deviceplugin-framework-overview.svg\"\nalt=\"Representation of the Device Plugin framework showing the relationship between the kubelet and a device plugin\"/> <figcaption>\n<p>Device Plugin framework Overview</p>\n</figcaption>\n</figure>\n<h3 id=\"change-in-semantics-of-device-plugin-registration-process\">Change in semantics of device plugin registration process</h3>\n<p>Device plugin code was refactored to separate 'plugin' package under the <code>devicemanager</code>\npackage to lay the groundwork for introducing a <code>v1beta2</code> device plugin API. This would\nallow adding support in <code>devicemanager</code> to service multiple device plugin APIs at the\nsame time.</p>\n<p>With this refactoring work, it is now mandatory for a device plugin to start serving its gRPC\nservice before registering itself with kubelet. Previously, these two operations were asynchronous\nand device plugin could register itself before starting its gRPC server which is no longer the\ncase. For more details, refer to <a href=\"https://github.com/kubernetes/kubernetes/pull/109016\">PR #109016</a> and <a href=\"https://github.com/kubernetes/kubernetes/issues/112395\">Issue #112395</a>.</p>\n<h3 id=\"dynamic-resource-allocation\">Dynamic resource allocation</h3>\n<p>In Kubernetes 1.26, inspired by how <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes\">Persistent Volumes</a>\nare handled in Kubernetes, <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/\">Dynamic Resource Allocation</a>\nhas been introduced to cater to devices that have more sophisticated resource requirements like:</p>\n<ol>\n<li>Decouple device initialization and allocation from the pod lifecycle.</li>\n<li>Facilitate dynamic sharing of devices between containers and pods.</li>\n<li>Support custom resource-specific parameters</li>\n<li>Enable resource-specific setup and cleanup actions</li>\n<li>Enable support for Network-attached resources, not just node-local resources</li>\n</ol>\n<h2 id=\"is-the-device-plugin-api-stable-now\">Is the Device Plugin API stable now?</h2>\n<p>No, the Device Plugin API is still not stable; the latest Device Plugin API version\navailable is <code>v1beta1</code>. There are plans in the community to introduce <code>v1beta2</code> API\nto service multiple plugin APIs at once. A per-API call with request/response types\nwould allow adding support for newer API versions without explicitly bumping the API.</p>\n<p>In addition to that, there are existing proposals in the community to introduce additional\nendpoints <a href=\"https://github.com/kubernetes/kubernetes/pull/109016\">KEP-3162: Add Deallocate and PostStopContainer to Device Manager API</a>.</p>","PublishedAt":"2022-12-19 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/19/devicemanager-ga/","SourceName":"Kubernetes"}},{"node":{"ID":2588,"Title":"Blog: Kubernetes 1.26: Non-Graceful Node Shutdown Moves to Beta","Description":"<p><strong>Author:</strong> Xing Yang (VMware), Ashutosh Kumar (VMware)</p>\n<p>Kubernetes v1.24 <a href=\"https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/\">introduced</a> an alpha quality implementation of improvements\nfor handling a <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown\">non-graceful node shutdown</a>.\nIn Kubernetes v1.26, this feature moves to beta. This feature allows stateful workloads to failover to a different node after the original node is shut down or in a non-recoverable state, such as the hardware failure or broken OS.</p>\n<h2 id=\"what-is-a-node-shutdown-in-kubernetes\">What is a node shutdown in Kubernetes?</h2>\n<p>In a Kubernetes cluster, it is possible for a node to shut down. This could happen either in a planned way or it could happen unexpectedly. You may plan for a security patch, or a kernel upgrade and need to reboot the node, or it may shut down due to preemption of VM instances. A node may also shut down due to a hardware failure or a software problem.</p>\n<p>To trigger a node shutdown, you could run a <code>shutdown</code> or <code>poweroff</code> command in a shell,\nor physically press a button to power off a machine.</p>\n<p>A node shutdown could lead to workload failure if the node is not drained before the shutdown.</p>\n<p>In the following, we will describe what is a graceful node shutdown and what is a non-graceful node shutdown.</p>\n<h2 id=\"what-is-a-graceful-node-shutdown\">What is a <em>graceful</em> node shutdown?</h2>\n<p>The kubelet's handling for a <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown\">graceful node shutdown</a>\nallows the kubelet to detect a node shutdown event, properly terminate the pods on that node,\nand release resources before the actual shutdown.\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical\">Critical pods</a>\nare terminated after all the regular pods are terminated, to ensure that the\nessential functions of an application can continue to work as long as possible.</p>\n<h2 id=\"what-is-a-non-graceful-node-shutdown\">What is a <em>non-graceful</em> node shutdown?</h2>\n<p>A Node shutdown can be graceful only if the kubelet's <em>node shutdown manager</em> can\ndetect the upcoming node shutdown action. However, there are cases where a kubelet\ndoes not detect a node shutdown action. This could happen because the <code>shutdown</code>\ncommand does not trigger the <a href=\"https://www.freedesktop.org/wiki/Software/systemd/inhibit\">Inhibitor Locks</a> mechanism used by the kubelet on Linux, or because of a user error. For example, if\nthe <code>shutdownGracePeriod</code> and <code>shutdownGracePeriodCriticalPods</code> details are not\nconfigured correctly for that node.</p>\n<p>When a node is shut down (or crashes), and that shutdown was <strong>not</strong> detected by the kubelet\nnode shutdown manager, it becomes a non-graceful node shutdown. Non-graceful node shutdown\nis a problem for stateful apps.\nIf a node containing a pod that is part of a StatefulSet is shut down in a non-graceful way, the Pod\nwill be stuck in <code>Terminating</code> status indefinitely, and the control plane cannot create a replacement\nPod for that StatefulSet on a healthy node.\nYou can delete the failed Pods manually, but this is not ideal for a self-healing cluster.\nSimilarly, pods that ReplicaSets created as part of a Deployment will be stuck in <code>Terminating</code> status, and\nthat were bound to the now-shutdown node, stay as <code>Terminating</code> indefinitely.\nIf you have set a horizontal scaling limit, even those terminating Pods count against the limit,\nso your workload may struggle to self-heal if it was already at maximum scale.\n(By the way: if the node that had done a non-graceful shutdown comes back up, the kubelet does delete\nthe old Pod, and the control plane can make a replacement.)</p>\n<h2 id=\"what-s-new-for-the-beta\">What's new for the beta?</h2>\n<p>For Kubernetes v1.26, the non-graceful node shutdown feature is beta and enabled by default.\nThe <code>NodeOutOfServiceVolumeDetach</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> is enabled by default\non <code>kube-controller-manager</code> instead of being opt-in; you can still disable it if needed\n(please also file an issue to explain the problem).</p>\n<p>On the instrumentation side, the kube-controller-manager reports two new metrics.</p>\n<dl>\n<dt><code>force_delete_pods_total</code></dt>\n<dd>number of pods that are being forcibly deleted (resets on Pod garbage collection controller restart)</dd>\n<dt><code>force_delete_pod_errors_total</code></dt>\n<dd>number of errors encountered when attempting forcible Pod deletion (also resets on Pod garbage collection controller restart)</dd>\n</dl>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>In the case of a node shutdown, if a graceful shutdown is not working or the node is in a\nnon-recoverable state due to hardware failure or broken OS, you can manually add an <code>out-of-service</code>\ntaint on the Node. For example, this can be <code>node.kubernetes.io/out-of-service=nodeshutdown:NoExecute</code>\nor <code>node.kubernetes.io/out-of-service=nodeshutdown:NoSchedule</code>. This taint trigger pods on the node to\nbe forcefully deleted if there are no matching tolerations on the pods. Persistent volumes attached to the shutdown node will be detached, and new pods will be created successfully on a different running node.</p>\n<pre tabindex=\"0\"><code>kubectl taint nodes &lt;node-name&gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute\n</code></pre><p><strong>Note:</strong> Before applying the out-of-service taint, you must verify that a node is already in shutdown\nor power-off state (not in the middle of restarting), either because the user intentionally shut it down\nor the node is down due to hardware failures, OS issues, etc.</p>\n<p>Once all the workload pods that are linked to the out-of-service node are moved to a new running node, and the shutdown node has been recovered, you should remove that taint on the affected node after the node is recovered.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>Depending on feedback and adoption, the Kubernetes team plans to push the Non-Graceful Node Shutdown implementation to GA in either 1.27 or 1.28.</p>\n<p>This feature requires a user to manually add a taint to the node to trigger the failover of workloads and remove the taint after the node is recovered.</p>\n<p>The cluster operator can automate this process by automatically applying the <code>out-of-service</code> taint\nif there is a programmatic way to determine that the node is really shut down and there isn‚Äôt IO between\nthe node and storage. The cluster operator can then automatically remove the taint after the workload\nfails over successfully to another running node and that the shutdown node has been recovered.</p>\n<p>In the future, we plan to find ways to automatically detect and fence nodes that are shut down or in a non-recoverable state and fail their workloads over to another node.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>To learn more, read <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown\">Non Graceful node shutdown</a> in the Kubernetes documentation.</p>\n<h2 id=\"how-to-get-involved\">How to get involved?</h2>\n<p>We offer a huge thank you to all the contributors who helped with design, implementation, and review of this feature:</p>\n<ul>\n<li>Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>)</li>\n<li>Derek Carr (<a href=\"https://github.com/derekwaynecarr\">derekwaynecarr</a>)</li>\n<li>Danielle Endocrimes (<a href=\"https://github.com/endocrimes\">endocrimes</a>)</li>\n<li>Tim Hockin (<a href=\"https://github.com/thockin\">thockin</a>)</li>\n<li>Ashutosh Kumar (<a href=\"https://github.com/sonasingh46\">sonasingh46</a>)</li>\n<li>Hemant Kumar (<a href=\"https://github.com/gnufied\">gnufied</a>)</li>\n<li>Yuiko Mouri(<a href=\"https://github.com/YuikoTakada\">YuikoTakada</a>)</li>\n<li>Mrunal Patel (<a href=\"https://github.com/mrunalp\">mrunalp</a>)</li>\n<li>David Porter (<a href=\"https://github.com/bobbypage\">bobbypage</a>)</li>\n<li>Yassine Tijani (<a href=\"https://github.com/yastij\">yastij</a>)</li>\n<li>Jing Xu (<a href=\"https://github.com/jingxu97\">jingxu97</a>)</li>\n<li>Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>)</li>\n</ul>\n<p>There are many people who have helped review the design and implementation along the way. We want to thank everyone who has contributed to this effort including the about 30 people who have reviewed the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown\">KEP</a> and implementation over the last couple of years.</p>\n<p>This feature is a collaboration between SIG Storage and SIG Node. For those interested in getting involved with the design and development of any part of the Kubernetes Storage system, join the Kubernetes Storage Special Interest Group (SIG). For those interested in getting involved with the design and development of the components that support the controlled interactions between pods and host resources, join the Kubernetes Node SIG.</p>","PublishedAt":"2022-12-16 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/16/kubernetes-1-26-non-graceful-node-shutdown-beta/","SourceName":"Kubernetes"}},{"node":{"ID":2556,"Title":"Blog: Kubernetes 1.26: Alpha API For Dynamic Resource Allocation","Description":"<p><strong>Authors:</strong> Patrick Ohly (Intel), Kevin Klues (NVIDIA)</p>\n<p>Dynamic resource allocation is a new API for requesting resources. It is a\ngeneralization of the persistent volumes API for generic resources, making it possible to:</p>\n<ul>\n<li>access the same resource instance in different pods and containers,</li>\n<li>attach arbitrary constraints to a resource request to get the exact resource\nyou are looking for,</li>\n<li>initialize a resource according to parameters provided by the user.</li>\n</ul>\n<p>Third-party resource drivers are responsible for interpreting these parameters\nas well as tracking and allocating resources as requests come in.</p>\n<p>Dynamic resource allocation is an <em>alpha feature</em> and only enabled when the\n<code>DynamicResourceAllocation</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature\ngate</a> and the\n<code>resource.k8s.io/v1alpha1</code> <a class='glossary-tooltip' title='A set of related paths in the Kubernetes API.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning' target='_blank' aria-label='API group'>API group</a> are enabled. For details, see the\n<code>--feature-gates</code> and <code>--runtime-config</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\">kube-apiserver\nparameters</a>.\nThe kube-scheduler, kube-controller-manager and kubelet components all need\nthe feature gate enabled as well.</p>\n<p>The default configuration of kube-scheduler enables the <code>DynamicResources</code>\nplugin if and only if the feature gate is enabled. Custom configurations may\nhave to be modified to include it.</p>\n<p>Once dynamic resource allocation is enabled, resource drivers can be installed\nto manage certain kinds of hardware. Kubernetes has a test driver that is used\nfor end-to-end testing, but also can be run manually. See\n<a href=\"#running-the-test-driver\">below</a> for step-by-step instructions.</p>\n<h2 id=\"api\">API</h2>\n<p>The new <code>resource.k8s.io/v1alpha1</code> <a class='glossary-tooltip' title='A set of related paths in the Kubernetes API.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning' target='_blank' aria-label='API group'>API group</a> provides four new types:</p>\n<dl>\n<dt>ResourceClass</dt>\n<dd>Defines which resource driver handles a certain kind of\nresource and provides common parameters for it. ResourceClasses\nare created by a cluster administrator when installing a resource\ndriver.</dd>\n<dt>ResourceClaim</dt>\n<dd>Defines a particular resource instances that is required by a\nworkload. Created by a user (lifecycle managed manually, can be shared\nbetween different Pods) or for individual Pods by the control plane based on\na ResourceClaimTemplate (automatic lifecycle, typically used by just one\nPod).</dd>\n<dt>ResourceClaimTemplate</dt>\n<dd>Defines the spec and some meta data for creating\nResourceClaims. Created by a user when deploying a workload.</dd>\n<dt>PodScheduling</dt>\n<dd>Used internally by the control plane and resource drivers\nto coordinate pod scheduling when ResourceClaims need to be allocated\nfor a Pod.</dd>\n</dl>\n<p>Parameters for ResourceClass and ResourceClaim are stored in separate objects,\ntypically using the type defined by a <a class='glossary-tooltip' title='Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/' target='_blank' aria-label='CRD'>CRD</a> that was created when\ninstalling a resource driver.</p>\n<p>With this alpha feature enabled, the <code>spec</code> of Pod defines ResourceClaims that are needed for a Pod\nto run: this information goes into a new\n<code>resourceClaims</code> field. Entries in that list reference either a ResourceClaim\nor a ResourceClaimTemplate. When referencing a ResourceClaim, all Pods using\nthis <code>.spec</code> (for example, inside a Deployment or StatefulSet) share the same\nResourceClaim instance. When referencing a ResourceClaimTemplate, each Pod gets\nits own ResourceClaim instance.</p>\n<p>For a container defined within a Pod, the <code>resources.claims</code> list\ndefines whether that container gets\naccess to these resource instances, which makes it possible to share resources\nbetween one or more containers inside the same Pod. For example, an init container could\nset up the resource before the application uses it.</p>\n<p>Here is an example of a fictional resource driver. Two ResourceClaim objects\nwill get created for this Pod and each container gets access to one of them.</p>\n<p>Assuming a resource driver called <code>resource-driver.example.com</code> was installed\ntogether with the following resource class:</p>\n<pre tabindex=\"0\"><code>apiVersion: resource.k8s.io/v1alpha1\nkind: ResourceClass\nname: resource.example.com\ndriverName: resource-driver.example.com\n</code></pre><p>An end-user could then allocate two specific resources of type\n<code>resource.example.com</code> as follows:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>cats.resource.example.com/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ClaimParameters<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>large-black-cats<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">color</span>:<span style=\"color:#bbb\"> </span>black<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">size</span>:<span style=\"color:#bbb\"> </span>large<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>resource.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ResourceClaimTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>large-black-cats<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceClassName</span>:<span style=\"color:#bbb\"> </span>resource.example.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">parametersRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>cats.resource.example.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ClaimParameters<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>large-black-cats<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>‚Äì--<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pod-with-cats<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># two example containers; each container claims one cat resource</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>first-example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>ubuntu:22.04<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;sleep&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;9999&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claims</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cat-0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>second-example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>ubuntu:22.04<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;sleep&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;9999&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claims</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cat-1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceClaims</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cat-0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceClaimTemplateName</span>:<span style=\"color:#bbb\"> </span>large-black-cats<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cat-1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceClaimTemplateName</span>:<span style=\"color:#bbb\"> </span>large-black-cats<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"scheduling\">Scheduling</h2>\n<p>In contrast to native resources (such as CPU or RAM) and\n<a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources\">extended resources</a>\n(managed by a\ndevice plugin, advertised by kubelet), the scheduler has no knowledge of what\ndynamic resources are available in a cluster or how they could be split up to\nsatisfy the requirements of a specific ResourceClaim. Resource drivers are\nresponsible for that. Drivers mark ResourceClaims as <em>allocated</em> once resources\nfor it are reserved. This also then tells the scheduler where in the cluster a\nclaimed resource is actually available.</p>\n<p>ResourceClaims can get resources allocated as soon as the ResourceClaim\nis created (<em>immediate allocation</em>), without considering which Pods will use\nthe resource. The default (<em>wait for first consumer</em>) is to delay allocation until\na Pod that relies on the ResourceClaim becomes eligible for scheduling.\nThis design with two allocation options is similar to how Kubernetes handles\nstorage provisioning with PersistentVolumes and PersistentVolumeClaims.</p>\n<p>In the wait for first consumer mode, the scheduler checks all ResourceClaims needed\nby a Pod. If the Pods has any ResourceClaims, the scheduler creates a PodScheduling\n(a special object that requests scheduling details on behalf of the Pod). The PodScheduling\nhas the same name and namespace as the Pod and the Pod as its as owner.\nUsing its PodScheduling, the scheduler informs the resource drivers\nresponsible for those ResourceClaims about nodes that the scheduler considers\nsuitable for the Pod. The resource drivers respond by excluding nodes that\ndon't have enough of the driver's resources left.</p>\n<p>Once the scheduler has that resource\ninformation, it selects one node and stores that choice in the PodScheduling\nobject. The resource drivers then allocate resources based on the relevant\nResourceClaims so that the resources will be available on that selected node.\nOnce that resource allocation is complete, the scheduler attempts to schedule the Pod\nto a suitable node. Scheduling can still fail at this point; for example, a different Pod could\nbe scheduled to the same node in the meantime. If this happens, already allocated\nResourceClaims may get deallocated to enable scheduling onto a different node.</p>\n<p>As part of this process, ResourceClaims also get reserved for the\nPod. Currently ResourceClaims can either be used exclusively by a single Pod or\nan unlimited number of Pods.</p>\n<p>One key feature is that Pods do not get scheduled to a node unless all of\ntheir resources are allocated and reserved. This avoids the scenario where\na Pod gets scheduled onto one node and then cannot run there, which is bad\nbecause such a pending Pod also blocks all other resources like RAM or CPU that were\nset aside for it.</p>\n<h2 id=\"limitations\">Limitations</h2>\n<p>The scheduler plugin must be involved in scheduling Pods which use\nResourceClaims. Bypassing the scheduler by setting the <code>nodeName</code> field leads\nto Pods that the kubelet refuses to start because the ResourceClaims are not\nreserved or not even allocated. It may be possible to remove this\n<a href=\"https://github.com/kubernetes/kubernetes/issues/114005\">limitation</a> in the\nfuture.</p>\n<h2 id=\"writing-a-resource-driver\">Writing a resource driver</h2>\n<p>A dynamic resource allocation driver typically consists of two separate-but-coordinating\ncomponents: a centralized controller, and a DaemonSet of node-local kubelet\nplugins. Most of the work required by the centralized controller to coordinate\nwith the scheduler can be handled by boilerplate code. Only the business logic\nrequired to actually allocate ResourceClaims against the ResourceClasses owned\nby the plugin needs to be customized. As such, Kubernetes provides\nthe following package, including APIs for invoking this boilerplate code as\nwell as a <code>Driver</code> interface that you can implement to provide their custom\nbusiness logic:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/dynamic-resource-allocation/tree/release-1.26/controller\">k8s.io/dynamic-resource-allocation/controller</a></li>\n</ul>\n<p>Likewise, boilerplate code can be used to register the node-local plugin with\nthe kubelet, as well as start a gRPC server to implement the kubelet plugin\nAPI. For drivers written in Go, the following package is recommended:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/dynamic-resource-allocation/tree/release-1.26/kubeletplugin\">k8s.io/dynamic-resource-allocation/kubeletplugin</a></li>\n</ul>\n<p>It is up to the driver developer to decide how these two components\ncommunicate. The <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\">KEP</a> outlines an <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation#implementing-a-plugin-for-node-resources\">approach using\nCRDs</a>.</p>\n<p>Within SIG Node, we also plan to provide a complete <a href=\"https://github.com/kubernetes-sigs/dra-example-driver\">example\ndriver</a> that can serve\nas a template for other drivers.</p>\n<h2 id=\"running-the-test-driver\">Running the test driver</h2>\n<p>The following steps bring up a local, one-node cluster directly from the\nKubernetes source code. As a prerequisite, your cluster must have nodes with a container\nruntime that supports the\n<a href=\"https://github.com/container-orchestrated-devices/container-device-interface\">Container Device Interface</a>\n(CDI). For example, you can run CRI-O <a href=\"https://github.com/cri-o/cri-o/releases/tag/v1.23.2\">v1.23.2</a> or later.\nOnce containerd v1.7.0 is released, we expect that you can run that or any later version.\nIn the example below, we use CRI-O.</p>\n<p>First, clone the Kubernetes source code. Inside that directory, run:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> hack/install-etcd.sh\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#000080;font-weight:bold\">$</span> <span style=\"color:#b8860b\">RUNTIME_CONFIG</span><span style=\"color:#666\">=</span>resource.k8s.io/v1alpha1 <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span><span style=\"color:#888\"> FEATURE_GATES=DynamicResourceAllocation=true \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> DNS_ADDON=&#34;coredns&#34; \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> CGROUP_DRIVER=systemd \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> CONTAINER_RUNTIME_ENDPOINT=unix:///var/run/crio/crio.sock \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> LOG_LEVEL=6 \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ENABLE_CSI_SNAPSHOTTER=false \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> API_SECURE_PORT=6444 \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ALLOW_PRIVILEGED=1 \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> PATH=$(pwd)/third_party/etcd:$PATH \\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ./hack/local-up-cluster.sh -O\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">To start using your cluster, you can open up another terminal/tab and run:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#888\"> export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span></code></pre></div><p>Once the cluster is up, in another\nterminal run the test driver controller. <code>KUBECONFIG</code> must be set for all of\nthe following commands.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> go run ./test/e2e/dra/test-driver --feature-gates <span style=\"color:#b8860b\">ContextualLogging</span><span style=\"color:#666\">=</span><span style=\"color:#a2f\">true</span> -v<span style=\"color:#666\">=</span><span style=\"color:#666\">5</span> controller\n</span></span></code></pre></div><p>In another terminal, run the kubelet plugin:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> sudo mkdir -p /var/run/cdi <span style=\"color:#666\">&amp;&amp;</span> <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span><span style=\"color:#888\"> sudo chmod a+rwx /var/run/cdi /var/lib/kubelet/plugins_registry /var/lib/kubelet/plugins/\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> go run ./test/e2e/dra/test-driver --feature-gates <span style=\"color:#b8860b\">ContextualLogging</span><span style=\"color:#666\">=</span><span style=\"color:#a2f\">true</span> -v<span style=\"color:#666\">=</span><span style=\"color:#666\">6</span> kubelet-plugin\n</span></span></code></pre></div><p>Changing the permissions of the directories makes it possible to run and (when\nusing delve) debug the kubelet plugin as a normal user, which is convenient\nbecause it uses the already populated Go cache. Remember to restore permissions\nwith <code>sudo chmod go-w</code> when done. Alternatively, you can also build the binary\nand run that as root.</p>\n<p>Now the cluster is ready to create objects:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl create -f test/e2e/dra/test-driver/deploy/example/resourceclass.yaml\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">resourceclass.resource.k8s.io/example created\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl create -f test/e2e/dra/test-driver/deploy/example/pod-inline.yaml\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">configmap/test-inline-claim-parameters created\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">resourceclaimtemplate.resource.k8s.io/test-inline-claim-template created\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pod/test-inline-claim created\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get resourceclaims\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME RESOURCECLASSNAME ALLOCATIONMODE STATE AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">test-inline-claim-resource example WaitForFirstConsumer allocated,reserved 8s\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl get pods\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">test-inline-claim 0/2 Completed 0 21s\n</span></span></span></code></pre></div><p>The test driver doesn't do much, it only sets environment variables as defined\nin the ConfigMap. The test pod dumps the environment, so the log can be checked\nto verify that everything worked:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> kubectl logs test-inline-claim with-resource | grep user_a\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">user_a=&#39;b&#39;\n</span></span></span></code></pre></div><h2 id=\"next-steps\">Next steps</h2>\n<ul>\n<li>See the\n<a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\">Dynamic Resource Allocation</a>\nKEP for more information on the design.</li>\n<li>Read <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/\">Dynamic Resource Allocation</a>\nin the official Kubernetes documentation.</li>\n<li>You can participate in\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-node/README.md\">SIG Node</a>\nand / or the <a href=\"https://github.com/cncf/tag-runtime/blob/master/wg/COD.md\">CNCF Container Orchestrated Device Working Group</a>.</li>\n<li>You can view or comment on the <a href=\"https://github.com/orgs/kubernetes/projects/95/views/1\">project board</a>\nfor dynamic resource allocation.</li>\n<li>In order to move this feature towards beta, we need feedback from hardware\nvendors, so here's a call to action: try out this feature, consider how it can help\nwith problems that your users are having, and write resource drivers‚Ä¶</li>\n</ul>","PublishedAt":"2022-12-15 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/15/dynamic-resource-allocation/","SourceName":"Kubernetes"}},{"node":{"ID":2521,"Title":"Blog: Kubernetes 1.26: Windows HostProcess Containers Are Generally Available","Description":"<p><strong>Authors</strong>: Brandon Smith (Microsoft) and Mark Rossetti (Microsoft)</p>\n<p>The long-awaited day has arrived: HostProcess containers, the Windows equivalent to Linux privileged\ncontainers, has finally made it to <strong>GA in Kubernetes 1.26</strong>!</p>\n<p>What are HostProcess containers and why are they useful?</p>\n<p>Cluster operators are often faced with the need to configure their nodes upon provisioning such as\ninstalling Windows services, configuring registry keys, managing TLS certificates,\nmaking network configuration changes, or even deploying monitoring tools such as a Prometheus's node-exporter.\nPreviously, performing these actions on Windows nodes was usually done by running PowerShell scripts\nover SSH or WinRM sessions and/or working with your cloud provider's virtual machine management tooling.\nHostProcess containers now enable you to do all of this and more with minimal effort using Kubernetes native APIs.</p>\n<p>With HostProcess containers you can now package any payload\ninto the container image, map volumes into containers at runtime, and manage them like any other Kubernetes workload.\nYou get all the benefits of containerized packaging and deployment methods combined with a reduction in\nboth administrative and development cost.\nGone are the days where cluster operators would need to manually log onto\nWindows nodes to perform administrative duties.</p>\n<p><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/\">HostProcess containers</a> differ\nquite significantly from regular Windows Server containers.\nThey are run directly as processes on the host with the access policies of\na user you specify. HostProcess containers run as either the built-in Windows system accounts or\nephemeral users within a user group defined by you. HostProcess containers also share\nthe host's network namespace and access/configure storage mounts visible to the host.\nOn the other hand, Windows Server containers are highly isolated and exist in a separate\nexecution namespace. Direct access to the host from a Windows Server container is explicitly disallowed\nby default.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>Windows HostProcess containers are implemented with Windows <a href=\"https://learn.microsoft.com/en-us/windows/win32/procthread/job-objects\"><em>Job Objects</em></a>,\na break from the previous container model which use server silos.\nJob Objects are components of the Windows OS which offer the ability to\nmanage a group of processes as a group (also known as a <em>job</em>) and assign resource constraints to the\ngroup as a whole. Job objects are specific to the Windows OS and are not associated with\nthe Kubernetes <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Job API</a>. They have no process\nor file system isolation,\nenabling the privileged payload to view and edit the host file system with the\ndesired permissions, among other host resources. The init process, and any processes\nit launches (including processes explicitly launched by the user) are all assigned to the\njob object of that container. When the init process exits or is signaled to exit,\nall the processes in the job will be signaled to exit, the job handle will be\nclosed and the storage will be unmounted.</p>\n<p>HostProcess and Linux privileged containers enable similar scenarios but differ\ngreatly in their implementation (hence the naming difference). HostProcess containers\nhave their own <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#windowssecuritycontextoptions-v1-core\">PodSecurityContext</a> fields.\nThose used to configure Linux privileged containers <strong>do not</strong> apply. Enabling privileged access to a Windows host is a\nfundamentally different process than with Linux so the configuration and\ncapabilities of each differ significantly. Below is a diagram detailing the\noverall architecture of Windows HostProcess containers:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2022/12/13/windows-host-process-containers-ga/hpc_architecture.svg\"\nalt=\"HostProcess Architecture\"/>\n</figure>\n<p>Two major features were added prior to moving to stable: the ability to run as local user accounts, and\na simplified method of accessing volume mounts. To learn more, read\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/\">Create a Windows HostProcess Pod</a>.</p>\n<h2 id=\"hostprocess-containers-in-action\">HostProcess containers in action</h2>\n<p>Kubernetes SIG Windows has been busy putting HostProcess containers to use - even before GA!\nThey've been very excited to use HostProcess containers for a number of important activities\nthat were a pain to perform in the past.</p>\n<p>Here are just a few of the many use use cases with example deployments:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/hostprocess/calico#calico-example\">CNI solutions and kube-proxy</a></li>\n<li><a href=\"https://github.com/prometheus-community/windows_exporter/blob/master/kubernetes/windows-exporter-daemonset.yaml\">windows-exporter</a></li>\n<li><a href=\"https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/hostprocess/csi-proxy\">csi-proxy</a></li>\n<li><a href=\"https://github.com/jsturtevant/windows-debug\">Windows-debug container</a></li>\n<li><a href=\"https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/hostprocess/eventflow-logger\">ETW event streaming</a></li>\n</ul>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>A HostProcess container can be built using any base image of your choosing, however, for convenience we have\ncreated <a href=\"https://github.com/microsoft/windows-host-process-containers-base-image\">a HostProcess container base image</a>.\nThis image is only a few KB in size and does not inherit any of the same compatibility requirements as regular Windows\nserver containers which allows it to run on any Windows server version.</p>\n<p>To use that Microsoft image, put this in your <code>Dockerfile</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-dockerfile\" data-lang=\"dockerfile\"><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">FROM</span><span style=\"color:#b44\"> mcr.microsoft.com/oss/kubernetes/windows-host-process-containers-base-image:v1.0.0</span><span style=\"\">\n</span></span></span></code></pre></div><p>You can run HostProcess containers from within a\n<a href=\"https://kubernetes.io/docs/concepts/workloads/pods/#privileged-mode-for-containers\">HostProcess Pod</a>.</p>\n<p>To get started with running Windows containers,\nsee the general guidance for <a href=\"https://kubernetes.io/docs/setup/production-environment/windows/\">deploying Windows nodes</a>.\nIf you have a compatible node (for example: Windows as the operating system\nwith containerd v1.7 or later as the container runtime), you can deploy a Pod with one\nor more HostProcess containers.\nSee the <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/#before-you-begin\">Create a Windows HostProcess Pod - Prerequisites</a>\nfor more information.</p>\n<p>Please note that within a Pod, you can't mix HostProcess containers with normal Windows containers.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>\n<p>Work through <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/\">Create a Windows HostProcess Pod</a></p>\n</li>\n<li>\n<p>Read about Kubernetes <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a> and <a href=\"docs/concepts/security/pod-security-admission/\">Pod Security Admission</a></p>\n</li>\n<li>\n<p>Read the enhancement proposal <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-windows/1981-windows-privileged-container-support\">Windows Privileged Containers and Host Networking Mode</a> (KEP-1981)</p>\n</li>\n<li>\n<p>Watch the <a href=\"https://www.youtube.com/watch?v=LcXT9pVkwvo\">Windows HostProcess for Configuration and Beyond</a> KubeCon NA 2022 talk</p>\n</li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>Get involved with <a href=\"https://github.com/kubernetes/community/tree/master/sig-windows\">SIG Windows</a>\nto contribute!</p>","PublishedAt":"2022-12-13 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/13/windows-host-process-containers-ga/","SourceName":"Kubernetes"}},{"node":{"ID":2504,"Title":"Blog: Kubernetes 1.26: We're now signing our binary release artifacts!","Description":"<p><strong>Author:</strong> Sascha Grunert</p>\n<p>The Kubernetes Special Interest Group (SIG) Release is proud to announce that we\nare digitally signing all release artifacts, and that this aspect of Kubernetes\nhas now reached <em>beta</em>.</p>\n<p>Signing artifacts provides end users a chance to verify the integrity of the\ndownloaded resource. It allows to mitigate man-in-the-middle attacks directly on\nthe client side and therefore ensures the trustfulness of the remote serving the\nartifacts. The overall goal of out past work was to define the used tooling for\nsigning all Kubernetes related artifacts as well as providing a standard signing\nprocess for related projects (for example for those in <a href=\"https://github.com/kubernetes-sigs\">kubernetes-sigs</a>).</p>\n<p>We already signed all officially released container images (from Kubernetes v1.24 onwards).\nImage signing was alpha for v1.24 and v1.25. For v1.26, we've added all\n<strong>binary artifacts</strong> to the signing process as well! This means that now all\n<a href=\"https://github.com/kubernetes/kubernetes/blob/release-1.26/CHANGELOG/CHANGELOG-1.26.md#downloads-for-v1260\">client, server and source tarballs</a>, <a href=\"https://gcsweb.k8s.io/gcs/kubernetes-release/release/v1.26.0/bin\">binary artifacts</a>,\n<a href=\"https://storage.googleapis.com/kubernetes-release/release/v1.26.0/kubernetes-release.spdx\">Software Bills of Material (SBOMs)</a> as well as the <a href=\"https://storage.googleapis.com/kubernetes-release/release/v1.26.0/provenance.json\">build\nprovenance</a> will be signed using <a href=\"https://github.com/sigstore/cosign\">cosign</a>. Technically\nspeaking, we now ship additional <code>*.sig</code> (signature) and <code>*.cert</code> (certificate)\nfiles side by side to the artifacts for verifying their integrity.</p>\n<p>To verify an artifact, for example <code>kubectl</code>, you can download the\nsignature and certificate alongside with the binary. I use the release candidate\n<code>rc.1</code> of v1.26 for demonstration purposes because the final has not been released yet:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>curl -sSfL https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl -o kubectl\n</span></span><span style=\"display:flex;\"><span>curl -sSfL https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.sig -o kubectl.sig\n</span></span><span style=\"display:flex;\"><span>curl -sSfL https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.cert -o kubectl.cert\n</span></span></code></pre></div><p>Then you can verify <code>kubectl</code> using <a href=\"https://github.com/sigstore/cosign\"><code>cosign</code></a>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#b8860b\">COSIGN_EXPERIMENTAL</span><span style=\"color:#666\">=</span><span style=\"color:#666\">1</span> cosign verify-blob kubectl --signature kubectl.sig --certificate kubectl.cert\n</span></span></code></pre></div><pre tabindex=\"0\"><code>tlog entry verified with uuid: 5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657 index: 8173886\nVerified OK\n</code></pre><p>The UUID can be used to query the <a href=\"https://github.com/sigstore/rekor\">rekor</a> transparency log:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>rekor-cli get --uuid 5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657\n</span></span></code></pre></div><pre tabindex=\"0\"><code>LogID: c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\nIndex: 8173886\nIntegratedTime: 2022-11-30T18:59:07Z\nUUID: 24296fb24b8ad77a5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657\nBody: {\n&#34;HashedRekordObj&#34;: {\n&#34;data&#34;: {\n&#34;hash&#34;: {\n&#34;algorithm&#34;: &#34;sha256&#34;,\n&#34;value&#34;: &#34;982dfe7eb5c27120de6262d30fa3e8029bc1da9e632ce70570e9c921d2851fc2&#34;\n}\n},\n&#34;signature&#34;: {\n&#34;content&#34;: &#34;MEQCIH0e1/0svxMoLzjeyhAaLFSHy5ZaYy0/2iQl2t3E0Pj4AiBsWmwjfLzrVyp9/v1sy70Q+FHE8miauOOVkAW2lTYVug==&#34;,\n&#34;publicKey&#34;: {\n&#34;content&#34;: &#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWthZ0F3SUJBZ0lVRldab0pLSUlFWkp3LzdsRkFrSVE2SHBQdi93d0NnWUlLb1pJemowRUF3TXcKTnpFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUjR3SEFZRFZRUURFeFZ6YVdkemRHOXlaUzFwYm5SbApjbTFsWkdsaGRHVXdIaGNOTWpJeE1UTXdNVGcxT1RBMldoY05Nakl4TVRNd01Ua3dPVEEyV2pBQU1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVDT3h5OXBwTFZzcVFPdHJ6RFgveTRtTHZSeU1scW9sTzBrS0EKTlJDM3U3bjMreHorYkhvWVkvMUNNRHpJQjBhRTA3NkR4ZWVaSkhVaWFjUXU4a0dDNktPQ0FXVXdnZ0ZoTUE0RwpBMVVkRHdFQi93UUVBd0lIZ0RBVEJnTlZIU1VFRERBS0JnZ3JCZ0VGQlFjREF6QWRCZ05WSFE0RUZnUVV5SmwxCkNlLzIzNGJmREJZQ2NzbXkreG5qdnpjd0h3WURWUjBqQkJnd0ZvQVUzOVBwejFZa0VaYjVxTmpwS0ZXaXhpNFkKWkQ4d1FnWURWUjBSQVFIL0JEZ3dOb0UwYTNKbGJDMXpkR0ZuYVc1blFHczRjeTF5Wld4bGJtY3RjSEp2WkM1cApZVzB1WjNObGNuWnBZMlZoWTJOdmRXNTBMbU52YlRBcEJnb3JCZ0VFQVlPL01BRUJCQnRvZEhSd2N6b3ZMMkZqClkyOTFiblJ6TG1kdmIyZHNaUzVqYjIwd2dZb0dDaXNHQVFRQjFua0NCQUlFZkFSNkFIZ0FkZ0RkUFRCcXhzY1IKTW1NWkhoeVpaemNDb2twZXVONDhyZitIaW5LQUx5bnVqZ0FBQVlUSjZDdlJBQUFFQXdCSE1FVUNJRXI4T1NIUQp5a25jRFZpNEJySklXMFJHS0pqNkQyTXFGdkFMb0I5SmNycXlBaUVBNW4xZ283cmQ2U3ZVeXNxeldhMUdudGZKCllTQnVTZHF1akVySFlMQTUrZTR3Q2dZSUtvWkl6ajBFQXdNRFpnQXdZd0l2Tlhub3pyS0pWVWFESTFiNUlqa1oKUWJJbDhvcmlMQ1M4MFJhcUlBSlJhRHNCNTFUeU9iYTdWcGVYdThuTHNjVUNNREU4ZmpPZzBBc3ZzSXp2azNRUQo0c3RCTkQrdTRVV1UrcjhYY0VxS0YwNGJjTFQwWEcyOHZGQjRCT2x6R204K093PT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=&#34;\n}\n}\n}\n}\n</code></pre><p>The <code>HashedRekordObj.signature.content</code> should match the content of the file\n<code>kubectl.sig</code> and <code>HashedRekordObj.signature.publicKey.content</code> should be\nidentical with the contents of <code>kubectl.cert</code>. It is also possible to specify\nthe remote certificate and signature locations without downloading them:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#b8860b\">COSIGN_EXPERIMENTAL</span><span style=\"color:#666\">=</span><span style=\"color:#666\">1</span> cosign verify-blob kubectl <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --signature https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.sig <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --certificate https://dl.k8s.io/release/v1.26.0-rc.1/bin/linux/amd64/kubectl.cert\n</span></span></code></pre></div><pre tabindex=\"0\"><code>tlog entry verified with uuid: 5d54b39222e3fa9a21bcb0badd8aac939b4b0d1d9085b37f1f10b18a8cd24657 index: 8173886\nVerified OK\n</code></pre><p>All of the mentioned steps as well as how to verify container images are\noutlined in the official documentation about how to <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts\">Verify Signed Kubernetes\nArtifacts</a>. In one of the next upcoming Kubernetes releases we will\nworking making the global story more mature by ensuring that truly all\nKubernetes artifacts are signed. Beside that, we are considering using Kubernetes\nowned infrastructure for the signing (root trust) and verification (transparency\nlog) process.</p>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>If you're interested in contributing to SIG Release, then consider applying for\nthe upcoming v1.27 shadowing program (watch for the announcement on\n<a href=\"https://groups.google.com/a/kubernetes.io/g/dev\">k-dev</a>) or join our <a href=\"http://bit.ly/k8s-sig-release-meeting\">weekly meeting</a> to say <em>hi</em>.</p>\n<p>We're looking forward to making even more of those awesome changes for future\nKubernetes releases. For example, we're working on the <a href=\"https://github.com/kubernetes/enhancements/issues/3027\">SLSA Level 3 Compliance\nin the Kubernetes Release Process</a> or the <a href=\"https://github.com/kubernetes/enhancements/issues/2853\">Renaming of the kubernetes/kubernetes\ndefault branch name to <code>main</code></a>.</p>\n<p>Thank you for reading this blog post! I'd like to use this opportunity to give\nall involved SIG Release folks a special shout-out for shipping this feature in\ntime!</p>\n<p>Feel free to reach out to us by using the <a href=\"https://groups.google.com/g/kubernetes-sig-release\">SIG Release mailing list</a> or\nthe <a href=\"http://slack.k8s.io\">#sig-release</a> Slack channel.</p>\n<h2 id=\"additional-resources\">Additional resources</h2>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3031\">Signing Release Artifacts Enhancement Proposal</a></li>\n</ul>","PublishedAt":"2022-12-12 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/12/kubernetes-release-artifact-signing/","SourceName":"Kubernetes"}},{"node":{"ID":2499,"Title":"Blog: Kubernetes v1.26: Electrifying","Description":"<p><strong>Authors</strong>: <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.26/release-team.md\">Kubernetes 1.26 Release Team</a></p>\n<p>It's with immense joy that we announce the release of Kubernetes v1.26!</p>\n<p>This release includes a total of 37 enhancements: eleven of them are graduating to Stable, ten are\ngraduating to Beta, and sixteen of them are entering Alpha. We also have twelve features being\ndeprecated or removed, three of which we better detail in this announcement.</p>\n<h2 id=\"release-theme-and-logo\">Release theme and logo</h2>\n<p><strong>Kubernetes 1.26: Electrifying</strong></p>\n<figure class=\"release-logo\">\n<img src=\"https://kubernetes.io/images/blog/2022-12-08-kubernetes-1.26-release/kubernetes-1.26.png\"\nalt=\"Kubernetes 1.26 Electrifying logo\"/>\n</figure>\n<p>The theme for Kubernetes v1.26 is <em>Electrifying</em>.</p>\n<p>Each Kubernetes release is the result of the coordinated effort of dedicated volunteers, and only\nmade possible due to the use of a diverse and complex set of computing resources, spread out through\nmultiple datacenters and regions worldwide. The end result of a release - the binaries, the image\ncontainers, the documentation - are then deployed on a growing number of personal, on-premises, and\ncloud computing resources.</p>\n<p>In this release we want to recognise the importance of all these building blocks on which Kubernetes\nis developed and used, while at the same time raising awareness on the importance of taking the\nenergy consumption footprint into account: environmental sustainability is an inescapable concern of\ncreators and users of any software solution, and the environmental footprint of sofware, like\nKubernetes, an area which we believe will play a significant role in future releases.</p>\n<p>As a community, we always work to make each new release process better than before (in this release,\nwe have <a href=\"https://github.com/orgs/kubernetes/projects/98/views/1\">started to use Projects for tracking\nenhancements</a>, for example). If <a href=\"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/\">v1.24\n&quot;Stargazer&quot;</a> <em>had us looking upwards, to\nwhat is possible when our community comes together</em>, and <a href=\"https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/\">v1.25\n&quot;Combiner&quot;</a> <em>what the combined efforts of our community\nare capable of</em>, this v1.26 &quot;Electrifying&quot; is also dedicated to all of those whose individual\nmotion, integrated into the release flow, made all of this possible.</p>\n<h2 id=\"major-themes\">Major themes</h2>\n<p>Kubernetes v1.26 is composed of many changes, brought to you by a worldwide team of volunteers. For\nthis release, we have identified several major themes.</p>\n<h3 id=\"change-in-container-image-registry\">Change in container image registry</h3>\n<p>In the previous release, <a href=\"https://github.com/kubernetes/kubernetes/pull/109938\">Kubernetes changed the container\nregistry</a>, allowing the spread of the load\nacross multiple Cloud Providers and Regions, a change that reduced the reliance on a single entity\nand provided a faster download experience for a large number of users.</p>\n<p>This release of Kubernetes is the first that is exclusively published in the new <code>registry.k8s.io</code>\ncontainer image registry. In the (now legacy) <code>k8s.gcr.io</code> image registry, no container images tags\nfor v1.26 will be published, and only tags from releases before v1.26 will continue to be\nupdated. Refer to <a href=\"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/\">registry.k8s.io: faster, cheaper and Generally\nAvailable</a> for more information on the\nmotivation, advantages, and implications of this significant change.</p>\n<h3 id=\"cri-v1alpha2-removed\">CRI v1alpha2 removed</h3>\n<p>With the adoption of the <a href=\"https://kubernetes.io/docs/concepts/architecture/cri/\">Container Runtime Interface</a> (CRI) and\nthe <a href=\"https://kubernetes.io/blog/2022/02/17/dockershim-faq/\">removal of dockershim</a> in v1.24, the CRI is the only\nsupported and documented way through which Kubernetes interacts with different container\nruntimes. Each kubelet negotiates which version of CRI to use with the container runtime on that\nnode.</p>\n<p>In the previous release, the Kubernetes project recommended using CRI version <code>v1</code>, but kubelet\ncould still negotiate the use of CRI <code>v1alpha2</code>, which was deprecated.</p>\n<p>Kubernetes v1.26 drops support for CRI <code>v1alpha2</code>. That\n<a href=\"https://github.com/kubernetes/kubernetes/pull/110618\">removal</a> will result in the kubelet not\nregistering the node if the container runtime doesn't support CRI <code>v1</code>. This means that containerd\nminor version 1.5 and older are not supported in Kubernetes 1.26; if you use containerd, you will\nneed to upgrade to containerd version 1.6.0 or later <strong>before</strong> you upgrade that node to Kubernetes\nv1.26. This applies equally to any other container runtimes that only support the <code>v1alpha2</code>: if\nthat affects you, you should contact the container runtime vendor for advice or check their website\nfor additional instructions in how to move forward.</p>\n<h3 id=\"storage-improvements\">Storage improvements</h3>\n<p>Following the GA of the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration\">core Container Storage Interface (CSI)\nMigration</a>\nfeature in the previous release, CSI migration is an on-going effort that we've been working on for\na few releases now, and this release continues to add (and remove) features aligned with the\nmigration's goals, as well as other improvements to Kubernetes storage.</p>\n<h4 id=\"csi-migration-for-azure-file-and-vsphere-graduated-to-stable\">CSI migration for Azure File and vSphere graduated to stable</h4>\n<p>Both the <a href=\"https://github.com/kubernetes/enhancements/issues/1491\">vSphere</a> and\n<a href=\"https://github.com/kubernetes/enhancements/issues/1885\">Azure</a> in-tree driver migration to CSI have\ngraduated to Stable. You can find more information about them in the <a href=\"https://github.com/kubernetes-sigs/vsphere-csi-driver\">vSphere CSI\ndriver</a> and <a href=\"https://github.com/kubernetes-sigs/azurefile-csi-driver\">Azure File CSI\ndriver</a> repositories.</p>\n<h4 id=\"delegate-fsgroup-to-csi-driver-graduated-to-stable\"><em>Delegate FSGroup to CSI Driver</em> graduated to stable</h4>\n<p>This feature allows Kubernetes to <a href=\"https://github.com/kubernetes/enhancements/issues/2317\">supply the pod's <code>fsGroup</code> to the CSI driver when a volume is\nmounted</a> so that the driver can utilize\nmount options to control volume permissions. Previously, the kubelet would always apply the\n<code>fsGroup</code>ownership and permission change to files in the volume according to the policy specified in\nthe Pod's <code>.spec.securityContext.fsGroupChangePolicy</code> field. Starting with this release, CSI\ndrivers have the option to apply the <code>fsGroup</code> settings during attach or mount time of the volumes.</p>\n<h4 id=\"in-tree-glusterfs-driver-removal\">In-tree GlusterFS driver removal</h4>\n<p>Already deprecated in the v1.25 release, the in-tree GlusterFS driver was\n<a href=\"https://github.com/kubernetes/enhancements/issues/3446\">removed</a> in this release.</p>\n<h4 id=\"in-tree-openstack-cinder-driver-removal\">In-tree OpenStack Cinder driver removal</h4>\n<p>This release removed the deprecated in-tree storage integration for OpenStack (the <code>cinder</code> volume\ntype). You should migrate to external cloud provider and CSI driver from\n<a href=\"https://github.com/kubernetes/cloud-provider-openstack\">https://github.com/kubernetes/cloud-provider-openstack</a> instead. For more information, visit <a href=\"https://github.com/kubernetes/enhancements/issues/1489\">Cinder\nin-tree to CSI driver migration</a>.</p>\n<h3 id=\"signing-kubernetes-release-artifacts-graduates-to-beta\">Signing Kubernetes release artifacts graduates to beta</h3>\n<p>Introduced in Kubernetes v1.24, <a href=\"https://github.com/kubernetes/enhancements/issues/3031\">this\nfeature</a> constitutes a significant milestone\nin improving the security of the Kubernetes release process. All release artifacts are signed\nkeyless using <a href=\"https://github.com/sigstore/cosign/\">cosign</a>, and both binary artifacts and images\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/\">can be verified</a>.</p>\n<h3 id=\"support-for-windows-privileged-containers-graduates-to-stable\">Support for Windows privileged containers graduates to stable</h3>\n<p>Privileged container support allows containers to run with similar access to the host as processes\nthat run on the host directly. Support for this feature in Windows nodes, called <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/\">HostProcess\ncontainers</a>, will now <a href=\"https://github.com/kubernetes/enhancements/issues/1981\">graduate to Stable</a>,\nenabling access to host resources (including network resources) from privileged containers.</p>\n<h3 id=\"improvements-to-kubernetes-metrics\">Improvements to Kubernetes metrics</h3>\n<p>This release has several noteworthy improvements on metrics.</p>\n<h4 id=\"metrics-framework-extension-graduates-to-alpha\">Metrics framework extension graduates to alpha</h4>\n<p>The metrics framework extension <a href=\"https://github.com/kubernetes/enhancements/issues/3498\">graduates to\nAlpha</a>, and\n<a href=\"https://kubernetes.io/docs/reference/instrumentation/metrics/\">documentation is now published for every metric in the\nKubernetes codebase</a>.This enhancement adds two additional metadata\nfields to Kubernetes metrics: <code>Internal</code> and <code>Beta</code>, representing different stages of metric maturity.</p>\n<h4 id=\"component-health-service-level-indicators-graduates-to-alpha\">Component Health Service Level Indicators graduates to alpha</h4>\n<p>Also improving on the ability to consume Kubernetes metrics, <a href=\"https://kubernetes.io/docs/reference/instrumentation/slis/\">component health Service Level\nIndicators (SLIs)</a> have <a href=\"https://github.com/kubernetes/kubernetes/pull/112884\">graduated to\nAlpha</a>: by enabling the <code>ComponentSLIs</code>\nfeature flag there will be an additional metrics endpoint which allows the calculation of Service\nLevel Objectives (SLOs) from raw healthcheck data converted into metric format.</p>\n<h4 id=\"feature-metrics-are-now-available\">Feature metrics are now available</h4>\n<p>Feature metrics are now available for each Kubernetes component, making it possible to <a href=\"https://github.com/kubernetes/kubernetes/pull/112690\">track\nwhether each active feature gate is enabled</a>\nby checking the component's metric endpoint for <code>kubernetes_feature_enabled</code>.</p>\n<h3 id=\"dynamic-resource-allocation-graduates-to-alpha\">Dynamic Resource Allocation graduates to alpha</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/\">Dynamic Resource\nAllocation</a>\nis a <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\">new feature</a>\nthat puts resource scheduling in the hands of third-party developers: it offers an\nalternative to the limited &quot;countable&quot; interface for requesting access to resources\n(e.g. <code>nvidia.com/gpu: 2</code>), providing an API more akin to that of persistent volumes. Under the\nhood, it uses the <a href=\"https://github.com/container-orchestrated-devices/container-device-interface\">Container Device\nInterface</a> (CDI) to do\nits device injection. This feature is blocked by the <code>DynamicResourceAllocation</code> feature gate.</p>\n<h3 id=\"cel-in-admission-control-graduates-to-alpha\">CEL in Admission Control graduates to alpha</h3>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/3488\">This feature</a> introduces a <code>v1alpha1</code> API for <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/\">validating admission\npolicies</a>, enabling extensible admission\ncontrol via <a href=\"https://github.com/google/cel-spec\">Common Expression Language</a> expressions. Currently,\ncustom policies are enforced via <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\">admission\nwebhooks</a>,\nwhich, while flexible, have a few drawbacks when compared to in-process policy enforcement. To use,\nenable the <code>ValidatingAdmissionPolicy</code> feature gate and the <code>admissionregistration.k8s.io/v1alpha1</code>\nAPI via <code>--runtime-config</code>.</p>\n<h3 id=\"pod-scheduling-improvements\">Pod scheduling improvements</h3>\n<p>Kubernetes v1.26 introduces some relevant enhancements to the ability to better control scheduling\nbehavior.</p>\n<h4 id=\"podschedulingreadiness-graduates-to-alpha\"><code>PodSchedulingReadiness</code> graduates to alpha</h4>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/3521\">This feature</a> introduces a <code>.spec.schedulingGates</code>\nfield to Pod's API, to <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/\">indicate whether the Pod is allowed to be scheduled or not</a>. External users/controllers can use this field to hold a Pod from scheduling based on their policies and\nneeds.</p>\n<h4 id=\"nodeinclusionpolicyinpodtopologyspread-graduates-to-beta\"><code>NodeInclusionPolicyInPodTopologySpread</code> graduates to beta</h4>\n<p>By specifying a <code>nodeInclusionPolicy</code> in <code>topologySpreadConstraints</code>, you can control whether to\n<a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\">take taints/tolerations into consideration</a>\nwhen calculating Pod Topology Spread skew.</p>\n<h2 id=\"other-updates\">Other Updates</h2>\n<h3 id=\"graduations-to-stable\">Graduations to stable</h3>\n<p>This release includes a total of eleven enhancements promoted to Stable:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1981\">Support for Windows privileged containers</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1491\">vSphere in-tree to CSI driver migration</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2317\">Allow Kubernetes to supply pod's fsgroup to CSI driver on mount</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1885\">Azure file in-tree to CSI driver migration</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2307\">Job tracking without lingering Pods</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2086\">Service Internal Traffic Policy</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2133\">Kubelet Credential Provider</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1435\">Support of mixed protocols in Services with type=LoadBalancer</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3070\">Reserve Service IP Ranges For Dynamic and Static IP Allocation</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3570\">CPUManager</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3573\">DeviceManager</a></li>\n</ul>\n<h3 id=\"deprecations-and-removals\">Deprecations and removals</h3>\n<p>12 features were <a href=\"https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/\">deprecated or removed</a> from\nKubernetes with this release.</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/110618\">CRI <code>v1alpha2</code> API is removed</a></li>\n<li><a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v126\">Removal of the <code>v1beta1</code> flow control API group</a></li>\n<li><a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#horizontalpodautoscaler-v126\">Removal of the <code>v2beta2</code> HorizontalPodAutoscaler API</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3446\">GlusterFS plugin removed from available in-tree drivers</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/112120\">Removal of legacy command line arguments relating to logging</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/112133\">Removal of <code>kube-proxy</code> userspace modes</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/112341\">Removal of in-tree credential management code</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1489\">The in-tree OpenStack cloud provider is removed</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/112643\">Removal of dynamic kubelet configuration</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/113116\">Deprecation of non-inclusive <code>kubectl</code> flag</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/38186\">Deprecations for <code>kube-apiserver</code> command line arguments</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/112261\">Deprecations for <code>kubectl run</code> command line arguments</a></li>\n</ul>\n<h3 id=\"release-notes\">Release notes</h3>\n<p>The complete details of the Kubernetes v1.26 release are available in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md\">release\nnotes</a>.</p>\n<h3 id=\"availability\">Availability</h3>\n<p>Kubernetes v1.26 is available for download on <a href=\"https://k8s.io/releases/download/\">the Kubernetes site</a>.\nTo get started with Kubernetes, check out these <a href=\"https://kubernetes.io/docs/tutorials/\">interactive tutorials</a> or run local\nKubernetes clusters using containers as &quot;nodes&quot;, with <a href=\"https://kind.sigs.k8s.io/\">kind</a>. You can also\neasily install v1.26 using <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h3 id=\"release-team\">Release team</h3>\n<p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each\nrelease team is made up of dedicated community volunteers who work together to build the many pieces\nthat make up the Kubernetes releases you rely on. This requires the specialized skills of people\nfrom all corners of our community, from the code itself to its documentation and project management.</p>\n<p>We would like to thank the entire <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.26/release-team.md\">release team</a>\nfor the hours spent hard at work to ensure we deliver a solid Kubernetes v1.26 release for our community.</p>\n<p>A very special thanks is in order for our Release Lead, Leonard Pahlke, for successfully steering\nthe entire release team throughout the entire release cycle, by making sure that we could all\ncontribute in the best way possible to this release through his constant support and attention to\nthe many and diverse details that make up the path to a successful release.</p>\n<h3 id=\"user-highlights\">User highlights</h3>\n<ul>\n<li>Wortell faced increasingly higher ammounts of developer expertise and time for daily\ninfrastructure management. <a href=\"https://www.cncf.io/case-studies/wortell/\">They used Dapr to reduce the complexity and amount of required\ninfrastructure-related code, allowing them to focus more time on new\nfeatures</a>.</li>\n<li>Utmost handles sensitive personal data and needed SOC 2 Type II attestation, ISO 27001\ncertification, and zero trust networking. <a href=\"https://www.cncf.io/case-studies/utmost/\">Using Cilium, they created automated pipelines that\nallowed developers to create new policies, supporting over 4,000 flows per\nsecond</a>.</li>\n<li>Global cybersecurity company Ericom‚Äôs solutions depend on hyper-low latency and data\nsecurity. <a href=\"https://www.cncf.io/case-studies/ericom/\">With Ridge's managed Kubernetes service they were able to deploy, through a single API,\nto a network of service providers worldwide</a>.</li>\n<li>Lunar, a Scandinavian online bank, wanted to implement quarterly production cluster failover\ntesting to prepare for disaster recovery, and needed a better way to managed their platform\nservices.<a href=\"https://www.cncf.io/case-studies/lunar/\">They started by centralizing their log management system, and followed-up with the\ncentralization of all platform services, using Linkerd to connect the\nclusters</a>.</li>\n<li>Datadog runs 10s of clusters with 10,000+ of nodes and 100,000+ pods across multiple cloud\nproviders.<a href=\"https://www.cncf.io/case-studies/datadog/\">They turned to Cilium as their CNI and kube-proxy replacement to take advantage of the\npower of eBPF and provide a consistent networking experience for their users across any\ncloud</a>.</li>\n<li>Insiel wanted to update their software production methods and introduce a cloud native paradigm in\ntheir software production. <a href=\"https://www.cncf.io/case-studies/insiel/\">Their digital transformation project with Kiratech and Microsoft Azure\nallowed them to develop a cloud-first culture</a>.</li>\n</ul>\n<h3 id=\"ecosystem-updates\">Ecosystem updates</h3>\n<ul>\n<li>KubeCon + CloudNativeCon Europe 2023 will take place in Amsterdam, The Netherlands, from 17 ‚Äì 21\nApril 2023! You can find more information about the conference and registration on the <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/\">event\nsite</a>.</li>\n<li>CloudNativeSecurityCon North America, a two-day event designed to foster collaboration, discussion\nand knowledge sharing of cloud native security projects and how to best use these to address\nsecurity challenges and opportunities, will take place in Seattle, Washington (USA), from 1-2\nFebruary 2023. See the <a href=\"https://events.linuxfoundation.org/cloudnativesecuritycon-north-america/\">event\npage</a> for more\ninformation.</li>\n<li>The CNCF announced <a href=\"https://www.cncf.io/announcements/2022/10/28/cloud-native-computing-foundation-reveals-2022-community-awards-winners/\">the 2022 Community Awards\nWinners</a>:\nthe Community Awards recognize CNCF community members that are going above and beyond to advance\ncloud native technology.</li>\n</ul>\n<h3 id=\"project-velocity\">Project velocity</h3>\n<p>The <a href=\"https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m\">CNCF K8s DevStats</a> project\naggregates a number of interesting data points related to the velocity of Kubernetes and various\nsub-projects. This includes everything from individual contributions to the number of companies that\nare contributing, and is an illustration of the depth and breadth of effort that goes into evolving\nthis ecosystem.</p>\n<p>In the v1.26 release cycle, which <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.26\">ran for 14 weeks</a>\n(September 5 to December 9), we saw contributions from <a href=\"https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.25.0%20-%20v1.26.0&amp;var-metric=contributions\">976 companies</a> and <a href=\"https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.25.0%20-%20v1.26.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes\">6877 individuals</a>.</p>\n<h2 id=\"upcoming-release-webinar\">Upcoming Release Webinar</h2>\n<p>Join members of the Kubernetes v1.26 release team on Tuesday January 17, 2023 10am - 11am EST (3pm - 4pm UTC) to learn about the major features\nof this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the <a href=\"https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v126-release/\">event\npage</a>.</p>\n<h2 id=\"get-involved\">Get Involved</h2>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest\nGroups</a> (SIGs) that align with your\ninterests.</p>\n<p>Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly\n<a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through\nthe channels below:</p>\n<ul>\n<li>Find out more about contributing to Kubernetes at the <a href=\"https://www.kubernetes.dev/\">Kubernetes\nContributors</a> website</li>\n<li>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for the latest updates</li>\n<li>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a></li>\n<li>Join the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Post questions (or answer questions) on <a href=\"https://serverfault.com/questions/tagged/kubernetes\">Server\nFault</a></li>\n<li><a href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">Share</a> your Kubernetes story</li>\n<li>Read more about what‚Äôs happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a></li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release\nTeam</a></li>\n</ul>","PublishedAt":"2022-12-09 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/09/kubernetes-v1-26-release/","SourceName":"Kubernetes"}},{"node":{"ID":2450,"Title":"Blog: Forensic container checkpointing in Kubernetes","Description":"<p><strong>Authors:</strong> Adrian Reber (Red Hat)</p>\n<p>Forensic container checkpointing is based on <a href=\"https://criu.org/\">Checkpoint/Restore In\nUserspace</a> (CRIU) and allows the creation of stateful copies\nof a running container without the container knowing that it is being\ncheckpointed. The copy of the container can be analyzed and restored in a\nsandbox environment multiple times without the original container being aware\nof it. Forensic container checkpointing was introduced as an alpha feature in\nKubernetes v1.25.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>With the help of CRIU it is possible to checkpoint and restore containers.\nCRIU is integrated in runc, crun, CRI-O and containerd and forensic container\ncheckpointing as implemented in Kubernetes uses these existing CRIU\nintegrations.</p>\n<h2 id=\"why-is-it-important\">Why is it important?</h2>\n<p>With the help of CRIU and the corresponding integrations it is possible to get\nall information and state about a running container on disk for later forensic\nanalysis. Forensic analysis might be important to inspect a suspicious\ncontainer without stopping or influencing it. If the container is really under\nattack, the attacker might detect attempts to inspect the container. Taking a\ncheckpoint and analysing the container in a sandboxed environment offers the\npossibility to inspect the container without the original container and maybe\nattacker being aware of the inspection.</p>\n<p>In addition to the forensic container checkpointing use case, it is also\npossible to migrate a container from one node to another node without loosing\nthe internal state. Especially for stateful containers with long initialization\ntimes restoring from a checkpoint might save time after a reboot or enable much\nfaster startup times.</p>\n<h2 id=\"how-do-i-use-container-checkpointing\">How do I use container checkpointing?</h2>\n<p>The feature is behind a <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a>, so\nmake sure to enable the <code>ContainerCheckpoint</code> gate before you can use the new\nfeature.</p>\n<p>The runtime must also support container checkpointing:</p>\n<ul>\n<li>\n<p>containerd: support is currently under discussion. See containerd\npull request <a href=\"https://github.com/containerd/containerd/pull/6965\">#6965</a> for more details.</p>\n</li>\n<li>\n<p>CRI-O: v1.25 has support for forensic container checkpointing.</p>\n</li>\n</ul>\n<h3 id=\"usage-example-with-cri-o\">Usage example with CRI-O</h3>\n<p>To use forensic container checkpointing in combination with CRI-O, the runtime\nneeds to be started with the command-line option <code>--enable-criu-support=true</code>.\nFor Kubernetes, you need to run your cluster with the <code>ContainerCheckpoint</code>\nfeature gate enabled. As the checkpointing functionality is provided by CRIU it\nis also necessary to install CRIU. Usually runc or crun depend on CRIU and\ntherefore it is installed automatically.</p>\n<p>It is also important to mention that at the time of writing the checkpointing functionality is\nto be considered as an alpha level feature in CRI-O and Kubernetes and the\nsecurity implications are still under consideration.</p>\n<p>Once containers and pods are running it is possible to create a checkpoint.\n<a href=\"https://kubernetes.io/docs/reference/node/kubelet-checkpoint-api/\">Checkpointing</a>\nis currently only exposed on the <strong>kubelet</strong> level. To checkpoint a container,\nyou can run <code>curl</code> on the node where that container is running, and trigger a\ncheckpoint:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>curl -X POST <span style=\"color:#b44\">&#34;https://localhost:10250/checkpoint/namespace/podId/container&#34;</span>\n</span></span></code></pre></div><p>For a container named <em>counter</em> in a pod named <em>counters</em> in a namespace named\n<em>default</em> the <strong>kubelet</strong> API endpoint is reachable at:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>curl -X POST <span style=\"color:#b44\">&#34;https://localhost:10250/checkpoint/default/counters/counter&#34;</span>\n</span></span></code></pre></div><p>For completeness the following <code>curl</code> command-line options are necessary to\nhave <code>curl</code> accept the <em>kubelet</em>'s self signed certificate and authorize the\nuse of the <em>kubelet</em> <code>checkpoint</code> API:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>--insecure --cert /var/run/kubernetes/client-admin.crt --key /var/run/kubernetes/client-admin.key\n</span></span></code></pre></div><p>Triggering this <strong>kubelet</strong> API will request the creation of a checkpoint from\nCRI-O. CRI-O requests a checkpoint from your low-level runtime (for example,\n<code>runc</code>). Seeing that request, <code>runc</code> invokes the <code>criu</code> tool\nto do the actual checkpointing.</p>\n<p>Once the checkpointing has finished the checkpoint should be available at\n<code>/var/lib/kubelet/checkpoints/checkpoint-&lt;pod-name&gt;_&lt;namespace-name&gt;-&lt;container-name&gt;-&lt;timestamp&gt;.tar</code></p>\n<p>You could then use that tar archive to restore the container somewhere else.</p>\n<h3 id=\"restore-checkpointed-container-standalone\">Restore a checkpointed container outside of Kubernetes (with CRI-O)</h3>\n<p>With the checkpoint tar archive it is possible to restore the container outside\nof Kubernetes in a sandboxed instance of CRI-O. For better user experience\nduring restore, I recommend that you use the latest version of CRI-O from the\n<em>main</em> CRI-O GitHub branch. If you're using CRI-O v1.25, you'll need to\nmanually create certain directories Kubernetes would create before starting the\ncontainer.</p>\n<p>The first step to restore a container outside of Kubernetes is to create a pod sandbox\nusing <em>crictl</em>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>crictl runp pod-config.json\n</span></span></code></pre></div><p>Then you can restore the previously checkpointed container into the newly created pod sandbox:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>crictl create &lt;POD_ID&gt; container-config.json pod-config.json\n</span></span></code></pre></div><p>Instead of specifying a container image in a registry in <code>container-config.json</code>\nyou need to specify the path to the checkpoint archive that you created earlier:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;metadata&#34;</span>: {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;name&#34;</span>: <span style=\"color:#b44\">&#34;counter&#34;</span>\n</span></span><span style=\"display:flex;\"><span> },\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;image&#34;</span>:{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;image&#34;</span>: <span style=\"color:#b44\">&#34;/var/lib/kubelet/checkpoints/&lt;checkpoint-archive&gt;.tar&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>Next, run <code>crictl start &lt;CONTAINER_ID&gt;</code> to start that container, and then a\ncopy of the previously checkpointed container should be running.</p>\n<h3 id=\"restore-checkpointed-container-k8s\">Restore a checkpointed container within of Kubernetes</h3>\n<p>To restore the previously checkpointed container directly in Kubernetes it is\nnecessary to convert the checkpoint archive into an image that can be pushed to\na registry.</p>\n<p>One possible way to convert the local checkpoint archive consists of the\nfollowing steps with the help of <a href=\"https://buildah.io/\">buildah</a>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#b8860b\">newcontainer</span><span style=\"color:#666\">=</span><span style=\"color:#a2f;font-weight:bold\">$(</span>buildah from scratch<span style=\"color:#a2f;font-weight:bold\">)</span>\n</span></span><span style=\"display:flex;\"><span>buildah add <span style=\"color:#b8860b\">$newcontainer</span> /var/lib/kubelet/checkpoints/checkpoint-&lt;pod-name&gt;_&lt;namespace-name&gt;-&lt;container-name&gt;-&lt;timestamp&gt;.tar /\n</span></span><span style=\"display:flex;\"><span>buildah config --annotation<span style=\"color:#666\">=</span>io.kubernetes.cri-o.annotations.checkpoint.name<span style=\"color:#666\">=</span>&lt;container-name&gt; <span style=\"color:#b8860b\">$newcontainer</span>\n</span></span><span style=\"display:flex;\"><span>buildah commit <span style=\"color:#b8860b\">$newcontainer</span> checkpoint-image:latest\n</span></span><span style=\"display:flex;\"><span>buildah rm <span style=\"color:#b8860b\">$newcontainer</span>\n</span></span></code></pre></div><p>The resulting image is not standardized and only works in combination with\nCRI-O. Please consider this image format as pre-alpha. There are ongoing\n<a href=\"https://github.com/opencontainers/image-spec/issues/962\">discussions</a> to standardize the format of checkpoint\nimages like this. Important to remember is that this not yet standardized image\nformat only works if CRI-O has been started with <code>--enable-criu-support=true</code>.\nThe security implications of starting CRI-O with CRIU support are not yet clear\nand therefore the functionality as well as the image format should be used with\ncare.</p>\n<p>Now, you'll need to push that image to a container image registry. For example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>buildah push localhost/checkpoint-image:latest container-image-registry.example/user/checkpoint-image:latest\n</span></span></code></pre></div><p>To restore this checkpoint image (<code>container-image-registry.example/user/checkpoint-image:latest</code>), the\nimage needs to be listed in the specification for a Pod. Here's an example\nmanifest:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namePrefix</span>:<span style=\"color:#bbb\"> </span>example-<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>&lt;container-name&gt;<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>container-image-registry.example/user/checkpoint-image:latest<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodeName</span>:<span style=\"color:#bbb\"> </span>&lt;destination-node&gt;<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Kubernetes schedules the new Pod onto a node. The kubelet on that node\ninstructs the container runtime (CRI-O in this example) to create and start a\ncontainer based on an image specified as <code>registry/user/checkpoint-image:latest</code>.\nCRI-O detects that <code>registry/user/checkpoint-image:latest</code>\nis a reference to checkpoint data rather than a container image. Then,\ninstead of the usual steps to create and start a container,\nCRI-O fetches the checkpoint data and restores the container from that\nspecified checkpoint.</p>\n<p>The application in that Pod would continue running as if the checkpoint had not been taken;\nwithin the container, the application looks and behaves like any other container that had been\nstarted normally and not restored from a checkpoint.</p>\n<p>With these steps, it is possible to replace a Pod running on one node\nwith a new equivalent Pod that is running on a different node,\nand without losing the state of the containers in that Pod.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>You can reach SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n</ul>","PublishedAt":"2022-12-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":2443,"Title":"Blog: Finding suspicious syscalls with the seccomp notifier","Description":"<p><strong>Authors:</strong> Sascha Grunert</p>\n<p>Debugging software in production is one of the biggest challenges we have to\nface in our containerized environments. Being able to understand the impact of\nthe available security options, especially when it comes to configuring our\ndeployments, is one of the key aspects to make the default security in\nKubernetes stronger. We have all those logging, tracing and metrics data already\nat hand, but how do we assemble the information they provide into something\nhuman readable and actionable?</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Seccomp\">Seccomp</a> is one of the standard mechanisms to protect a Linux based\nKubernetes application from malicious actions by interfering with its <a href=\"https://en.wikipedia.org/wiki/Syscall\">system\ncalls</a>. This allows us to restrict the application to a defined set of\nactionable items, like modifying files or responding to HTTP requests. Linking\nthe knowledge of which set of syscalls is required to, for example, modify a\nlocal file, to the actual source code is in the same way non-trivial. Seccomp\nprofiles for Kubernetes have to be written in <a href=\"https://www.json.org\">JSON</a> and can be understood\nas an architecture specific allow-list with superpowers, for example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-json\" data-lang=\"json\"><span style=\"display:flex;\"><span>{\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;defaultAction&#34;</span>: <span style=\"color:#b44\">&#34;SCMP_ACT_ERRNO&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;defaultErrnoRet&#34;</span>: <span style=\"color:#666\">38</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;defaultErrno&#34;</span>: <span style=\"color:#b44\">&#34;ENOSYS&#34;</span>,\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;syscalls&#34;</span>: [\n</span></span><span style=\"display:flex;\"><span> {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;names&#34;</span>: [<span style=\"color:#b44\">&#34;chmod&#34;</span>, <span style=\"color:#b44\">&#34;chown&#34;</span>, <span style=\"color:#b44\">&#34;open&#34;</span>, <span style=\"color:#b44\">&#34;write&#34;</span>],\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#008000;font-weight:bold\">&#34;action&#34;</span>: <span style=\"color:#b44\">&#34;SCMP_ACT_ALLOW&#34;</span>\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span> ]\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>The above profile errors by default specifying the <code>defaultAction</code> of\n<code>SCMP_ACT_ERRNO</code>. This means we have to allow a set of syscalls via\n<code>SCMP_ACT_ALLOW</code>, otherwise the application would not be able to do anything at\nall. Okay cool, for being able to allow file operations, all we have to do is\nadding a bunch of file specific syscalls like <code>open</code> or <code>write</code>, and probably\nalso being able to change the permissions via <code>chmod</code> and <code>chown</code>, right?\nBasically yes, but there are issues with the simplicity of that approach:</p>\n<p>Seccomp profiles need to include the minimum set of syscalls required to start\nthe application. This also includes some syscalls from the lower level\n<a href=\"https://opencontainers.org\">Open Container Initiative (OCI)</a> container runtime, for example\n<a href=\"https://github.com/opencontainers/runc\">runc</a> or <a href=\"https://github.com/containers/crun\">crun</a>. Beside that, we can only guarantee the required\nsyscalls for a very specific version of the runtimes and our application,\nbecause the code parts can change between releases. The same applies to the\ntermination of the application as well as the target architecture we're\ndeploying on. Features like executing commands within containers also require\nanother subset of syscalls. Not to mention that there are multiple versions for\nsyscalls doing slightly different things and the seccomp profiles are able to\nmodify their arguments. It's also not always clearly visible to the developers\nwhich syscalls are used by their own written code parts, because they rely on\nprogramming language abstractions or frameworks.</p>\n<p><em>How can we know which syscalls are even required then? Who should create and\nmaintain those profiles during its development life-cycle?</em></p>\n<p>Well, recording and distributing seccomp profiles is one of the problem domains\nof the <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator\">Security Profiles Operator</a>, which is already solving that. The\noperator is able to record <a href=\"https://en.wikipedia.org/wiki/Seccomp\">seccomp</a>, <a href=\"https://en.wikipedia.org/wiki/Security-Enhanced_Linux\">SELinux</a> and even\n<a href=\"https://en.wikipedia.org/wiki/AppArmor\">AppArmor</a> profiles into a <a href=\"https://k8s.io/docs/concepts/extend-kubernetes/api-extension/custom-resources\">Custom Resource Definition (CRD)</a>,\nreconciles them to each node and makes them available for usage.</p>\n<p>The biggest challenge about creating security profiles is to catch all code\npaths which execute syscalls. We could achieve that by having <strong>100%</strong> logical\ncoverage of the application when running an end-to-end test suite. You get the\nproblem with the previous statement: It's too idealistic to be ever fulfilled,\neven without taking all the moving parts during application development and\ndeployment into account.</p>\n<p>Missing a syscall in the seccomp profiles' allow list can have tremendously\nnegative impact on the application. It's not only that we can encounter crashes,\nwhich are trivially detectable. It can also happen that they slightly change\nlogical paths, change the business logic, make parts of the application\nunusable, slow down performance or even expose security vulnerabilities. We're\nsimply not able to see the whole impact of that, especially because blocked\nsyscalls via <code>SCMP_ACT_ERRNO</code> do not provide any additional <a href=\"https://linux.die.net/man/8/auditd\">audit</a>\nlogging on the system.</p>\n<p>Does that mean we're lost? Is it just not realistic to dream about a Kubernetes\nwhere <a href=\"https://github.com/kubernetes/enhancements/issues/2413\">everyone uses the default seccomp profile</a>? Should we\nstop striving towards maximum security in Kubernetes and accept that it's not\nmeant to be secure by default?</p>\n<p><strong>Definitely not.</strong> Technology evolves over time and there are many folks\nworking behind the scenes of Kubernetes to indirectly deliver features to\naddress such problems. One of the mentioned features is the <em>seccomp notifier</em>,\nwhich can be used to find suspicious syscalls in Kubernetes.</p>\n<p>The seccomp notify feature consists of a set of changes introduced in Linux 5.9.\nIt makes the kernel capable of communicating seccomp related events to the user\nspace. That allows applications to act based on the syscalls and opens for a\nwide range of possible use cases. We not only need the right kernel version,\nbut also at least runc v1.1.0 (or crun v0.19) to be able to make the notifier\nwork at all. The Kubernetes container runtime <a href=\"https://cri-o.io\">CRI-O</a> gets <a href=\"https://github.com/cri-o/cri-o/pull/6120\">support for\nthe seccomp notifier in v1.26.0</a>. The new feature allows us to\nidentify possibly malicious syscalls in our application, and therefore makes it\npossible to verify profiles for consistency and completeness. Let's give that a\ntry.</p>\n<p>First of all we need to run the latest <code>main</code> version of CRI-O, because v1.26.0\nhas not been released yet at time of writing. You can do that by either\ncompiling it from the <a href=\"https://github.com/cri-o/cri-o/blob/main/install.md#build-and-install-cri-o-from-source\">source code</a> or by using the pre-built binary\nbundle via <a href=\"https://github.com/cri-o/cri-o#installing-cri-o\">the get-script</a>. The seccomp notifier feature of CRI-O is\nguarded by an annotation, which has to be explicitly allowed, for example by\nusing a configuration drop-in like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> cat /etc/crio/crio.conf.d/02-runtimes.conf\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex;\"><span>[crio.runtime]\n</span></span><span style=\"display:flex;\"><span>default_runtime = <span style=\"color:#b44\">&#34;runc&#34;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>[crio.runtime.runtimes.runc]\n</span></span><span style=\"display:flex;\"><span>allowed_annotations = [ <span style=\"color:#b44\">&#34;io.kubernetes.cri-o.seccompNotifierAction&#34;</span> ]\n</span></span></code></pre></div><p>If CRI-O is up and running, then it should indicate that the seccomp notifier is\navailable as well:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> sudo ./bin/crio --enable-metrics\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">INFO[‚Ä¶] Starting seccomp notifier watcher\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">INFO[‚Ä¶] Serving metrics on :9090 via HTTP\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span></code></pre></div><p>We also enable the metrics, because they provide additional telemetry data about\nthe notifier. Now we need a running Kubernetes cluster for demonstration\npurposes. For this demo, we mainly stick to the\n<a href=\"https://github.com/cri-o/cri-o#running-kubernetes-with-cri-o\"><code>hack/local-up-cluster.sh</code></a> approach to locally spawn a single node\nKubernetes cluster.</p>\n<p>If everything is up and running, then we would have to define a seccomp profile\nfor testing purposes. But we do not have to create our own, we can just use the\n<code>RuntimeDefault</code> profile which gets shipped with each container runtime. For\nexample the <code>RuntimeDefault</code> profile for CRI-O can be found in the\n<a href=\"https://github.com/containers/common/blob/afff1d6/pkg/seccomp/seccomp.json\">containers/common</a> library.</p>\n<p>Now we need a test container, which can be a simple <a href=\"https://www.nginx.com\">nginx</a> pod like\nthis:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">io.kubernetes.cri-o.seccompNotifierAction</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;stop&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">restartPolicy</span>:<span style=\"color:#bbb\"> </span>Never<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>nginx:1.23.2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">securityContext</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">seccompProfile</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>RuntimeDefault<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Please note the annotation <code>io.kubernetes.cri-o.seccompNotifierAction</code>, which\nenables the seccomp notifier for this workload. The value of the annotation can\nbe either <code>stop</code> for stopping the workload or anything else for doing nothing\nelse than logging and throwing metrics. Because of the termination we also use\nthe <code>restartPolicy: Never</code> to not automatically recreate the container on\nfailure.</p>\n<p>Let's run the pod and check if it works:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl apply -f nginx.yaml\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl get pods -o wide\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">nginx 1/1 Running 0 3m39s 10.85.0.3 127.0.0.1 &lt;none&gt; &lt;none&gt;\n</span></span></span></code></pre></div><p>We can also test if the web server itself works as intended:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> curl 10.85.0.3\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&lt;!DOCTYPE html&gt;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&lt;html&gt;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&lt;head&gt;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span></code></pre></div><p>While everything is now up and running, CRI-O also indicates that it has started\nthe seccomp notifier:</p>\n<pre tabindex=\"0\"><code>‚Ä¶\nINFO[‚Ä¶] Injecting seccomp notifier into seccomp profile of container 662a3bb0fdc7dd1bf5a88a8aa8ef9eba6296b593146d988b4a9b85822422febb\n‚Ä¶\n</code></pre><p>If we would now run a forbidden syscall inside of the container, then we can\nexpect that the workload gets terminated. Let's give that a try by running\n<code>chroot</code> in the containers namespaces:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl <span style=\"color:#a2f\">exec</span> -it nginx -- bash\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">root@nginx:/# chroot /tmp\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">chroot: cannot change root directory to &#39;/tmp&#39;: Function not implemented\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">root@nginx:/# command terminated with exit code 137\n</span></span></span></code></pre></div><p>The exec session got terminated, so it looks like the container is not running\nany more:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl get pods\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READY STATUS RESTARTS AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">nginx 0/1 seccomp killed 0 96s\n</span></span></span></code></pre></div><p>Alright, the container got killed by seccomp, do we get any more information\nabout what was going on?</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl describe pod nginx\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Name: nginx\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">Containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> nginx:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ‚Ä¶\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> State: Terminated\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Reason: seccomp killed\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Message: Used forbidden syscalls: chroot (1x)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Exit Code: 137\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Started: Mon, 14 Nov 2022 12:19:46 +0100\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Finished: Mon, 14 Nov 2022 12:20:26 +0100\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">‚Ä¶\n</span></span></span></code></pre></div><p>The seccomp notifier feature of CRI-O correctly set the termination reason and\nmessage, including which forbidden syscall has been used how often (<code>1x</code>). How\noften? Yes, the notifier gives the application up to 5 seconds after the last\nseen syscall until it starts the termination. This means that it's possible to\ncatch multiple forbidden syscalls within one test by avoiding time-consuming\ntrial and errors.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl <span style=\"color:#a2f\">exec</span> -it nginx -- chroot /tmp\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">chroot: cannot change root directory to &#39;/tmp&#39;: Function not implemented\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">command terminated with exit code 125\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl <span style=\"color:#a2f\">exec</span> -it nginx -- chroot /tmp\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">chroot: cannot change root directory to &#39;/tmp&#39;: Function not implemented\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">command terminated with exit code 125\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl <span style=\"color:#a2f\">exec</span> -it nginx -- swapoff -a\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">command terminated with exit code 32\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl <span style=\"color:#a2f\">exec</span> -it nginx -- swapoff -a\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">command terminated with exit code 32\n</span></span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> kubectl describe pod nginx | grep Message\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> Message: Used forbidden syscalls: chroot (2x), swapoff (2x)\n</span></span></span></code></pre></div><p>The CRI-O metrics will also reflect that:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">&gt;</span> curl -sf localhost:9090/metrics | grep seccomp_notifier\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">#</span> HELP container_runtime_crio_containers_seccomp_notifier_count_total Amount of containers stopped because they used a forbidden syscalls by their name\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">#</span> TYPE container_runtime_crio_containers_seccomp_notifier_count_total counter\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">container_runtime_crio_containers_seccomp_notifier_count_total{name=&#34;‚Ä¶&#34;,syscalls=&#34;chroot (1x)&#34;} 1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">container_runtime_crio_containers_seccomp_notifier_count_total{name=&#34;‚Ä¶&#34;,syscalls=&#34;chroot (2x), swapoff (2x)&#34;} 1\n</span></span></span></code></pre></div><p>How does it work in detail? CRI-O uses the chosen seccomp profile and injects\nthe action <code>SCMP_ACT_NOTIFY</code> instead of <code>SCMP_ACT_ERRNO</code>, <code>SCMP_ACT_KILL</code>,\n<code>SCMP_ACT_KILL_PROCESS</code> or <code>SCMP_ACT_KILL_THREAD</code>. It also sets a local listener\npath which will be used by the lower level OCI runtime (runc or crun) to create\nthe seccomp notifier socket. If the connection between the socket and CRI-O has\nbeen established, then CRI-O will receive notifications for each syscall being\ninterfered by seccomp. CRI-O stores the syscalls, allows a bit of timeout for\nthem to arrive and then terminates the container if the chosen\n<code>seccompNotifierAction=stop</code>. Unfortunately, the seccomp notifier is not able to\nnotify on the <code>defaultAction</code>, which means that it's required to have\na list of syscalls to test for custom profiles. CRI-O does also state that\nlimitation in the logs:</p>\n<pre tabindex=\"0\"><code class=\"language-log\" data-lang=\"log\">INFO[‚Ä¶] The seccomp profile default action SCMP_ACT_ERRNO cannot be overridden to SCMP_ACT_NOTIFY,\nwhich means that syscalls using that default action can&#39;t be traced by the notifier\n</code></pre><p>As a conclusion, the seccomp notifier implementation in CRI-O can be used to\nverify if your applications behave correctly when using <code>RuntimeDefault</code> or any\nother custom profile. Alerts can be created based on the metrics to create long\nrunning test scenarios around that feature. Making seccomp understandable and\neasier to use will increase adoption as well as help us to move towards a more\nsecure Kubernetes by default!</p>\n<p>Thank you for reading this blog post. If you'd like to read more about the\nseccomp notifier, checkout the following resources:</p>\n<ul>\n<li>The Seccomp Notifier - New Frontiers in Unprivileged Container Development: <a href=\"https://brauner.io/2020/07/23/seccomp-notify.html\">https://brauner.io/2020/07/23/seccomp-notify.html</a></li>\n<li>Bringing Seccomp Notify to Runc and Kubernetes: <a href=\"https://kinvolk.io/blog/2022/03/bringing-seccomp-notify-to-runc-and-kubernetes\">https://kinvolk.io/blog/2022/03/bringing-seccomp-notify-to-runc-and-kubernetes</a></li>\n<li>Seccomp Agent reference implementation: <a href=\"https://github.com/opencontainers/runc/tree/6b16d00/contrib/cmd/seccompagent\">https://github.com/opencontainers/runc/tree/6b16d00/contrib/cmd/seccompagent</a></li>\n</ul>","PublishedAt":"2022-12-02 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/02/seccomp-notifier/","SourceName":"Kubernetes"}},{"node":{"ID":2432,"Title":"Blog: Boosting Kubernetes container runtime observability with OpenTelemetry","Description":"<p><strong>Authors:</strong> Sascha Grunert</p>\n<p>When speaking about observability in the cloud native space, then probably\neveryone will mention <a href=\"https://opentelemetry.io\">OpenTelemetry (OTEL)</a> at some point in the\nconversation. That's great, because the community needs standards to rely on\nfor developing all cluster components into the same direction. OpenTelemetry\nenables us to combine logs, metrics, traces and other contextual information\n(called baggage) into a single resource. Cluster administrators or software\nengineers can use this resource to get a viewport about what is going on in the\ncluster over a defined period of time. But how can Kubernetes itself make use of\nthis technology stack?</p>\n<p>Kubernetes consists of multiple components where some are independent and others\nare stacked together. Looking at the architecture from a container runtime\nperspective, then there are from the top to the bottom:</p>\n<ul>\n<li><strong>kube-apiserver</strong>: Validates and configures data for the API objects</li>\n<li><strong>kubelet</strong>: Agent running on each node</li>\n<li><strong>CRI runtime</strong>: Container Runtime Interface (CRI) compatible container runtime\nlike <a href=\"https://cri-o.io\">CRI-O</a> or <a href=\"https://containerd.io\">containerd</a></li>\n<li><strong>OCI runtime</strong>: Lower level <a href=\"https://opencontainers.org\">Open Container Initiative (OCI)</a> runtime\nlike <a href=\"https://github.com/opencontainers/runc\">runc</a> or <a href=\"https://github.com/containers/crun\">crun</a></li>\n<li><strong>Linux kernel</strong> or <strong>Microsoft Windows</strong>: Underlying operating system</li>\n</ul>\n<p>That means if we encounter a problem with running containers in Kubernetes, then\nwe start looking at one of those components. Finding the root cause for problems\nis one of the most time consuming actions we face with the increased\narchitectural complexity from today's cluster setups. Even if we know the\ncomponent which seems to cause the issue, we still have to take the others into\naccount to maintain a mental timeline of events which are going on. How do we\nachieve that? Well, most folks will probably stick to scraping logs, filtering\nthem and assembling them together over the components borders. We also have\nmetrics, right? Correct, but bringing metrics values in correlation with plain\nlogs makes it even harder to track what is going on. Some metrics are also not\nmade for debugging purposes. They have been defined based on the end user\nperspective of the cluster for linking usable alerts and not for developers\ndebugging a cluster setup.</p>\n<p>OpenTelemetry to the rescue: the project aims to combine signals such as\n<a href=\"https://opentelemetry.io/docs/concepts/signals/traces\">traces</a>, <a href=\"https://opentelemetry.io/docs/concepts/signals/metrics\">metrics</a> and <a href=\"https://opentelemetry.io/docs/concepts/signals/logs\">logs</a> together to maintain the\nright viewport on the cluster state.</p>\n<p>What is the current state of OpenTelemetry tracing in Kubernetes? From an API\nserver perspective, we have alpha support for tracing since Kubernetes v1.22,\nwhich will graduate to beta in one of the upcoming releases. Unfortunately the\nbeta graduation has missed the v1.26 Kubernetes release. The design proposal can\nbe found in the <a href=\"https://github.com/kubernetes/enhancements/issues/647\"><em>API Server Tracing</em> Kubernetes Enhancement Proposal\n(KEP)</a> which provides more information about it.</p>\n<p>The kubelet tracing part is tracked <a href=\"https://github.com/kubernetes/enhancements/issues/2831\">in another KEP</a>, which was\nimplemented in an alpha state in Kubernetes v1.25. A beta graduation is not\nplanned as time of writing, but more may come in the v1.27 release cycle.\nThere are other side-efforts going on beside both KEPs, for example <a href=\"https://github.com/kubernetes/klog/issues/356\">klog is\nconsidering OTEL support</a>, which would boost the observability by\nlinking log messages to existing traces. Within SIG Instrumentation and SIG Node,\nwe're also discussing <a href=\"https://github.com/kubernetes/kubernetes/issues/113414\">how to link the\nkubelet traces together</a>, because right now they're focused on the\n<a href=\"https://grpc.io\">gRPC</a> calls between the kubelet and the CRI container runtime.</p>\n<p>CRI-O features OpenTelemetry tracing support <a href=\"https://github.com/cri-o/cri-o/pull/4883\">since v1.23.0</a> and is\nworking on continuously improving them, for example by <a href=\"https://github.com/cri-o/cri-o/pull/6294\">attaching the logs to the\ntraces</a> or extending the <a href=\"https://github.com/cri-o/cri-o/pull/6343\">spans to logical parts of the\napplication</a>. This helps users of the traces to gain the same\ninformation like parsing the logs, but with enhanced capabilities of scoping and\nfiltering to other OTEL signals. The CRI-O maintainers are also working on a\ncontainer monitoring replacement for <a href=\"https://github.com/containers/conmon\">conmon</a>, which is called\n<a href=\"https://github.com/containers/conmon-rs\">conmon-rs</a> and is purely written in <a href=\"https://www.rust-lang.org\">Rust</a>. One benefit of\nhaving a Rust implementation is to be able to add features like OpenTelemetry\nsupport, because the crates (libraries) for those already exist. This allows a\ntight integration with CRI-O and lets consumers see the most low level tracing\ndata from their containers.</p>\n<p>The <a href=\"https://containerd.io\">containerd</a> folks added tracing support since v1.6.0, which is\navailable <a href=\"https://github.com/containerd/containerd/blob/7def13d/docs/tracing.md\">by using a plugin</a>. Lower level OCI runtimes like\n<a href=\"https://github.com/opencontainers/runc\">runc</a> or <a href=\"https://github.com/containers/crun\">crun</a> feature no support for OTEL at all and it does not\nseem to exist a plan for that. We always have to consider that there is a\nperformance overhead when collecting the traces as well as exporting them to a\ndata sink. I still think it would be worth an evaluation on how extended\ntelemetry collection could look like in OCI runtimes. Let's see if the Rust OCI\nruntime <a href=\"https://github.com/containers/youki/issues/1348\">youki</a> is considering something like that in the future.</p>\n<p>I'll show you how to give it a try. For my demo I'll stick to a stack with a single local node\nthat has runc, conmon-rs, CRI-O, and a kubelet. To enable tracing in the kubelet, I need to\napply the following <code>KubeletConfiguration</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeletConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">featureGates</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">KubeletTracing</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">tracing</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">samplingRatePerMillion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1000000</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>A <code>samplingRatePerMillion</code> equally to one million will internally translate to\nsampling everything. A similar configuration has to be applied to CRI-O; I can\neither start the <code>crio</code> binary with <code>--enable-tracing</code> and\n<code>--tracing-sampling-rate-per-million 1000000</code> or we use a drop-in configuration\nlike this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat /etc/crio/crio.conf.d/99-tracing.conf\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex;\"><span>[crio.tracing]\n</span></span><span style=\"display:flex;\"><span>enable_tracing = <span style=\"color:#a2f;font-weight:bold\">true</span>\n</span></span><span style=\"display:flex;\"><span>tracing_sampling_rate_per_million = <span style=\"color:#666\">1000000</span>\n</span></span></code></pre></div><p>To configure CRI-O to use conmon-rs, you require at least the latest CRI-O\nv1.25.x and conmon-rs v0.4.0. Then a configuration drop-in like this can be used\nto make CRI-O use conmon-rs:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat /etc/crio/crio.conf.d/99-runtimes.conf\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex;\"><span>[crio.runtime]\n</span></span><span style=\"display:flex;\"><span>default_runtime = <span style=\"color:#b44\">&#34;runc&#34;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>[crio.runtime.runtimes.runc]\n</span></span><span style=\"display:flex;\"><span>runtime_type = <span style=\"color:#b44\">&#34;pod&#34;</span>\n</span></span><span style=\"display:flex;\"><span>monitor_path = <span style=\"color:#b44\">&#34;/path/to/conmonrs&#34;</span> <span style=\"color:#080;font-style:italic\"># or will be looked up in $PATH</span>\n</span></span></code></pre></div><p>That's it, the default configuration will point to an <a href=\"https://opentelemetry.io/docs/collector/getting-started\">OpenTelemetry\ncollector</a> <a href=\"https://grpc.io\">gRPC</a> endpoint of <code>localhost:4317</code>, which has to be up and\nrunning as well. There are multiple ways to run OTLP as <a href=\"https://opentelemetry.io/docs/collector/getting-started\">described in the\ndocs</a>, but it's also possible to <code>kubectl proxy</code> into an existing\ninstance running within Kubernetes.</p>\n<p>If everything is set up, then the collector should log that there are incoming\ntraces:</p>\n<pre tabindex=\"0\"><code>ScopeSpans #0\nScopeSpans SchemaURL:\nInstrumentationScope go.opentelemetry.io/otel/sdk/tracer\nSpan #0\nTrace ID : 71896e69f7d337730dfedb6356e74f01\nParent ID : a2a7714534c017e6\nID : 1d27dbaf38b9da8b\nName : github.com/cri-o/cri-o/server.(*Server).filterSandboxList\nKind : SPAN_KIND_INTERNAL\nStart time : 2022-11-15 09:50:20.060325562 +0000 UTC\nEnd time : 2022-11-15 09:50:20.060326291 +0000 UTC\nStatus code : STATUS_CODE_UNSET\nStatus message :\nSpan #1\nTrace ID : 71896e69f7d337730dfedb6356e74f01\nParent ID : a837a005d4389579\nID : a2a7714534c017e6\nName : github.com/cri-o/cri-o/server.(*Server).ListPodSandbox\nKind : SPAN_KIND_INTERNAL\nStart time : 2022-11-15 09:50:20.060321973 +0000 UTC\nEnd time : 2022-11-15 09:50:20.060330602 +0000 UTC\nStatus code : STATUS_CODE_UNSET\nStatus message :\nSpan #2\nTrace ID : fae6742709d51a9b6606b6cb9f381b96\nParent ID : 3755d12b32610516\nID : 0492afd26519b4b0\nName : github.com/cri-o/cri-o/server.(*Server).filterContainerList\nKind : SPAN_KIND_INTERNAL\nStart time : 2022-11-15 09:50:20.0607746 +0000 UTC\nEnd time : 2022-11-15 09:50:20.060795505 +0000 UTC\nStatus code : STATUS_CODE_UNSET\nStatus message :\nEvents:\nSpanEvent #0\n-&gt; Name: log\n-&gt; Timestamp: 2022-11-15 09:50:20.060778668 +0000 UTC\n-&gt; DroppedAttributesCount: 0\n-&gt; Attributes::\n-&gt; id: Str(adf791e5-2eb8-4425-b092-f217923fef93)\n-&gt; log.message: Str(No filters were applied, returning full container list)\n-&gt; log.severity: Str(DEBUG)\n-&gt; name: Str(/runtime.v1.RuntimeService/ListContainers)\n</code></pre><p>I can see that the spans have a trace ID and typically have a parent attached.\nEvents such as logs are part of the output as well. In the above case, the kubelet is\nperiodically triggering a <code>ListPodSandbox</code> RPC to CRI-O caused by the Pod\nLifecycle Event Generator (PLEG). Displaying those traces can be done via,\nfor example, <a href=\"https://www.jaegertracing.io/\">Jaeger</a>. When running the tracing stack locally, then a Jaeger\ninstance should be exposed on <code>http://localhost:16686</code> per default.</p>\n<p>The <code>ListPodSandbox</code> requests are directly visible within the Jaeger UI:</p>\n<p><img src=\"list_pod_sandbox.png\" alt=\"ListPodSandbox RPC in the Jaeger UI\"></p>\n<p>That's not too exciting, so I'll run a workload directly via <code>kubectl</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl run -it --rm --restart<span style=\"color:#666\">=</span>Never --image<span style=\"color:#666\">=</span>alpine alpine -- <span style=\"color:#a2f\">echo</span> hi\n</span></span></code></pre></div><pre tabindex=\"0\"><code>hi\npod &#34;alpine&#34; deleted\n</code></pre><p>Looking now at Jaeger, we can see that we have traces for <code>conmonrs</code>, <code>crio</code> as\nwell as the <code>kubelet</code> for the <code>RunPodSandbox</code> and <code>CreateContainer</code> CRI RPCs:</p>\n<p><img src=\"create_container.png\" alt=\"Container creation in the Jaeger UI\"></p>\n<p>The kubelet and CRI-O spans are connected to each other to make investigation\neasier. If we now take a closer look at the spans, then we can see that CRI-O's\nlogs are correctly accosted with the corresponding functionality. For example we\ncan extract the container user from the traces like this:</p>\n<p><img src=\"crio_spans.png\" alt=\"CRI-O in the Jaeger UI\"></p>\n<p>The lower level spans of conmon-rs are also part of this trace. For example\nconmon-rs maintains an internal <code>read_loop</code> for handling IO between the\ncontainer and the end user. The logs for reading and writing bytes are part of\nthe span. The same applies to the <code>wait_for_exit_code</code> span, which tells us that\nthe container exited successfully with code <code>0</code>:</p>\n<p><img src=\"conmonrs_spans.png\" alt=\"conmon-rs in the Jaeger UI\"></p>\n<p>Having all that information at hand side by side to the filtering capabilities\nof Jaeger makes the whole stack a great solution for debugging container issues!\nMentioning the &quot;whole stack&quot; also shows the biggest downside of the overall\napproach: Compared to parsing logs it adds a noticeable overhead on top of the\ncluster setup. Users have to maintain a sink like <a href=\"https://www.elastic.co\">Elasticsearch</a> to\npersist the data, expose the Jaeger UI and possibly take the performance\ndrawback into account. Anyways, it's still one of the best ways to increase the\nobservability aspect of Kubernetes.</p>\n<p>Thank you for reading this blog post, I'm pretty sure we're looking into a\nbright future for OpenTelemetry support in Kubernetes to make troubleshooting\nsimpler.</p>","PublishedAt":"2022-12-01 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/12/01/runtime-observability-opentelemetry/","SourceName":"Kubernetes"}},{"node":{"ID":2337,"Title":"Blog: registry.k8s.io: faster, cheaper and Generally Available (GA)","Description":"<p><strong>Authors</strong>: Adolfo Garc√≠a Veytia (Chainguard), Bob Killen (Google)</p>\n<p>Starting with Kubernetes 1.25, our container image registry has changed from k8s.gcr.io to <a href=\"https://registry.k8s.io\">registry.k8s.io</a>. This new registry spreads the load across multiple Cloud Providers &amp; Regions, functioning as a sort of content delivery network (CDN) for Kubernetes container images. This change reduces the project‚Äôs reliance on a single entity and provides a faster download experience for a large number of users.</p>\n<h2 id=\"tl-dr-what-you-need-to-know-about-this-change\">TL;DR: What you need to know about this change</h2>\n<ul>\n<li>Container images for Kubernetes releases from 1.25 onward are no longer published to k8s.gcr.io, only to registry.k8s.io.</li>\n<li>In the upcoming December patch releases, the new registry domain default will be backported to all branches still in support (1.22, 1.23, 1.24).</li>\n<li>If you run in a restricted environment and apply strict domain/IP address access policies limited to k8s.gcr.io, the <strong>image pulls will not function</strong> after the migration to this new registry. For these users, the recommended method is to mirror the release images to a private registry.</li>\n</ul>\n<p>If you‚Äôd like to know more about why we made this change, or some potential issues you might run into, keep reading.</p>\n<h2 id=\"why-has-kubernetes-changed-to-a-different-image-registry\">Why has Kubernetes changed to a different image registry?</h2>\n<p>k8s.gcr.io is hosted on a custom <a href=\"https://cloud.google.com/container-registry\">Google Container Registry</a> (GCR) domain that was setup solely for the Kubernetes project. This has worked well since the inception of the project, and we thank Google for providing these resources, but today there are other cloud providers and vendors that would like to host images to provide a better experience for the people on their platforms. In addition to Google‚Äôs <a href=\"https://www.cncf.io/google-cloud-recommits-3m-to-kubernetes/\">renewed commitment to donate $3 million</a> to support the project's infrastructure, Amazon announced a matching donation during their Kubecon NA 2022 keynote in Detroit. This will provide a better experience for users (closer servers = faster downloads) and will reduce the egress bandwidth and costs from GCR at the same time. registry.k8s.io will spread the load between Google and Amazon, with other providers to follow in the future.</p>\n<h2 id=\"why-isn-t-there-a-stable-list-of-domains-ips-why-can-t-i-restrict-image-pulls\">Why isn‚Äôt there a stable list of domains/IPs? Why can‚Äôt I restrict image pulls?</h2>\n<p>registry.k8s.io is a <a href=\"https://github.com/kubernetes/registry.k8s.io/blob/main/cmd/archeio/docs/request-handling.md\">secure blob redirector</a> that connects clients to the closest cloud provider. The nature of this change means that a client pulling an image could be redirected to any one of a large number of backends. We expect the set of backends to keep changing and will only increase as more and more cloud providers and vendors come on board to help mirror the release images.</p>\n<p>Restrictive control mechanisms like man-in-the-middle proxies or network policies that restrict access to a specific list of IPs/domains will break with this change. For these scenarios, we encourage you to mirror the release images to a local registry that you have strict control over.</p>\n<p>For more information on this policy, please see the <a href=\"https://github.com/kubernetes/registry.k8s.io#stability\">stability section of the registry.k8s.io documentation</a>.</p>\n<h2 id=\"what-kind-of-errors-will-i-see-how-will-i-know-if-i-m-still-using-the-old-address\">What kind of errors will I see? How will I know if I‚Äôm still using the old address?</h2>\n<p>Errors may depend on what kind of container runtime you are using, and what endpoint you are routed to, but it should present as a container failing to be created with the warning <code>FailedCreatePodSandBox</code>.</p>\n<p>Below is an example error message showing a proxied deployment failing to pull due to an unknown certificate:</p>\n<pre tabindex=\"0\"><code>FailedCreatePodSandBox: Failed to create pod sandbox: rpc error: code = Unknown desc = Error response from daemon: Head ‚Äúhttps://us-west1-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.8‚Äù: x509: certificate signed by unknown authority\n</code></pre><h2 id=\"i-m-impacted-by-this-change-how-do-i-revert-to-the-old-registry-address\">I‚Äôm impacted by this change, how do I revert to the old registry address?</h2>\n<p>If using the new registry domain name is not an option, you can revert to the old domain name for cluster versions less than 1.25. Keep in mind that, eventually, you will have to switch to the new registry, as new image tags will no longer be pushed to GCR.</p>\n<h3 id=\"reverting-the-registry-name-in-kubeadm\">Reverting the registry name in kubeadm</h3>\n<p>The registry used by kubeadm to pull its images can be controlled by two methods:</p>\n<p>Setting the <code>--image-repository</code> flag.</p>\n<pre tabindex=\"0\"><code>kubeadm init --image-repository=k8s.gcr.io\n</code></pre><p>Or in <a href=\"https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/\">kubeadm config</a> <code>ClusterConfiguration</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta3<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ClusterConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">imageRepository</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;k8s.gcr.io&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"reverting-the-registry-name-in-kubelet\">Reverting the Registry Name in kubelet</h3>\n<p>The image used by kubelet for the pod sandbox (<code>pause</code>) can be overridden by setting the <code>--pod-infra-container-image</code> flag. For example:</p>\n<pre tabindex=\"0\"><code>kubelet --pod-infra-container-image=k8s.gcr.io/pause:3.5\n</code></pre><h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p><strong>Change is hard</strong>, and evolving our image-serving platform is needed to ensure a sustainable future for the project. We strive to make things better for everyone using Kubernetes. Many contributors from all corners of our community have been working long and hard to ensure we are making the best decisions possible, executing plans, and doing our best to communicate those plans.</p>\n<p>Thanks to Aaron Crickenberger, Arnaud Meukam, Benjamin Elder, Caleb Woodbine, Davanum Srinivas, Mahamed Ali, and Tim Hockin from SIG K8s Infra, Brian McQueen, and Sergey Kanzhelev from SIG Node, Lubomir Ivanov from SIG Cluster Lifecycle, Adolfo Garc√≠a Veytia, Jeremy Rickard, Sascha Grunert, and Stephen Augustus from SIG Release, Bob Killen and Kaslin Fields from SIG Contribex, Tim Allclair from the Security Response Committee. Also a big thank you to our friends acting as liaisons with our cloud provider partners: Jay Pipes from Amazon and Jon Johnson Jr. from Google.</p>","PublishedAt":"2022-11-28 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/","SourceName":"Kubernetes"}},{"node":{"ID":2295,"Title":"Blog: Kubernetes Removals, Deprecations, and Major Changes in 1.26","Description":"<p><strong>Author</strong>: Frederico Mu√±oz (SAS)</p>\n<p>Change is an integral part of the Kubernetes life-cycle: as Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements for the health of the project. For Kubernetes v1.26 there are several planned: this article identifies and describes some of them, based on the information available at this mid-cycle point in the v1.26 release process, which is still ongoing and can introduce additional changes.</p>\n<h2 id=\"k8s-api-deprecation-process\">The Kubernetes API Removal and Deprecation process</h2>\n<p>The Kubernetes project has a <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">well-documented deprecation policy</a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</p>\n<ul>\n<li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</li>\n<li>Beta or pre-release API versions must be supported for 3 releases after deprecation.</li>\n<li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</li>\n</ul>\n<p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.</p>\n<h2 id=\"cri-api-removal\">A note about the removal of the CRI <code>v1alpha2</code> API and containerd 1.5 support</h2>\n<p>Following the adoption of the <a href=\"https://kubernetes.io/docs/concepts/architecture/cri/\">Container Runtime Interface</a> (CRI) and the [removal of dockershim] in v1.24 , the CRI is the supported and documented way through which Kubernetes interacts withdifferent container runtimes. Each kubelet negotiates which version of CRI to use with the container runtime on that node.</p>\n<p>The Kubernetes project recommends using CRI version <code>v1</code>; in Kubernetes v1.25 the kubelet can also negotiate the use of CRI <code>v1alpha2</code> (which was deprecated along at the same time as adding support for the stable <code>v1</code> interface).</p>\n<p>Kubernetes v1.26 will not support CRI <code>v1alpha2</code>. That <a href=\"https://github.com/kubernetes/kubernetes/pull/110618\">removal</a> will result in the kubelet not registering the node if the container runtime doesn't support CRI <code>v1</code>. This means that containerd minor version 1.5 and older will not be supported in Kubernetes 1.26; if you use containerd, you will need to upgrade to containerd version 1.6.0 or later <strong>before</strong> you upgrade that node to Kubernetes v1.26. Other container runtimes that only support the <code>v1alpha2</code> are equally affected: if that affects you, you should contact the container runtime vendor for advice or check their website for additional instructions in how to move forward.</p>\n<p>If you want to benefit from v1.26 features and still use an older container runtime, you can run an older kubelet. The <a href=\"https://kubernetes.io/releases/version-skew-policy/#kubelet\">supported skew</a> for the kubelet allows you to run a v1.25 kubelet, which still is still compatible with <code>v1alpha2</code> CRI support, even if you upgrade the control plane to the 1.26 minor release of Kubernetes.</p>\n<p>As well as container runtimes themselves, that there are tools like <a href=\"https://github.com/containerd/stargz-snapshotter\">stargz-snapshotter</a> that act as a proxy between kubelet and container runtime and those also might be affected.</p>\n<h2 id=\"deprecations-removals\">Deprecations and removals in Kubernetes v1.26</h2>\n<p>In addition to the above, Kubernetes v1.26 is targeted to include several additional removals and deprecations.</p>\n<h3 id=\"removal-of-the-v1beta1-flow-control-api-group\">Removal of the <code>v1beta1</code> flow control API group</h3>\n<p>The <code>flowcontrol.apiserver.k8s.io/v1beta1</code> API version of FlowSchema and PriorityLevelConfiguration <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v126\">will no longer be served in v1.26</a>. Users should migrate manifests and API clients to use the <code>flowcontrol.apiserver.k8s.io/v1beta2</code> API version, available since v1.23.</p>\n<h3 id=\"removal-of-the-v2beta2-horizontalpodautoscaler-api\">Removal of the <code>v2beta2</code> HorizontalPodAutoscaler API</h3>\n<p>The <code>autoscaling/v2beta2</code> API version of HorizontalPodAutoscaler <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#horizontalpodautoscaler-v126\">will no longer be served in v1.26</a>. Users should migrate manifests and API clients to use the <code>autoscaling/v2</code> API version, available since v1.23.</p>\n<h3 id=\"removal-of-in-tree-credential-management-code\">Removal of in-tree credential management code</h3>\n<p>In this upcoming release, legacy vendor-specific authentication code that is part of Kubernetes\nwill be <a href=\"https://github.com/kubernetes/kubernetes/pull/112341\">removed</a> from both\n<code>client-go</code> and <code>kubectl</code>.\nThe existing mechanism supports authentication for two specific cloud providers:\nAzure and Google Cloud.\nIn its place, Kubernetes already offers a vendor-neutral\n<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins\">authentication plugin mechanism</a> -\nyou can switch over right now, before the v1.26 release happens.\nIf you're affected, you can find additional guidance on how to proceed for\n<a href=\"https://github.com/Azure/kubelogin#readme\">Azure</a> and for\n<a href=\"https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke\">Google Cloud</a>.</p>\n<h3 id=\"removal-of-kube-proxy-userspace-modes\">Removal of <code>kube-proxy</code> userspace modes</h3>\n<p>The <code>userspace</code> proxy mode, deprecated for over a year, is <a href=\"https://github.com/kubernetes/kubernetes/pull/112133\">no longer supported on either Linux or Windows</a> and will be removed in this release. Users should use <code>iptables</code> or <code>ipvs</code> on Linux, or <code>kernelspace</code> on Windows: using <code>--mode userspace</code> will now fail.</p>\n<h3 id=\"removal-of-in-tree-openstack-cloud-provider\">Removal of in-tree OpenStack cloud provider</h3>\n<p>Kubernetes is switching from in-tree code for storage integrations, in favor of the Container Storage Interface (CSI).\nAs part of this, Kubernetes v1.26 will remove the the deprecated in-tree storage integration for OpenStack\n(the <code>cinder</code> volume type). You should migrate to external cloud provider and CSI driver from\n<a href=\"https://github.com/kubernetes/cloud-provider-openstack\">https://github.com/kubernetes/cloud-provider-openstack</a> instead.\nFor more information, visit <a href=\"https://github.com/kubernetes/enhancements/issues/1489\">Cinder in-tree to CSI driver migration</a>.</p>\n<h3 id=\"removal-of-the-glusterfs-in-tree-driver\">Removal of the GlusterFS in-tree driver</h3>\n<p>The in-tree GlusterFS driver was <a href=\"https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/#deprecations-and-removals\">deprecated in v1.25</a>, and will be removed from Kubernetes v1.26.</p>\n<h3 id=\"deprecation-of-non-inclusive-kubectl-flag\">Deprecation of non-inclusive <code>kubectl</code> flag</h3>\n<p>As part of the implementation effort of the <a href=\"https://www.cncf.io/announcements/2021/10/13/inclusive-naming-initiative-announces-new-community-resources-for-a-more-inclusive-future/\">Inclusive Naming Initiative</a>,\nthe <code>--prune-whitelist</code> flag will be <a href=\"https://github.com/kubernetes/kubernetes/pull/113116\">deprecated</a>, and replaced with <code>--prune-allowlist</code>.\nUsers that use this flag are strongly advised to make the necessary changes prior to the final removal of the flag, in a future release.</p>\n<h3 id=\"removal-of-dynamic-kubelet-configuration\">Removal of dynamic kubelet configuration</h3>\n<p><em>Dynamic kubelet configuration</em> allowed <a href=\"https://github.com/kubernetes/enhancements/tree/2cd758cc6ab617a93f578b40e97728261ab886ed/keps/sig-node/281-dynamic-kubelet-configuration\">new kubelet configurations to be rolled out via the Kubernetes API</a>, even in a live cluster.\nA cluster operator could reconfigure the kubelet on a Node by specifying a ConfigMap\nthat contained the configuration data that the kubelet should use.\nDynamic kubelet configuration was removed from the kubelet in v1.24, and will be\n<a href=\"https://github.com/kubernetes/kubernetes/pull/112643\">removed from the API server</a> in the v1.26 release.</p>\n<h3 id=\"deprecations-for-kube-apiserver-command-line-arguments\">Deprecations for <code>kube-apiserver</code> command line arguments</h3>\n<p>The <code>--master-service-namespace</code> command line argument to the kube-apiserver doesn't have\nany effect, and was already informally <a href=\"https://github.com/kubernetes/kubernetes/pull/38186\">deprecated</a>.\nThat command line argument wil be formally marked as deprecated in v1.26, preparing for its\nremoval in a future release.\nThe Kubernetes project does not expect any impact from this deprecation and removal.</p>\n<h3 id=\"deprecations-for-kubectl-run-command-line-arguments\">Deprecations for <code>kubectl run</code> command line arguments</h3>\n<p>Several unused option arguments for the <code>kubectl run</code> subcommand will be <a href=\"https://github.com/kubernetes/kubernetes/pull/112261\">marked as deprecated</a>, including:</p>\n<ul>\n<li><code>--cascade</code></li>\n<li><code>--filename</code></li>\n<li><code>--force</code></li>\n<li><code>--grace-period</code></li>\n<li><code>--kustomize</code></li>\n<li><code>--recursive</code></li>\n<li><code>--timeout</code></li>\n<li><code>--wait</code></li>\n</ul>\n<p>These arguments are already ignored so no impact is expected: the explicit deprecation sets a warning message and prepares the removal of the argumentsin a future release.</p>\n<h3 id=\"removal-of-legacy-command-line-arguments-relating-to-logging\">Removal of legacy command line arguments relating to logging</h3>\n<p>Kubernetes v1.26 will <a href=\"https://github.com/kubernetes/kubernetes/pull/112120\">remove</a> some\ncommand line arguments relating to logging. These command line arguments were\nalready deprecated.\nFor more information, see [Deprecate klog specific flags in Kubernetes Components] (<a href=\"https://github.com/kubernetes/enhancements/tree/3cb66bd0a1ef973ebcc974f935f0ac5cba9db4b2/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)\">https://github.com/kubernetes/enhancements/tree/3cb66bd0a1ef973ebcc974f935f0ac5cba9db4b2/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)</a>.</p>\n<h2 id=\"looking-ahead\">Looking ahead</h2>\n<p>The official list of <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-27\">API removals</a>) planned for Kubernetes 1.27 includes:</p>\n<ul>\n<li>All beta versions of the CSIStorageCapacity API; specifically: <code>storage.k8s.io/v1beta1</code></li>\n</ul>\n<h3 id=\"want-to-know-more\">Want to know more?</h3>\n<p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation\">Kubernetes 1.21</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation\">Kubernetes 1.22</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation\">Kubernetes 1.23</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation\">Kubernetes 1.24</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation\">Kubernetes 1.25</a></li>\n</ul>\n<p>We will formally announce the deprecations that come with <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#deprecation\">Kubernetes 1.26</a> as part of the CHANGELOG for that release.</p>","PublishedAt":"2022-11-18 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/","SourceName":"Kubernetes"}},{"node":{"ID":2135,"Title":"Blog: Live and let live with Kluctl and Server Side Apply","Description":"<p><strong>Author:</strong> Alexander Block</p>\n<p>This blog post was inspired by a previous Kubernetes blog post about\n<a href=\"https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/\">Advanced Server Side Apply</a>.\nThe author of said blog post listed multiple benefits for applications and\ncontrollers when switching to server-side apply (from now on abbreviated with\nSSA). Especially the chapter about\n<a href=\"https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/#ci-cd-systems\">CI/CD systems</a>\nmotivated me to respond and write down my thoughts and experiences.</p>\n<p>These thoughts and experiences are the results of me working on <a href=\"https://kluctl.io\">Kluctl</a>\nfor the past 2 years. I describe Kluctl as &quot;The missing glue to put together\nlarge Kubernetes deployments, composed of multiple smaller parts\n(Helm/Kustomize/...) in a manageable and unified way.&quot;</p>\n<p>To get a basic understanding of Kluctl, I suggest to visit the <a href=\"https://kluctlio\">kluctl.io</a>\nwebsite and read through the documentation and tutorials, for example the\n<a href=\"https://kluctl.io/docs/guides/tutorials/microservices-demo/\">microservices demo tutorial</a>.\nAs an alternative, you can watch <a href=\"https://www.youtube.com/watch?v=9LoYLjDjOdg\">Hands-on Introduction to kluctl</a>\nfrom the Rawkode Academy YouTube channel which shows a hands-on demo session.</p>\n<p>There is also a <a href=\"https://github.com/codablock/podtato-head/tree/kluctl/delivery/kluctl\">Kluctl delivery scenario</a>\navailable in my fork of the <a href=\"https://github.com/codablock/podtato-head\">podtato-head</a> demo project.</p>\n<h2 id=\"live-and-let-live\">Live and let live</h2>\n<p>One of the main philosophies that Kluctl follows is <a href=\"https://kluctl.io/docs/philosophy/#live-and-let-live\">&quot;live and let live&quot;</a>,\nmeaning that it will try its best to work in conjunction with any other tool or\ncontroller running outside or inside your clusters. Kluctl will not overwrite\nany fields that it lost ownership of, unless you explicitly tell it to do so.</p>\n<p>Achieving this would not have been possible (or at least several magnitudes\nharder) without the use of SSA. Server-side apply allows Kluctl\nto detect when ownership for a field got lost, for example when another controller\nor operator updates that field to another value. Kluctl can then decide on a\nfield-by-field basis if force-applying is required before retrying based on these\ndecisions.</p>\n<h2 id=\"the-days-before-ssa\">The days before SSA</h2>\n<p>The first versions of Kluctl were based on shelling out to <code>kubectl</code> and thus\nimplicitly relied on client-side apply. At that time, SSA was\nstill alpha and quite buggy. And to be honest, I didn't even know it was a\nthing at that time.</p>\n<p>The way client-side apply worked had some serious drawbacks. The most obvious one\n(it was guaranteed that you'd stumble on this by yourself if enough time passed)\nis that it relied on an annotation (<code>kubectl.kubernetes.io/last-applied-configuration</code>)\nbeing added to the object, bringing in all the limitations and issues with huge\nannotation values. A good example of such issues are\n<a href=\"https://github.com/prometheus-operator/prometheus-operator/issues/4439\">CRDs being so large</a>,\nthat they don't fit into the annotation's value anymore.</p>\n<p>Another drawback can be seen just by looking at the name (<strong>client</strong>-side apply).\nBeing <strong>client</strong> side means that each client has to provide the apply-logic on\nits own, which at that time was only properly implemented inside <code>kubectl</code>,\nmaking it hard to be replicated inside controllers.</p>\n<p>This added <code>kubectl</code> as a dependency (either as an executable or in the form of\nGo packages) to all controllers that wanted to leverage the apply-logic.</p>\n<p>However, even if one managed to get client-side apply running from inside a\ncontroller, you ended up with a solution that gave no control over how it\nworked internally. As an example, there was no way to individually decide which\nfields to overwrite in case of external changes and which ones to let go.</p>\n<h2 id=\"discovering-ssa-apply\">Discovering SSA apply</h2>\n<p>I was never happy with the solution described above and then somehow stumbled\nacross <a href=\"https://kubernetes.io/docs/reference/using-api/server-side-apply/\">server-side apply</a>,\nwhich was still in beta at that time. Experimenting with it via\n<code>kubectl apply --server-side</code> revealed immediately that the true power of\nSSA can not be easily leveraged by shelling out to <code>kubectl</code>.</p>\n<p>The way SSA is implemented in <code>kubectl</code> does not allow enough\ncontrol over conflict resolution as it can only switch between\n&quot;not force-applying anything and erroring out&quot; and &quot;force-applying everything\nwithout showing any mercy!&quot;.</p>\n<p>The API documentation however made it clear that SSA is able to\ncontrol conflict resolution on field level, simply by choosing which fields\nto include and which fields to omit from the supplied object.</p>\n<h2 id=\"moving-away-from-kubectl\">Moving away from kubectl</h2>\n<p>This meant that Kluctl had to move away from shelling out to <code>kubectl</code> first. Only\nafter that was done, I would have been able to properly implement SSA\nwith its powerful conflict resolution.</p>\n<p>To achieve this, I first implemented access to the target clusters via a\nKubernetes client library. This had the nice side effect of dramatically\nspeeding up Kluctl as well. It also improved the security and usability of\nKluctl by ensuring that a running Kluctl command could not be messed around\nwith by externally modifying the kubeconfig while it was running.</p>\n<h2 id=\"implementing-ssa\">Implementing SSA</h2>\n<p>After switching to a Kubernetes client library, leveraging SSA\nfelt easy. Kluctl now has to send each manifest to the API server as part of a\n<code>PATCH</code> request, which signals\nthat Kluctl wants to perform a SSA operation. The API server then\nresponds with an OK response (HTTP status code 200), or with a Conflict response\n(HTTP status 409).</p>\n<p>In case of a Conflict response, the body of that response includes machine-readable\ndetails about the conflicts. Kluctl can then use these details to figure out\nwhich fields are in conflict and which actors (field managers) have taken\nownership of the conflicted fields.</p>\n<p>Then, for each field, Kluctl will decide if the conflict should be ignored or\nif it should be force-applied. If any field needs to be force-applied, Kluctl\nwill retry the apply operation with the ignored fields omitted and the <code>force</code>\nflag being set on the API call.</p>\n<p>In case a conflict is ignored, Kluctl will issue a warning to the user so that\nthe user can react properly (or ignore it forever...).</p>\n<p>That's basically it. That is all that is required to leverage SSA.\nBig thanks and thumbs-up to the Kubernetes developers who made this possible!</p>\n<h2 id=\"conflict-resolution\">Conflict Resolution</h2>\n<p>Kluctl has a few simple rules to figure out if a conflict should be ignored\nor force-applied.</p>\n<p>It first checks the field's actor (the field manager) against a list of known\nfield manager strings from tools that are frequently used to perform manual modifications. These\nare for example <code>kubectl</code> and <code>k9s</code>. Any modifications performed with these tools\nare considered &quot;temporary&quot; and will be overwritten by Kluctl.</p>\n<p>If you're using Kluctl along with <code>kubectl</code> where you don't want the changes from\n<code>kubectl</code> to be overwritten (for example, using in a script) then you can specify\n<code>--field-manager=&lt;manager-name&gt;</code> on the command line to <code>kubectl</code>, and Kluctl\ndoesn't apply its special heuristic.</p>\n<p>If the field manager is not known by Kluctl, it will check if force-applying is\nrequested for that field. Force-applying can be requested in different ways:</p>\n<ol>\n<li>By passing <code>--force-apply</code> to Kluctl. This will cause ALL fields to be force-applied on conflicts.</li>\n<li>By adding the <a href=\"https://kluctl.io/docs/reference/deployments/annotations/all-resources/#kluctlioforce-apply\"><code>kluctl.io/force-apply=true</code></a> annotation to the object in question. This will cause all fields of that object to be force-applied on conflicts.</li>\n<li>By adding the <a href=\"https://kluctl.io/docs/reference/deployments/annotations/all-resources/#kluctlioforce-apply-field\"><code>kluctl.io/force-apply-field=my.json.path</code></a> annotation to the object in question. This causes only fields matching the JSON path to be force-applied on conflicts.</li>\n</ol>\n<p>Marking a field to be force-applied is required whenever some other actor is\nknown to erroneously claim fields (the ECK operator does this to the nodeSets\nfield for example), you can ensure that Kluctl always overwrites these fields\nto the original or a new value.</p>\n<p>In the future, Kluctl will allow even more control about conflict resolution.\nFor example, the CLI will allow to control force-applying on field level.</p>\n<h2 id=\"devops-vs-controllers\">DevOps vs Controllers</h2>\n<p>So how does SSA in Kluctl lead to &quot;live and let live&quot;?</p>\n<p>It allows the co-existence of classical pipelines (e.g. Github Actions or\nGitlab CI), controllers (e.g. the HPA controller or GitOps style controllers)\nand even admins running deployments from their local machines.</p>\n<p>Wherever you are on your infrastructure automation journey, Kluctl has a place\nfor you. From running deployments using a script on your PC, all the way to\nfully automated CI/CD with the pipelines themselves defined in code, Kluctl\naims to complement the workflow that's right for you.</p>\n<p>And even after fully automating everything, you can intervene with your admin\npermissions if required and run a <code>kubectl</code> command that will modify a field\nand prevent Kluctl from overwriting it. You'd just have to switch to a\nfield-manager (e.g. &quot;admin-override&quot;) that is not overwritten by Kluctl.</p>\n<h2 id=\"a-few-takeaways\">A few takeaways</h2>\n<p>Server-side apply is a great feature and essential for the future of\ncontrollers and tools in Kubernetes. The amount of controllers involved\nwill only get more and proper modes of working together are a must.</p>\n<p>I believe that CI/CD-related controllers and tools should leverage\nSSA to perform proper conflict resolution. I also believe that\nother controllers (e.g. Flux and ArgoCD) would benefit from the same kind\nof conflict resolution control on field-level.</p>\n<p>It might even be a good idea to come together and work on a standardized\nset of annotations to control conflict resolution for CI/CD-related tooling.</p>\n<p>On the other side, non CI/CD-related controllers should ensure that they don't\ncause unnecessary conflicts when modifying objects. As of\n<a href=\"https://kubernetes.io/docs/reference/using-api/server-side-apply/#using-server-side-apply-in-a-controller\">the server-side apply documentation</a>,\nit is strongly recommended for controllers to always perform force-applying. When\nfollowing this recommendation, controllers should really make sure that only\nfields related to the controller are included in the applied object.\nOtherwise, unnecessary conflicts are guaranteed.</p>\n<p>In many cases, controllers are meant to only modify the status subresource\nof the objects they manage. In this case, controllers should only patch the\nstatus subresource and not touch the actual object. If this is followed,\nconflicts become impossible to occur.</p>\n<p>If you are a developer of such a controller and unsure about your controller\nadhering to the above, simply try to retrieve an object managed by your\ncontroller and look at the <code>managedFields</code> (you'll need to pass\n<code>--show-managed-fields -oyaml</code> to <code>kubectl get</code>) to see if some field got\nclaimed unexpectedly.</p>","PublishedAt":"2022-11-04 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/11/04/live-and-let-live-with-kluctl-and-ssa/","SourceName":"Kubernetes"}},{"node":{"ID":2011,"Title":"Blog: Server Side Apply Is Great And You Should Be Using It","Description":"<p><strong>Author:</strong> Daniel Smith (Google)</p>\n<p><a href=\"https://kubernetes.io/docs/reference/using-api/server-side-apply/\">Server-side apply</a> (SSA) has now\nbeen <a href=\"https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/\">GA for a few releases</a>, and I\nhave found myself in a number of conversations, recommending that people / teams\nin various situations use it. So I‚Äôd like to write down some of those reasons.</p>\n<h2 id=\"benefits\">Obvious (and not-so-obvious) benefits of SSA</h2>\n<p>A list of improvements / niceties you get from switching from various things to\nServer-side apply!</p>\n<ul>\n<li>Versus client-side-apply (that is, plain <code>kubectl apply</code>):\n<ul>\n<li>The system gives you conflicts when you accidentally fight with another\nactor over the value of a field!</li>\n<li>When combined with <code>--dry-run</code>, there‚Äôs no chance of accidentally running a\nclient-side dry run instead of a server side dry run.</li>\n</ul>\n</li>\n<li>Versus hand-rolling patches:\n<ul>\n<li>The SSA patch format is extremely natural to write, with no weird syntax.\nIt‚Äôs just a regular object, but you can (and should) omit any field you\ndon‚Äôt care about.</li>\n<li>The old patch format (‚Äústrategic merge patch‚Äù) was ad-hoc and still has some\nbugs; JSON-patch and JSON merge-patch fail to handle some cases that are\ncommon in the Kubernetes API, namely lists with items that should be\nrecursively merged based on a ‚Äúname‚Äù or other identifying field.</li>\n<li>There‚Äôs also now great <a href=\"https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#using-server-side-apply-in-a-controller\">go-language library support</a>\nfor building apply calls programmatically!</li>\n<li>You can use SSA to explicitly delete fields you don‚Äôt ‚Äúown‚Äù by setting them\nto <code>null</code>, which makes it a feature-complete replacement for all of the old\npatch formats.</li>\n</ul>\n</li>\n<li>Versus shelling out to kubectl:\n<ul>\n<li>You can use the <strong>apply</strong> API call from any language without shelling out to\nkubectl!</li>\n<li>As stated above, the <a href=\"https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#server-side-apply-support-in-client-go\">Go library has dedicated mechanisms</a>\nto make this easy now.</li>\n</ul>\n</li>\n<li>Versus GET-modify-PUT:\n<ul>\n<li>(This one is more complicated and you can skip it if you've never written a\ncontroller!)</li>\n<li>To use GET-modify-PUT correctly, you have to handle and retry a write\nfailure in the case that someone else has modified the object in any way\nbetween your GET and PUT. This is an ‚Äúoptimistic concurrency failure‚Äù when\nit happens.</li>\n<li>SSA offloads this task to the server‚Äì you only have to retry if there‚Äôs a\nconflict, and the conflicts you can get are all meaningful, like when you‚Äôre\nactually trying to take a field away from another actor in the system.</li>\n<li>To put it another way, if 10 actors do a GET-modify-PUT cycle at the same\ntime, 9 will get an optimistic concurrency failure and have to retry, then\n8, etc, for up to 50 total GET-PUT attempts in the worst case (that‚Äôs .5N^2\nGET and PUT calls for N actors making simultaneous changes). If the actors\nare using SSA instead, and the changes don‚Äôt actually conflict over specific\nfields, then all the changes can go in in any order. Additionally, SSA\nchanges can often be done without a GET call at all. That‚Äôs only N <strong>apply</strong>\nrequests for N actors, which is a drastic improvement!</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"how-can-i-use-ssa\">How can I use SSA?</h2>\n<h3 id=\"users\">Users</h3>\n<p>Use <code>kubectl apply --server-side</code>! Soon we (SIG API Machinery) hope to make this\nthe default and remove the ‚Äúclient side‚Äù apply completely!</p>\n<h3 id=\"controller-authors\">Controller authors</h3>\n<p>There‚Äôs two main categories here, but for both of them, <strong>you should probably\n<em>force conflicts</em> when using SSA</strong>. This is because your controller probably\ndoesn‚Äôt know what to do when some other entity in the system has a different\ndesire than your controller about a particular field. (See the <a href=\"#ci-cd-systems\">CI/CD\nsection</a>, though!)</p>\n<h4 id=\"get-modify-put-patch-controllers\">Controllers that use either a GET-modify-PUT sequence or a PATCH</h4>\n<p>This kind of controller GETs an object (possibly from a\n<a href=\"https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes\"><strong>watch</strong></a>),\nmodifies it, and then PUTs it back to write its changes. Sometimes it constructs\na custom PATCH, but the semantics are the same. Most existing controllers\n(especially those in-tree) work like this.</p>\n<p>If your controller is perfect, great! You don‚Äôt need to change it. But if you do\nwant to change it, you can take advantage of the new client library‚Äôs <em>extract</em>\nworkflow‚Äì that is, <strong>get</strong> the existing object, extract your existing desires,\nmake modifications, and re-<strong>apply</strong>. For many controllers that were computing\nthe smallest API changes possible, this will be a minor update to the existing\nimplementation.</p>\n<p>This workflow avoids the failure mode of accidentally trying to own every field\nin the object, which is what happens if you just GET the object, make changes,\nand then <strong>apply</strong>. (Note that the server will notice you did this and reject\nyour change!)</p>\n<h4 id=\"reconstructive-controllers\">Reconstructive controllers</h4>\n<p>This kind of controller wasn't really possible prior to SSA. The idea here is to\n(whenever something changes etc) reconstruct from scratch the fields of the\nobject as the controller wishes them to be, and then <strong>apply</strong> the change to the\nserver, letting it figure out the result. I now recommend that new controllers\nstart out this way‚Äìit's less fiddly to say what you want an object to look like\nthan it is to say how you want it to change.</p>\n<p>The client library supports this method of operation by default.</p>\n<p>The only downside is that you may end up sending unneeded <strong>apply</strong> requests to\nthe API server, even if actually the object already matches your controller‚Äôs\ndesires. This doesn't matter if it happens once in a while, but for extremely\nhigh-throughput controllers, it might cause a performance problem for the\ncluster‚Äìspecifically, the API server. No-op writes are not written to storage\n(etcd) or broadcast to any watchers, so it‚Äôs not really that big of a deal. If\nyou‚Äôre worried about this anyway, today you could use the method explained in\nthe previous section, or you could still do it this way for now, and wait for an\nadditional client-side mechanism to suppress zero-change applies.</p>\n<p>To get around this downside, why not GET the object and only send your <strong>apply</strong>\nif the object needs it? Surprisingly, it doesn't help much ‚Äì a no-op <strong>apply</strong> is\nnot very much more work for the API server than an extra GET; and an <strong>apply</strong>\nthat changes things is cheaper than that same <strong>apply</strong> with a preceding GET.\nWorse, since it is a distributed system, something could change between your GET\nand <strong>apply</strong>, invalidating your computation. Instead, you can use this\noptimization on an object retrieved from a cache‚Äìthen it legitimately will\nreduce load on the system (at the cost of a delay when a change is needed and\nthe cache is a bit behind).</p>\n<h4 id=\"ci-cd-systems\">CI/CD systems</h4>\n<p>Continuous integration (CI) and/or continuous deployment (CD) systems are a\nspecial kind of controller which is doing something like reading manifests from\nsource control (such as a Git repo) and automatically pushing them into the\ncluster. Perhaps the CI / CD process first generates manifests from a template,\nthen runs some tests, and then deploys a change. Typically, users are the\nentities pushing changes into source control, although that‚Äôs not necessarily\nalways the case.</p>\n<p>Some systems like this continuously reconcile with the cluster, others may only\noperate when a change is pushed to the source control system. The following\nconsiderations are important for both, but more so for the continuously\nreconciling kind.</p>\n<p>CI/CD systems are literally controllers, but for the purpose of <strong>apply</strong>, they\nare more like users, and unlike other controllers, they need to pay attention to\nconflicts. Reasoning:</p>\n<ul>\n<li>Abstractly, CI/CD systems can change anything, which means they could conflict\nwith <strong>any</strong> controller out there. The recommendation that controllers force\nconflicts is assuming that controllers change a limited number of things and\nyou can be reasonably sure that they won‚Äôt fight with other controllers about\nthose things; that‚Äôs clearly not the case for CI/CD controllers.</li>\n<li>Concrete example: imagine the CI/CD system wants <code>.spec.replicas</code> for some\nDeployment to be 3, because that is the value that is checked into source\ncode; however there is also a HorizontalPodAutoscaler (HPA) that targets the\nsame deployment. The HPA computes a target scale and decides that there should\nbe 10 replicas. Which should win? I just said that most controllers‚Äìincluding\nthe HPA‚Äìshould ignore conflicts. The HPA has no idea if it has been enabled\nincorrectly, and the HPA has no convenient way of informing users of errors.</li>\n<li>The other common cause of a CI/CD system getting a conflict is probably when\nit is trying to overwrite a hot-fix (hand-rolled patch) placed there by a\nsystem admin / SRE / dev-on-call. You almost certainly don‚Äôt want to override\nthat automatically.</li>\n<li>Of course, sometimes SRE makes an accidental change, or a dev makes an\nunauthorized change ‚Äì those you do want to notice and overwrite; however, the\nCI/CD system can‚Äôt tell the difference between these last two cases.</li>\n</ul>\n<p>Hopefully this convinces you that CI/CD systems need error paths‚Äìa way to\nback-propagate these conflict errors to humans; in fact, they should have this\nalready, certainly continuous integration systems need some way to report that\ntests are failing. But maybe I can also say something about how <em>humans</em> can\ndeal with errors:</p>\n<ul>\n<li>\n<p>Reject the hotfix: the (human) administrator of the CI/CD system observes the\nerror, and manually force-applies the manifest in question. Then the CI/CD\nsystem will be able to apply the manifest successfully and become a co-owner.</p>\n<p>Optional: then the administrator applies a blank manifest (just the object\ntype / namespace / name) to relinquish any fields they became a manager for.\nif this step is omitted, there's some chance the administrator will end up\nowning fields and causing an unwanted future conflict.</p>\n<p><strong>Note</strong>: why an administrator? I'm assuming that developers which ordinarily\npush to the CI/CD system and / or its source control system may not have\npermissions to push directly to the cluster.</p>\n</li>\n<li>\n<p>Accept the hotfix: the author of the change in question sees the conflict, and\nedits their change to accept the value running in production.</p>\n</li>\n<li>\n<p>Accept then reject: as in the accept option, but after that manifest is\napplied, and the CI/CD queue owns everything again (so no conflicts), re-apply\nthe original manifest.</p>\n</li>\n<li>\n<p>I can also imagine the CI/CD system permitting you to mark a manifest as\n‚Äúforce conflicts‚Äù somehow‚Äì if there‚Äôs demand for this we could consider making\na more standardized way to do this. A rigorous version of this which lets you\ndeclare exactly which conflicts you intend to force would require support from\nthe API server; in lieu of that, you can make a second manifest with only that\nsubset of fields.</p>\n</li>\n<li>\n<p>Future work: we could imagine an especially advanced CI/CD system that could\nparse <code>metadata.managedFields</code> data to see who or what they are conflicting\nwith, over what fields, and decide whether or not to ignore the conflict. In\nfact, this information is also presented in any conflict errors, though\nperhaps not in an easily machine-parseable format. We (SIG API Machinery)\nmostly didn't expect that people would want to take this approach ‚Äî so we\nwould love to know if in fact people want/need the features implied by this\napproach, such as the ability, when <strong>apply</strong>ing to request to override\ncertain conflicts but not others.</p>\n<p>If this sounds like an approach you'd want to take for your own controller,\ncome talk to SIG API Machinery!</p>\n</li>\n</ul>\n<p>Happy <strong>apply</strong>ing!</p>","PublishedAt":"2022-10-20 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/","SourceName":"Kubernetes"}},{"node":{"ID":1874,"Title":"Blog: Current State: 2019 Third Party Security Audit of Kubernetes","Description":"<p><strong>Authors</strong> (in alphabetical order): Cailyn Edwards (Shopify), Pushkar Joglekar (VMware), Rey Lejano (SUSE) and Rory McCune (DataDog)</p>\n<p>We expect the brand new Third Party Security Audit of Kubernetes will be\npublished later this month (Oct 2022).</p>\n<p>In preparation for that, let's look at the state of findings that were made\npublic as part of the last <a href=\"https://github.com/kubernetes/sig-security/tree/main/sig-security-external-audit/security-audit-2019\">third party security audit of\n2019</a>\nthat was based on <a href=\"https://github.com/kubernetes/kubernetes/tree/release-1.13\">Kubernetes v1.13.4</a>.</p>\n<h2 id=\"motivation\">Motivation</h2>\n<p><a href=\"https://github.com/cji\">Craig Ingram</a> has graciously attempted over the years to keep track of the\nstatus of the findings reported in the last audit in this issue:\n<a href=\"https://github.com/kubernetes/kubernetes/issues/81146\">kubernetes/kubernetes#81146</a>.\nThis blog post will attempt to dive deeper into this, address any gaps\nin tracking and become a point in time summary of the state of the\nfindings reported from 2019.</p>\n<p>This article should also help readers gain confidence through transparent\ncommunication, of work done by the community to address these findings and\nbubble up any findings that need help from community contributors.</p>\n<h2 id=\"current-state\">Current State</h2>\n<p>The status of each issue / finding here is represented in a best effort manner.\nAuthors do not claim to be 100% accurate on the status and welcome any\ncorrections or feedback if the current state is not reflected accurately by\ncommenting directly on the relevant issue.</p>\n<table>\n<thead>\n<tr>\n<th><strong>#</strong></th>\n<th><strong>Title</strong></th>\n<th><strong>Issue</strong></th>\n<th><strong>Status</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>hostPath PersistentVolumes enable PodSecurityPolicy bypass</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81110\">#81110</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/website/pull/15756\">kubernetes/website#15756</a> and <a href=\"https://github.com/kubernetes/kubernetes/pull/109798\">kubernetes/kubernetes#109798</a></td>\n</tr>\n<tr>\n<td>2</td>\n<td>Kubernetes does not facilitate certificate revocation</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81111\">#81111</a></td>\n<td>duplicate of <a href=\"https://github.com/kubernetes/kubernetes/issues/18982\">#18982</a> and <strong>needs a KEP</strong></td>\n</tr>\n<tr>\n<td>3</td>\n<td>HTTPS connections are not authenticated</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81112\">#81112</a></td>\n<td>Largely left as an end user exercise in setting up the right configuration</td>\n</tr>\n<tr>\n<td>4</td>\n<td><abbr title=\"Time-of-check to time-of-use bug\">TOCTOU</abbr> when moving PID to manager's cgroup via kubelet</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81113\">#81113</a></td>\n<td>Requires Node access for successful exploitation. Fix needed</td>\n</tr>\n<tr>\n<td>5</td>\n<td>Improperly patched directory traversal in <code>kubectl cp</code></td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/pull/76788\">#76788</a></td>\n<td>closed, assigned <a href=\"https://github.com/advisories/GHSA-v8c4-hw4j-x4pr\">CVE-2019-11249</a>, fixed in <a href=\"https://github.com/kubernetes/kubernetes/pull/80436\">#80436</a></td>\n</tr>\n<tr>\n<td>6</td>\n<td>Bearer tokens are revealed in logs</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81114\">#81114</a></td>\n<td>closed, assigned <a href=\"https://github.com/advisories/GHSA-jmrx-5g74-6v2f\">CVE-2019-11250</a>, fixed in <a href=\"https://github.com/kubernetes/kubernetes/pull/81330\">#81330</a></td>\n</tr>\n<tr>\n<td>7</td>\n<td>Seccomp is disabled by default</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81115\">#81115</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/kubernetes/pull/101943\">#101943</a></td>\n</tr>\n<tr>\n<td>8</td>\n<td>Pervasive world-accessible file permissions</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81116\">#81116</a></td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/pull/112384\">#112384</a> ( in progress)</td>\n</tr>\n<tr>\n<td>9</td>\n<td>Environment variables expose sensitive data</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81117\">#81117</a></td>\n<td>closed, addressed by <a href=\"https://github.com/kubernetes/kubernetes/pull/84992\">#84992</a> and <a href=\"https://github.com/kubernetes/kubernetes/pull/84677\">#84677</a></td>\n</tr>\n<tr>\n<td>10</td>\n<td>Use of InsecureIgnoreHostKey in SSH connections</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81118\">#81118</a></td>\n<td>This feature was removed in v1.22: <a href=\"https://github.com/kubernetes/kubernetes/pull/102297\">#102297</a></td>\n</tr>\n<tr>\n<td>11</td>\n<td>Use of InsecureSkipVerify and other TLS weaknesses</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81119\">#81119</a></td>\n<td><strong>Needs a KEP</strong></td>\n</tr>\n<tr>\n<td>12</td>\n<td><code>kubeadm</code> performs potentially-dangerous reset operations</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81120\">#81120</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/81495\">#81495</a>, <a href=\"https://github.com/kubernetes/kubernetes/pull/81494\">#81494</a>, and <a href=\"https://github.com/kubernetes/website/pull/15881\">kubernetes/website#15881</a></td>\n</tr>\n<tr>\n<td>13</td>\n<td>Overflows when using strconv.Atoi and downcasting the result</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81121\">#81121</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/89120\">#89120</a></td>\n</tr>\n<tr>\n<td>14</td>\n<td>kubelet can cause an Out of Memory error with a malicious manifest</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81122\">#81122</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/76518\">#76518</a></td>\n</tr>\n<tr>\n<td>15</td>\n<td><code>kubectl</code> can cause an Out Of Memory error with a malicious Pod specification</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81123\">#81123</a></td>\n<td>Fix needed</td>\n</tr>\n<tr>\n<td>16</td>\n<td>Improper fetching of PIDs allows incorrect cgroup movement</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81124\">#81124</a></td>\n<td>Fix needed</td>\n</tr>\n<tr>\n<td>17</td>\n<td>Directory traversal of host logs running kube-apiserver and kubelet</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81125\">#81125</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/87273\">#87273</a></td>\n</tr>\n<tr>\n<td>18</td>\n<td>Non-constant time password comparison</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81126\">#81126</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/81152\">#81152</a></td>\n</tr>\n<tr>\n<td>19</td>\n<td>Encryption recommendations not in accordance with best practices</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81127\">#81127</a></td>\n<td>Work in Progress</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Adding credentials to containers by default is unsafe</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81128\">#81128</a></td>\n<td>Closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/89193\">#89193</a></td>\n</tr>\n<tr>\n<td>21</td>\n<td>kubelet liveness probes can be used to enumerate host network</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81129\">#81129</a></td>\n<td><strong>Needs a KEP</strong></td>\n</tr>\n<tr>\n<td>22</td>\n<td>iSCSI volume storage cleartext secrets in logs</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81130\">#81130</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/81215\">#81215</a></td>\n</tr>\n<tr>\n<td>23</td>\n<td>Hard coded credential paths</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81131\">#81131</a></td>\n<td>closed, awaiting more evidence</td>\n</tr>\n<tr>\n<td>24</td>\n<td>Log rotation is not atomic</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81132\">#81132</a></td>\n<td>Fix needed</td>\n</tr>\n<tr>\n<td>25</td>\n<td>Arbitrary file paths without bounding</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81133\">#81133</a></td>\n<td>Fix needed.</td>\n</tr>\n<tr>\n<td>26</td>\n<td>Unsafe JSON construction</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81134\">#81134</a></td>\n<td>Partially fixed</td>\n</tr>\n<tr>\n<td>27</td>\n<td>kubelet crash due to improperly handled errors</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81135\">#81135</a></td>\n<td>Closed. Fixed by <a href=\"https://github.com/kubernetes/kubernetes/issues/81135\">#81135</a></td>\n</tr>\n<tr>\n<td>28</td>\n<td>Legacy tokens do not expire</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81136\">#81136</a></td>\n<td>closed, fixed as part of <a href=\"https://github.com/kubernetes/kubernetes/issues/70679\">#70679</a></td>\n</tr>\n<tr>\n<td>29</td>\n<td>CoreDNS leaks internal cluster information across namespaces</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81137\">#81137</a></td>\n<td>Closed, resolved with CoreDNS v1.6.2. <a href=\"https://github.com/kubernetes/kubernetes/issues/81137\">#81137</a> (comment)</td>\n</tr>\n<tr>\n<td>30</td>\n<td>Services use questionable default functions</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81138\">#81138</a></td>\n<td>Fix needed</td>\n</tr>\n<tr>\n<td>31</td>\n<td>Incorrect docker daemon process name in container manager</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81139\">#81139</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/81083\">#81083</a></td>\n</tr>\n<tr>\n<td>32</td>\n<td>Use standard formats everywhere</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81140\">#81140</a></td>\n<td><strong>Needs a KEP</strong></td>\n</tr>\n<tr>\n<td>33</td>\n<td>Superficial health check provides false sense of safety</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81141\">#81141</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/81319\">#81319</a></td>\n</tr>\n<tr>\n<td>34</td>\n<td>Hardcoded use of insecure gRPC transport</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81142\">#81142</a></td>\n<td><strong>Needs a KEP</strong></td>\n</tr>\n<tr>\n<td>35</td>\n<td>Incorrect handling of <code>Retry-After</code></td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81143\">#81143</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/91048\">#91048</a></td>\n</tr>\n<tr>\n<td>36</td>\n<td>Incorrect isKernelPid check</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81144\">#81144</a></td>\n<td>closed, fixed by <a href=\"https://github.com/kubernetes/kubernetes/pull/81086\">#81086</a></td>\n</tr>\n<tr>\n<td>37</td>\n<td>Kubelet supports insecure TLS ciphersuites</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81145\">#81145</a></td>\n<td>closed but fix needed for <a href=\"https://github.com/kubernetes/kubernetes/issues/91444\">#91444</a> (see <a href=\"https://github.com/kubernetes/kubernetes/issues/81145#issuecomment-630291221\">this comment</a>)</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"inspired-outcomes\">Inspired outcomes</h3>\n<p>Apart from fixes to the specific issues, the 2019 third party security audit\nalso motivated security focussed enhancements in the next few releases of\nKubernetes. One such example is\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-security/1933-secret-logging-static-analysis\">Kubernetes Enhancement Proposal (KEP) 1933 Defend Against Logging Secrets via Static Analysis</a> to prevent exposing\nsecrets to logs with <a href=\"@PurelyApplied\">Patrick Rhomberg</a> driving the\nimplementation. As a result of this KEP,\n<a href=\"https://github.com/google/go-flow-levee\"><code>go-flow-levee</code></a>, a taint propagation\nanalysis tool configured to detect logging of secrets, is executed in a\n<a href=\"https://github.com/kubernetes/kubernetes/blob/master/hack/verify-govet-levee.sh\">script</a>\nas a Prow presubmit job. This KEP was introduced in v1.20.0 as an alpha\nfeature, then graduated to beta in v1.21.0, and graduated to stable in\nv1.23.0. As stable, the analysis runs as a blocking presubmit test. This\nKEP also helped resolve the following issues from the 2019 third party security audit:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/81114\">#81114 Bearer tokens are revealed in logs</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/81117\">#81117 Environment variables expose sensitive data</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/81130\">#81130 iSCSI volume storage cleartext secrets in logs</a></li>\n</ul>\n<h2 id=\"remaining-work\">Remaining Work</h2>\n<p>Many of the 37 findings identified were fixed by work from\nour community members over the last 3 years. However, we still have some work\nleft to do. Here's a breakdown of remaining work with rough estimates on\ntime commitment, complexity and benefits to the ecosystem on fixing\nthese pending issues.</p>\n<div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> Anything requiring a KEP (Kubernetes Enhancement Proposal) is considered\n<em>high</em> time commitment and <em>high</em> complexity. Benefits to Ecosystem are\nroughly equivalent to risk of keeping the finding unfixed which is\ndetermined by Severity Level + Likelihood of a successful vulnerability\nexploit. These estimates and values in the table below are the authors'\npersonal opinion. An individual or end users' threat model may rate the\nbenefits to fix a particular issue higher or lower.\n</div>\n<table>\n<thead>\n<tr>\n<th>Title</th>\n<th>Issue</th>\n<th>Time Commitment</th>\n<th>Complexity</th>\n<th>Benefit to Ecosystem</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Kubernetes does not facilitate certificate revocation</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81111\">#81111</a></td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Use of InsecureSkipVerify and other TLS weaknesses</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81119\">#81119</a></td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><code>kubectl</code> can cause a local Out Of Memory error with a malicious Pod specification</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81123\">#81123</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Improper fetching of PIDs allows incorrect cgroup movement</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81124\">#81124</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>kubelet liveness probes can be used to enumerate host network</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81129\">#81129</a></td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>API Server supports insecure TLS ciphersuites</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81145\">#81145</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><abbr title=\"Time-of-check to time-of-use bug\">TOCTOU</abbr> when moving PID to manager's cgroup via kubelet</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81113\">#81113</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Log rotation is not atomic</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81132\">#81132</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Arbitrary file paths without bounding</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81133\">#81133</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Services use questionable default functions</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81138\">#81138</a></td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Use standard formats everywhere</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81140\">#81140</a></td>\n<td>High</td>\n<td>High</td>\n<td>Very Low</td>\n</tr>\n<tr>\n<td>Hardcoded use of insecure gRPC transport</td>\n<td><a href=\"https://github.com/kubernetes/kubernetes/issues/81142\">#81142</a></td>\n<td>High</td>\n<td>High</td>\n<td>Very Low</td>\n</tr>\n</tbody>\n</table>\n<p>To get started on fixing any of these findings that need help, please\nconsider getting involved in <a href=\"https://github.com/kubernetes/community/tree/master/sig-security#contact\">Kubernetes SIG\nSecurity</a>\nby joining our bi-weekly meetings or hanging out with us on our Slack\nChannel.</p>","PublishedAt":"2022-10-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/","SourceName":"Kubernetes"}},{"node":{"ID":1852,"Title":"Blog: Introducing Kueue","Description":"<p><strong>Authors:</strong> Abdullah Gharaibeh (Google), Aldo Culquicondor (Google)</p>\n<p>Whether on-premises or in the cloud, clusters face real constraints for resource usage, quota, and cost management reasons. Regardless of the autoscalling capabilities, clusters have finite capacity. As a result, users want an easy way to fairly and\nefficiently share resources.</p>\n<p>In this article, we introduce <a href=\"https://github.com/kubernetes-sigs/kueue/tree/main/docs#readme\">Kueue</a>,\nan open source job queueing controller designed to manage batch jobs as a single unit.\nKueue leaves pod-level orchestration to existing stable components of Kubernetes.\nKueue natively supports the Kubernetes <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Job</a>\nAPI and offers hooks for integrating other custom-built APIs for batch jobs.</p>\n<h2 id=\"why-kueue\">Why Kueue?</h2>\n<p>Job queueing is a key feature to run batch workloads at scale in both on-premises and cloud environments. The main goal\nof job queueing is to manage access to a limited pool of resources shared by multiple tenants. Job queueing decides which\njobs should wait, which can start immediately, and what resources they can use.</p>\n<p>Some of the most desired job queueing requirements include:</p>\n<ul>\n<li>Quota and budgeting to control who can use what and up to what limit. This is not only needed in clusters with static resources like on-premises,\nbut it is also needed in cloud environments to control spend or usage of scarce resources.</li>\n<li>Fair sharing of resources between tenants. To maximize the usage of available resources, any unused quota assigned to inactive tenants should be\nallowed to be shared fairly between active tenants.</li>\n<li>Flexible placement of jobs across different resource types based on availability. This is important in cloud environments which have heterogeneous\nresources such as different architectures (GPU or CPU models) and different provisioning modes (spot vs on-demand).</li>\n<li>Support for autoscaled environments where resources can be provisioned on demand.</li>\n</ul>\n<p>Plain Kubernetes doesn't address the above requirements. In normal circumstances, once a Job is created, the job-controller instantly creates the\npods and kube-scheduler continuously attempts to assign the pods to nodes. At scale, this situation can work the control plane to death. There is\nalso currently no good way to control at the job level which jobs should get which resources first, and no way to express order or fair sharing. The\ncurrent ResourceQuota model is not a good fit for these needs because quotas are enforced on resource creation, and there is no queueing of requests. The\nintent of ResourceQuotas is to provide a builtin reliability mechanism with policies needed by admins to protect clusters from failing over.</p>\n<p>In the Kubernetes ecosystem, there are several solutions for job scheduling. However, we found that these alternatives have one or more of the following problems:</p>\n<ul>\n<li>They replace existing stable components of Kubernetes, like kube-scheduler or the job-controller. This is problematic not only from an operational point of view, but\nalso the duplication in the job APIs causes fragmentation of the ecosystem and reduces portability.</li>\n<li>They don't integrate with autoscaling, or</li>\n<li>They lack support for resource flexibility.</li>\n</ul>\n<h2 id=\"overview\">How Kueue works</h2>\n<p>With Kueue we decided to take a different approach to job queueing on Kubernetes that is anchored around the following aspects:</p>\n<ul>\n<li>Not duplicating existing functionalities already offered by established Kubernetes components for pod scheduling, autoscaling and job\nlifecycle management.</li>\n<li>Adding key features that are missing to existing components. For example, we invested in the Job API to cover more use cases like\n<a href=\"https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs\">IndexedJob</a> and <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-tracking-with-finalizers\">fixed long standing issues related to pod\ntracking</a>. While this path takes longer to\nland features, we believe it is the more sustainable long term solution.</li>\n<li>Ensuring compatibility with cloud environments where compute resources are elastic and heterogeneous.</li>\n</ul>\n<p>For this approach to be feasible, Kueue needs knobs to influence the behavior of those established components so it can effectively manage\nwhen and where to start a job. We added those knobs to the Job API in the form of two features:</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#suspending-a-job\">Suspend field</a>, which allows Kueue to signal to the job-controller\nwhen to start or stop a Job.</li>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#mutable-scheduling-directives\">Mutable scheduling directives</a>, which allows Kueue to\nupdate a Job's <code>.spec.template.spec.nodeSelector</code> before starting the Job. This way, Kueue can control Pod placement while still\ndelegating to kube-scheduler the actual pod-to-node scheduling.</li>\n</ul>\n<p>Note that any custom job API can be managed by Kueue if that API offers the above two capabilities.</p>\n<h3 id=\"resource-model\">Resource model</h3>\n<p>Kueue defines new APIs to address the requirements mentioned at the beginning of this post. The three main APIs are:</p>\n<ul>\n<li>ResourceFlavor: a cluster-scoped API to define resource flavor available for consumption, like a GPU model. At its core, a ResourceFlavor is\na set of labels that mirrors the labels on the nodes that offer those resources.</li>\n<li>ClusterQueue: a cluster-scoped API to define resource pools by setting quotas for one or more ResourceFlavor.</li>\n<li>LocalQueue: a namespaced API for grouping and managing single tenant jobs. In its simplest form, a LocalQueue is a pointer to the ClusterQueue\nthat the tenant (modeled as a namespace) can use to start their jobs.</li>\n</ul>\n<p>For more details, take a look at the <a href=\"https://sigs.k8s.io/kueue/docs/concepts\">API concepts documentation</a>. While the three APIs may look overwhelming,\nmost of Kueue‚Äôs operations are centered around ClusterQueue; the ResourceFlavor and LocalQueue APIs are mainly organizational wrappers.</p>\n<h3 id=\"example-use-case\">Example use case</h3>\n<p>Imagine the following setup for running batch workloads on a Kubernetes cluster on the cloud:</p>\n<ul>\n<li>You have <a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\">cluster-autoscaler</a> installed in the cluster to automatically\nadjust the size of your cluster.</li>\n<li>There are two types of autoscaled node groups that differ on their provisioning policies: spot and on-demand. The nodes of each group are\ndifferentiated by the label <code>instance-type=spot</code> or <code>instance-type=ondemand</code>.\nMoreover, since not all Jobs can tolerate running on spot nodes, the nodes are tainted with <code>spot=true:NoSchedule</code>.</li>\n<li>To strike a balance between cost and resource availability, imagine you want Jobs to use up to 1000 cores of on-demand nodes, then use up to\n2000 cores of spot nodes.</li>\n</ul>\n<p>As an admin for the batch system, you define two ResourceFlavors that represent the two types of nodes:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kueue.x-k8s.io/v1alpha2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ResourceFlavor<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>ondemand<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">instance-type</span>:<span style=\"color:#bbb\"> </span>ondemand <span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kueue.x-k8s.io/v1alpha2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ResourceFlavor<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>spot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">instance-type</span>:<span style=\"color:#bbb\"> </span>spot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">taints</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>- <span style=\"color:#008000;font-weight:bold\">effect</span>:<span style=\"color:#bbb\"> </span>NoSchedule<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">key</span>:<span style=\"color:#bbb\"> </span>spot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;true&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Then you define the quotas by creating a ClusterQueue as follows:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kueue.x-k8s.io/v1alpha2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ClusterQueue<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>research-pool<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespaceSelector</span>:<span style=\"color:#bbb\"> </span>{}<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;cpu&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">flavors</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>ondemand<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">quota</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">min</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>spot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">quota</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">min</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">2000</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Note that the order of flavors in the ClusterQueue resources matters: Kueue will attempt to fit jobs in the available quotas according to\nthe order unless the job has an explicit affinity to specific flavors.</p>\n<p>For each namespace, you define a LocalQueue that points to the ClusterQueue above:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kueue.x-k8s.io/v1alpha2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>LocalQueue<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>training<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>team-ml<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">clusterQueue</span>:<span style=\"color:#bbb\"> </span>research-pool<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Admins create the above setup once. Batch users are able to find the queues they are allowed to\nsubmit to by listing the LocalQueues in their namespace(s). The command is similar to the following: <code>kubectl get -n my-namespace localqueues</code></p>\n<p>To submit work, create a Job and set the <code>kueue.x-k8s.io/queue-name</code> annotation as follows:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>batch/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">generateName</span>:<span style=\"color:#bbb\"> </span>sample-job-<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kueue.x-k8s.io/queue-name</span>:<span style=\"color:#bbb\"> </span>training<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">parallelism</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">3</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">completions</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">3</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">tolerations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">key</span>:<span style=\"color:#bbb\"> </span>spot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operator</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;Exists&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">effect</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;NoSchedule&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-batch-workload<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>registry.example/batch/calculate-pi:3.14<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">args</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;30s&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">restartPolicy</span>:<span style=\"color:#bbb\"> </span>Never<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Kueue intervenes to suspend the Job as soon as it is created. Once the Job is at the head of the ClusterQueue, Kueue evaluates if it can start\nby checking if the resources requested by the job fit the available quota.</p>\n<p>In the above example, the Job tolerates spot resources. If there are previously admitted Jobs consuming all existing on-demand quota but\nnot all of spot‚Äôs, Kueue admits the Job using the spot quota. Kueue does this by issuing a single update to the Job object that:</p>\n<ul>\n<li>Changes the <code>.spec.suspend</code> flag to false</li>\n<li>Adds the term <code>instance-type: spot</code> to the job's <code>.spec.template.spec.nodeSelector</code> so that when the pods are created by the job controller, those pods can only schedule\nonto spot nodes.</li>\n</ul>\n<p>Finally, if there are available empty nodes with matching node selector terms, then kube-scheduler will directly schedule the pods. If not, then\nkube-scheduler will initially mark the pods as unschedulable, which will trigger the cluster-autoscaler to provision new nodes.</p>\n<h2 id=\"future-work-and-getting-involved\">Future work and getting involved</h2>\n<p>The example above offers a glimpse of some of Kueue's features including support for quota, resource flexibility, and integration with cluster\nautoscaler. Kueue also supports fair-sharing, job priorities, and different queueing strategies. Take a look at the\n<a href=\"https://github.com/kubernetes-sigs/kueue/tree/main/docs\">Kueue documentation</a> to learn more about those features and how to use Kueue.</p>\n<p>We have a number of features that we plan to add to Kueue, such as hierarchical quota, budgets, and support for dynamically sized jobs. In\nthe more immediate future, we are focused on adding support for job preemption.</p>\n<p>The latest <a href=\"https://github.com/kubernetes-sigs/kueue/releases\">Kueue release</a> is available on Github;\ntry it out if you run batch workloads on Kubernetes (requires v1.22 or newer).\nWe are in the early stages of this project and we are seeking feedback of all levels, major or minor, so please don‚Äôt hesitate to reach out. We‚Äôre\nalso open to additional contributors, whether it is to fix or report bugs, or help add new features or write documentation. You can get in touch with\nus via our <a href=\"http://sigs.k8s.io/kueue\">repo</a>, <a href=\"https://groups.google.com/a/kubernetes.io/g/wg-batch\">mailing list</a> or on\n<a href=\"https://kubernetes.slack.com/messages/wg-batch\">Slack</a>.</p>\n<p>Last but not least, thanks to all <a href=\"https://github.com/kubernetes-sigs/kueue/graphs/contributors\">our contributors</a> who made this project possible!</p>","PublishedAt":"2022-10-04 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/10/04/introducing-kueue/","SourceName":"Kubernetes"}},{"node":{"ID":1836,"Title":"Blog: Kubernetes 1.25: alpha support for running Pods with user namespaces","Description":"<p><strong>Authors:</strong> Rodrigo Campos (Microsoft), Giuseppe Scrivano (Red Hat)</p>\n<p>Kubernetes v1.25 introduces the support for user namespaces.</p>\n<p>This is a major improvement for running secure workloads in\nKubernetes. Each pod will have access only to a limited subset of the\navailable UIDs and GIDs on the system, thus adding a new security\nlayer to protect from other pods running on the same system.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>A process running on Linux can use up to 4294967296 different UIDs and\nGIDs.</p>\n<p>User namespaces is a Linux feature that allows mapping a set of users\nin the container to different users in the host, thus restricting what\nIDs a process can effectively use.\nFurthermore, the capabilities granted in a new user namespace do not\napply in the host initial namespaces.</p>\n<h2 id=\"why-is-it-important\">Why is it important?</h2>\n<p>There are mainly two reasons why user namespaces are important:</p>\n<ul>\n<li>\n<p>improve security since they restrict the IDs a pod can use, so each\npod can run in its own separate environment with unique IDs.</p>\n</li>\n<li>\n<p>enable running workloads as root in a safer manner.</p>\n</li>\n</ul>\n<p>In a user namespace we can map the root user inside the pod to a\nnon-zero ID outside the container, containers believe in running as\nroot while they are a regular unprivileged ID from the host point of\nview.</p>\n<p>The process can keep capabilities that are usually restricted to\nprivileged pods and do it in a safe way since the capabilities granted\nin a new user namespace do not apply in the host initial namespaces.</p>\n<h2 id=\"how-do-i-enable-user-namespaces\">How do I enable user namespaces?</h2>\n<p>At the moment, user namespaces support is opt-in, so you must enable\nit for a pod setting <code>hostUsers</code> to <code>false</code> under the pod spec stanza:</p>\n<pre tabindex=\"0\"><code>apiVersion: v1\nkind: Pod\nspec:\nhostUsers: false\ncontainers:\n- name: nginx\nimage: docker.io/nginx\n</code></pre><p>The feature is behind a feature gate, so make sure to enable\nthe <code>UserNamespacesStatelessPodsSupport</code> gate before you can use\nthe new feature.</p>\n<p>The runtime must also support user namespaces:</p>\n<ul>\n<li>\n<p>containerd: support is planned for the 1.7 release. See containerd\nissue <a href=\"https://github.com/containerd/containerd/issues/7063\">#7063</a> for more details.</p>\n</li>\n<li>\n<p>CRI-O: v1.25 has support for user namespaces.</p>\n</li>\n</ul>\n<p>Support for this in <code>cri-dockerd</code> is <a href=\"https://github.com/Mirantis/cri-dockerd/issues/74\">not planned</a> yet.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>You can reach SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnode\">Open Community Issues/PRs</a></li>\n</ul>\n<p>You can also contact us directly:</p>\n<ul>\n<li>GitHub / Slack: @rata @giuseppe</li>\n</ul>","PublishedAt":"2022-10-03 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/10/03/userns-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":1812,"Title":"Blog: Enforce CRD Immutability with CEL Transition Rules","Description":"<p><strong>Author:</strong> <a href=\"https://github.com/alexzielenski\">Alexander Zielenski</a> (Google)</p>\n<p>Immutable fields can be found in a few places in the built-in Kubernetes types.\nFor example, you can't change the <code>.metadata.name</code> of an object. Specific objects\nhave fields where changes to existing objects are constrained; for example, the\n<code>.spec.selector</code> of a Deployment.</p>\n<p>Aside from simple immutability, there are other common design patterns such as\nlists which are append-only, or a map with mutable values and immutable keys.</p>\n<p>Until recently the best way to restrict field mutability for CustomResourceDefinitions\nhas been to create a validating\n<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks\">admission webhook</a>:\nthis means a lot of complexity for the common case of making a field immutable.</p>\n<p>Beta since Kubernetes 1.25, CEL Validation Rules allow CRD authors to express\nvalidation constraints on their fields using a rich expression language,\n<a href=\"https://github.com/google/cel-spec\">CEL</a>. This article explores how you can\nuse validation rules to implement a few common immutability patterns directly in\nthe manifest for a CRD.</p>\n<h2 id=\"basics-of-validation-rules\">Basics of validation rules</h2>\n<p>The new support for CEL validation rules in Kubernetes allows CRD authors to add\ncomplicated admission logic for their resources without writing any code!</p>\n<p>For example, A CEL rule to constrain a field <code>maximumSize</code> to be greater than a\n<code>minimumSize</code> for a CRD might look like the following:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span>|<span style=\"color:#b44;font-style:italic\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44;font-style:italic\"> </span><span style=\"color:#bbb\"> </span>self.maximumSize &gt; self.minimumSize<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;Maximum size must be greater than minimum size.&#39;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The rule field contains an expression written in CEL. <code>self</code> is a special keyword\nin CEL which refers to the object whose type contains the rule.</p>\n<p>The message field is an error message which will be sent to Kubernetes clients\nwhenever this particular rule is not satisfied.</p>\n<p>For more details about the capabilities and limitations of Validation Rules using\nCEL, please refer to\n<a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">validation rules</a>.\nThe <a href=\"https://github.com/google/cel-spec\">CEL specification</a> is also a good\nreference for information specifically related to the language.</p>\n<h2 id=\"immutability-patterns-with-cel-validation-rules\">Immutability patterns with CEL validation rules</h2>\n<p>This section implements several common use cases for immutability in Kubernetes\nCustomResourceDefinitions, using validation rules expressed as\n<a href=\"https://book.kubebuilder.io/reference/markers/crd.html\">kubebuilder marker comments</a>.\nResultant OpenAPI generated by the kubebuilder marker comments will also be\nincluded so that if you are writing your CRD manifests by hand you can still\nfollow along.</p>\n<h2 id=\"project-setup\">Project setup</h2>\n<p>To use CEL rules with kubebuilder comments, you first need to set up a Golang\nproject structure with the CRD defined in Go.</p>\n<p>You may skip this step if you are not using kubebuilder or are only interested\nin the resultant OpenAPI extensions.</p>\n<p>Begin with a folder structure of a Go module set up like the following. If\nyou have your own project already set up feel free to adapt this tutorial to your liking:</p>\n<figure>\n<div class=\"mermaid\">\ngraph LR\n. --> generate.go\n. --> pkg --> apis --> stable.example.com --> v1\nv1 --> doc.go\nv1 --> types.go\n. --> tools.go\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<p>This is the typical folder structure used by Kubernetes projects for defining new API resources.</p>\n<p><code>doc.go</code> contains package-level metadata such as the group and the version:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +groupName=stable.example.com\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +versionName=v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span><span style=\"color:#a2f;font-weight:bold\">package</span> v1\n</span></span></code></pre></div><p><code>types.go</code> contains all type definitions in stable.example.com/v1</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">package</span> v1\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">import</span> (\n</span></span><span style=\"display:flex;\"><span> metav1 <span style=\"color:#b44\">&#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;</span>\n</span></span><span style=\"display:flex;\"><span>)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// An empty CRD as an example of defining a type using controller tools\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +kubebuilder:storageversion\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +kubebuilder:subresource:status\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span><span style=\"color:#a2f;font-weight:bold\">type</span> TestCRD <span style=\"color:#a2f;font-weight:bold\">struct</span> {\n</span></span><span style=\"display:flex;\"><span> metav1.TypeMeta <span style=\"color:#b44\">`json:&#34;,inline&#34;`</span>\n</span></span><span style=\"display:flex;\"><span> metav1.ObjectMeta <span style=\"color:#b44\">`json:&#34;metadata,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> Spec TestCRDSpec <span style=\"color:#b44\">`json:&#34;spec,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span> Status TestCRDStatus <span style=\"color:#b44\">`json:&#34;status,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">type</span> TestCRDStatus <span style=\"color:#a2f;font-weight:bold\">struct</span> {}\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">type</span> TestCRDSpec <span style=\"color:#a2f;font-weight:bold\">struct</span> {\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#080;font-style:italic\">// You will fill this in as you go along\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span>}\n</span></span></code></pre></div><p><code>tools.go</code> contains a dependency on <a href=\"https://book.kubebuilder.io/reference/generating-crd.html#generating-crds\">controller-gen</a> which will be used to generate the CRD definition:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">//go:build tools\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">package</span> celimmutabilitytutorial\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// Force direct dependency on code-generator so that it may be executed with go run\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span><span style=\"color:#a2f;font-weight:bold\">import</span> (\n</span></span><span style=\"display:flex;\"><span> _ <span style=\"color:#b44\">&#34;sigs.k8s.io/controller-tools/cmd/controller-gen&#34;</span>\n</span></span><span style=\"display:flex;\"><span>)\n</span></span></code></pre></div><p>Finally, <code>generate.go</code>contains a <code>go:generate</code> directive to make use of\n<code>controller-gen</code>. <code>controller-gen</code> parses our <code>types.go</code> and creates generates\nCRD yaml files into a <code>crd</code> folder:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">package</span> celimmutabilitytutorial\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">//go:generate go run sigs.k8s.io/controller-tools/cmd/controller-gen crd paths=./pkg/apis/... output:dir=./crds\n</span></span></span></code></pre></div><p>You may now want to add dependencies for our definitions and test the code generation:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#a2f\">cd</span> cel-immutability-tutorial\n</span></span><span style=\"display:flex;\"><span>go mod init &lt;your-org&gt;/&lt;your-module-name&gt;\n</span></span><span style=\"display:flex;\"><span>go mod tidy\n</span></span><span style=\"display:flex;\"><span>go generate ./...\n</span></span></code></pre></div><p>After running these commands you now have completed the basic project structure.\nYour folder tree should look like the following:</p>\n<figure>\n<div class=\"mermaid\">\ngraph LR\n. --> crds --> stable.example.com_testcrds.yaml\n. --> generate.go\n. --> go.mod\n. --> go.sum\n. --> pkg --> apis --> stable.example.com --> v1\nv1 --> doc.go\nv1 --> types.go\n. --> tools.go\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<p>The manifest for the example CRD is now available in <code>crds/stable.example.com_testcrds.yaml</code>.</p>\n<h2 id=\"immutablility-after-first-modification\">Immutablility after first modification</h2>\n<p>A common immutability design pattern is to make the field immutable once it has\nbeen first set. This example will throw a validation error if the field after\nchanges after being first initialized.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;!has(oldSelf.value) || has(self.value)&#34;, message=&#34;Value is required once set&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span><span style=\"color:#a2f;font-weight:bold\">type</span> ImmutableSinceFirstWrite <span style=\"color:#a2f;font-weight:bold\">struct</span> {\n</span></span><span style=\"display:flex;\"><span> metav1.TypeMeta <span style=\"color:#b44\">`json:&#34;,inline&#34;`</span>\n</span></span><span style=\"display:flex;\"><span> metav1.ObjectMeta <span style=\"color:#b44\">`json:&#34;metadata,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:Optional\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;self == oldSelf&#34;,message=&#34;Value is immutable&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:MaxLength=512\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> Value <span style=\"color:#0b0;font-weight:bold\">string</span> <span style=\"color:#b44\">`json:&#34;value&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>The <code>+kubebuilder</code> directives in the comments inform controller-gen how to\nannotate the generated OpenAPI. The <code>XValidation</code> rule causes the rule to appear\namong the <code>x-kubernetes-validations</code> OpenAPI extension. Kubernetes then\nrespects the OpenAPI spec to enforce our constraints.</p>\n<p>To enforce a field's immutability after its first write, you need to apply the following constraints:</p>\n<ol>\n<li>Field must be allowed to be initially unset <code>+kubebuilder:validation:Optional</code></li>\n<li>Once set, field must not be allowed to be removed: <code>!has(oldSelf.value) | has(self.value)</code> (type-scoped rule)</li>\n<li>Once set, field must not be allowed to change value <code>self == oldSelf</code> (field-scoped rule)</li>\n</ol>\n<p>Also note the additional directive <code>+kubebuilder:validation:MaxLength</code>. CEL\nrequires that all strings have attached max length so that it may estimate the\ncomputation cost of the rule. Rules that are too expensive will be rejected.\nFor more information on CEL cost budgeting, check out the other tutorial.</p>\n<h3 id=\"example-usage\">Example usage</h3>\n<p>Generating and installing the CRD should succeed:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Ensure the CRD yaml is generated by controller-gen</span>\n</span></span><span style=\"display:flex;\"><span>go generate ./...\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f crds/stable.example.com_immutablesincefirstwrites.yaml\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">customresourcedefinition.apiextensions.k8s.io/immutablesincefirstwrites.stable.example.com created\n</span></span></span></code></pre></div><p>Creating initial empty object with no <code>value</code> is permitted since <code>value</code> is <code>optional</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceFirstWrite\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">immutablesincefirstwrite.stable.example.com/test1 created\n</span></span></span></code></pre></div><p>The initial modification of <code>value</code> succeeds:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceFirstWrite\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value: Hello, world!\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">immutablesincefirstwrite.stable.example.com/test1 configured\n</span></span></span></code></pre></div><p>An attempt to change <code>value</code> is blocked by the field-level validation rule. Note\nthe error message shown to the user comes from the validation rule.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceFirstWrite\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value: Hello, new world!\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The ImmutableSinceFirstWrite &#34;test1&#34; is invalid: value: Invalid value: &#34;string&#34;: Value is immutable\n</span></span></span></code></pre></div><p>An attempt to remove the <code>value</code> field altogether is blocked by the other validation rule\non the type. The error message also comes from the rule.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceFirstWrite\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The ImmutableSinceFirstWrite &#34;test1&#34; is invalid: &lt;nil&gt;: Invalid value: &#34;object&#34;: Value is required once set\n</span></span></span></code></pre></div><h3 id=\"generated-schema\">Generated schema</h3>\n<p>Note that in the generated schema there are two separate rule locations.\nOne is directly attached to the property <code>immutable_since_first_write</code>.\nThe other rule is associated with the crd type itself.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">openAPIV3Schema</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">properties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">maxLength</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">512</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>string<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Value is immutable<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span>self == oldSelf<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Value is required once set<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;!has(oldSelf.value) || has(self.value)&#39;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"immutability-upon-object-creation\">Immutability upon object creation</h2>\n<p>A field which is immutable upon creation time is implemented similarly to the\nearlier example. The difference is that that field is marked required, and the\ntype-scoped rule is no longer necessary.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">type</span> ImmutableSinceCreation <span style=\"color:#a2f;font-weight:bold\">struct</span> {\n</span></span><span style=\"display:flex;\"><span> metav1.TypeMeta <span style=\"color:#b44\">`json:&#34;,inline&#34;`</span>\n</span></span><span style=\"display:flex;\"><span> metav1.ObjectMeta <span style=\"color:#b44\">`json:&#34;metadata,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:Required\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;self == oldSelf&#34;,message=&#34;Value is immutable&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:MaxLength=512\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> Value <span style=\"color:#0b0;font-weight:bold\">string</span> <span style=\"color:#b44\">`json:&#34;value&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>This field will be required when the object is created, and after that point will\nnot be allowed to be modified. Our CEL Validation Rule <code>self == oldSelf</code></p>\n<h3 id=\"usage-example\">Usage example</h3>\n<p>Generating and installing the CRD should succeed:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Ensure the CRD yaml is generated by controller-gen</span>\n</span></span><span style=\"display:flex;\"><span>go generate ./...\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f crds/stable.example.com_immutablesincecreations.yaml\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">customresourcedefinition.apiextensions.k8s.io/immutablesincecreations.stable.example.com created\n</span></span></span></code></pre></div><p>Applying an object without the required field should fail:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceCreation\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The ImmutableSinceCreation &#34;test1&#34; is invalid:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">* value: Required value\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">* &lt;nil&gt;: Invalid value: &#34;null&#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation\n</span></span></span></code></pre></div><p>Now that the field has been added, the operation is permitted:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceCreation\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value: Hello, world!\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">immutablesincecreation.stable.example.com/test1 created\n</span></span></span></code></pre></div><p>If you attempt to change the <code>value</code>, the operation is blocked due to the\nvalidation rules in the CRD. Note that the error message is as it was defined\nin the validation rule.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceCreation\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value: Hello, new world!\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The ImmutableSinceCreation &#34;test1&#34; is invalid: value: Invalid value: &#34;string&#34;: Value is immutable\n</span></span></span></code></pre></div><p>Also if you attempted to remove <code>value</code> altogether after adding it, you will\nsee an error as expected:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: ImmutableSinceCreation\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: test1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The ImmutableSinceCreation &#34;test1&#34; is invalid:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">* value: Required value\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">* &lt;nil&gt;: Invalid value: &#34;null&#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation\n</span></span></span></code></pre></div><h3 id=\"generated-schema-1\">Generated schema</h3>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">openAPIV3Schema</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">properties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">maxLength</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">512</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>string<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Value is immutable<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span>self == oldSelf<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">required</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- value<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"append-only-list-of-containers\">Append-only list of containers</h2>\n<p>In the case of ephemeral containers on Pods, Kubernetes enforces that the\nelements in the list are immutable, and can‚Äôt be removed. The following example\nshows how you could use CEL to achieve the same behavior.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;!has(oldSelf.value) || has(self.value)&#34;, message=&#34;Value is required once set&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span><span style=\"color:#a2f;font-weight:bold\">type</span> AppendOnlyList <span style=\"color:#a2f;font-weight:bold\">struct</span> {\n</span></span><span style=\"display:flex;\"><span> metav1.TypeMeta <span style=\"color:#b44\">`json:&#34;,inline&#34;`</span>\n</span></span><span style=\"display:flex;\"><span> metav1.ObjectMeta <span style=\"color:#b44\">`json:&#34;metadata,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:Optional\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:MaxItems=100\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;oldSelf.all(x, x in self)&#34;,message=&#34;Values may only be added&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> Values []v1.EphemeralContainer <span style=\"color:#b44\">`json:&#34;value&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><ol>\n<li>Once set, field must not be deleted: <code>!has(oldSelf.value) || has(self.value)</code> (type-scoped)</li>\n<li>Once a value is added it is not removed: <code>oldSelf.all(x, x in self)</code> (field-scoped)</li>\n<li>Value may be initially unset: <code>+kubebuilder:validation:Optional</code></li>\n</ol>\n<p>Note that for cost-budgeting purposes, <code>MaxItems</code> is also required to be specified.</p>\n<h3 id=\"example-usage-1\">Example usage</h3>\n<p>Generating and installing the CRD should succeed:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Ensure the CRD yaml is generated by controller-gen</span>\n</span></span><span style=\"display:flex;\"><span>go generate ./...\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f crds/stable.example.com_appendonlylists.yaml\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">customresourcedefinition.apiextensions.k8s.io/appendonlylists.stable.example.com created\n</span></span></span></code></pre></div><p>Creating an inital list with one element inside should succeed without problem:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: AppendOnlyList\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testlist\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: container1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: nginx/nginx\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">appendonlylist.stable.example.com/testlist created\n</span></span></span></code></pre></div><p>Adding an element to the list should also proceed without issue as expected:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: AppendOnlyList\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testlist\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: container1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: nginx/nginx\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: container2\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: mongodb/mongodb\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">appendonlylist.stable.example.com/testlist configured\n</span></span></span></code></pre></div><p>But if you now attempt to remove an element, the error from the validation rule\nis triggered:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: AppendOnlyList\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testlist\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">value:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: container1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: nginx/nginx\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The AppendOnlyList &#34;testlist&#34; is invalid: value: Invalid value: &#34;array&#34;: Values may only be added\n</span></span></span></code></pre></div><p>Additionally, to attempt to remove the field once it has been set is also disallowed\nby the type-scoped validation rule.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: AppendOnlyList\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testlist\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The AppendOnlyList &#34;testlist&#34; is invalid: &lt;nil&gt;: Invalid value: &#34;object&#34;: Value is required once set\n</span></span></span></code></pre></div><h3 id=\"generated-schema-2\">Generated schema</h3>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">openAPIV3Schema</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">properties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">value</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">items</span>:<span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">maxItems</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">100</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>array<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Values may only be added<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span>oldSelf.all(x, x in self)<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Value is required once set<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;!has(oldSelf.value) || has(self.value)&#39;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"map-with-append-only-keys-immutable-values\">Map with append-only keys, immutable values</h2>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// A map which does not allow keys to be removed or their values changed once set. New keys may be added, however.\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;!has(oldSelf.values) || has(self.values)&#34;, message=&#34;Value is required once set&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span><span style=\"color:#a2f;font-weight:bold\">type</span> MapAppendOnlyKeys <span style=\"color:#a2f;font-weight:bold\">struct</span> {\n</span></span><span style=\"display:flex;\"><span> metav1.TypeMeta <span style=\"color:#b44\">`json:&#34;,inline&#34;`</span>\n</span></span><span style=\"display:flex;\"><span> metav1.ObjectMeta <span style=\"color:#b44\">`json:&#34;metadata,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:Optional\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:MaxProperties=10\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> <span style=\"color:#080;font-style:italic\">// +kubebuilder:validation:XValidation:rule=&#34;oldSelf.all(key, key in self &amp;&amp; self[key] == oldSelf[key])&#34;,message=&#34;Keys may not be removed and their values must stay the same&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"></span> Values <span style=\"color:#a2f;font-weight:bold\">map</span>[<span style=\"color:#0b0;font-weight:bold\">string</span>]<span style=\"color:#0b0;font-weight:bold\">string</span> <span style=\"color:#b44\">`json:&#34;values,omitempty&#34;`</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><ol>\n<li>Once set, field must not be deleted: <code>!has(oldSelf.values) || has(self.values)</code> (type-scoped)</li>\n<li>Once a key is added it is not removed nor is its value modified: <code>oldSelf.all(key, key in self &amp;&amp; self[key] == oldSelf[key])</code> (field-scoped)</li>\n<li>Value may be initially unset: <code>+kubebuilder:validation:Optional</code></li>\n</ol>\n<h3 id=\"example-usage-2\">Example usage</h3>\n<p>Generating and installing the CRD should succeed:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># Ensure the CRD yaml is generated by controller-gen</span>\n</span></span><span style=\"display:flex;\"><span>go generate ./...\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f crds/stable.example.com_mapappendonlykeys.yaml\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">customresourcedefinition.apiextensions.k8s.io/mapappendonlykeys.stable.example.com created\n</span></span></span></code></pre></div><p>Creating an initial object with one key within <code>values</code> should be permitted:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: MapAppendOnlyKeys\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testmap\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">values:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> key1: value1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">mapappendonlykeys.stable.example.com/testmap created\n</span></span></span></code></pre></div><p>Adding new keys to the map should also be permitted:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: MapAppendOnlyKeys\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testmap\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">values:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> key1: value1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> key2: value2\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">mapappendonlykeys.stable.example.com/testmap configured\n</span></span></span></code></pre></div><p>But if a key is removed, the error messagr from the validation rule should be\nreturned:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: MapAppendOnlyKeys\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testmap\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">values:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> key1: value1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The MapAppendOnlyKeys &#34;testmap&#34; is invalid: values: Invalid value: &#34;object&#34;: Keys may not be removed and their values must stay the same\n</span></span></span></code></pre></div><p>If the entire field is removed, the other validation rule is triggered and the\noperation is prevented. Note that the error message for the validation rule is\nshown to the user.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f - <span style=\"color:#b44\">&lt;&lt;EOF\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">---\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: stable.example.com/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: MapAppendOnlyKeys\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: testmap\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">The MapAppendOnlyKeys &#34;testmap&#34; is invalid: &lt;nil&gt;: Invalid value: &#34;object&#34;: Value is required once set\n</span></span></span></code></pre></div><h3 id=\"generated-schema-3\">Generated schema</h3>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">openAPIV3Schema</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">description</span>:<span style=\"color:#bbb\"> </span>A map which does not allow keys to be removed or their values<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>changed once set. New keys may be added, however.<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">properties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">values</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">additionalProperties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>string<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">maxProperties</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">10</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Keys may not be removed and their values must stay the same<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span>oldSelf.all(key, key in self &amp;&amp; self[key] == oldSelf[key])<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span>Value is required once set<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;!has(oldSelf.values) || has(self.values)&#39;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h1 id=\"going-further\">Going further</h1>\n<p>The above examples showed how CEL rules can be added to kubebuilder types.\nThe same rules can be added directly to OpenAPI if writing a manifest for a CRD by hand.</p>\n<p>For native types, the same behavior can be achieved using kube-openapi‚Äôs marker\n<a href=\"https://github.com/kubernetes/kube-openapi/blob/923526ac052c59656d41710b45bbcb03748aa9d6/pkg/generators/extension.go#L69\"><code>+validations</code></a>.</p>\n<p>Usage of CEL within Kubernetes Validation Rules is so much more powerful than\nwhat has been shown in this article. For more information please check out\n<a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">validation rules</a>\nin the Kubernetes documentation and <a href=\"https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/\">CRD Validation Rules Beta</a> blog post.</p>","PublishedAt":"2022-09-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/","SourceName":"Kubernetes"}},{"node":{"ID":1755,"Title":"Blog: Kubernetes 1.25: Kubernetes In-Tree to CSI Volume Migration Status Update","Description":"<p><strong>Author:</strong> Jiawei Wang (Google)</p>\n<p>The Kubernetes in-tree storage plugin to <a href=\"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/\">Container Storage Interface (CSI)</a> migration infrastructure has already been <a href=\"https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/\">beta</a> since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.\nSince then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for CSI Migration feature to go GA.</p>\n<p>SIG Storage is excited to announce that the core CSI Migration feature is <strong>generally available</strong> in Kubernetes v1.25 release!</p>\n<p>SIG Storage wrote a blog post in v1.23 for <a href=\"https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/\">CSI Migration status update</a> which discussed the CSI migration status for each storage driver. It has been a while and this article is intended to give a latest status update on each storage driver for their CSI Migration status in Kubernetes v1.25.</p>\n<h2 id=\"quick-recap-what-is-csi-migration-and-why-migrate\">Quick recap: What is CSI Migration, and why migrate?</h2>\n<p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.\nKubernetes support for the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md#README\">Container Storage Interface</a> has been\n<a href=\"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/\">generally available</a> since Kubernetes v1.13.\nSupport for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).</p>\n<p>As more CSI Drivers were created and became production ready, SIG Storage wanted all Kubernetes users to benefit from the CSI model. However, we could not break API compatibility with the existing storage API types due to k8s architecture conventions. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.</p>\n<p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as <code>kubernetes.io/gce-pd</code> or <code>kubernetes.io/aws-ebs</code> with a corresponding <a href=\"https://kubernetes-csi.github.io/docs/introduction.html\">CSI driver</a> from the storage backend.\nIf CSI Migration is working properly, Kubernetes end users shouldn‚Äôt notice a difference. Existing <code>StorageClass</code>, <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> objects should continue to work.\nWhen a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.\nHowever, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.</p>\n<p>For example, suppose you are a <code>kubernetes.io/gce-pd</code> user; after CSI migration, you can still use <code>kubernetes.io/gce-pd</code> to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing APIs and Interface will still function correctly. However, the underlying function calls are all going through the <a href=\"https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver\">GCE PD CSI driver</a> instead of the in-tree Kubernetes function.</p>\n<p>This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.</p>\n<h2 id=\"timeline-and-status\">What is the timeline / status?</h2>\n<p>The current and targeted releases for each individual driver is shown in the table below:</p>\n<table>\n<thead>\n<tr>\n<th>Driver</th>\n<th>Alpha</th>\n<th>Beta (in-tree deprecated)</th>\n<th>Beta (on-by-default)</th>\n<th>GA</th>\n<th>Target &quot;in-tree plugin&quot; removal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AWS EBS</td>\n<td>1.14</td>\n<td>1.17</td>\n<td>1.23</td>\n<td>1.25</td>\n<td>1.27 (Target)</td>\n</tr>\n<tr>\n<td>Azure Disk</td>\n<td>1.15</td>\n<td>1.19</td>\n<td>1.23</td>\n<td>1.24</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>Azure File</td>\n<td>1.15</td>\n<td>1.21</td>\n<td>1.24</td>\n<td>1.26 (Target)</td>\n<td>1.28 (Target)</td>\n</tr>\n<tr>\n<td>Ceph FS</td>\n<td>1.26 (Target)</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Ceph RBD</td>\n<td>1.23</td>\n<td>1.26 (Target)</td>\n<td>1.27 (Target)</td>\n<td>1.28 (Target)</td>\n<td>1.30 (Target)</td>\n</tr>\n<tr>\n<td>GCE PD</td>\n<td>1.14</td>\n<td>1.17</td>\n<td>1.23</td>\n<td>1.25</td>\n<td>1.27 (Target)</td>\n</tr>\n<tr>\n<td>OpenStack Cinder</td>\n<td>1.14</td>\n<td>1.18</td>\n<td>1.21</td>\n<td>1.24</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>Portworx</td>\n<td>1.23</td>\n<td>1.25</td>\n<td>1.26 (Target)</td>\n<td>1.27 (Target)</td>\n<td>1.29 (Target)</td>\n</tr>\n<tr>\n<td>vSphere</td>\n<td>1.18</td>\n<td>1.19</td>\n<td>1.25</td>\n<td>1.26 (Target)</td>\n<td>1.28 (Target)</td>\n</tr>\n</tbody>\n</table>\n<p>The following storage drivers will not have CSI migration support.\nThe <code>scaleio</code>, <code>flocker</code>, <code>quobyte</code> and <code>storageos</code> drivers were removed; the others are deprecated and will be removed from core Kubernetes in the coming releases.</p>\n<table>\n<thead>\n<tr>\n<th>Driver</th>\n<th>Deprecated</th>\n<th>Code Removal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Flocker</td>\n<td>1.22</td>\n<td>1.25</td>\n</tr>\n<tr>\n<td>GlusterFS</td>\n<td>1.25</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>Quobyte</td>\n<td>1.22</td>\n<td>1.25</td>\n</tr>\n<tr>\n<td>ScaleIO</td>\n<td>1.16</td>\n<td>1.22</td>\n</tr>\n<tr>\n<td>StorageOS</td>\n<td>1.22</td>\n<td>1.25</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"what-does-it-mean-for-the-core-csi-migration-feature-to-go-ga\">What does it mean for the core CSI Migration feature to go GA?</h2>\n<p>Core CSI Migration goes to GA means that the general framework, core library and API for CSI migration is\nstable for Kubernetes v1.25 and will be part of future Kubernetes releases as well.</p>\n<ul>\n<li>If you are a Kubernetes distribution maintainer, this means if you disabled <code>CSIMigration</code> feature gate previously, you are no longer allowed to do so because the feature gate has been locked.</li>\n<li>If you are a Kubernetes storage driver developer, this means you can expect no backwards incompatibility changes in the CSI migration library.</li>\n<li>If you are a Kubernetes maintainer, expect nothing changes from your day to day development flows.</li>\n<li>If you are a Kubernetes user, expect nothing to change from your day-to-day usage flows. If you encounter any storage related issues, contact the people who operate your cluster (if that's you, contact the provider of your Kubernetes distribution, or get help from the <a href=\"https://kubernetes.io/community/#discuss\">community</a>).</li>\n</ul>\n<h2 id=\"what-does-it-mean-for-the-storage-driver-csi-migration-to-go-ga\">What does it mean for the storage driver CSI migration to go GA?</h2>\n<p>Storage Driver CSI Migration goes to GA means that the specific storage driver supports CSI Migration. Expect feature parity between the in-tree plugin with the CSI driver.</p>\n<ul>\n<li>If you are a Kubernetes distribution maintainer, make sure you install the corresponding\nCSI driver on the distribution. And make sure you are not disabling the specific <code>CSIMigration{provider}</code> flag, as they are locked.</li>\n<li>If you are a Kubernetes storage driver maintainer, make sure the CSI driver can ensure feature parity if it supports CSI migration.</li>\n<li>If you are a Kubernetes maintainer/developer, expect nothing to change from your day-to-day development flows.</li>\n<li>If you are a Kubernetes user, the CSI Migration feature should be completely transparent\nto you, the only requirement is to install the corresponding CSI driver.</li>\n</ul>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>We are expecting cloud provider in-tree storage plugins code removal to start to happen as part of the v1.26 and v1.27 releases of Kubernetes. More and more drivers that support CSI migration will go GA in the upcoming releases.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>The Kubernetes Slack channel <a href=\"https://kubernetes.slack.com/messages/csi-migration\">#csi-migration</a> along with any of the standard <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact\">SIG Storage communication channels</a> are great ways to reach out to the SIG Storage and migration working group teams.</p>\n<p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:</p>\n<ul>\n<li>Xing Yang (xing-yang)</li>\n<li>Hemant Kumar (gnufied)</li>\n</ul>\n<p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:</p>\n<ul>\n<li>Andy Zhang (andyzhangz)</li>\n<li>Divyen Patel (divyenpatel)</li>\n<li>Deep Debroy (ddebroy)</li>\n<li>Humble Devassy Chirammal (humblec)</li>\n<li>Ismail Alidzhikov (ialidzhikov)</li>\n<li>Jordan Liggitt (liggitt)</li>\n<li>Matthew Cary (mattcary)</li>\n<li>Matthew Wong (wongma7)</li>\n<li>Neha Arora (nearora-msft)</li>\n<li>Oksana Naumov (trierra)</li>\n<li>Saad Ali (saad-ali)</li>\n<li>Michelle Au (msau42)</li>\n</ul>\n<p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group (SIG)</a>. We‚Äôre rapidly growing and always welcome new contributors.</p>","PublishedAt":"2022-09-26 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/","SourceName":"Kubernetes"}},{"node":{"ID":1744,"Title":"Blog: Kubernetes 1.25: CustomResourceDefinition Validation Rules Graduate to Beta","Description":"<p><strong>Authors:</strong> Joe Betz (Google), Cici Huang (Google), Kermit Alexander (Google)</p>\n<p>In Kubernetes 1.25, <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">Validation rules for CustomResourceDefinitions</a> (CRDs) have graduated to Beta!</p>\n<p>Validation rules make it possible to declare how custom resources are validated using the <a href=\"https://github.com/google/cel-spec\">Common Expression Language</a> (CEL). For example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>apiextensions.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>CustomResourceDefinition<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">openAPIV3Schema</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">properties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>object<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">x-kubernetes-validations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">rule</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;self.minReplicas &lt;= self.replicas &amp;&amp; self.replicas &lt;= self.maxReplicas&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">message</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;replicas should be in the range minReplicas..maxReplicas.&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">properties</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">replicas</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>integer<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Validation rules support a wide range of use cases. To get a sense of some of the capabilities, let's look at a few examples:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>self.minReplicas &lt;= self.replicas</code></td>\n<td>Validate an integer field is less than or equal to another integer field</td>\n</tr>\n<tr>\n<td><code>'Available' in self.stateCounts</code></td>\n<td>Validate an entry with the 'Available' key exists in a map</td>\n</tr>\n<tr>\n<td><code>self.set1.all(e, !(e in self.set2))</code></td>\n<td>Validate that the elements of two sets are disjoint</td>\n</tr>\n<tr>\n<td><code>self == oldSelf</code></td>\n<td>Validate that a required field is immutable once it is set</td>\n</tr>\n<tr>\n<td><code>self.created + self.ttl &lt; self.expired</code></td>\n<td>Validate that 'expired' date is after a 'create' date plus a 'ttl' duration</td>\n</tr>\n</tbody>\n</table>\n<p>Validation rules are expressive and flexible. See the <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">Validation Rules documentation</a> to learn more about what validation rules are capable of.</p>\n<h2 id=\"why-cel\">Why CEL?</h2>\n<p>CEL was chosen as the language for validation rules for a couple reasons:</p>\n<ul>\n<li>CEL expressions can easily be inlined into CRD schemas. They are sufficiently expressive to replace the vast majority of CRD validation checks currently implemented in admission webhooks. This results in CRDs that are self-contained and are easier to understand.</li>\n<li>CEL expressions are compiled and type checked against a CRD's schema &quot;ahead-of-time&quot; (when CRDs are created and updated) allowing them to be evaluated efficiently and safely &quot;runtime&quot; (when custom resources are validated). Even regex string literals in CEL are validated and pre-compiled when CRDs are created or updated.</li>\n</ul>\n<h2 id=\"why-not-use-validation-webhooks\">Why not use validation webhooks?</h2>\n<p>Benefits of using validation rules when compared with validation webhooks:</p>\n<ul>\n<li>CRD authors benefit from a simpler workflow since validation rules eliminate the need to develop and maintain a webhook.</li>\n<li>Cluster administrators benefit by no longer having to install, upgrade and operate webhooks for the purposes of CRD validation.</li>\n<li>Cluster operability improves because CRD validation no longer requires a remote call to a webhook endpoint, eliminating a potential point of failure in the request-serving-path of the Kubernetes API server. This allows clusters to retain high availability while scaling to larger amounts of installed CRD extensions, since expected control plane availability would otherwise decrease with each additional webhook installed.</li>\n</ul>\n<h2 id=\"getting-started-with-validation-rules\">Getting started with validation rules</h2>\n<h3 id=\"writing-validation-rules-in-openapiv3-schemas\">Writing validation rules in OpenAPIv3 schemas</h3>\n<p>You can define validation rules for any level of a CRD's OpenAPIv3 schema. Validation rules are automatically scoped to their location in the schema where they are declared.</p>\n<p>Good practices for CRD validation rules:</p>\n<ul>\n<li>Scope validation rules as close as possible to the fields(s) they validate.</li>\n<li>Use multiple rules when validating independent constraints.</li>\n<li>Do not use validation rules for validations already</li>\n<li>Use OpenAPIv3 <a href=\"https://swagger.io/specification/#properties\">value validations</a> (<code>maxLength</code>, <code>maxItems</code>, <code>maxProperties</code>, <code>required</code>, <code>enum</code>, <code>minimum</code>, <code>maximum</code>, ..) and <a href=\"https://swagger.io/docs/specification/data-models/data-types/#format\">string formats</a> where available.</li>\n<li>Use <code>x-kubernetes-int-or-string</code>, <code>x-kubernetes-embedded-type</code> and <code>x-kubernetes-list-type=(set|map)</code> were appropriate.</li>\n</ul>\n<p>Examples of good practice:</p>\n<table>\n<thead>\n<tr>\n<th>Validation</th>\n<th>Best Practice</th>\n<th>Example(s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Validate an integer is between 0 and 100.</td>\n<td>Use OpenAPIv3 value validations.</td>\n<td><pre>type: integer<br>minimum: 0<br>maximum: 100</pre></td>\n</tr>\n<tr>\n<td>Constraint the max size limits on maps (objects with additionalProperties), arrays and string.</td>\n<td>Use OpenAPIv3 value validations. Recommended for all maps, arrays and strings. This best practice is essential for rule cost estimation (explained below).</td>\n<td><pre>type:<br>maxItems: 100</pre></td>\n</tr>\n<tr>\n<td>Require a date-time be more recent than a particular timestamp.</td>\n<td>Use OpenAPIv3 string formats to declare that the field is a date-time. Use validation rules to compare it to a particular timestamp.</td>\n<td><pre>type: string<br>format: date-time<br>x-kubernetes-validations:<br> - rule: &quot;self &gt;= timestamp('2000-01-01T00:00:00.000Z')&quot;</pre></td>\n</tr>\n<tr>\n<td>Require two sets to be disjoint.</td>\n<td>Use x-kubernetes-list-type to validate that the arrays are sets. <br>Use validation rules to validate the sets are disjoint.</td>\n<td><pre>type: object<br>properties:<br> set1:<br> type: array<br> x-kubernetes-list-type: set<br> set2: ...<br> x-kubernetes-validations:<br> - rule: &quot;!self.set1.all(e, !(e in self.set2))&quot;</pre></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"crd-transition-rules\">CRD transition rules</h2>\n<p><a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#transition-rules\">Transition Rules</a> make it possible to compare the new state against the old state of a resource in validation rules. You use transition rules to make sure that the cluster's API server does not accept invalid state transitions. A transition rule is a validation rule that references 'oldSelf'. The API server only evaluates transition rules when both an old value and new value exist.</p>\n<p>Transition rule examples:</p>\n<table>\n<thead>\n<tr>\n<th>Transition Rule</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>self == oldSelf</code></td>\n<td>For a required field, make that field immutable once it is set. For an optional field, only allow transitioning from unset to set, or from set to unset.</td>\n</tr>\n<tr>\n<td>(on parent of field) <code>has(self.field) == has(oldSelf.field)</code><br>on field: <code>self == oldSelf</code></td>\n<td>Make a field immutable: validate that a field, even if optional, never changes after the resource is created (for a required field, the previous rule is simpler).</td>\n</tr>\n<tr>\n<td><code>self.all(x, x in oldSelf)</code></td>\n<td>Only allow adding items to a field that represents a set (prevent removals).</td>\n</tr>\n<tr>\n<td><code>self &gt;= oldSelf</code></td>\n<td>Validate that a number is monotonically increasing.</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"using-the-functions-libraries\">Using the Functions Libraries</h2>\n<p>Validation rules have access to a couple different function libraries:</p>\n<ul>\n<li>CEL standard functions, defined in the <a href=\"https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#list-of-standard-definitions\">list of standard definitions</a></li>\n<li>CEL standard <a href=\"https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#macros\">macros</a></li>\n<li>CEL <a href=\"https://pkg.go.dev/github.com/google/cel-go/ext#Strings\">extended string function library</a></li>\n<li>Kubernetes <a href=\"https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions\">CEL extension library</a> which includes supplemental functions for <a href=\"https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions\">lists</a>, <a href=\"https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex\">regex</a>, and <a href=\"https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex\">URLs</a>.</li>\n</ul>\n<p>Examples of function libraries in use:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>!(self.getDayOfWeek() in [0, 6])</code></td>\n<td>Validate that a date is not a Sunday or Saturday.</td>\n</tr>\n<tr>\n<td><code>isUrl(self) &amp;&amp; url(self).getHostname() in [a.example.com', 'b.example.com']</code></td>\n<td>Validate that a URL has an allowed hostname.</td>\n</tr>\n<tr>\n<td><code>self.map(x, x.weight).sum() == 1</code></td>\n<td>Validate that the weights of a list of objects sum to 1.</td>\n</tr>\n<tr>\n<td><code>int(self.find('^[0-9]*')) &lt; 100</code></td>\n<td>Validate that a string starts with a number less than 100.</td>\n</tr>\n<tr>\n<td><code>self.isSorted()</code></td>\n<td>Validates that a list is sorted.</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"resource-use-and-limits\">Resource use and limits</h2>\n<p>To prevent CEL evaluation from consuming excessive compute resources, validation rules impose some limits. These limits are based on CEL <em>cost units</em>, a platform and machine independent measure of execution cost. As a result, the limits are the same regardless of where they are enforced.</p>\n<h3 id=\"estimated-cost-limit\">Estimated cost limit</h3>\n<p>CEL is, by design, non-Turing-complete in such a way that the halting problem isn‚Äôt a concern. CEL takes advantage of this design choice to include an &quot;estimated cost&quot; subsystem that can statically compute the worst case run time cost of any CEL expression. Validation rules are <a href=\"o/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#resource-use-by-validation-functions\">integrated with the estimated cost system</a> and disallow CEL expressions from being included in CRDs if they have a sufficiently poor (high) estimated cost. The estimated cost limit is set quite high and typically requires an O(n^2) or worse operation, across something of unbounded size, to be exceeded. Fortunately the fix is usually quite simple: because the cost system is aware of size limits declared in the CRD's schema, CRD authors can add size limits to the CRD's schema (<code>maxItems</code> for arrays, <code>maxProperties</code> for maps, <code>maxLength</code> for strings) to reduce the estimated cost.</p>\n<p>Good practice:</p>\n<p>Set <code>maxItems</code>, <code>maxProperties</code> and <code>maxLength</code> on all array, map (<code>object</code> with <code>additionalProperties</code>) and string types in CRD schemas! This results in lower and more accurate estimated costs and generally makes a CRD safer to use.</p>\n<h3 id=\"runtime-cost-limits-for-crd-validation-rules\">Runtime cost limits for CRD validation rules</h3>\n<p>In addition to the estimated cost limit, CEL keeps track of actual cost while evaluating a CEL expression and will halt execution of the expression if a limit is exceeded.</p>\n<p>With the estimated cost limit already in place, the runtime cost limit is rarely encountered. But it is possible. For example, it might be encountered for a large resource composed entirely of a single large list and a validation rule that is either evaluated on each element in the list, or traverses the entire list.</p>\n<p>CRD authors can ensure the runtime cost limit will not be exceeded in much the same way the estimated cost limit is avoided: by setting <code>maxItems</code>, <code>maxProperties</code> and <code>maxLength</code> on array, map and string types.</p>\n<h2 id=\"future-work\">Future work</h2>\n<p>We look forward to working with the community on the adoption of CRD Validation Rules, and hope to see this feature promoted to general availability in an upcoming Kubernetes release!</p>\n<p>There is a growing community of Kubernetes contributors thinking about how to make it possible to write extensible admission controllers using CEL as a substitute for admission webhooks for policy enforcement use cases. Anyone interested should reach out to us on the usual <a href=\"https://github.com/kubernetes/community/tree/master/sig-api-machinery\">SIG API Machinery</a> channels or via slack at <a href=\"https://kubernetes.slack.com/archives/C02TTBG6LF4\">#sig-api-machinery-cel-dev</a>.</p>\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n<p>Special thanks to Cici Huang, Ben Luddy, Jordan Liggitt, David Eads, Daniel Smith, Dr. Stefan Schimanski, Leila Jalali and everyone who contributed to Validation Rules!</p>","PublishedAt":"2022-09-23 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/","SourceName":"Kubernetes"}},{"node":{"ID":1719,"Title":"Blog: Kubernetes 1.25: Use Secrets for Node-Driven Expansion of CSI Volumes","Description":"<p><strong>Author:</strong> Humble Chirammal (Red Hat), Louis Koo (deeproute.ai)</p>\n<p>Kubernetes v1.25, released earlier this month, introduced a new feature\nthat lets your cluster expand storage volumes, even when access to those\nvolumes requires a secret (for example: a credential for accessing a SAN fabric)\nto perform node expand operation. This new behavior is in alpha and you\nmust enable a feature gate (<code>CSINodeExpandSecret</code>) to make use of it.\nYou must also be using <a href=\"https://kubernetes-csi.github.io/docs/\">CSI</a>\nstorage; this change isn't relevant to storage drivers that are built in to Kubernetes.</p>\n<p>To turn on this new, alpha feature, you enable the <code>CSINodeExpandSecret</code> feature\ngate for the kube-apiserver and kubelet, which turns on a mechanism to send <code>secretRef</code>\nconfiguration as part of NodeExpansion by the CSI drivers thus make use of\nthe same to perform node side expansion operation with the underlying\nstorage system.</p>\n<h2 id=\"what-is-this-all-about\">What is this all about?</h2>\n<p>Before Kubernetes v1.24, you were able to define a cluster-level StorageClass\nthat made use of <a href=\"https://kubernetes-csi.github.io/docs/secrets-and-credentials-storage-class.html\">StorageClass Secrets</a>,\nbut you didn't have any mechanism to specify the credentials that would be used for\noperations that take place when the storage was mounted onto a node and when\nthe volume has to be expanded at node side.</p>\n<p>The Kubernetes CSI already implemented a similar mechanism specific kinds of\nvolume resizes; namely, resizes of PersistentVolumes where the resizes take place\nindependently from any node referred as Controller Expansion. In that case, you\nassociate a PersistentVolume with a Secret that contains credentials for volume resize\nactions, so that controller expansion can take place. CSI also supports a <code>nodeExpandVolume</code>\noperation which CSI drivers can make use independent of Controller Expansion or along with\nController Expansion on which, where the resize is driven from a node in your cluster where\nthe volume is attached. Please read <a href=\"https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/\">Kubernetes 1.24: Volume Expansion Now A Stable Feature</a></p>\n<ul>\n<li>\n<p>At times, the CSI driver needs to check the actual size of the backend block storage (or image)\nbefore proceeding with a node-level filesystem expand operation. This avoids false positive returns\nfrom the backend storage cluster during filesystem expands.</p>\n</li>\n<li>\n<p>When a PersistentVolume represents encrypted block storage (for example using LUKS)\nyou need to provide a passphrase in order to expand the device, and also to make it possible\nto grow the filesystem on that device.</p>\n</li>\n<li>\n<p>For various validations at time of node expansion, the CSI driver has to be connected\nto the backend storage cluster. If the <code>nodeExpandVolume</code> request includes a <code>secretRef</code>\nthen the CSI driver can make use of the same and connect to the storage cluster to\nperform the cluster operations.</p>\n</li>\n</ul>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>To enable this functionality from this version of Kubernetes, SIG Storage have introduced\na new feature gate called <code>CSINodeExpandSecret</code>. Once the feature gate is enabled\nin the cluster, NodeExpandVolume requests can include a <code>secretRef</code> field. The NodeExpandVolume request\nis part of CSI; for example, in a request which has been sent from the Kubernetes\ncontrol plane to the CSI driver.</p>\n<p>As a cluster operator, you admin can specify these secrets as an opaque parameter in a StorageClass,\nthe same way that you can already specify other CSI secret data. The StorageClass needs to have some\nCSI-specific parameters set. Here's an example of those parameters:</p>\n<pre tabindex=\"0\"><code>csi.storage.k8s.io/node-expand-secret-name: test-secret\ncsi.storage.k8s.io/node-expand-secret-namespace: default\n</code></pre><p>If feature gates are enabled and storage class carries the above secret configuration,\nthe CSI provisioner receives the credentials from the Secret as part of the NodeExpansion request.</p>\n<p>CSI volumes that require secrets for online expansion will have NodeExpandSecretRef\nfield set. If not set, the NodeExpandVolume CSI RPC call will be made without a secret.</p>\n<h2 id=\"trying-it-out\">Trying it out</h2>\n<ol>\n<li>\n<p>Enable the <code>CSINodeExpandSecret</code> feature gate (please refer to\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">Feature Gates</a>).</p>\n</li>\n<li>\n<p>Create a Secret, and then a StorageClass that uses that Secret.</p>\n</li>\n</ol>\n<p>Here's an example manifest for a Secret that holds credentials:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Secret<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-secret<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>default<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">data</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">stringData</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">username</span>:<span style=\"color:#bbb\"> </span>admin<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">password</span>:<span style=\"color:#bbb\"> </span>t0p-Secret<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Here's an example manifest for a StorageClass that refers to those credentials:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>storage.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>StorageClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>csi-blockstorage-sc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">parameters</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">csi.storage.k8s.io/node-expand-secret-name</span>:<span style=\"color:#bbb\"> </span>test-secret <span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># the name of the Secret</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">csi.storage.k8s.io/node-expand-secret-namespace</span>:<span style=\"color:#bbb\"> </span>default <span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># the namespace that the Secret is in</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">provisioner</span>:<span style=\"color:#bbb\"> </span>blockstorage.cloudprovider.example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">reclaimPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">volumeBindingMode</span>:<span style=\"color:#bbb\"> </span>Immediate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">allowVolumeExpansion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"example-output\">Example output</h2>\n<p>If the PersistentVolumeClaim (PVC) was created successfully, you can see that\nconfiguration within the <code>spec.csi</code> field of the PersistentVolume (look for\n<code>spec.csi.nodeExpandSecretRef</code>).\nCheck that it worked by running <code>kubectl get persistentvolume &lt;pv_name&gt; -o yaml</code>.\nYou should see something like.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolume<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">pv.kubernetes.io/provisioned-by</span>:<span style=\"color:#bbb\"> </span>blockstorage.cloudprovider.example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">creationTimestamp</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2022-08-26T15:14:07Z&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">finalizers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- kubernetes.io/pv-protection<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;420263&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">uid</span>:<span style=\"color:#bbb\"> </span>6fa824d7-8a06-4e0c-b722-d3f897dcbd65<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">capacity</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>6Gi<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claimRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>csi-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>default<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;419862&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">uid</span>:<span style=\"color:#bbb\"> </span>95eb531a-d675-49f6-940b-9bc3fde83eb0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">csi</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>blockstorage.cloudprovider.example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodeExpandSecretRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-secret<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>default<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeAttributes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage.kubernetes.io/csiProvisionerIdentity</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1648042783218-8081</span>-blockstorage.cloudprovider.example<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeHandle</span>:<span style=\"color:#bbb\"> </span>e21c7809-aabb-11ec-917a-2e2e254eb4cf<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodeAffinity</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">required</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">nodeSelectorTerms</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">matchExpressions</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">key</span>:<span style=\"color:#bbb\"> </span>topology.hostpath.csi/node<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operator</span>:<span style=\"color:#bbb\"> </span>In<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">values</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- racknode01<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">persistentVolumeReclaimPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>csi-blockstorage-sc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">status</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">phase</span>:<span style=\"color:#bbb\"> </span>Bound<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>If you then trigger online storage expansion, the kubelet passes the appropriate credentials\nto the CSI driver, by loading that Secret and passing the data to the storage driver.</p>\n<p>Here's an example debug log:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">I0330 03:29:51.966241 1 server.go:101] GRPC call: /csi.v1.Node/NodeExpandVolume\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">I0330 03:29:51.966261 1 server.go:105] GRPC request: {&#34;capacity_range&#34;:{&#34;required_bytes&#34;:7516192768},&#34;secrets&#34;:&#34;***stripped***&#34;,&#34;staging_target_path&#34;:&#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&#34;,&#34;volume_capability&#34;:{&#34;AccessType&#34;:{&#34;Mount&#34;:{}},&#34;access_mode&#34;:{&#34;mode&#34;:7}},&#34;volume_id&#34;:&#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&#34;,&#34;volume_path&#34;:&#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&#34;}\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">I0330 03:29:51.966360 1 nodeserver.go:459] req:volume_id:&#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&#34; volume_path:&#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&#34; capacity_range:&lt;required_bytes:7516192768 &gt; staging_target_path:&#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&#34; volume_capability:&lt;mount:&lt;&gt; access_mode:&lt;mode:SINGLE_NODE_MULTI_WRITER &gt; &gt; secrets:&lt;key:&#34;XXXXXX&#34; value:&#34;XXXXX&#34; &gt; secrets:&lt;key:&#34;XXXXX&#34; value:&#34;XXXXXX&#34; &gt;\n</span></span></span></code></pre></div><h2 id=\"the-future\">The future</h2>\n<p>As this feature is still in alpha, Kubernetes Storage SIG expect to update or get feedback from CSI driver\nauthors with more tests and implementation. The community plans to eventually\npromote the feature to Beta in upcoming releases.</p>\n<h2 id=\"get-involved-or-learn-more\">Get involved or learn more?</h2>\n<p>The enhancement proposal includes lots of detail about the history and technical\nimplementation of this feature.</p>\n<p>To learn more about StorageClass based dynamic provisioning in Kubernetes, please refer to\n<a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/\">Storage Classes</a> and\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">Persistent Volumes</a>.</p>\n<p>Please get involved by joining the Kubernetes\n<a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md\">Storage SIG</a>\n(Special Interest Group) to help us enhance this feature.\nThere are a lot of good ideas already and we'd be thrilled to have more!</p>","PublishedAt":"2022-09-21 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":1698,"Title":"Blog: Kubernetes 1.25: Local Storage Capacity Isolation Reaches GA","Description":"<p><strong>Author:</strong> Jing Xu (Google)</p>\n<p>Local ephemeral storage capacity isolation was introduced as a alpha feature in Kubernetes 1.7 and it went beta in 1.9. With Kubernetes 1.25 we are excited to announce general availability(GA) of this feature.</p>\n<p>Pods use ephemeral local storage for scratch space, caching, and logs. The lifetime of local ephemeral storage does not extend beyond the life of the individual pod. It is exposed to pods using the container‚Äôs writable layer, logs directory, and <code>EmptyDir</code> volumes. Before this feature was introduced, there were issues related to the lack of local storage accounting and isolation, such as Pods not knowing how much local storage is available and being unable to request guaranteed local storage. Local storage is a best-effort resource and pods can be evicted due to other pods filling the local storage.</p>\n<p>The <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage\">local storage capacity isolation feature</a> allows users to manage local ephemeral storage in the same way as managing CPU and memory. It provides support for capacity isolation of shared storage between pods, such that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of shared storage exceeds that limit. It also allows setting ephemeral storage requests for resource reservation. The limits and requests for shared <code>ephemeral-storage</code> are similar to those for memory and CPU consumption.</p>\n<h3 id=\"how-to-use-local-storage-capacity-isolation\">How to use local storage capacity isolation</h3>\n<p>A typical configuration for local ephemeral storage is to place all different kinds of ephemeral local data (emptyDir volumes, writeable layers, container images, logs) into one filesystem. Typically, both /var/lib/kubelet and /var/log are on the system's root filesystem. If users configure the local storage in different ways, kubelet might not be able to correctly measure disk usage and use this feature.</p>\n<h4 id=\"setting-requests-and-limits-for-local-ephemeral-storage\">Setting requests and limits for local ephemeral storage</h4>\n<p>You can specify <code>ephemeral-storage</code> for managing local ephemeral storage. Each container of a Pod can specify either or both of the following:</p>\n<ul>\n<li><code>spec.containers[].resources.limits.ephemeral-storage</code></li>\n<li><code>spec.containers[].resources.requests.ephemeral-storage</code></li>\n</ul>\n<p>In the following example, the Pod has two containers. The first container has a request of 8GiB of local ephemeral storage and a limit of 12GiB. The second container requests 2GiB of local storage, but no limit setting. Therefore, the Pod requests a total of 10GiB (8GiB+2GiB) of local ephemeral storage and enforces a limit of 12GiB of local ephemeral storage. It also sets emptyDir sizeLimit to 5GiB. With this setting in pod spec, it will affect how the scheduler makes a decision on scheduling pods and also how kubelet evict pods.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>frontend<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>app<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>images.my-company.example/app:v4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ephemeral-storage</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;8Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">limits</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ephemeral-storage</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;12Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMounts</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>ephemeral<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">mountPath</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;/tmp&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>log-aggregator<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>images.my-company.example/log-aggregator:v6<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ephemeral-storage</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2Gi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMounts</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>ephemeral<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">mountPath</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;/tmp&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>ephemeral<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">emptyDir</span>:<span style=\"color:#bbb\"> </span>{}<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">sizeLimit</span>:<span style=\"color:#bbb\"> </span>5Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>First of all, the scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node. In this case, the pod can be assigned to a node only if its available ephemeral storage (allocatable resource) has more than 10GiB.</p>\n<p>Secondly, at container level, since one of the container sets resource limit, kubelet eviction manager will measure the disk usage of this container and evict the pod if the storage usage of the first container exceeds its limit (12GiB). At pod level, kubelet works out an overall Pod storage limit by\nadding up the limits of all the containers in that Pod. In this case, the total storage usage at pod level is the sum of the disk usage from all containers plus the Pod's <code>emptyDir</code>volumes. If this total usage exceeds the overall Pod storage limit (12GiB), then the kubelet also marks the Pod for eviction.</p>\n<p>Last, in this example, emptyDir volume sets its sizeLimit to 5Gi. It means that if this pod's emptyDir used up more local storage than 5GiB, the pod will be evicted from the node.</p>\n<h4 id=\"setting-resource-quota-and-limitrange-for-local-ephemeral-storage\">Setting resource quota and limitRange for local ephemeral storage</h4>\n<p>This feature adds two more resource quotas for storage. The request and limit set constraints on the total requests/limits of all containers‚Äô in a namespace.</p>\n<ul>\n<li>requests.ephemeral-storage</li>\n<li>limits.ephemeral-storage</li>\n</ul>\n<pre tabindex=\"0\"><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: storage-resources\nspec:\nhard:\nrequests.ephemeral-storage: &#34;10Gi&#34;\nlimits.ephemeral-storage: &#34;20Gi&#34;\n</code></pre><p>Similar to CPU and memory, admin could use LimitRange to set default container‚Äôs local storage request/limit, and/or minimum/maximum resource constraints for a namespace.</p>\n<p>apiVersion: v1\nkind: LimitRange\nmetadata:\nname: storage-limit-range\nspec:\nlimits:</p>\n<ul>\n<li>default:\nephemeral-storage: 10Gi\ndefaultRequest:\nephemeral-storage: 5Gi\ntype: Container</li>\n</ul>\n<p>Also, ephemeral-storage may be specified to reserve for kubelet or system. example, --system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=10Gi][,][pid=1000] --kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=5Gi][,][pid=1000]. If your cluster node root disk capacity is 100Gi, after setting system-reserved and kube-reserved value, the available allocatable ephemeral storage would become 85Gi. The schedule will use this information to assign pods based on request and allocatable resources from each node. The eviction manager will also use allocatable resource to determine pod eviction. See more details from <a href=\"docs/tasks/administer-cluster/reserve-compute-resources/\">Reserve Compute Resources for System Daemons</a></p>\n<h3 id=\"how-do-i-get-involved\">How do I get involved?</h3>\n<p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p>\n<p>We offer a huge thank you to all the contributors in <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage SIG</a> and CSI community who helped review the design and implementation of the project, including but not limited to the following:</p><ul><li>Benjamin Elder (<a href=https://github.com/BenTheElder>BenTheElder</a>)</li><li>Michelle Au (<a href=https://github.com/msau42>msau42</a>)</li><li>Tim Hockin (<a href=https://github.com/thockin>thockin</a>)</li><li>Jordan Liggitt (<a href=https://github.com/liggitt>liggitt</a>)</li><li>Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>)</li></p>","PublishedAt":"2022-09-19 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/","SourceName":"Kubernetes"}},{"node":{"ID":1672,"Title":"Blog: Kubernetes 1.25: Two Features for Apps Rollouts Graduate to Stable","Description":"<p><strong>Authors:</strong> Ravi Gudimetla (Apple), Filip K≈ôepinsk√Ω (Red Hat), Maciej Szulik (Red Hat)</p>\n<p>This blog describes the two features namely <code>minReadySeconds</code> for StatefulSets and <code>maxSurge</code> for DaemonSets that SIG Apps is happy to graduate to stable in Kubernetes 1.25.</p>\n<p>Specifying <code>minReadySeconds</code> slows down a rollout of a StatefulSet, when using a <code>RollingUpdate</code> value in <code>.spec.updateStrategy</code> field, by waiting for each pod for a desired time.\nThis time can be used for initializing the pod (e.g. warming up the cache) or as a delay before acknowledging the pod.</p>\n<p><code>maxSurge</code> allows a DaemonSet workload to run multiple instances of the same pod on a node during a rollout when using a <code>RollingUpdate</code> value in <code>.spec.updateStrategy</code> field.\nThis helps to minimize the downtime of the DaemonSet for consumers.</p>\n<p>These features were already available in a Deployment and other workloads. This graduation helps to align this functionality across the workloads.</p>\n<h2 id=\"what-problems-do-these-features-solve\">What problems do these features solve?</h2>\n<h3 id=\"solved-problem-statefulset-minreadyseconds\">minReadySeconds for StatefulSets</h3>\n<p><code>minReadySeconds</code> ensures that the StatefulSet workload is <code>Ready</code> for the given number of seconds before reporting the\npod as <code>Available</code>. The notion of being <code>Ready</code> and <code>Available</code> is quite important for workloads. For example, some workloads, like Prometheus with multiple instances of Alertmanager, should be considered <code>Available</code> only when the Alertmanager's state transfer is complete. <code>minReadySeconds</code> also helps when using loadbalancers with cloud providers. Since the pod should be <code>Ready</code> for the given number of seconds, it provides buffer time to prevent killing pods in rotation before new pods show up.</p>\n<h3 id=\"how-use-daemonset-maxsurge\">maxSurge for DaemonSets</h3>\n<p>Kubernetes system-level components like CNI, CSI are typically run as DaemonSets. These components can have impact on the availability of the workloads if those DaemonSets go down momentarily during the upgrades. The feature allows DaemonSet pods to temporarily increase their number, thereby ensuring zero-downtime for the DaemonSets.</p>\n<p>Please note that the usage of <code>hostPort</code> in conjunction with <code>maxSurge</code> in DaemonSets is not allowed as DaemonSet pods are tied to a single node and two active pods cannot share the same port on the same node.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<h3 id=\"how-does-statefulset-minreadyseconds-work\">minReadySeconds for StatefulSets</h3>\n<p>The StatefulSet controller watches for the StatefulSet pods and counts how long a particular pod has been in the <code>Running</code> state, if this value is greater than or equal to the time specified in <code>.spec.minReadySeconds</code> field of the StatefulSet, the StatefulSet controller updates the <code>AvailableReplicas</code> field in the StatefulSet's status.</p>\n<h3 id=\"how-does-daemonset-maxsurge-work\">maxSurge for DaemonSets</h3>\n<p>The DaemonSet controller creates the additional pods (above the desired number resulting from DaemonSet spec) based on the value given in <code>.spec.strategy.rollingUpdate.maxSurge</code>. The additional pods would run on the same node where the old DaemonSet pod is running till the old pod gets killed.</p>\n<ul>\n<li>The default value is 0.</li>\n<li>The value cannot be <code>0</code> when <code>MaxUnavailable</code> is 0.</li>\n<li>The value can be specified either as an absolute number of pods, or a percentage (rounded up) of desired pods.</li>\n</ul>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<h3 id=\"how-use-statefulset-minreadyseconds\">minReadySeconds for StatefulSets</h3>\n<p>Specify a value for <code>minReadySeconds</code> for any StatefulSet and check if pods are available or not by inspecting\n<code>AvailableReplicas</code> field using:</p>\n<p><code>kubectl get statefulset/&lt;name_of_the_statefulset&gt; -o yaml</code></p>\n<p>Please note that the default value of <code>minReadySeconds</code> is 0.</p>\n<h3 id=\"how-use-daemonset-maxsurge\">maxSurge for DaemonSets</h3>\n<p>Specify a value for <code>.spec.updateStrategy.rollingUpdate.maxSurge</code> and set <code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code> to <code>0</code>.</p>\n<p>Then observe a faster rollout and higher number of pods running at the same time in the next rollout.</p>\n<pre tabindex=\"0\"><code>kubectl rollout restart daemonset &lt;name_of_the_daemonset&gt;\nkubectl get pods -w\n</code></pre><h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<h3 id=\"learn-more-statefulset-minreadyseconds\">minReadySeconds for StatefulSets</h3>\n<ul>\n<li>Documentation: <a href=\"https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds\">https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds</a></li>\n<li>KEP: <a href=\"https://github.com/kubernetes/enhancements/issues/2599\">https://github.com/kubernetes/enhancements/issues/2599</a></li>\n<li>API Changes: <a href=\"https://github.com/kubernetes/kubernetes/pull/100842\">https://github.com/kubernetes/kubernetes/pull/100842</a></li>\n</ul>\n<h3 id=\"learn-more-daemonset-maxsurge\">maxSurge for DaemonSets</h3>\n<ul>\n<li>Documentation: <a href=\"https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/\">https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/</a></li>\n<li>KEP: <a href=\"https://github.com/kubernetes/enhancements/issues/1591\">https://github.com/kubernetes/enhancements/issues/1591</a></li>\n<li>API Changes: <a href=\"https://github.com/kubernetes/kubernetes/pull/96375\">https://github.com/kubernetes/kubernetes/pull/96375</a></li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>Please reach out to us on <a href=\"https://kubernetes.slack.com/archives/C18NZM5K9\">#sig-apps</a> channel on Slack, or through the SIG Apps mailing list <a href=\"https://groups.google.com/g/kubernetes-sig-apps\">kubernetes-sig-apps@googlegroups.com</a>.</p>","PublishedAt":"2022-09-15 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/","SourceName":"Kubernetes"}},{"node":{"ID":1646,"Title":"Blog: Kubernetes 1.25: PodHasNetwork condition for pods","Description":"<p><strong>Author:</strong>\nDeep Debroy (Apple)</p>\n<p>Kubernetes 1.25 introduces Alpha support for a new kubelet-managed pod condition\nin the status field of a pod: <code>PodHasNetwork</code>. The kubelet, for a worker node,\nwill use the <code>PodHasNetwork</code> condition to accurately surface the initialization\nstate of a pod from the perspective of pod sandbox creation and network\nconfiguration by a container runtime (typically in coordination with CNI\nplugins). The kubelet starts to pull container images and start individual\ncontainers (including init containers) after the status of the <code>PodHasNetwork</code>\ncondition is set to <code>True</code>. Metrics collection services that report latency of\npod initialization from a cluster infrastructural perspective (i.e. agnostic of\nper container characteristics like image size or payload) can utilize the\n<code>PodHasNetwork</code> condition to accurately generate Service Level Indicators\n(SLIs). Certain operators or controllers that manage underlying pods may utilize\nthe <code>PodHasNetwork</code> condition to optimize the set of actions performed when pods\nrepeatedly fail to come up.</p>\n<h3 id=\"how-is-this-different-from-the-existing-initialized-condition-reported-for-pods\">How is this different from the existing Initialized condition reported for pods?</h3>\n<p>The kubelet sets the status of the existing <code>Initialized</code> condition reported in\nthe status field of a pod depending on the presence of init containers in a pod.</p>\n<p>If a pod specifies init containers, the status of the <code>Initialized</code> condition in\nthe pod status will not be set to <code>True</code> until all init containers for the pod\nhave succeeded. However, init containers, configured by users, may have errors\n(payload crashing, invalid image, etc) and the number of init containers\nconfigured in a pod may vary across different workloads. Therefore,\ncluster-wide, infrastructural SLIs around pod initialization cannot depend on\nthe <code>Initialized</code> condition of pods.</p>\n<p>If a pod does not specify init containers, the status of the <code>Initialized</code>\ncondition in the pod status is set to <code>True</code> very early in the lifecycle of the\npod. This occurs before the kubelet initiates any pod runtime sandbox creation\nand network configuration steps. As a result, a pod without init containers will\nreport the status of the <code>Initialized</code> condition as <code>True</code> even if the container\nruntime is not able to successfully initialize the pod sandbox environment.</p>\n<p>Relative to either situation above, the <code>PodHasNetwork</code> condition surfaces more\naccurate data around when the pod runtime sandbox was initialized with\nnetworking configured so that the kubelet can proceed to launch user-configured\ncontainers (including init containers) in the pod.</p>\n<p>Note that a node agent may dynamically re-configure network interface(s) for a\npod by watching changes in pod annotations that specify additional networking\nconfiguration (e.g. <code>k8s.v1.cni.cncf.io/networks</code>). Dynamic updates of pod\nnetworking configuration after the pod sandbox is initialized by Kubelet (in\ncoordination with a container runtime) are not reflected by the <code>PodHasNetwork</code>\ncondition.</p>\n<h3 id=\"try-out-the-podhasnetwork-condition-for-pods\">Try out the PodHasNetwork condition for pods</h3>\n<p>In order to have the kubelet report the <code>PodHasNetwork</code> condition in the status\nfield of a pod, please enable the <code>PodHasNetworkCondition</code> feature gate on the\nkubelet.</p>\n<p>For a pod whose runtime sandbox has been successfully created and has networking\nconfigured, the kubelet will report the <code>PodHasNetwork</code> condition with status set to <code>True</code>:</p>\n<pre tabindex=\"0\"><code>$ kubectl describe pod nginx1\nName: nginx1\nNamespace: default\n...\nConditions:\nType Status\nPodHasNetwork True\nInitialized True\nReady True\nContainersReady True\nPodScheduled True\n</code></pre><p>For a pod whose runtime sandbox has not been created yet (and networking not\nconfigured either), the kubelet will report the <code>PodHasNetwork</code> condition with\nstatus set to <code>False</code>:</p>\n<pre tabindex=\"0\"><code>$ kubectl describe pod nginx2\nName: nginx2\nNamespace: default\n...\nConditions:\nType Status\nPodHasNetwork False\nInitialized True\nReady False\nContainersReady False\nPodScheduled True\n</code></pre><h3 id=\"what-s-next\">What‚Äôs next?</h3>\n<p>Depending on feedback and adoption, the Kubernetes team plans to push the\nreporting of the <code>PodHasNetwork</code> condition to Beta in 1.26 or 1.27.</p>\n<h3 id=\"how-can-i-learn-more\">How can I learn more?</h3>\n<p>Please check out the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/\">documentation</a> for the\n<code>PodHasNetwork</code> condition to learn more about it and how it fits in relation to\nother pod conditions.</p>\n<h3 id=\"how-to-get-involved\">How to get involved?</h3>\n<p>This feature is driven by the SIG Node community. Please join us to connect with\nthe community and share your ideas and feedback around the above feature and\nbeyond. We look forward to hearing from you!</p>\n<h3 id=\"acknowledgements\">Acknowledgements</h3>\n<p>We want to thank the following people for their insightful and helpful reviews\nof the KEP and PRs around this feature: Derek Carr (@derekwaynecarr), Mrunal\nPatel (@mrunalp), Dawn Chen (@dchen1107), Qiutong Song (@qiutongs), Ruiwen Zhao\n(@ruiwen-zhao), Tim Bannister (@sftim), Danielle Lancashire (@endocrimes) and\nAgam Dua (@agamdua).</p>","PublishedAt":"2022-09-14 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/","SourceName":"Kubernetes"}},{"node":{"ID":1617,"Title":"Blog: Announcing the Auto-refreshing Official Kubernetes CVE Feed","Description":"<p><strong>Author</strong>: Pushkar Joglekar (VMware)</p>\n<p>A long-standing request from the Kubernetes community has been to have a\nprogrammatic way for end users to keep track of Kubernetes security issues\n(also called &quot;CVEs&quot;, after the database that tracks public security issues across\ndifferent products and vendors). Accompanying the release of Kubernetes v1.25,\nwe are excited to announce availability of such\na <a href=\"https://kubernetes.io/docs/reference/issues-security/official-cve-feed/\">feed</a> as an <code>alpha</code>\nfeature. This blog will cover the background and scope of this new service.</p>\n<h2 id=\"motivation\">Motivation</h2>\n<p>With the growing number of eyes on Kubernetes, the number of CVEs related to\nKubernetes have increased. Although most CVEs that directly, indirectly, or\ntransitively impact Kubernetes are regularly fixed, there is no single place for\nthe end users of Kubernetes to programmatically subscribe or pull the data of\nfixed CVEs. Current options are either broken or incomplete.</p>\n<h2 id=\"scope\">Scope</h2>\n<h3 id=\"what-this-does\">What This Does</h3>\n<p>Create a periodically auto-refreshing, human and machine-readable list of\nofficial Kubernetes CVEs</p>\n<h3 id=\"what-this-doesn-t-do\">What This Doesn't Do</h3>\n<ul>\n<li>Triage and vulnerability disclosure will continue to be done by SRC (Security\nResponse Committee).</li>\n<li>Listing CVEs that are identified in build time dependencies and container\nimages are out of scope.</li>\n<li>Only official CVEs announced by the Kubernetes SRC will be published in the\nfeed.</li>\n</ul>\n<h3 id=\"who-it-s-for\">Who It's For</h3>\n<ul>\n<li><strong>End Users</strong>: Persons or teams who <em>use</em> Kubernetes to deploy applications\nthey own</li>\n<li><strong>Platform Providers</strong>: Persons or teams who <em>manage</em> Kubernetes clusters</li>\n<li><strong>Maintainers</strong>: Persons or teams who <em>create</em> and <em>support</em> Kubernetes\nreleases through their work in Kubernetes Community - via various Special\nInterest Groups and Committees.</li>\n</ul>\n<h2 id=\"what-s-next\">What's Next?</h2>\n<p>In order to graduate this feature, SIG Security\nis gathering feedback from end users who are using this alpha feed.</p>\n<p>So in order to improve the feed in future Kubernetes Releases, if you have any\nfeedback, please let us know by adding a comment to\nthis <a href=\"https://github.com/kubernetes/sig-security/issues/1\">tracking issue</a> or\nlet us know on\n<a href=\"https://kubernetes.slack.com/archives/C01CUSVMHPY\">#sig-security-tooling</a>\nKubernetes Slack channel.\n(Join <a href=\"https://slack.k8s.io\">Kubernetes Slack here</a>)</p>\n<p><em>A special shout out and massive thanks to Neha Lohia\n<a href=\"https://github.com/nehalohia27\">(@nehalohia27)</a> and Tim\nBannister <a href=\"https://github.com/sftim\">(@sftim)</a> for their stellar collaboration\nfor many months from &quot;ideation to implementation&quot; of this feature.</em></p>","PublishedAt":"2022-09-12 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":1607,"Title":"Blog: Kubernetes 1.25: KMS V2 Improvements","Description":"<p><strong>Authors:</strong> Anish Ramasekar, Rita Zhang, Mo Khan, and Xander Grzywinski (Microsoft)</p>\n<p>With Kubernetes v1.25, SIG Auth is introducing a new <code>v2alpha1</code> version of the Key Management Service (KMS) API. There are a lot of improvements in the works, and we're excited to be able to start down the path of a new and improved KMS!</p>\n<h2 id=\"what-is-kms\">What is KMS?</h2>\n<p>One of the first things to consider when securing a Kubernetes cluster is encrypting persisted API data at rest. KMS provides an interface for a provider to utilize a key stored in an external key service to perform this encryption.</p>\n<p>Encryption at rest using KMS v1 has been a feature of Kubernetes since version v1.10, and is currently in beta as of version v1.12.</p>\n<h2 id=\"what-s-new-in-v2alpha1\">What‚Äôs new in <code>v2alpha1</code>?</h2>\n<p>While the original v1 implementation has been successful in helping Kubernetes users encrypt etcd data, it did fall short in a few key ways:</p>\n<ol>\n<li><strong>Performance:</strong> When starting a cluster, all resources are serially fetched and decrypted to fill the <code>kube-apiserver</code> cache. When using a KMS plugin, this can cause slow startup times due to the large number of requests made to the remote vault. In addition, there is the potential to hit API rate limits on external key services depending on how many encrypted resources exist in the cluster.</li>\n<li><strong>Key Rotation:</strong> With KMS v1, rotation of a key-encrypting key is a manual and error-prone process. It can be difficult to determine what encryption keys are in-use on a cluster.</li>\n<li><strong>Health Check &amp; Status:</strong> Before the KMS v2 API, the <code>kube-apiserver</code> was forced to make encrypt and decrypt calls as a proxy to determine if the KMS plugin is healthy. With cloud services these operations usually cost actual money with cloud service. Whatever the cost, those operations on their own do not provide a holistic view of the service's health.</li>\n<li><strong>Observability:</strong> Without some kind of trace ID, it's has been difficult to correlate events found in the various logs across <code>kube-apiserver</code>, KMS, and KMS plugins.</li>\n</ol>\n<p>The KMS v2 enhancement attempts to address all of these shortcomings, though not all planned features are implemented in the initial alpha release. Here are the improvements that arrived in Kubernetes v1.25:</p>\n<ol>\n<li>Support for KMS plugins that use a key hierarchy to reduce network requests made to the remote vault. To learn more, check out the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#key-hierachy\">design details for how a KMS plugin can leverage key hierarchy</a>.</li>\n<li>Extra metadata is now tracked to allow a KMS plugin to communicate what key it is currently using with the <code>kube-apiserver</code>, allowing for rotation without API server restart. Data stored in etcd follows a more standard proto format to allow external tools to observe its state. To learn more, check out the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#metadata\">details for metadata</a>.</li>\n<li>A dedicated status API is used to communicate the health of the KMS plugin with the API server. To learn more, check out the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#status-api\">details for status API</a>.</li>\n<li>To improve observability, a new <code>UID</code> field is included in <code>EncryptRequest</code> and <code>DecryptRequest</code> of the v2 API. The UID is generated for each envelope operation. To learn more, check out the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#Observability\">details for observability</a>.</li>\n</ol>\n<h3 id=\"sequence-diagram\">Sequence Diagram</h3>\n<h4 id=\"encrypt-request\">Encrypt Request</h4>\n<p><a href=\"https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O\"><img src=\"https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O\" alt=\"\"></a></p>\n<h4 id=\"decrypt-request\">Decrypt Request</h4>\n<p><a href=\"https://mermaid.live/edit#pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk\"><img src=\"https://mermaid.ink/img/pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk\" alt=\"\"></a></p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>For Kubernetes v1.26, we expect to ship another alpha version. As of right now, the alpha API will be ready to be used by KMS plugin authors. We hope to include a reference plugin implementation with the next release, and you'll be able to try out the feature at that time.</p>\n<p>You can learn more about KMS v2 by reading <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/\">Using a KMS provider for data encryption</a>. You can also follow along on the <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/3299-kms-v2-improvements/#readme\">KEP</a> to track progress across the coming Kubernetes releases.</p>\n<h2 id=\"how-to-get-involved\">How to get involved</h2>\n<p>If you are interested in getting involved in the development of this feature or would like to share feedback, please reach out on the <a href=\"https://kubernetes.slack.com/archives/C03035EH4VB\">#sig-auth-kms-dev</a> channel on Kubernetes Slack.</p>\n<p>You are also welcome to join the bi-weekly <a href=\"https://github.com/kubernetes/community/blob/master/sig-auth/README.md#meetings\">SIG Auth meetings</a>, held every-other Wednesday.</p>\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n<p>This feature has been an effort driven by contributors from several different companies. We would like to extend a huge thank you to everyone that contributed their time and effort to help make this possible.</p>","PublishedAt":"2022-09-09 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/","SourceName":"Kubernetes"}},{"node":{"ID":1586,"Title":"Blog: Kubernetes‚Äôs IPTables Chains Are Not API","Description":"<p><strong>Author:</strong> Dan Winship (Red Hat)</p>\n<p>Some Kubernetes components (such as kubelet and kube-proxy) create\niptables chains and rules as part of their operation. These chains\nwere never intended to be part of any Kubernetes API/ABI guarantees,\nbut some external components nonetheless make use of some of them (in\nparticular, using <code>KUBE-MARK-MASQ</code> to mark packets as needing to be\nmasqueraded).</p>\n<p>As a part of the v1.25 release, SIG Network made this declaration\nexplicit: that (with one exception), the iptables chains that\nKubernetes creates are intended only for Kubernetes‚Äôs own internal\nuse, and third-party components should not assume that Kubernetes will\ncreate any specific iptables chains, or that those chains will contain\nany specific rules if they do exist.</p>\n<p>Then, in future releases, as part of <a href=\"https://github.com/kubernetes/enhancements/issues/3178\">KEP-3178</a>, we will begin phasing\nout certain chains that Kubernetes itself no longer needs. Components\noutside of Kubernetes itself that make use of <code>KUBE-MARK-MASQ</code>,\n<code>KUBE-MARK-DROP</code>, or other Kubernetes-generated iptables chains should\nstart migrating away from them now.</p>\n<h2 id=\"background\">Background</h2>\n<p>In addition to various service-specific iptables chains, kube-proxy\ncreates certain general-purpose iptables chains that it uses as part\nof service proxying. In the past, kubelet also used iptables for a few\nfeatures (such as setting up <code>hostPort</code> mapping for pods) and so it\nalso redundantly created some of the same chains.</p>\n<p>However, with <a href=\"https://kubernetes.io/blog/2022/02/17/dockershim-faq/\">the removal of dockershim</a> in Kubernetes in 1.24,\nkubelet now no longer ever uses any iptables rules for its own\npurposes; the things that it used to use iptables for are now always\nthe responsibility of the container runtime or the network plugin, and\nthere is no reason for kubelet to be creating any iptables rules.</p>\n<p>Meanwhile, although <code>iptables</code> is still the default kube-proxy backend\non Linux, it is unlikely to remain the default forever, since the\nassociated command-line tools and kernel APIs are essentially\ndeprecated, and no longer receiving improvements. (RHEL 9\n<a href=\"https://access.redhat.com/solutions/6739041\">logs a warning</a> if you use the iptables API, even via\n<code>iptables-nft</code>.)</p>\n<p>Although as of Kubernetes 1.25 iptables kube-proxy remains popular,\nand kubelet continues to create the iptables rules that it\nhistorically created (despite no longer <em>using</em> them), third party\nsoftware cannot assume that core Kubernetes components will keep\ncreating these rules in the future.</p>\n<h2 id=\"upcoming-changes\">Upcoming changes</h2>\n<p>Starting a few releases from now, kubelet will no longer create the\nfollowing iptables chains in the <code>nat</code> table:</p>\n<ul>\n<li><code>KUBE-MARK-DROP</code></li>\n<li><code>KUBE-MARK-MASQ</code></li>\n<li><code>KUBE-POSTROUTING</code></li>\n</ul>\n<p>Additionally, the <code>KUBE-FIREWALL</code> chain in the <code>filter</code> table will no\nlonger have the functionality currently associated with\n<code>KUBE-MARK-DROP</code> (and it may eventually go away entirely).</p>\n<p>This change will be phased in via the <code>IPTablesOwnershipCleanup</code>\nfeature gate. That feature gate is available and can be manually\nenabled for testing in Kubernetes 1.25. The current plan is that it\nwill become enabled-by-default in Kubernetes 1.27, though this may be\ndelayed to a later release. (It will not happen sooner than Kubernetes\n1.27.)</p>\n<h2 id=\"what-to-do-if-you-use-kubernetes-s-iptables-chains\">What to do if you use Kubernetes‚Äôs iptables chains</h2>\n<p>(Although the discussion below focuses on short-term fixes that are\nstill based on iptables, you should probably also start thinking about\neventually migrating to nftables or another API).</p>\n<h3 id=\"use-case-kube-mark-masq\">If you use <code>KUBE-MARK-MASQ</code>...</h3>\n<p>If you are making use of the <code>KUBE-MARK-MASQ</code> chain to cause packets\nto be masqueraded, you have two options: (1) rewrite your rules to use\n<code>-j MASQUERADE</code> directly, (2) create your own alternative ‚Äúmark for\nmasquerade‚Äù chain.</p>\n<p>The reason kube-proxy uses <code>KUBE-MARK-MASQ</code> is because there are lots\nof cases where it needs to call both <code>-j DNAT</code> and <code>-j MASQUERADE</code> on\na packet, but it‚Äôs not possible to do both of those at the same time\nin iptables; <code>DNAT</code> must be called from the <code>PREROUTING</code> (or <code>OUTPUT</code>)\nchain (because it potentially changes where the packet will be routed\nto) while <code>MASQUERADE</code> must be called from <code>POSTROUTING</code> (because the\nmasqueraded source IP that it picks depends on what the final routing\ndecision was).</p>\n<p>In theory, kube-proxy could have one set of rules to match packets in\n<code>PREROUTING</code>/<code>OUTPUT</code> and call <code>-j DNAT</code>, and then have a second set\nof rules to match the same packets in <code>POSTROUTING</code> and call <code>-j MASQUERADE</code>. But instead, for efficiency, it only matches them once,\nduring <code>PREROUTING</code>/<code>OUTPUT</code>, at which point it calls <code>-j DNAT</code> and\nthen calls <code>-j KUBE-MARK-MASQ</code> to set a bit on the kernel packet mark\nas a reminder to itself. Then later, during <code>POSTROUTING</code>, it has a\nsingle rule that matches all previously-marked packets, and calls <code>-j MASQUERADE</code> on them.</p>\n<p>If you have <em>a lot</em> of rules where you need to apply both DNAT and\nmasquerading to the same packets like kube-proxy does, then you may\nwant a similar arrangement. But in many cases, components that use\n<code>KUBE-MARK-MASQ</code> are only doing it because they copied kube-proxy‚Äôs\nbehavior without understanding why kube-proxy was doing it that way.\nMany of these components could easily be rewritten to just use\nseparate DNAT and masquerade rules. (In cases where no DNAT is\noccurring then there is even less point to using <code>KUBE-MARK-MASQ</code>;\njust move your rules from <code>PREROUTING</code> to <code>POSTROUTING</code> and call <code>-j MASQUERADE</code> directly.)</p>\n<h3 id=\"use-case-kube-mark-drop\">If you use <code>KUBE-MARK-DROP</code>...</h3>\n<p>The rationale for <code>KUBE-MARK-DROP</code> is similar to the rationale for\n<code>KUBE-MARK-MASQ</code>: kube-proxy wanted to make packet-dropping decisions\nalongside other decisions in the <code>nat</code> <code>KUBE-SERVICES</code> chain, but you\ncan only call <code>-j DROP</code> from the <code>filter</code> table. So instead, it uses\n<code>KUBE-MARK-DROP</code> to mark packets to be dropped later on.</p>\n<p>In general, the approach for removing a dependency on <code>KUBE-MARK-DROP</code>\nis the same as for removing a dependency on <code>KUBE-MARK-MASQ</code>. In\nkube-proxy‚Äôs case, it is actually quite easy to replace the usage of\n<code>KUBE-MARK-DROP</code> in the <code>nat</code> table with direct calls to <code>DROP</code> in the\n<code>filter</code> table, because there are no complicated interactions between\nDNAT rules and drop rules, and so the drop rules can simply be moved\nfrom <code>nat</code> to <code>filter</code>.</p>\n<p>In more complicated cases, it might be necessary to ‚Äúre-match‚Äù the\nsame packets in both <code>nat</code> and <code>filter</code>.</p>\n<h3 id=\"use-case-iptables-mode\">If you use Kubelet‚Äôs iptables rules to figure out <code>iptables-legacy</code> vs <code>iptables-nft</code>...</h3>\n<p>Components that manipulate host-network-namespace iptables rules from\ninside a container need some way to figure out whether the host is\nusing the old <code>iptables-legacy</code> binaries or the newer <code>iptables-nft</code>\nbinaries (which talk to a different kernel API underneath).</p>\n<p>The <a href=\"https://github.com/kubernetes-sigs/iptables-wrappers/\"><code>iptables-wrappers</code></a> module provides a way for such components to\nautodetect the system iptables mode, but in the past it did this by\nassuming that Kubelet will have created ‚Äúa bunch‚Äù of iptables rules\nbefore any containers start, and so it can guess which mode the\niptables binaries in the host filesystem are using by seeing which\nmode has more rules defined.</p>\n<p>In future releases, Kubelet will no longer create many iptables rules,\nso heuristics based on counting the number of rules present may fail.</p>\n<p>However, as of 1.24, Kubelet always creates a chain named\n<code>KUBE-IPTABLES-HINT</code> in the <code>mangle</code> table of whichever iptables\nsubsystem it is using. Components can now look for this specific chain\nto know which iptables subsystem Kubelet (and thus, presumably, the\nrest of the system) is using.</p>\n<p>(Additionally, since Kubernetes 1.17, kubelet has created a chain\ncalled <code>KUBE-KUBELET-CANARY</code> in the <code>mangle</code> table. While this chain\nmay go away in the future, it will of course still be there in older\nreleases, so in any recent version of Kubernetes, at least one of\n<code>KUBE-IPTABLES-HINT</code> or <code>KUBE-KUBELET-CANARY</code> will be present.)</p>\n<p>The <code>iptables-wrappers</code> package has <a href=\"https://github.com/kubernetes-sigs/iptables-wrappers/pull/3\">already been updated</a> with this new\nheuristic, so if you were previously using that, you can rebuild your\ncontainer images with an updated version of that.</p>\n<h2 id=\"further-reading\">Further reading</h2>\n<p>The project to clean up iptables chain ownership and deprecate the old\nchains is tracked by <a href=\"https://github.com/kubernetes/enhancements/issues/3178\">KEP-3178</a>.</p>","PublishedAt":"2022-09-07 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/","SourceName":"Kubernetes"}},{"node":{"ID":1554,"Title":"Blog: Introducing COSI: Object Storage Management using Kubernetes APIs","Description":"<p><strong>Authors:</strong> Sidhartha Mani (<a href=\"https://min.io\">Minio, Inc</a>)</p>\n<p>This article introduces the Container Object Storage Interface (COSI), a standard for provisioning and consuming object storage in Kubernetes. It is an alpha feature in Kubernetes v1.25.</p>\n<p>File and block storage are treated as first class citizens in the Kubernetes ecosystem via <a href=\"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/\">Container Storage Interface</a> (CSI). Workloads using CSI volumes enjoy the benefits of portability across vendors and across Kubernetes clusters without the need to change application manifests. An equivalent standard does not exist for Object storage.</p>\n<p>Object storage has been rising in popularity in recent years as an alternative form of storage to filesystems and block devices. Object storage paradigm promotes disaggregation of compute and storage. This is done by making data available over the network, rather than locally. Disaggregated architectures allow compute workloads to be stateless, which consequently makes them easier to manage, scale and automate.</p>\n<h2 id=\"cosi\">COSI</h2>\n<p>COSI aims to standardize consumption of object storage to provide the following benefits:</p>\n<ul>\n<li>Kubernetes Native - Use the Kubernetes API to provision, configure and manage buckets</li>\n<li>Self Service - A clear delineation between administration and operations (DevOps) to enable self-service capability for DevOps personnel</li>\n<li>Portability - Vendor neutrality enabled through portability across Kubernetes Clusters and across Object Storage vendors</li>\n</ul>\n<p><em>Portability across vendors is only possible when both vendors support a common datapath-API. Eg. it is possible to port from AWS S3 to Ceph, or AWS S3 to MinIO and back as they all use S3 API. In contrast, it is not possible to port from AWS S3 and Google Cloud‚Äôs GCS or vice versa.</em></p>\n<h2 id=\"architecture\">Architecture</h2>\n<p>COSI is made up of three components:</p>\n<ul>\n<li>COSI Controller Manager</li>\n<li>COSI Sidecar</li>\n<li>COSI Driver</li>\n</ul>\n<p>The COSI Controller Manager acts as the main controller that processes changes to COSI API objects. It is responsible for fielding requests for bucket creation, updates, deletion and access management. One instance of the controller manager is required per kubernetes cluster. Only one is needed even if multiple object storage providers are used in the cluster.</p>\n<p>The COSI Sidecar acts as a translator between COSI API requests and vendor-specific COSI Drivers. This component uses a standardized gRPC protocol that vendor drivers are expected to satisfy.</p>\n<p>The COSI Driver is the vendor specific component that receives requests from the sidecar and calls the appropriate vendor APIs to create buckets, manage their lifecycle and manage access to them.</p>\n<h2 id=\"api\">API</h2>\n<p>The COSI API is centered around buckets, since bucket is the unit abstraction for object storage. COSI defines three Kubernetes APIs aimed at managing them</p>\n<ul>\n<li>Bucket</li>\n<li>BucketClass</li>\n<li>BucketClaim</li>\n</ul>\n<p>In addition, two more APIs for managing access to buckets are also defined:</p>\n<ul>\n<li>BucketAccess</li>\n<li>BucketAccessClass</li>\n</ul>\n<p>In a nutshell, Bucket and BucketClaim can be considered to be similar to PersistentVolume and PersistentVolumeClaim respectively. The BucketClass‚Äô counterpart in the file/block device world is StorageClass.</p>\n<p>Since Object Storage is always authenticated, and over the network, access credentials are required to access buckets. The two APIs, namely, BucketAccess and BucketAccessClass are used to denote access credentials and policies for authentication. More info about these APIs can be found in the official COSI proposal - <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support\">https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support</a></p>\n<h2 id=\"self-service\">Self-Service</h2>\n<p>Other than providing kubernetes-API driven bucket management, COSI also aims to empower DevOps personnel to provision and manage buckets on their own, without admin intervention. This, further enabling dev teams to realize faster turn-around times and faster time-to-market.</p>\n<p>COSI achieves this by dividing bucket provisioning steps among two different stakeholders, namely the administrator (admin), and the cluster operator. The administrator will be responsible for setting broad policies and limits on how buckets are provisioned, and how access is obtained for them. The cluster operator will be free to create and utilize buckets within the limits set by the admin.</p>\n<p>For example, a cluster operator could use an admin policy could be used to restrict maximum provisioned capacity to 100GB, and developers would be allowed to create buckets and store data upto that limit. Similarly for access credentials, admins would be able to restrict who can access which buckets, and developers would be able to access all the buckets available to them.</p>\n<h2 id=\"portability\">Portability</h2>\n<p>The third goal of COSI is to achieve vendor neutrality for bucket management. COSI enables two kinds of portability:</p>\n<ul>\n<li>Cross Cluster</li>\n<li>Cross Provider</li>\n</ul>\n<p>Cross Cluster portability is allowing buckets provisioned in one cluster to be available in another cluster. This is only valid when the object storage backend itself is accessible from both clusters.</p>\n<p>Cross-provider portability is about allowing organizations or teams to move from one object storage provider to another seamlessly, and without requiring changes to application definitions (PodTemplates, StatefulSets, Deployment and so on). This is only possible if the source and destination providers use the same data.</p>\n<p><em>COSI does not handle data migration as it is outside of its scope. In case porting between providers requires data to be migrated as well, then other measures need to be taken to ensure data availability.</em></p>\n<h2 id=\"what-s-next\">What‚Äôs next</h2>\n<p>The amazing sig-storage-cosi community has worked hard to bring the COSI standard to alpha status. We are looking forward to onboarding a lot of vendors to write COSI drivers and become COSI compatible!</p>\n<p>We want to add more authentication mechanisms for COSI buckets, we are designing advanced bucket sharing primitives, multi-cluster bucket management and much more. Lots of great ideas and opportunities ahead!</p>\n<p>Stay tuned for what comes next, and if you have any questions, comments or suggestions</p>\n<ul>\n<li>Chat with us on the Kubernetes <a href=\"https://kubernetes.slack.com/archives/C017EGC1C6N\">Slack:#sig-storage-cosi</a></li>\n<li>Join our <a href=\"https://zoom.us/j/614261834?pwd=Sk1USmtjR2t0MUdjTGVZeVVEV1BPQT09\">Zoom meeting</a>, every Thursday at 10:00 Pacific Time</li>\n<li>Participate in the <a href=\"https://github.com/kubernetes/enhancements/pull/2813\">bucket API proposal PR</a> to add your ideas, suggestions and more.</li>\n</ul>","PublishedAt":"2022-09-02 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/","SourceName":"Kubernetes"}},{"node":{"ID":1529,"Title":"Blog: Kubernetes 1.25: cgroup v2 graduates to GA","Description":"<p><strong>Authors:</strong>: David Porter (Google), Mrunal Patel (Red Hat)</p>\n<p>Kubernetes 1.25 brings cgroup v2 to GA (general availability), letting the\n<a href=\"https://kubernetes.io/docs/concepts/overview/components/#kubelet\">kubelet</a> use the latest container resource\nmanagement capabilities.</p>\n<h2 id=\"what-are-cgroups\">What are cgroups?</h2>\n<p>Effective <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\">resource management</a> is a\ncritical aspect of Kubernetes. This involves managing the finite resources in\nyour nodes, such as CPU, memory, and storage.</p>\n<p><em>cgroups</em> are a Linux kernel capability that establish resource management\nfunctionality like limiting CPU usage or setting memory limits for running\nprocesses.</p>\n<p>When you use the resource management capabilities in Kubernetes, such as configuring\n<a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits\">requests and limits for Pods and containers</a>,\nKubernetes uses cgroups to enforce your resource requests and limits.</p>\n<p>The Linux kernel offers two versions of cgroups: cgroup v1 and cgroup v2.</p>\n<h2 id=\"what-is-cgroup-v2\">What is cgroup v2?</h2>\n<p>cgroup v2 is the latest version of the Linux cgroup API. cgroup v2 provides a\nunified control system with enhanced resource management capabilities.</p>\n<p>cgroup v2 has been development in the Linux Kernel since 2016 and in recent\nyears has matured across the container ecosystem. With Kubernetes 1.25, cgroup\nv2 support has graduated to general availability.</p>\n<p>Many recent releases of Linux distributions have switched over to cgroup v2 by\ndefault so it's important that Kubernetes continues to work well on these new\nupdated distros.</p>\n<p>cgroup v2 offers several improvements over cgroup v1, such as the following:</p>\n<ul>\n<li>Single unified hierarchy design in API</li>\n<li>Safer sub-tree delegation to containers</li>\n<li>Newer features like <a href=\"https://www.kernel.org/doc/html/latest/accounting/psi.html\">Pressure Stall Information</a></li>\n<li>Enhanced resource allocation management and isolation across multiple resources\n<ul>\n<li>Unified accounting for different types of memory allocations (network and kernel memory, etc)</li>\n<li>Accounting for non-immediate resource changes such as page cache write backs</li>\n</ul>\n</li>\n</ul>\n<p>Some Kubernetes features exclusively use cgroup v2 for enhanced resource\nmanagement and isolation. For example,\nthe <a href=\"https://kubernetes.io/blog/2021/11/26/qos-memory-resources/\">MemoryQoS feature</a> improves\nmemory utilization and relies on cgroup v2 functionality to enable it. New\nresource management features in the kubelet will also take advantage of the new\ncgroup v2 features moving forward.</p>\n<h2 id=\"how-do-you-use-cgroup-v2\">How do you use cgroup v2?</h2>\n<p>Many Linux distributions are switching to cgroup v2 by default; you might start\nusing it the next time you update the Linux version of your control plane and\nnodes!</p>\n<p>Using a Linux distribution that uses cgroup v2 by default is the recommended\nmethod. Some of the popular Linux distributions that use cgroup v2 include the\nfollowing:</p>\n<ul>\n<li>Container Optimized OS (since M97)</li>\n<li>Ubuntu (since 21.10)</li>\n<li>Debian GNU/Linux (since Debian 11 Bullseye)</li>\n<li>Fedora (since 31)</li>\n<li>Arch Linux (since April 2021)</li>\n<li>RHEL and RHEL-like distributions (since 9)</li>\n</ul>\n<p>To check if your distribution uses cgroup v2 by default,\nrefer to <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/#check-cgroup-version\">Check your cgroup version</a> or\nconsult your distribution's documentation.</p>\n<p>If you're using a managed Kubernetes offering, consult your provider to\ndetermine how they're adopting cgroup v2, and whether you need to take action.</p>\n<p>To use cgroup v2 with Kubernetes, you must meet the following requirements:</p>\n<ul>\n<li>Your Linux distribution enables cgroup v2 on kernel version 5.8 or later</li>\n<li>Your container runtime supports cgroup v2. For example:\n<ul>\n<li><a href=\"https://containerd.io/\">containerd</a> v1.4 or later</li>\n<li><a href=\"https://cri-o.io/\">cri-o</a> v1.20 or later</li>\n</ul>\n</li>\n<li>The kubelet and the container runtime are configured to use the <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver\">systemd cgroup driver</a></li>\n</ul>\n<p>The kubelet and container runtime use a <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes#cgroup-drivers\">cgroup driver</a>\nto set cgroup paramaters. When using cgroup v2, it's strongly recommended that both\nthe kubelet and your container runtime use the\n<a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver\">systemd cgroup driver</a>,\nso that there's a single cgroup manager on the system. To configure the kubelet\nand the container runtime to use the driver, refer to the\n<a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver\">systemd cgroup driver documentation</a>.</p>\n<h2 id=\"migrate-to-cgroup-v2\">Migrate to cgroup v2</h2>\n<p>When you run Kubernetes with a Linux distribution that enables cgroup v2, the\nkubelet should automatically adapt without any additional configuration\nrequired, as long as you meet the requirements.</p>\n<p>In most cases, you won't see a difference in the user experience when you\nswitch to using cgroup v2 unless your users access the cgroup file system\ndirectly.</p>\n<p>If you have applications that access the cgroup file system directly, either on\nthe node or from inside a container, you must update the applications to use\nthe cgroup v2 API instead of the cgroup v1 API.</p>\n<p>Scenarios in which you might need to update to cgroup v2 include the following:</p>\n<ul>\n<li>If you run third-party monitoring and security agents that depend on the cgroup file system, update the\nagents to versions that support cgroup v2.</li>\n<li>If you run <a href=\"https://github.com/google/cadvisor\">cAdvisor</a> as a stand-alone\nDaemonSet for monitoring pods and containers, update it to v0.43.0 or later.</li>\n<li>If you deploy Java applications with the JDK, prefer to use JDK 11.0.16 and\nlater or JDK 15 and later, which <a href=\"https://bugs.openjdk.org/browse/JDK-8230305\">fully support cgroup v2</a>.</li>\n</ul>\n<h2 id=\"learn-more\">Learn more</h2>\n<ul>\n<li>Read the <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">Kubernetes cgroup v2 documentation</a></li>\n<li>Read the enhancement proposal, <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2254-cgroup-v2/README.md\">KEP 2254</a></li>\n<li>Learn more about\n<a href=\"https://man7.org/linux/man-pages/man7/cgroups.7.html\">cgroups</a> on Linux Manual Pages\nand <a href=\"https://docs.kernel.org/admin-guide/cgroup-v2.html\">cgroup v2</a> on the Linux Kernel documentation</li>\n</ul>\n<h2 id=\"get-involved\">Get involved</h2>\n<p>Your feedback is always welcome! SIG Node meets regularly and are available in\nthe <code>#sig-node</code> channel in the Kubernetes <a href=\"https://slack.k8s.io/\">Slack</a>, or\nusing the SIG <a href=\"https://github.com/kubernetes/community/tree/master/sig-node#contact\">mailing list</a>.</p>\n<p>cgroup v2 has had a long journey and is a great example of open source\ncommunity collaboration across the industry because it required work across the\nstack, from the Linux Kernel to systemd to various container runtimes, and (of\ncourse) Kubernetes.</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>We would like to thank <a href=\"https://github.com/giuseppe\">Giuseppe Scrivano</a> who\ninitiated cgroup v2 support in Kubernetes, and reviews and leadership from the\nSIG Node community including chairs <a href=\"https://github.com/dchen1107\">Dawn Chen</a>\nand <a href=\"https://github.com/derekwaynecarr\">Derek Carr</a>.</p>\n<p>We'd also like to thank the maintainers of container runtimes like Docker,\ncontainerd and CRI-O, and the maintainers of components like\n<a href=\"https://github.com/google/cadvisor\">cAdvisor</a>\nand <a href=\"https://github.com/opencontainers/runc\">runc, libcontainer</a>,\nwhich underpin many container runtimes. Finally, this wouldn't have been\npossible without support from systemd and upstream Linux Kernel maintainers.</p>\n<p>It's a team effort!</p>","PublishedAt":"2022-08-31 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/","SourceName":"Kubernetes"}},{"node":{"ID":1472,"Title":"Blog: Kubernetes 1.25: CSI Inline Volumes have graduated to GA","Description":"<p><strong>Author:</strong> Jonathan Dobson (Red Hat)</p>\n<p>CSI Inline Volumes were introduced as an alpha feature in Kubernetes 1.15 and have been beta since 1.16. We are happy to announce that this feature has graduated to General Availability (GA) status in Kubernetes 1.25.</p>\n<p>CSI Inline Volumes are similar to other ephemeral volume types, such as <code>configMap</code>, <code>downwardAPI</code> and <code>secret</code>. The important difference is that the storage is provided by a CSI driver, which allows the use of ephemeral storage provided by third-party vendors. The volume is defined as part of the pod spec and follows the lifecycle of the pod, meaning the volume is created once the pod is scheduled and destroyed when the pod is destroyed.</p>\n<h2 id=\"what-s-new-in-1-25\">What's new in 1.25?</h2>\n<p>There are a couple of new bug fixes related to this feature in 1.25, and the <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">CSIInlineVolume feature gate</a> has been locked to <code>True</code> with the graduation to GA. There are no new API changes, so users of this feature during beta should not notice any significant changes aside from these bug fixes.</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/89290\">#89290 - CSI inline volumes should support fsGroup</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/79980\">#79980 - CSI volume reconstruction does not work for ephemeral volumes</a></li>\n</ul>\n<h2 id=\"when-to-use-this-feature\">When to use this feature</h2>\n<p>CSI inline volumes are meant for simple local volumes that should follow the lifecycle of the pod. They may be useful for providing secrets, configuration data, or other special-purpose storage to the pod from a CSI driver.</p>\n<p>A CSI driver is not suitable for inline use when:</p>\n<ul>\n<li>The volume needs to persist longer than the lifecycle of a pod</li>\n<li>Volume snapshots, cloning, or volume expansion are required</li>\n<li>The CSI driver requires <code>volumeAttributes</code> that should be restricted to an administrator</li>\n</ul>\n<h2 id=\"how-to-use-this-feature\">How to use this feature</h2>\n<p>In order to use this feature, the <code>CSIDriver</code> spec must explicitly list <code>Ephemeral</code> as one of the supported <code>volumeLifecycleModes</code>. Here is a simple example from the <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver\">Secrets Store CSI Driver</a>.</p>\n<pre tabindex=\"0\"><code>apiVersion: storage.k8s.io/v1\nkind: CSIDriver\nmetadata:\nname: secrets-store.csi.k8s.io\nspec:\npodInfoOnMount: true\nattachRequired: false\nvolumeLifecycleModes:\n- Ephemeral\n</code></pre><p>Any pod spec may then reference that CSI driver to create an inline volume, as in this example.</p>\n<pre tabindex=\"0\"><code>kind: Pod\napiVersion: v1\nmetadata:\nname: my-csi-app-inline\nspec:\ncontainers:\n- name: my-frontend\nimage: busybox\nvolumeMounts:\n- name: secrets-store-inline\nmountPath: &#34;/mnt/secrets-store&#34;\nreadOnly: true\ncommand: [ &#34;sleep&#34;, &#34;1000000&#34; ]\nvolumes:\n- name: secrets-store-inline\ncsi:\ndriver: secrets-store.csi.k8s.io\nreadOnly: true\nvolumeAttributes:\nsecretProviderClass: &#34;my-provider&#34;\n</code></pre><p>If the driver supports any volume attributes, you can provide these as part of the <code>spec</code> for the Pod as well:</p>\n<pre tabindex=\"0\"><code> csi:\ndriver: block.csi.vendor.example\nvolumeAttributes:\nfoo: bar\n</code></pre><h2 id=\"example-use-cases\">Example Use Cases</h2>\n<p>Two existing CSI drivers that support the <code>Ephemeral</code> volume lifecycle mode are the Secrets Store CSI Driver and the Cert-Manager CSI Driver.</p>\n<p>The <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver\">Secrets Store CSI Driver</a> allows users to mount secrets from external secret stores into a pod as an inline volume. This can be useful when the secrets are stored in an external managed service or Vault instance.</p>\n<p>The <a href=\"https://github.com/cert-manager/csi-driver\">Cert-Manager CSI Driver</a> works along with <a href=\"https://cert-manager.io/\">cert-manager</a> to seamlessly request and mount certificate key pairs into a pod. This allows the certificates to be renewed and updated in the application pod automatically.</p>\n<h2 id=\"security-considerations\">Security Considerations</h2>\n<p>Special consideration should be given to which CSI drivers may be used as inline volumes. <code>volumeAttributes</code> are typically controlled through the <code>StorageClass</code>, and may contain attributes that should remain restricted to the cluster administrator. Allowing a CSI driver to be used for inline ephmeral volumes means that any user with permission to create pods may also provide <code>volumeAttributes</code> to the driver through a pod spec.</p>\n<p>Cluster administrators may choose to omit (or remove) <code>Ephemeral</code> from <code>volumeLifecycleModes</code> in the CSIDriver spec to prevent the driver from being used as an inline ephemeral volume, or use an <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\">admission webhook</a> to restrict how the driver is used.</p>\n<h2 id=\"references\">References</h2>\n<p>For more information on this feature, see:</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes\">Kubernetes documentation</a></li>\n<li><a href=\"https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html\">CSI documentation</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/596-csi-inline-volumes/README.md\">KEP-596</a></li>\n<li><a href=\"https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/\">Beta blog post for CSI Inline Volumes</a></li>\n</ul>","PublishedAt":"2022-08-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/","SourceName":"Kubernetes"}},{"node":{"ID":1463,"Title":"Blog: Kubernetes v1.25: Pod Security Admission Controller in Stable","Description":"<p><strong>Authors:</strong> Tim Allclair (Google), Sam Stoelinga (Google)</p>\n<p>The release of Kubernetes v1.25 marks a major milestone for Kubernetes out-of-the-box pod security\ncontrols: Pod Security admission (PSA) graduated to stable, and Pod Security Policy (PSP) has been\nremoved.\n<a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">PSP was deprecated in Kubernetes v1.21</a>,\nand no longer functions in Kubernetes v1.25 and later.</p>\n<p>The Pod Security admission controller replaces PodSecurityPolicy, making it easier to enforce predefined\n<a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a> by\nsimply adding a label to a namespace. The Pod Security Standards are maintained by the K8s\ncommunity, which means you automatically get updated security policies whenever new\nsecurity-impacting Kubernetes features are introduced.</p>\n<h2 id=\"what-s-new-since-beta\">What‚Äôs new since Beta?</h2>\n<p>Pod Security Admission hasn‚Äôt changed much since the Beta in Kubernetes v1.23. The focus has been on\nimproving the user experience, while continuing to maintain a high quality bar.</p>\n<h3 id=\"improved-violation-messages\">Improved violation messages</h3>\n<p>We improved violation messages so that you get\n<a href=\"https://github.com/kubernetes/kubernetes/pull/107698\">fewer duplicate messages</a>. For example,\ninstead of the following message when the Baseline and Restricted policies check the same\ncapability:</p>\n<pre tabindex=\"0\"><code>pods &#34;admin-pod&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: non-default capabilities (container &#34;admin&#34; must not include &#34;SYS_ADMIN&#34; in securityContext.capabilities.add), unrestricted capabilities (container &#34;admin&#34; must not include &#34;SYS_ADMIN&#34; in securityContext.capabilities.add)\n</code></pre><p>You get this message:</p>\n<pre tabindex=\"0\"><code>pods &#34;admin-pod&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: unrestricted capabilities (container &#34;admin&#34; must not include &#34;SYS_ADMIN&#34; in securityContext.capabilities.add)\n</code></pre><h3 id=\"improved-namespace-warnings\">Improved namespace warnings</h3>\n<p>When you modify the <code>enforce</code> Pod Security labels on a namespace, the Pod Security\nadmission controller checks all existing pods for\nviolations and surfaces a <a href=\"https://kubernetes.io/blog/2020/09/03/warnings/\">warning</a> if any are out of compliance. These\n<a href=\"https://github.com/kubernetes/kubernetes/pull/105889\">warnings are now aggregated</a> for pods with\nidentical violations, making large namespaces with many replicas much more manageable. For example:</p>\n<pre tabindex=\"0\"><code>Warning: frontend-h23gf2: allowPrivilegeEscalation != false\nWarning: myjob-g342hj (and 6 other pods): host namespaces, allowPrivilegeEscalation != false Warning: backend-j23h42 (and 1 other pod): non-default capabilities, unrestricted capabilities\n</code></pre><p>Additionally, when you apply a non-privileged label to a namespace that has been\n<a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/#exemptions\">configured to be exempt</a>,\nyou will now get a warning alerting you to this fact:</p>\n<pre tabindex=\"0\"><code>Warning: namespace &#39;kube-system&#39; is exempt from Pod Security, and the policy (enforce=baseline:latest) will be ignored\n</code></pre><h3 id=\"changes-to-the-pod-security-standards\">Changes to the Pod Security Standards</h3>\n<p>The <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a>,\nwhich Pod Security admission enforces, have been updated with support for the new Pod OS\nfield. In v1.25 and later, if you use the Restricted policy, the following Linux-specific restrictions will no\nlonger be required if you explicitly set the pod's <code>.spec.os.name</code> field to <code>windows</code>:</p>\n<ul>\n<li>Seccomp - The <code>seccompProfile.type</code> field for Pod and container security contexts</li>\n<li>Privilege escalation - The <code>allowPrivilegeEscalation</code> field on container security contexts</li>\n<li>Capabilities - The requirement to drop <code>ALL</code> capabilities in the <code>capabilities</code> field on containers</li>\n</ul>\n<p>In Kubernetes v1.23 and earlier, the kubelet didn't enforce the Pod OS field.\nIf your cluster includes nodes running a v1.23 or older kubelet, you should explicitly\n<a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces\">pin Restricted policies</a>\nto a version prior to v1.25.</p>\n<h2 id=\"migrating-from-podsecuritypolicy-to-the-pod-security-admission-controller\">Migrating from PodSecurityPolicy to the Pod Security admission controller</h2>\n<p>For instructions to migrate from PodSecurityPolicy to the Pod Security admission controller, and\nfor help choosing a migration strategy, refer to the\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/\">migration guide</a>.\nWe're also developing a tool called\n<a href=\"https://github.com/kubernetes-sigs/pspmigrator\">pspmigrator</a> to automate parts\nof the migration process.</p>\n<p>We'll be talking about PSP migration in more detail at our upcoming KubeCon 2022 NA talk,\n<a href=\"https://sched.co/182Jx\"><em>Migrating from Pod Security Policy</em></a>. Use the\n<a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/\">KubeCon NA schedule</a>\nto learn more.</p>","PublishedAt":"2022-08-25 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/","SourceName":"Kubernetes"}},{"node":{"ID":1444,"Title":"Blog: PodSecurityPolicy: The Historical Context","Description":"<p><strong>Author:</strong> Mah√© Tardy (Quarkslab)</p>\n<p>The PodSecurityPolicy (PSP) admission controller has been removed, as of\nKubernetes v1.25. Its deprecation was announced and detailed in the blog post\n<a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">PodSecurityPolicy Deprecation: Past, Present, and Future</a>,\npublished for the Kubernetes v1.21 release.</p>\n<p>This article aims to provide historical context on the birth and evolution of\nPSP, explain why the feature never made it to stable, and show why it was\nremoved and replaced by Pod Security admission control.</p>\n<p>PodSecurityPolicy, like other specialized admission control plugins, provided\nfine-grained permissions on specific fields concerning the pod security settings\nas a built-in policy API. It acknowledged that cluster administrators and\ncluster users are usually not the same people, and that creating workloads in\nthe form of a Pod or any resource that will create a Pod should not equal being\n&quot;root on the cluster&quot;. It could also encourage best practices by configuring\nmore secure defaults through mutation and decoupling low-level Linux security\ndecisions from the deployment process.</p>\n<h2 id=\"the-birth-of-podsecuritypolicy\">The birth of PodSecurityPolicy</h2>\n<p>PodSecurityPolicy originated from OpenShift's SecurityContextConstraints\n(SCC) that were in the very first release of the Red Hat OpenShift Container Platform,\neven before Kubernetes 1.0. PSP was a stripped-down version of the SCC.</p>\n<p>The origin of the creation of PodSecurityPolicy is difficult to track, notably\nbecause it was mainly added before Kubernetes Enhancements Proposal (KEP)\nprocess, when design proposals were still a thing. Indeed, the archive of the final\n<a href=\"https://github.com/kubernetes/design-proposals-archive/blob/main/auth/pod-security-policy.md\">design proposal</a>\nis still available. Nevertheless, a <a href=\"https://github.com/kubernetes/enhancements/issues/5\">KEP issue number five</a>\nwas created after the first pull requests were merged.</p>\n<p>Before adding the first piece of code that created PSP, two main pull\nrequests were merged into Kubernetes, a <a href=\"https://github.com/kubernetes/kubernetes/pull/7343\"><code>SecurityContext</code> subresource</a>\nthat defined new fields on pods' containers, and the first iteration of the <a href=\"https://github.com/kubernetes/kubernetes/pull/7101\">ServiceAccount</a>\nAPI.</p>\n<p>Kubernetes 1.0 was released on 10 July 2015 without any mechanism to restrict the\nsecurity context and sensitive options of workloads, other than an alpha-quality\nSecurityContextDeny admission plugin (then known as <code>scdeny</code>).\nThe <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny\">SecurityContextDeny plugin</a>\nis still in Kubernetes today (as an alpha feature) and creates an admission controller that\nprevents the usage of some fields in the security context.</p>\n<p>The roots of the PodSecurityPolicy were added with\n<a href=\"https://github.com/kubernetes/kubernetes/pull/7893\">the very first pull request on security policy</a>,\nwhich added the design proposal with the new PSP object, based on the SCC (Security Context Constraints). It\nwas a long discussion of nine months, with back and forth from OpenShift's SCC,\nmany rebases, and the rename to PodSecurityPolicy that finally made it to\nupstream Kubernetes in February 2016. Now that the PSP object\nhad been created, the next step was to add an admission controller that could enforce\nthese policies. The first step was to add the admission\n<a href=\"https://github.com/kubernetes/kubernetes/pull/7893#issuecomment-180410539\">without taking into account the users or groups</a>.\nA specific <a href=\"https://github.com/kubernetes/kubernetes/issues/23217\">issue to bring PodSecurityPolicy to a usable state</a>\nwas added to keep track of the progress and a first version of the admission\ncontroller was merged in <a href=\"https://github.com/kubernetes/kubernetes/pull/24600\">pull request named PSP admission</a>\nin May 2016. Then around two months later, Kubernetes 1.3 was released.</p>\n<p>Here is a timeline that recaps the main pull requests of the birth of the\nPodSecurityPolicy and its admission controller with 1.0 and 1.3 releases as\nreference points.</p>\n<figure>\n<img src=\"./timeline.svg\"\nalt=\"Timeline of the PodSecurityPolicy creation pull requests\"/>\n</figure>\n<p>After that, the PSP admission controller was enhanced by adding what was initially\nleft aside. <a href=\"https://github.com/kubernetes/kubernetes/pull/33080\">The authorization mechanism</a>,\nmerged in early November 2016 allowed administrators to use multiple policies\nin a cluster to grant different levels of access for different types of users.\nLater, a <a href=\"https://github.com/kubernetes/kubernetes/pull/52849\">pull request</a>\nmerged in October 2017 fixed <a href=\"https://github.com/kubernetes/kubernetes/issues/36184\">a design issue</a>\non ordering PodSecurityPolicies between mutating and alphabetical order, and continued to\nbuild the PSP admission as we know it. After that, many improvements and fixes\nfollowed to build the PodSecurityPolicy feature of recent Kubernetes releases.</p>\n<h2 id=\"the-rise-of-pod-security-admission\">The rise of Pod Security Admission</h2>\n<p>Despite the crucial issue it was trying to solve, PodSecurityPolicy presented\nsome major flaws:</p>\n<ul>\n<li><strong>Flawed authorization model</strong> - users can create a pod if they have the\n<strong>use</strong> verb on the PSP that allows that pod or the pod's service account has\nthe <strong>use</strong> permission on the allowing PSP.</li>\n<li><strong>Difficult to roll out</strong> - PSP fail-closed. That is, in the absence of a policy,\nall pods are denied. It mostly means that it cannot be enabled by default and\nthat users have to add PSPs for all workloads before enabling the feature,\nthus providing no audit mode to discover which pods would not be allowed by\nthe new policy. The opt-in model also leads to insufficient test coverage and\nfrequent breakage due to cross-feature incompatibility. And unlike RBAC,\nthere was no strong culture of shipping PSP manifests with projects.</li>\n<li><strong>Inconsistent unbounded API</strong> - the API has grown with lots of\ninconsistencies notably because of many requests for niche use cases: e.g.\nlabels, scheduling, fine-grained volume controls, etc. It has poor\ncomposability with a weak prioritization model, leading to unexpected\nmutation priority. It made it really difficult to combine PSP with other\nthird-party admission controllers.</li>\n<li><strong>Require security knowledge</strong> - effective usage still requires an\nunderstanding of Linux security primitives. e.g. MustRunAsNonRoot +\nAllowPrivilegeEscalation.</li>\n</ul>\n<p>The experience with PodSecurityPolicy concluded that most users care for two or three\npolicies, which led to the creation of the <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a>,\nthat define three policies:</p>\n<ul>\n<li><strong>Privileged</strong> - unrestricted policy.</li>\n<li><strong>Baseline</strong> - minimally restrictive policy, allowing the default pod\nconfiguration.</li>\n<li><strong>Restricted</strong> - security best practice policy.</li>\n</ul>\n<p>The replacement for PSP, the new <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Pod Security Admission</a>\nis an in-tree, stable for Kubernetes v1.25, admission plugin to enforce these\nstandards at the namespace level. It makes it easier to enforce basic pod\nsecurity without deep security knowledge. For more sophisticated use cases, you\nmight need a third-party solution that can be easily combined with Pod Security\nAdmission.</p>\n<h2 id=\"what-s-next\">What's next</h2>\n<p>For further details on the SIG Auth processes, covering PodSecurityPolicy removal and\ncreation of Pod Security admission, the\n<a href=\"https://www.youtube.com/watch?v=SFtHRmPuhEw\">SIG auth update at KubeCon NA 2019</a>\nand the <a href=\"https://www.youtube.com/watch?v=HsRRmlTJpls\">PodSecurityPolicy Replacement: Past, Present, and Future</a>\npresentation at KubeCon NA 2021 records are available.</p>\n<p>Particularly on the PSP removal, the\n<a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">PodSecurityPolicy Deprecation: Past, Present, and Future</a>\nblog post is still accurate.</p>\n<p>And for the new Pod Security admission,\n<a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">documentation is available</a>.\nIn addition, the blog post\n<a href=\"https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/\">Kubernetes 1.23: Pod Security Graduates to Beta</a>\nalong with the KubeCon EU 2022 presentation\n<a href=\"https://www.youtube.com/watch?v=gcz5VsvOYmI\">The Hitchhiker's Guide to Pod Security</a>\ngive great hands-on tutorials to learn.</p>","PublishedAt":"2022-08-23 23:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/","SourceName":"Kubernetes"}},{"node":{"ID":1445,"Title":"Blog: Kubernetes v1.25: Combiner","Description":"<p><strong>Authors</strong>: <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md\">Kubernetes 1.25 Release Team</a></p>\n<p>Announcing the release of Kubernetes v1.25!</p>\n<p>This release includes a total of 40 enhancements. Fifteen of those enhancements are entering Alpha, ten are graduating to Beta, and thirteen are graduating to Stable. We also have two features being deprecated or removed.</p>\n<h2 id=\"release-theme-and-logo\">Release theme and logo</h2>\n<p><strong>Kubernetes 1.25: Combiner</strong></p>\n<figure class=\"release-logo\">\n<img src=\"https://kubernetes.io/images/blog/2022-08-23-kubernetes-1.25-release/kubernetes-1.25.png\"\nalt=\"Combiner logo\"/>\n</figure>\n<p>The theme for Kubernetes v1.25 is <em>Combiner</em>.</p>\n<p>The Kubernetes project itself is made up of many, many individual components that, when combined, take the form of the project you see today. It is also built and maintained by many individuals, all of them with different skills, experiences, histories, and interests, who join forces not just as the release team but as the many SIGs that support the project and the community year-round.</p>\n<p>With this release we wish to honor the collaborative, open spirit that takes us from isolated developers, writers, and users spread around the globe to a combined force capable of changing the world. Kubernetes v1.25 includes a staggering 40 enhancements, none of which would exist without the incredible power we have when we work together.</p>\n<p>Inspired by our release lead's son, Albert Song, Kubernetes v1.25 is named for each and every one of you, no matter how you choose to contribute your unique power to the combined force that becomes Kubernetes.</p>\n<h2 id=\"what-s-new-major-themes\">What's New (Major Themes)</h2>\n<h3 id=\"pod-security-changes\">PodSecurityPolicy is removed; Pod Security Admission graduates to Stable</h3>\n<p>PodSecurityPolicy was initially <a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">deprecated in v1.21</a>, and with the release of v1.25, it has been removed. The updates required to improve its usability would have introduced breaking changes, so it became necessary to remove it in favor of a more friendly replacement. That replacement is <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Pod Security Admission</a>, which graduates to Stable with this release. If you are currently relying on PodSecurityPolicy, please follow the instructions for <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/\">migration to Pod Security Admission</a>.</p>\n<h3 id=\"ephemeral-containers-graduate-to-stable\">Ephemeral Containers Graduate to Stable</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/\">Ephemeral Containers</a> are containers that exist for only a limited time within an existing pod. This is particularly useful for troubleshooting when you need to examine another container but cannot use <code>kubectl exec</code> because that container has crashed or its image lacks debugging utilities. Ephemeral containers graduated to Beta in Kubernetes v1.23, and with this release, the feature graduates to Stable.</p>\n<h3 id=\"support-for-cgroups-v2-graduates-to-stable\">Support for cgroups v2 Graduates to Stable</h3>\n<p>It has been more than two years since the Linux kernel cgroups v2 API was declared stable. With some distributions now defaulting to this API, Kubernetes must support it to continue operating on those distributions. cgroups v2 offers several improvements over cgroups v1, for more information see the <a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">cgroups v2</a> documentation. While cgroups v1 will continue to be supported, this enhancement puts us in a position to be ready for its eventual deprecation and replacement.</p>\n<h3 id=\"improved-windows-support\">Improved Windows support</h3>\n<ul>\n<li><a href=\"http://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019\">Performance dashboards</a> added support for Windows</li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/51540\">Unit tests</a> added support for Windows</li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/108592\">Conformance tests</a> added support for Windows</li>\n<li>New GitHub repository created for <a href=\"https://github.com/kubernetes-sigs/windows-operational-readiness\">Windows Operational Readiness</a></li>\n</ul>\n<h3 id=\"moved-container-registry-service-from-k8s-gcr-io-to-registry-k8s-io\">Moved container registry service from k8s.gcr.io to registry.k8s.io</h3>\n<p><a href=\"https://github.com/kubernetes/kubernetes/pull/109938\">Moving container registry from k8s.gcr.io to registry.k8s.io</a> got merged. For more details, see the <a href=\"https://github.com/kubernetes/k8s.io/wiki/New-Registry-url-for-Kubernetes-(registry.k8s.io)\">wiki page</a>, <a href=\"https://groups.google.com/a/kubernetes.io/g/dev/c/DYZYNQ_A6_c/m/oD9_Q8Q9AAAJ\">annoucement</a> was sent to the kubernetes development mailing list.</p>\n<h3 id=\"promoted-seccompdefault-to-beta\">Promoted SeccompDefault to Beta</h3>\n<p>SeccompDefault promoted to beta, see the tutorial <a href=\"https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads\">Restrict a Container's Syscalls with seccomp</a> for more details.</p>\n<h3 id=\"promoted-endport-in-network-policy-to-stable\">Promoted endPort in Network Policy to Stable</h3>\n<p>Promoted <code>endPort</code> in <a href=\"https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports\">Network Policy</a> to GA. Network Policy providers that support <code>endPort</code> field now can use it to specify a range of ports to apply a Network Policy. Previously, each Network Policy could only target a single port.</p>\n<p>Please be aware that <code>endPort</code> field <strong>must be supported</strong> by the Network Policy provider. If your provider does not support <code>endPort</code>, and this field is specified in a Network Policy, the Network Policy will be created covering only the port field (single port).</p>\n<h3 id=\"promoted-local-ephemeral-storage-capacity-isolation-to-stable\">Promoted Local Ephemeral Storage Capacity Isolation to Stable</h3>\n<p>The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/361-local-ephemeral-storage-isolation\">Local Ephemeral Storage Capacity Isolation</a> feature moved to GA. This was introduced as alpha in 1.8, moved to beta in 1.10, and it is now a stable feature. It provides support for capacity isolation of local ephemeral storage between pods, such as <code>EmptyDir</code>, so that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of local ephemeral storage exceeds that limit.</p>\n<h3 id=\"promoted-core-csi-migration-to-stable\">Promoted core CSI Migration to Stable</h3>\n<p><a href=\"https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#quick-recap-what-is-csi-migration-and-why-migrate\">CSI Migration</a> is an ongoing effort that SIG Storage has been working on for a few releases. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration\">core CSI Migration</a> feature moved to GA. CSI Migration for GCE PD and AWS EBS also moved to GA. CSI Migration for vSphere remains in beta (but is on by default). CSI Migration for Portworx moved to Beta (but is off-by-default).</p>\n<h3 id=\"promoted-csi-ephemeral-volume-to-stable\">Promoted CSI Ephemeral Volume to Stable</h3>\n<p>The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/596-csi-inline-volumes\">CSI Ephemeral Volume</a> feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it moved to GA. This feature is used by some CSI drivers such as the <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver\">secret-store CSI driver</a>.</p>\n<h3 id=\"promoted-crd-validation-expression-language-to-beta\">Promoted CRD Validation Expression Language to Beta</h3>\n<p><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md\">CRD Validation Expression Language</a> is promoted to beta, which makes it possible to declare how custom resources are validated using the <a href=\"https://github.com/google/cel-spec\">Common Expression Language (CEL)</a>. Please see the <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules\">validation rules</a> guide.</p>\n<h3 id=\"promoted-server-side-unknown-field-validation-to-beta\">Promoted Server Side Unknown Field Validation to Beta</h3>\n<p>Promoted the <code>ServerSideFieldValidation</code> feature gate to beta (on by default). This allows optionally triggering schema validation on the API server that errors when unknown fields are detected. This allows the removal of client-side validation from kubectl while maintaining the same core functionality of erroring out on requests that contain unknown or invalid fields.</p>\n<h3 id=\"introduced-kms-v2-api\">Introduced KMS v2 API</h3>\n<p>Introduce KMS v2alpha1 API to add performance, rotation, and observability improvements. Encrypt data at rest (ie Kubernetes <code>Secrets</code>) with DEK using AES-GCM instead of AES-CBC for kms data encryption. No user action is required. Reads with AES-GCM and AES-CBC will continue to be allowed. See the guide <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/\">Using a KMS provider for data encryption</a> for more information.</p>\n<h2 id=\"other-updates\">Other Updates</h2>\n<h3 id=\"graduations-to-stable\">Graduations to Stable</h3>\n<p>This release includes a total of thirteen enhancements promoted to stable:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/277\">Ephemeral Containers</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/361\">Local Ephemeral Storage Resource Management</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/596\">CSI Ephemeral Volumes</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/625\">CSI Migration - Core</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/785\">Graduate the kube-scheduler ComponentConfig to GA</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1487\">CSI Migration - AWS</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1488\">CSI Migration - GCE</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1591\">DaemonSets Support MaxSurge</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2079\">NetworkPolicy Port Range</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2254\">cgroups v2</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2579\">Pod Security Admission</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2599\">Add <code>minReadySeconds</code> to Statefulsets</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2802\">Identify Windows pods at API admission level authoritatively</a></li>\n</ul>\n<h3 id=\"deprecations-and-removals\">Deprecations and Removals</h3>\n<p>Two features were <a href=\"https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/\">deprecated or removed</a> from Kubernetes with this release.</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/5\">PodSecurityPolicy is removed</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3446\">GlusterFS plugin deprecated from available in-tree drivers</a></li>\n</ul>\n<h3 id=\"release-notes\">Release Notes</h3>\n<p>The complete details of the Kubernetes v1.25 release are available in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md\">release notes</a>.</p>\n<h3 id=\"availability\">Availability</h3>\n<p>Kubernetes v1.25 is available for download on <a href=\"https://github.com/kubernetes/kubernetes/releases/tag/v1.25.0\">GitHub</a>.\nTo get started with Kubernetes, check out these <a href=\"https://kubernetes.io/docs/tutorials/\">interactive tutorials</a> or run local\nKubernetes clusters using containers as ‚Äúnodes‚Äù, with <a href=\"https://kind.sigs.k8s.io/\">kind</a>.\nYou can also easily install 1.25 using <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h3 id=\"release-team\">Release Team</h3>\n<p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that, when combined, make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.</p>\n<p>We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.25 release for our community. Every one of you had a part to play in building this, and you all executed beautifully. We would like to extend special thanks to our fearless release lead, Cici Huang, for all she did to guarantee we had what we needed to succeed.</p>\n<h3 id=\"user-highlights\">User Highlights</h3>\n<ul>\n<li>Finleap Connect operates in a highly regulated environment. <a href=\"https://www.cncf.io/case-studies/finleap-connect/\">In 2019, they had five months to implement mutual TLS (mTLS) across all services in their clusters for their business code to comply with the new European PSD2 payment directive</a>.</li>\n<li>PNC sought to develop a way to ensure new code would meet security standards and audit compliance requirements automatically‚Äîreplacing the cumbersome 30-day manual process they had in place. Using Knative, <a href=\"https://www.cncf.io/case-studies/pnc-bank/\">PNC developed internal tools to automatically check new code and changes to existing code</a>.</li>\n<li>Nexxiot needed highly-reliable, secure, performant, and cost efficient Kubernetes clusters. <a href=\"https://www.cncf.io/case-studies/nexxiot/\">They turned to Cilium as the CNI to lock down their clusters and enable resilient networking with reliable day two operations</a>.</li>\n<li>Because the process of creating cyber insurance policies is a complicated multi-step process, At-Bay sought to improve operations by using asynchronous message-based communication patterns/facilities. <a href=\"https://www.cncf.io/case-studies/at-bay/\">They determined that Dapr fulfilled its desired list of requirements and much more</a>.</li>\n</ul>\n<h3 id=\"ecosystem-updates\">Ecosystem Updates</h3>\n<ul>\n<li>KubeCon + CloudNativeCon North America 2022 will take place in Detroit, Michigan from 24 ‚Äì 28 October 2022! You can find more information about the conference and registration on the <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\">event site</a>.</li>\n<li>KubeDay event series kicks off with KubeDay Japan December 7! Register or submit a proposal on the <a href=\"https://events.linuxfoundation.org/kubeday-japan/\">event site</a></li>\n<li>In the <a href=\"https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/\">2021 Cloud Native Survey</a>, the CNCF saw record Kubernetes and container adoption. Take a look at the <a href=\"https://www.cncf.io/reports/cncf-annual-survey-2021/\">results of the survey</a>.</li>\n</ul>\n<h3 id=\"project-velocity\">Project Velocity</h3>\n<p>The <a href=\"https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m\">CNCF K8s DevStats</a> project\naggregates a number of interesting data points related to the velocity of Kubernetes and various\nsub-projects. This includes everything from individual contributions to the number of companies that\nare contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>\n<p>In the v1.24 release cycle, which <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25\">ran for 15 weeks</a> (May 23 to August 23), we saw contributions from <a href=\"https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;var-metric=contributions\">1065 companies</a> and <a href=\"https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes\">1620 individuals</a>.</p>\n<h2 id=\"upcoming-release-webinar\">Upcoming Release Webinar</h2>\n<p>Join members of the Kubernetes v1.25 release team on Thursday September 22, 2022 10am ‚Äì 11am PT to learn about\nthe major features of this release, as well as deprecations and removals to help plan for upgrades.\nFor more information and registration, visit the <a href=\"https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v125-release/\">event page</a>.</p>\n<h2 id=\"get-involved\">Get Involved</h2>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests.\nHave something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below:</p>\n<ul>\n<li>Find out more about contributing to Kubernetes at the <a href=\"https://www.kubernetes.dev/\">Kubernetes Contributors</a> website</li>\n<li>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for the latest updates</li>\n<li>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a></li>\n<li>Join the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Post questions (or answer questions) on <a href=\"https://serverfault.com/questions/tagged/kubernetes\">Server Fault</a>.</li>\n<li>Share your Kubernetes <a href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">story</a></li>\n<li>Read more about what‚Äôs happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a></li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a></li>\n</ul>","PublishedAt":"2022-08-23 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/","SourceName":"Kubernetes"}},{"node":{"ID":1439,"Title":"Blog: Spotlight on SIG Storage","Description":"<p><strong>Author</strong>: Frederico Mu√±oz (SAS)</p>\n<p>Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes.</p>\n<p>Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage's umbrella.</p>\n<p>In this SIG Storage spotlight, <a href=\"https://twitter.com/fredericomunoz\">Frederico Mu√±oz</a> (Cloud &amp; Architecture Lead at SAS) talked with <a href=\"https://twitter.com/2000xyang\">Xing Yang</a>, Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.</p>\n<h2 id=\"about-sig-storage\">About SIG Storage</h2>\n<p><strong>Frederico (FSM)</strong>: Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage.</p>\n<p><strong>Xing Yang (XY)</strong>: I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the <a href=\"https://kubernetes.io/docs/concepts/storage/volume-snapshots/\">VolumeSnapshot</a> project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage. It needed contributors. So I volunteered to help. Then I worked with other community members to bring VolumeSnapshot to Alpha in K8s 1.12 release in 2018, Beta in K8s 1.17 in 2019, and eventually GA in 1.20 in 2020.</p>\n<p><strong>FSM</strong>: Reading the <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/charter.md\">SIG Storage charter</a> alone it‚Äôs clear that SIG Storage covers a lot of ground, could you describe how the SIG is organised?</p>\n<p><strong>XY</strong>: In SIG Storage, there are two Co-Chairs and two Tech Leads. Saad Ali from Google and myself are Co-Chairs. Michelle Au from Google and Jan ≈†afr√°nek from Red Hat are Tech Leads.</p>\n<p>We have bi-weekly meetings where we go through features we are working on for each particular release, getting the statuses, making sure each feature has dev owners and reviewers working on it, and reminding people about the release deadlines, etc. More information on the SIG is on the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">community page</a>. People can also add PRs that need attention, design proposals that need discussion, and other topics to the meeting agenda doc. We will go over them after project tracking is done.</p>\n<p>We also have other regular meetings, i.e., CSI Implementation meeting, Object Bucket API design meeting, and one-off meetings for specific topics if needed. There is also a <a href=\"https://github.com/kubernetes/community/blob/master/wg-data-protection/README.md\">K8s Data Protection Workgroup</a> that is sponsored by SIG Storage and SIG Apps. SIG Storage owns or co-owns features that are being discussed at the Data Protection WG.</p>\n<h2 id=\"storage-and-kubernetes\">Storage and Kubernetes</h2>\n<p><strong>FSM</strong>: Storage is such a foundational component in so many things, not least in Kubernetes: what do you think are the Kubernetes-specific challenges in terms of storage management?</p>\n<p><strong>XY</strong>: In Kubernetes, there are multiple components involved for a volume operation. For example, creating a Pod to use a PVC has multiple components involved. There are the Attach Detach Controller and the external-attacher working on attaching the PVC to the pod. There‚Äôs the Kubelet that works on mounting the PVC to the pod. Of course the CSI driver is involved as well. There could be race conditions sometimes when coordinating between multiple components.</p>\n<p>Another challenge is regarding core vs <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">Custom Resource Definitions</a> (CRD), not really storage specific. CRD is a great way to extend Kubernetes capabilities while not adding too much code to the Kubernetes core itself. However, this also means there are many external components that are needed when running a Kubernetes cluster.</p>\n<p>From the SIG Storage side, one most notable example is Volume Snapshot. Volume Snapshot APIs are defined as CRDs. API definitions and controllers are out-of-tree. There is a common snapshot controller and a snapshot validation webhook that should be deployed on the control plane, similar to how kube-controller-manager is deployed. Although Volume Snapshot is a CRD, it is a core feature of SIG Storage. It is recommended for the K8s cluster distros to deploy Volume Snapshot CRDs, the snapshot controller, and the snapshot validation webhook, however, most of the time we don‚Äôt see distros deploy them. So this becomes a problem for the storage vendors: now it becomes their responsibility to deploy these non-driver specific common components. This could cause conflicts if a customer wants to use more than one storage system and deploy more than one CSI driver.</p>\n<p><strong>FSM</strong>: Not only the complexity of a single storage system, you have to consider how they will be used together in Kubernetes?</p>\n<p><strong>XY</strong>: Yes, there are many different storage systems that can provide storage to containers in Kubernetes. They don‚Äôt work the same way. It is challenging to find a solution that works for everyone.</p>\n<p><strong>FSM</strong>: Storage in Kubernetes also involves interacting with external solutions, perhaps more so than other parts of Kubernetes. Is this interaction with vendors and external providers challenging? Has it evolved with time in any way?</p>\n<p><strong>XY</strong>: Yes, it is definitely challenging. Initially Kubernetes storage had in-tree volume plugin interfaces. Multiple storage vendors implemented in-tree interfaces and have volume plugins in the Kubernetes core code base. This caused lots of problems. If there is a bug in a volume plugin, it affects the entire Kubernetes code base. All volume plugins must be released together with Kubernetes. There was no flexibility if storage vendors need to fix a bug in their plugin or want to align with their own product release.</p>\n<p><strong>FSM</strong>: That‚Äôs where CSI enters the game?</p>\n<p><strong>XY</strong>: Exactly, then there comes <a href=\"https://kubernetes-csi.github.io/docs/\">Container Storage Interface</a> (CSI). This is an industry standard trying to design common storage interfaces so that a storage vendor can write one plugin and have it work across a range of container orchestration systems (CO). Now Kubernetes is the main CO, but back when CSI just started, there were Docker, Mesos, Cloud Foundry, in addition to Kubernetes. CSI drivers are out-of-tree so bug fixes and releases can happen at their own pace.</p>\n<p>CSI is definitely a big improvement compared to in-tree volume plugins. Kubernetes implementation of CSI has been GA <a href=\"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/\">since the 1.13 release</a>. It has come a long way. SIG Storage has been working on moving in-tree volume plugins to out-of-tree CSI drivers for several releases now.</p>\n<p><strong>FSM</strong>: Moving drivers away from the Kubernetes main tree and into CSI was an important improvement.</p>\n<p><strong>XY</strong>: CSI interface is an improvement over the in-tree volume plugin interface, however, there are still challenges. There are lots of storage systems. Currently <a href=\"https://kubernetes-csi.github.io/docs/drivers.html\">there are more than 100 CSI drivers listed in CSI driver docs</a>. These storage systems are also very diverse. So it is difficult to design a common API that works for all. We introduced capabilities at CSI driver level, but we also have challenges when volumes provisioned by the same driver have different behaviors. The other day we just had a meeting discussing Per Volume CSI Driver Capabilities. We have a problem differentiating some CSI driver capabilities when the same driver supports both block and file volumes. We are going to have follow up meetings to discuss this problem.</p>\n<h2 id=\"ongoing-challenges\">Ongoing challenges</h2>\n<p><strong>FSM</strong>: Specifically for the <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25\">1.25 release</a> we can see that there are a relevant number of storage-related <a href=\"https://bit.ly/k8s125-enhancements\">KEPs</a> in the pipeline, would you say that this release is particularly important for the SIG?</p>\n<p><strong>XY</strong>: I wouldn‚Äôt say one release is more important than other releases. In any given release, we are working on a few very important things.</p>\n<p><strong>FSM</strong>: Indeed, but are there any 1.25 specific specificities and highlights you would like to point out though?</p>\n<p><strong>XY</strong>: Yes. For the 1.25 release, I want to highlight the following:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration\">CSI Migration</a> is an on-going effort that SIG Storage has been working on for a few releases now. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. There are 7 KEPs that we are targeting in 1.25 are related to CSI migration. There is one core KEP for the general CSI Migration feature. That is targeting GA in 1.25. CSI Migration for GCE PD and AWS EBS are targeting GA. CSI Migration for vSphere is targeting to have the feature gate on by default while staying in 1.25 that are in Beta. Ceph RBD and PortWorx are targeting Beta, with feature gate off by default. Ceph FS is targeting Alpha.</li>\n<li>The second one I want to highlight is <a href=\"https://github.com/kubernetes-sigs/container-object-storage-interface-spec\">COSI, the Container Object Storage Interface</a>. This is a sub-project under SIG Storage. COSI proposes object storage Kubernetes APIs to support orchestration of object store operations for Kubernetes workloads. It also introduces gRPC interfaces for object storage providers to write drivers to provision buckets. The COSI team has been working on this project for more than two years now. The COSI feature is targeting Alpha in 1.25. The KEP just got merged. The COSI team is working on updating the implementation based on the updated KEP.</li>\n<li>Another feature I want to mention is <a href=\"https://github.com/kubernetes/enhancements/issues/596\">CSI Ephemeral Volume</a> support. This feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it is now targeting GA in 1.25.</li>\n</ul>\n<p><strong>FSM</strong>: If you had to single something out, what would be the most pressing areas the SIG is working on?</p>\n<p><strong>XY</strong>: CSI migration is definitely one area that the SIG has put in lots of effort and it has been on-going for multiple releases now. It involves work from multiple cloud providers and storage vendors as well.</p>\n<h2 id=\"community-involvement\">Community involvement</h2>\n<p><strong>FSM</strong>: Kubernetes is a community-driven project. Any recommendation for anyone looking into getting involved in SIG Storage work? Where should they start?</p>\n<p><strong>XY</strong>: Take a look at the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">SIG Storage community page</a>, it has lots of information on how to get started. There are <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/annual-report-2021.md\">SIG annual reports</a> that tell you what we did each year. Take a look at the Contributing guide. It has links to presentations that can help you get familiar with Kubernetes storage concepts.</p>\n<p>Join our <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage#meetings\">bi-weekly meetings on Thursdays</a>. Learn how the SIG operates and what we are working on for each release. Find a project that you are interested in and help out. As I mentioned earlier, I got started in SIG Storage by contributing to the Volume Snapshot project.</p>\n<p><strong>FSM</strong>: Any closing thoughts you would like to add?</p>\n<p><strong>XY</strong>: SIG Storage always welcomes new contributors. We need contributors to help with building new features, fixing bugs, doing code reviews, writing tests, monitoring test grid health, and improving documentation, etc.</p>\n<p><strong>FSM</strong>: Thank you so much for your time and insights into the workings of SIG Storage!</p>","PublishedAt":"2022-08-22 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/","SourceName":"Kubernetes"}},{"node":{"ID":1427,"Title":"Blog: Stargazing, solutions and staycations: the Kubernetes 1.24 release interview","Description":"<p><strong>Author</strong>: Craig Box (Google)</p>\n<p>The Kubernetes project has participants from all around the globe. Some are friends, some are colleagues, and some are strangers. The one thing that unifies them, no matter their differences, are that they all have an interesting story. It is my pleasure to be the documentarian for the stories of the Kubernetes community in the weekly <a href=\"https://kubernetespodcast.com/\">Kubernetes Podcast from Google</a>. With every new Kubernetes release comes an interview with the release team lead, telling the story of that release, but also their own personal story.</p>\n<p>With 1.25 around the corner, <a href=\"https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog\">the tradition continues</a> with a look back at the story of 1.24. That release was led by <a href=\"https://twitter.com/jameslaverack\">James Laverack</a> of Jetstack. <a href=\"https://kubernetespodcast.com/episode/178-kubernetes-1.24/\">James was on the podcast</a> in May, and while you can read his story below, if you can, please do listen to it in his own voice.</p>\n<p>Make sure you <a href=\"https://kubernetespodcast.com/subscribe/\">subscribe, wherever you get your podcasts</a>, so you hear all our stories from the cloud native community, including the story of 1.25 next week.</p>\n<p><em>This transcript has been lightly edited and condensed for clarity.</em></p>\n<hr>\n<p><strong>CRAIG BOX: Your journey to Kubernetes went through the financial technology (fintech) industry. Tell me a little bit about how you came to software?</strong></p>\n<p>JAMES LAVERACK: I took a pretty traditional path to software engineering. I went through school and then I did a computer science degree at the University of Bristol, and then I just ended up taking a software engineer job from there. Somewhat rather by accident, I ended up doing fintech work, which is pretty interesting, pretty engaging.</p>\n<p>But in my most recent fintech job before I joined <a href=\"https://www.jetstack.io/\">Jetstack</a>, I ended up working on a software project. We needed Kubernetes to solve a technical problem. So we implemented Kubernetes, and as often happens, I ended up as the one person of a team that understood the infrastructure, while everyone else was doing all of the application development.</p>\n<p>I ended up enjoying the infrastructure side so much that I decided to move and do that full time. So I looked around and I found Jetstack, whose offices were literally across the road. I could see them out of our office window. And so I decided to just hop across the road and join them, and do all of this Kubernetes stuff more.</p>\n<p><strong>CRAIG BOX: What's the tech scene like in Bristol? You went there for school and never left?</strong></p>\n<p>JAMES LAVERACK: Pretty much. It's happened to a lot of people I know and a lot of my friends, is that you go to University somewhere and you're just kind of stuck there forever, so to speak. It's been known for being quite hot in the area in terms of that part of the UK. It has a lot of tech companies, obviously, it was a fintech company I worked at before. I think some larger companies have offices there. For &quot;not London&quot;, it's not doing too bad, I don't think.</p>\n<p><strong>CRAIG BOX: When you say hot, though, that's tech industry, not weather, I'm assuming.</strong></p>\n<p>JAMES LAVERACK: Yeah, weather is the usual UK. It's kind of a nice overcast and rainy, which I quite like. I'm quite fond of it.</p>\n<p><strong>CRAIG BOX: Public transport good?</strong></p>\n<p>JAMES LAVERACK: Buses are all right. We've got a new bus installed recently, which everyone hated while it was being built. And now it's complete, everyone loves. So, standard I think.</p>\n<p><strong>CRAIG BOX: That is the way. As someone who lived in London for a long time, it's very easy for me to say &quot;well, London's kind of like Singapore. It's its own little city-state.&quot; But whenever we did go out to that part of the world, Bath especially, a very lovely town</strong></p>\n<p>JAMES LAVERACK: Oh, Bath's lovely. I've been a couple of times.</p>\n<p><strong>CRAIG BOX: Have you been to Box?</strong></p>\n<p>JAMES LAVERACK: To where, sorry?</p>\n<p><strong>CRAIG BOX: There's <a href=\"https://en.wikipedia.org/wiki/Box,_Wiltshire\">a town called Box</a> just outside Bath. I had my picture taken outside all the buildings. Proclaimed myself the mayor.</strong></p>\n<p>JAMES LAVERACK: Oh, no, I don't think I have.</p>\n<p><strong>CRAIG BOX: Well, look it up if you're ever in the region, everybody. Let's get back to Jetstack, though. They were across the road. Great company, the <a href=\"https://www.jetstack.io/about/mattbarker/\">two</a> <a href=\"https://www.jetstack.io/about/mattbates/\">Matts</a>, the co-founders there. What was the interview process like for you?</strong></p>\n<p>JAMES LAVERACK: It was pretty relaxed. One lunchtime, I just walked down the road and went to a coffee shop with Matt and we had this lovely conversation talking about my background and Jetstack and what I was looking to achieve in a new role and all this. And I'd applied to be a software engineer. And then they kind of at the end of it, he looked over at me and was like, &quot;well, how about being a solutions engineer instead?&quot; And I was like, what's that?</p>\n<p>And he's like, &quot;well, you know, it's just effectively being a software consultant. You go, you help companies implement Kubernetes, users, saying all that stuff you enjoy. But you do it full time.&quot; I was like, &quot;well, maybe.&quot; And in the end he convinced me. I ended up joining as a solutions engineer with the idea of if I didn't like it, I could transfer to be a software engineer again.</p>\n<p>Nearly three years later, I've never taken them up on the offer. I've just <a href=\"https://www.jetstack.io/blog/life-as-a-solutions-engineer/\">stayed as a solutions engineer</a> the entire time.</p>\n<p><strong>CRAIG BOX: At the company you were working at, I guess you were effectively the consultant between the people writing the software and the deployment in Kubernetes. Did it make sense then for you to carry on in that role, as you moved to Jetstack?</strong></p>\n<p>JAMES LAVERACK: I think so. I think it's something that I enjoyed. Not that I didn't enjoy writing software applications. I always enjoyed it, and we had a really interesting product and a really fun team. But I just found that more interesting. And it was becoming increasingly difficult to justify spending time on it when we had an application to write.</p>\n<p>Which was just completely fine, and that made sense for the needs of the team at the time. But it's not what I wanted to do.</p>\n<p><strong>CRAIG BOX: Do you think that talks to the split between Kubernetes being for developers or for operators? Do you think there's always going to be the need to have a different set of people who are maintaining the running infrastructure versus the people who are writing the code that run on it?</strong></p>\n<p>JAMES LAVERACK: I think to some extent, yes, whether or not that's a separate platform team or whether or not that is because the people running it are consultants of some kind. Or whether or not this has been abstracted away from you in some of the more batteries-included versions of Kubernetes ‚Äî some of the cloud-hosted ones, especially, somewhat remove that need. So I don't think it's absolutely necessary to employ a platform team. But I think someone needs to do it or you need to implicitly or explicitly pay for someone to do it in some way.</p>\n<p><strong>CRAIG BOX: In the three years you have been at Jetstack now, how different are the jobs that you do for the customers? Is this just a case of learning one thing and rolling it out to multiple people, or is there always a different challenge with everyone you come across?</strong></p>\n<p>JAMES LAVERACK: I think there's always a different challenge. My role has varied drastically. For example, a long time ago, I did an Istio install. But it was a relatively complicated, single mesh, multi-cluster install. And that was before multi-cluster support was really as readily available as it is now. Conversely, I've worked building custom orchestration platforms on top of Kubernetes for specific customer use cases.</p>\n<p>It's all varied and every single customer engagement is different. That is an element I really like about the job, that variability in how things are and how things go.</p>\n<p><strong>CRAIG BOX: When the platform catches up and does things like makes it easier to manage multi-cluster environments, do you go back to the customers and bring them up to date with the newest methods?</strong></p>\n<p>JAMES LAVERACK: It depends. Most of our engagements are to solve a specific problem. And once we've solved that problem, they may have us back. But typically speaking, in my line of work, it's not an ongoing engagement. There are some within Jetstack that do that, but not so much in my team.</p>\n<p><strong>CRAIG BOX: Your bio suggests that you were once called &quot;the reason any corporate policy evolves.&quot; What's the story there?</strong></p>\n<p>JAMES LAVERACK: [CHUCKLES] I think I just couldn't leave things well enough alone. I was talking to our operations director inside of Jetstack, and he once said to me that whenever he's thinking of a new corporate policy, he asks will it pass the James Laverack test. That is, will I look at it and find some horrendous loophole?</p>\n<p>For example when I first joined, I took a look at our acceptable use policy for company equipment. And it stated that you're not allowed to have copyrighted material on your laptop. And of course, this makes sense, as you know, you don't want people doing software piracy or anything. But as written, that would imply you're not allowed to have anything that is copyrighted by anyone on your machine.</p>\n<p><strong>CRAIG BOX: Such as perhaps the operating system that comes installed on it?</strong></p>\n<p>JAMES LAVERACK: Such as perhaps the operating system, or anything. And you know, this clearly didn't make any sense. So he adjusted that, and I've kind of been fiddling with that sort of policy ever since.</p>\n<p><strong>CRAIG BOX: The release team is often seen as an administrative role versus a pure coding role. Does that speak to the kind of change you've had in career in previously being a software developer and now being more of a consultant, or was there something else that attracted you to get involved in that particular part of the community?</strong></p>\n<p>JAMES LAVERACK: I wouldn't really consider it less technical. I mean, yes, you do much less coding. This is something that constantly surprises my friends and some of my colleagues, when I tell them more detail about my role. There's not really any coding involved.</p>\n<p>I don't think my role has really changed to have less coding. In fact, one of my more recent projects at Jetstack, a client project, involved a lot of coding. But I think that what attracted me to this role within Kubernetes is really the community. I found it really rewarding to engage with SIG Release and to engage with the release team. So I've always just enjoyed doing it, even though there is, as you say, not all that much coding involved.</p>\n<p><strong>CRAIG BOX: Indeed; your wife said to you, <a href=\"https://twitter.com/JamesLaverack/status/1483201645286678529\">&quot;I don't think your job is to code anymore. You just talk to people all day.&quot;</a> How did that make you feel?</strong></p>\n<p>JAMES LAVERACK: Ahh, annoyed, because she was right. This was kind of a couple of months ago when I was in the middle of it with all of the Kubernetes meetings. Also, my client project at the time involved a lot of technical discussion. I was in three or four hours of calls every day. And I don't mind that. But I would come out, in part because of course you're working from home, so she sees me all the time. So I'd come out, I'd grab a coffee and be like, &quot;oh, I've got a meeting, I've got to go.&quot; And she'd be like, &quot;do you ever code anymore?&quot;\nI think it was in fact just after Christmas when she asked me, &quot;when was the last time you programmed anything?&quot; And I had to think about it. Then I realized that perhaps there was a problem there. Well, not a problem, but I realized that perhaps I don't code as much as I used to.</p>\n<p><strong>CRAIG BOX: Are you the kind of person who will pick up a hobby project to try and fix that?</strong></p>\n<p>JAMES LAVERACK: Absolutely. I've recently started writing <a href=\"https://github.com/JamesLaverack/kubernetes-minecraft-operator\">a Kubernetes operator for my Minecraft server</a>. That probably tells you about the state I'm in.</p>\n<p><strong>CRAIG BOX: If it's got Kubernetes in it, it doesn't sound that much of a hobby.</strong></p>\n<p>JAMES LAVERACK: [LAUGHING] Do you not consider Kubernetes to be a hobby?</p>\n<p><strong>CRAIG BOX: It depends.</strong></p>\n<p>JAMES LAVERACK: I think I do.</p>\n<p><strong>CRAIG BOX: I think by now.</strong></p>\n<p>JAMES LAVERACK: In some extents.</p>\n<p><strong>CRAIG BOX: You mentioned observing the release team in process before you decided to get involved. Was that as part of working with customers and looking to see whether a particular feature would make it into a release, or was there some other reason that that was how you saw the Kubernetes community?</strong></p>\n<p>JAMES LAVERACK: Just after I joined Jetstack, I got the opportunity to go to KubeCon San Diego. I think we actually met there.</p>\n<p><strong>CRAIG BOX: We did.</strong></p>\n<p>JAMES LAVERACK: We had dinner, didn't we? So when I went, I'd only been at Jetstack for a few months. I really wasn't involved in the community in any serious way at all. As a result, I just ended up following around my then colleague, James Munnelly. James is lovely. And, you know, I just kind of went around with him, because he knew everyone.</p>\n<p>I ended up in this hotel bar with a bunch of Kubernetes people, including Stephen Augustus, the co-chair of SIG Release and holder of a bunch of other roles within the community. I happened to ask him, I want to get involved. What is a good way to get involved with the Kubernetes community, if I've never been involved before? And he said, oh, you should join the release team.</p>\n<p><strong>CRAIG BOX: So it's all down to where you end up in the bar with someone.</strong></p>\n<p>JAMES LAVERACK: Yeah, pretty much.</p>\n<p><strong>CRAIG BOX: If I'd got to you sooner, you could have been working on Istio.</strong></p>\n<p>JAMES LAVERACK: Yeah, I could've been working on Istio, I could have ended up in some other SIG doing something. I just happened to be talking to Stephen. And Stephen suggested it, and I gave it a go. And here I am three years later.</p>\n<p><strong>CRAIG BOX: I think I remember at the time you were working on an etcd operator?</strong></p>\n<p>JAMES LAVERACK: Yeah, that's correct. That was part of a client project, which they, thankfully <a href=\"https://github.com/improbable-eng/etcd-cluster-operator\">let us open source</a>. This was an operator for etcd, where they had a requirement to run it in Kubernetes, which of course is the opposite way around to how you'd normally want to run it.</p>\n<p><strong>CRAIG BOX: And I remember having you up at the time, like I'm pretty sure those things exist already, and asking what the need was for there to be something different.</strong></p>\n<p>JAMES LAVERACK: It was that they needed something very specific. The ones that existed already were all designed to run clusters that couldn't be shut down. As long as one replica stayed up, you could keep running etcd. But they needed to be able to suspend and restart the entire cluster, which means it needs disk-persistence support, which it turns out is quite complicated.</p>\n<p><strong>CRAIG BOX: It's easier if you just throw all the data away.</strong></p>\n<p>JAMES LAVERACK: It's much easier to throw all the data away. We needed to be a little bit careful about how we managed it. We thought about forking and changing an existing one. But we realized it would probably just be as easy to start from scratch, so we did that.</p>\n<p><strong>CRAIG BOX: You've been a member of every release team since that point, since Kubernetes 1.18 in 2020, in a wide range of roles. Which set of roles have you been through?</strong></p>\n<p>JAMES LAVERACK: I started out as a release notes shadow, and did that for a couple of releases, in 1.18 and 1.19. In 1.20, I was the release notes lead. And then in 1.21, I moved into being a shadow again as an enhancement shadow, before in 1.22 becoming an enhancements lead, but in 1.23 a release lead shadow, and finally in 1.24, release lead as a whole.</p>\n<p><strong>CRAIG BOX: That's quite a long time to be with the release team. You're obviously going to move into an emeritus role after this release. Do you see yourself still remaining involved? Is it something that you're clearly very passionate about?</strong></p>\n<p>JAMES LAVERACK: I think I'm going to be around in SIG Release for as long as people want me there. I find it a really interesting part of the community. And I find the people super-interesting and super-inviting.</p>\n<p><strong>CRAIG BOX: Let's talk then about <a href=\"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/\">Kubernetes 1.24</a>. First, as always, congratulations on the release.</strong></p>\n<p>JAMES LAVERACK: Thank you.</p>\n<p><strong>CRAIG BOX: This release consists of 46 enhancements. 14 have graduated to stable, 15 have moved to beta, and 13 are in alpha. 2 are deprecated and 2 have been removed. How is that versus other releases recently? Is that an average number? That seems like a lot of stable enhancements, especially.</strong></p>\n<p>JAMES LAVERACK: I think it's pretty similar. Most of the recent releases have been quite similar in the number of enhancements they have and in what categories. For example, in 1.23, the previous release, there were 47. I think 1.22, before that, had 53, so slightly more. But it's around about that number.</p>\n<p><strong>CRAIG BOX: You didn't want to sneak in two extra so you could say you were one more than the last one?</strong></p>\n<p>JAMES LAVERACK: No, I don't think so. I think we had enough going on.</p>\n<p><strong>CRAIG BOX: The release team is obviously beholden to what features the SIGs are developing and what their plans are. Is there ever any coordination between the release process and the SIGs in terms of things like saying, this release is going to be a catch-up release, like the old Snow Leopard releases for macOS, for example, where we say we don't want as many new features, but we really want more stabilization, and could you please work on those kind of things?</strong></p>\n<p>JAMES LAVERACK: Not really. The cornerstone of a Kubernetes organization is the SIGs themselves, so the special interest groups that make up the organization. It's really up to them what they want to do. We don't do any particular coordination on the style of thing that should be implemented. A lot of SIGs have roadmaps that are looking over multiple releases to try to get features that they think are important in.</p>\n<p><strong>CRAIG BOX: Let's talk about some of the new features in 1.24. We have been hearing for many releases now about the impending doom which is the removal of Dockershim. <a href=\"https://github.com/kubernetes/enhancements/issues/2221\">It is gone in 1.24</a>. Do we worry?</strong></p>\n<p>JAMES LAVERACK: I don't think we worry. This is something that the community has been preparing for for a long time. <a href=\"https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/\">We've</a> <a href=\"https://kubernetes.io/blog/2022/02/17/dockershim-faq/\">published</a> a <a href=\"https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/\">lot</a> of <a href=\"https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/\">documentation</a> <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/\">about</a> <a href=\"https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/\">how</a> you need to approach this. The honest truth is that most users, most application developers in Kubernetes, will simply not notice a difference or have to worry about it.</p>\n<p>It's only really platform teams that administer Kubernetes clusters and people in very specific circumstances that are using Docker directly, not through the Kubernetes API, that are going to experience any issue at all.</p>\n<p><strong>CRAIG BOX: And I see that Mirantis and Docker have developed a CRI plugin for Docker anyway, so you can just switch over to that and everything continues.</strong></p>\n<p>JAMES LAVERACK: Yeah, absolutely, or you can use one of the many other CRI implementations. There are two in the CNCF, <a href=\"https://containerd.io/\">containerd</a>, and <a href=\"https://cri-o.io/\">CRI-O</a>.</p>\n<p><strong>CRAIG BOX: Having gone through the process of communicating this change over several releases, what has the team learnt in terms of how we will communicate a message like this in future?</strong></p>\n<p>JAMES LAVERACK: I think that this has been really interesting from the perspective that this is the biggest removal that the Kubernetes project has had to date. We've removed features before. In fact, we're removing another one in this release as well. But this is one of the most user-visible changes we've made.</p>\n<p>I think there are very good reasons for doing it. But I think we've learned a lot about how and when to communicate, and the importance of having migration guides, the importance of having official documentation that really clarifies the thing. I think that's the real, it's an area in which the Kubernetes project has matured a lot since I've been on the team.</p>\n<p><strong>CRAIG BOX: What is the other feature that's being removed?</strong></p>\n<p>JAMES LAVERACK: The other feature that we're removing is dynamic Kubelet configuration. This is a feature that was in beta for a while. But I believe we decided that it just wasn't being used enough to justify keeping it. So we're removing it. We deprecated it back in 1.22 and we're removing it this release.</p>\n<p><strong>CRAIG BOX: There was a change in policy a few releases ago that talked about features not being allowed to stay in beta forever. Have there been any features that were at risk of being removed due to lack of maintenance, or are all the SIGs pretty good now at keeping their features on track?</strong></p>\n<p>JAMES LAVERACK: I think the SIGs are getting pretty good at it. We had a spate of a long time when a lot of features were kind of perpetually in beta. As you remember, Ingress was in beta for a long, long time.</p>\n<p><strong>CRAIG BOX: I choose to believe it still is.</strong></p>\n<p>JAMES LAVERACK: [LAUGHTER] I think it's really good that we're moving towards that stability approach with things like Kubernetes. I think it's a very positive change.</p>\n<p><strong>CRAIG BOX: The fact that Ingress was in beta for so long, along with things like the main workload controllers, for example, did lead people to believing that beta APIs were stable and production ready, and could and should be used. Something that's changing in this release is that <a href=\"https://github.com/kubernetes/enhancements/issues/3136\">beta APIs are going to be off by default</a>. Why that change?</strong></p>\n<p>JAMES LAVERACK: This is really about encouraging the use of stable APIs. There was a perception, like you say, that beta APIs were actually stable. Because they can be removed very quickly, we often ended up in the state where we wanted to follow the policy and remove a beta API, but were unable to, because it was de facto stable, according to the community. This meant that cluster operators and users had a lot of breaking changes when doing upgrades that could have been avoided. This is really just to help stability as we go through more upgrades in the future.</p>\n<p><strong>CRAIG BOX: I understand that only applies now to new APIs. Things that are in beta at the moment will continue to be available. So there'll be no breaking changes again?</strong></p>\n<p>JAMES LAVERACK: That's correct. There's no breaking changes in beta APIs other than the ones we've documented this release. It's only new things.</p>\n<p><strong>CRAIG BOX: Now in this release, <a href=\"https://github.com/kubernetes/enhancements/issues/3031\">the artifacts are signed</a> using Cosign signatures, and there is <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-images/\">experimental support for verification of those signatures</a>. What needed to happen to make that process possible?</strong></p>\n<p>JAMES LAVERACK: This was a huge process from the other half of SIG Release. SIG Release has the release team, but it also has the release engineering team that handles the mechanics of actually pushing releases out. They have spent, and one of my friends over there, Adolfo, has spent a lot of time trying to bring us in line with <a href=\"https://slsa.dev/\">SLSA</a> compliance. I believe we're <a href=\"https://github.com/kubernetes/enhancements/issues/3027\">looking now at Level 3 compliance</a>.</p>\n<p>SLSA is a framework that describes software supply chain security. That is, of course, a really big issue in our industry at the moment. And it's really good to see the project adopting the best practices for this.</p>\n<p><strong>CRAIG BOX: I was looking back at <a href=\"https://kubernetespodcast.com/episode/167-kubernetes-1.23/\">the conversation I had with Rey Lejano about the 1.23 release</a>, and we were basically approaching Level 2. We're now obviously stepping up to Level 3. I think I asked Rey at the time was, is it fair to say that SLSA is inspired by large projects like Kubernetes, and in theory, it should be really easy for these projects to tick the boxes to get to that level, because the SLSA framework is written with a project like Kubernetes in mind?</strong></p>\n<p>JAMES LAVERACK: I think so. I think it's been somewhat difficult, just because it's one thing to do it, but it's another thing to prove that you're doing it, which is the whole point around these frameworks ‚Äî the assertation, that proof.</p>\n<p><strong>CRAIG BOX: As an end user of Kubernetes, whether I install it myself or I take it from a service like GKE, what will this provenance then let me prove? If we think back to <a href=\"https://kubernetespodcast.com/episode/174-in-toto/\">the orange juice example we talked to Santiago about recently</a>, how do I tell that my software is safe to run?</strong></p>\n<p>JAMES LAVERACK: If you're downloading and running Kubernetes yourself, you can use the verifying image signatures feature to verify the thing you've downloaded, and the thing you are running, is actually the thing that the Kubernetes project has released, and that it has been built from the actual source code in the Kubernetes GitHub repository. This can give you a lot of confidence in what you're running, especially if you're running in a highly secure or regulated environment of some kind.</p>\n<p>As an end user, this isn't something that will necessarily directly impact you. But it means that service providers that provide managed Kubernetes options, such as Google and GKE, can provide even greater levels of security and safety themselves about the services that they run.</p>\n<p><strong>CRAIG BOX: A lot of people get access to their Kubernetes server just by being granted an API endpoint, and they start running kubectl against it. They're not actually installing their own Kubernetes. They have a provider or a platform team do it for them. Do you think it's feasible to get to a world where there's something that you can run when you're deploying your workloads which queries the API server, for example, and gets access to that same provenance data?</strong></p>\n<p>JAMES LAVERACK: I think it's going to be very difficult to do it that way, simply because this provenance and assertation data implies that you actually have access to the underlying executables, which typically, when you're running in a managed platform, you don't. If you're having Kubernetes provided to you, I think you're still going to have to trust the platform team or the organization that's providing it to you.</p>\n<p><strong>CRAIG BOX: Just like when you go to the hotel breakfast bar, you have to trust that they've been good with their orange juice.</strong></p>\n<p>JAMES LAVERACK: Yeah, I think the orange juice example is great. If you're making it yourself, then you can use assertation. If you're not, if you've just been given a glass, then you're going to have to trust who's pouring it.</p>\n<p><strong>CRAIG BOX: Continuing with our exploration of new stable features, <a href=\"https://github.com/kubernetes/enhancements/issues/1472\">storage capacity tracking</a> and <a href=\"https://github.com/kubernetes/enhancements/issues/284\">volume expansion</a> are generally available. What do those features enable me to do?</strong></p>\n<p>JAMES LAVERACK: This is a really great set of stable features coming out of SIG Storage. Storage capacity tracking allows applications on Kubernetes to use the Kubernetes API to understand how much storage is available, which can drive application decisions. With volume expansion, that again allows an application to use the Kubernetes API to request additional storage, which can enable applications to make all kinds of operational decisions.</p>\n<p><strong>CRAIG BOX: SIG Storage are also working through <a href=\"https://github.com/kubernetes/enhancements/issues/625\">a project to migrate all of their in-tree storage plugins out to CSI plugins</a>. How are they going with that process?</strong></p>\n<p>JAMES LAVERACK: In 1.24 we have a couple of them that have been migrated out. The <a href=\"https://github.com/kubernetes/enhancements/issues/1490\">Azure Disk</a> and <a href=\"https://github.com/kubernetes/enhancements/issues/1489\">OpenStack Cinder</a> plugins have both been migrated. They're maintaining the original API, but the actual implementation now happens in those CSI plugins.</p>\n<p><strong>CRAIG BOX: Do they have a long way to go, or are they just cutting off a couple every release?</strong></p>\n<p>JAMES LAVERACK: They're just doing a couple every release from what I see. There are a couple of others to go. This is really part of a larger theme within Kubernetes, which is pushing application-specific things out behind interfaces, such as the container storage interface and the container runtime interface.</p>\n<p><strong>CRAIG BOX: That obviously sets up a situation where you have a stable interface and you can have beta implementations of that that are outside of Kubernetes and get around the problem we talked about before with not being able to run beta things.</strong></p>\n<p>JAMES LAVERACK: Yeah, exactly. It also makes it easy to expand Kubernetes. You don't have to try to get code in-tree in order to implement a new storage engine, for example.</p>\n<p><strong>CRAIG BOX: <a href=\"https://github.com/kubernetes/enhancements/issues/2727\">gRPC probes have graduated to beta in 1.24</a>. What does that functionality provide?</strong></p>\n<p>JAMES LAVERACK: This is one of the changes that's going to be most visible to application developers in Kubernetes, I think. Until now, Kubernetes has had the ability to do readiness and liveness checks on containers and be able to make intelligent routing and pod restart decisions based on those. But those checks had to be HTTP REST endpoints.</p>\n<p>With Kubernetes 1.24, we're enabling a beta feature that allows them to use gRPC. This means that if you're building an application that is primarily gRPC-based, as many microservices applications are, you can now use that same technology in order to implement your probes without having to bundle an HTTP server as well.</p>\n<p><strong>CRAIG BOX: Are there any other enhancements that are particularly notable or relevant perhaps to the work you've been doing?</strong></p>\n<p>JAMES LAVERACK: There's a really interesting one from SIG Network which is about <a href=\"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#avoiding-collisions-in-ip-allocation-to-services\">avoiding collisions in IP allocations to services</a>. In existing versions of Kubernetes, you can allocate a service to have a particular internal cluster IP, or you can leave it blank and it will generate its own IP.</p>\n<p>In Kubernetes 1.24, there's an opt-in feature, which allows you to specify a pool for dynamic IPs to be generated from. This means that you can statically allocate an IP to a service and know that IP can not be accidentally dynamically allocated. This is a problem I've actually had in my local Kubernetes cluster, where I use static IP addresses for a bunch of port forwarding rules. I've always worried that during server start-up, they're going to get dynamically allocated to one of the other services. Now, with 1.24, and this feature, I won't have to worry about it more.</p>\n<p><strong>CRAIG BOX: This is like the analog of allocating an IP in your DHCP server rather than just claiming it statically on your local machine?</strong></p>\n<p>JAMES LAVERACK: Pretty much. It means that you can't accidentally double allocate something.</p>\n<p><strong>CRAIG BOX: Why don't we all just use IPv6?</strong></p>\n<p>JAMES LAVERACK: That is a very deep question I don't think we have time for.</p>\n<p><strong>CRAIG BOX: The margins of this podcast would be unable to contain it even if we did.</strong></p>\n<p>JAMES LAVERACK: [LAUGHING]</p>\n<p><strong>CRAIG BOX: <a href=\"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#release-theme-and-logo\">The theme for Kubernetes 1.24 is Stargazer</a>. How did you pick that as the theme?</strong></p>\n<p>JAMES LAVERACK: Every release lead gets to pick their theme, pretty much by themselves. When I started, I asked Rey, the previous release lead, how he picked his theme, because he picked the Next Frontier for Kubernetes 1.23. And he told me that he'd actually picked it before the release even started, which meant for the first couple of weeks and months of the release, I was really worried about it, because I hadn't picked one yet, and I wasn't sure what to pick.</p>\n<p>Then again, I was speaking to another former release lead, and they told me that they picked theirs like two weeks out. It seems to really vary. About halfway through the release, I had some ideas down. I thought maybe we could talk about ‚Äî I live in a city called Bristol in the UK, which has a very famous bridge ‚Äî and I thought, oh, we could talk about bridges and architectural and a metaphor for community bridging gaps and things like this. I kind of liked the idea, but it didn't really grab me.</p>\n<p>One thing about me is that I am a serious night owl. I cannot work effectively in the mornings. I've always enjoyed the night. And that got me thinking about astronomy and the stars. I think one night I was trying to get to sleep, because I couldn't sleep, and I was watching <a href=\"https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g\">PBS Space Time</a>, which is this fantastic YouTube channel talking about physics. And I'm not a physicist. I don't understand any of the maths. But I find it really interesting as a topic.</p>\n<p>I just thought, well, why don't I make a theme about stars. Kubernetes has often had a space theme in many releases. As I'm sure you're aware, its original name was based off of Star Trek. The previous release had a Star Trek-based theme. I thought, well, let's do that. So I came up with the idea of Stargazer.</p>\n<p><strong>CRAIG BOX: Once you have a theme, you then need a release logo. I understand you have a household artist?</strong></p>\n<p>JAMES LAVERACK: [LAUGHS] I don't think she'd appreciate being called that, but, yes. My wife is an artist, and in particular, a digital artist. I had a bit of a conversation with the SIG Release folks to see if they'd be comfortable with my wife doing it, and they said they'd be completely fine with that.</p>\n<p>I asked if she would be willing to spend some time creating a logo for us. And thankfully for me, she was. She has produced this ‚Äî well, I'm somewhat obliged to say ‚Äî she produced us a beautiful logo, which you can see in our release blog and probably around social media. It is a telescope set over starry skies, and I absolutely love it.</p>\n<p><strong>CRAIG BOX: It is objectively very nice. It obviously has the seven stars or the Seven Sisters of the Pleiades. Do the colors have any particular meaning?</strong></p>\n<p>JAMES LAVERACK: The colors are based on the Kubernetes blue. If you look in the background, that haze is actually in the shape of a Kubernetes wheel from the original Kubernetes logo.</p>\n<p><strong>CRAIG BOX: You must have to squint at it the right way. Very abstract. As is the wont of art.</strong></p>\n<p>JAMES LAVERACK: As is the wont.</p>\n<p><strong>CRAIG BOX: You mentioned before Rey Lejano, the 1.23 release lead. We ask every interview what the person learned from the last lead and what they're going to put in the proverbial envelope for the next. At the time, Rey said that he would encourage you to use teachable moments in the release team meetings. Was that something you were able to do?</strong></p>\n<p>JAMES LAVERACK: Not as much as I would have liked. I think the thing that I really took from Rey was communicate more. I've made a big effort this time to put as much communication in the open as possible. I was actually worried that I was going to be spamming the SIG Release Slack channel too much. I asked our SIG Release chairs Stephen and Sasha about it. And they said, just don't worry about it. Just spam as much as you want.</p>\n<p>And so I think the majority of the conversation in SIG Release Slack over the past few months has just been me. [LAUGHING] That seemed to work out pretty well.</p>\n<p><strong>CRAIG BOX: That's what it's for.</strong></p>\n<p>JAMES LAVERACK: It is what it's for. But SIG Release does more than just the individual release process, of course. It's release engineering, too.</p>\n<p><strong>CRAIG BOX: I'm sure they'd be interested in what's going on anyway?</strong></p>\n<p>JAMES LAVERACK: It's true. It's true. It's been really nice to be able to talk to everyone that way, I think.</p>\n<p><strong>CRAIG BOX: We talked before about your introduction to Kubernetes being at a KubeCon, and meeting people in person. How has it been running the release almost entirely virtually?</strong></p>\n<p>JAMES LAVERACK: It's not been so bad. The release team has always been geographically distributed, somewhat by design. It's always been a very virtual engagement, so I don't think it's been impacted too, too much by the pandemic and travel restrictions. Of course, I'm looking forward to KubeCon Valencia and being able to see everyone again. But I think the release team has handled excellently in the current situation.</p>\n<p><strong>CRAIG BOX: What is the advice that you will pass on to <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md\">the next release lead</a>, which has been announced to be Cici Huang from Google?</strong></p>\n<p>JAMES LAVERACK: I would say to Cici that open communication is really important. I made a habit of posting every single week in SIG Release a summary of what's happened. I'm super-glad that I did that, and I'm going to encourage her to do the same if she wants to.</p>\n<p><strong>CRAIG BOX: This release was originally due out two weeks earlier, but <a href=\"https://groups.google.com/a/kubernetes.io/g/dev/c/9IZaUGVMnmo\">it was delayed</a>. What happened?</strong></p>\n<p>JAMES LAVERACK: That delay was the result of a release-blocking bug ‚Äî an absolute showstopper. This was in the underlying Go implementation of TLS certificate verification. It meant that a lot of clients simply would not be able to connect to clusters or anything else. So we took the decision that we can't release with a bug this big. Thus the term release-blocking.</p>\n<p>The fix had to be merged upstream in Go 1.18.1, and then we had to, of course, rebuild and release release candidates. Given the time we like to have things to sit and stabilize after we make a lot of changes like that, we felt it was more prudent to push out the release by a couple of weeks than risk shipping a broken point-zero.</p>\n<p><strong>CRAIG BOX: Go 1.18 is itself quite new. How does the project decide how quickly to upgrade its underlying programming language?</strong></p>\n<p>JAMES LAVERACK: A lot of it is driven by support requirements. We support each release for three releases. So Kubernetes 1.24 will be most likely in support until this time next year, in 2023, as we do three releases per year. That means that right up until May, 2023, we're probably going to be shipping updates for Kubernetes 1.24, which means that the version of Go we're using, and other dependencies, have to be supported as well. My understanding is that the older version of Go, Go 1.17, just wouldn't be supported long enough.</p>\n<p>Any underlying critical bug fixes that were coming in, they wouldn't have been back ported to Go 1.17, and therefore we might not be able to adequately support Kubernetes 1.24.</p>\n<p><strong>CRAIG BOX: A side effect of the unfortunate delay was an unfortunate holiday situation, where you were booked to take the week after the release off and instead you ended up taking the week before the release off. Were you able to actually have any holiday and relax in that situation?</strong></p>\n<p>JAMES LAVERACK: Well, I didn't go anywhere, if that's what you're asking.</p>\n<p><strong>CRAIG BOX: No one ever does. This is what the pandemic's been, staycations.</strong></p>\n<p>JAMES LAVERACK: Yeah, staycations. It's been interesting. On the one hand, I've done a lot of Kubernetes work in that time. So you could argue it's not really been a holiday. On the other hand, my highly annoying friends have gotten me into playing an MMO, so I've been spending a lot of time playing that.</p>\n<p><strong>CRAIG BOX: I hear also you have a new vacuum cleaner?</strong></p>\n<p>JAMES LAVERACK: [LAUGHS] You've been following my Twitter. Yes, I couldn't find the charging cord for my old vacuum cleaner. And so I decided just to buy a new one. I decided, at long last, just to buy one of the nice brand-name ones. And it is just better.</p>\n<p><strong>CRAIG BOX: This isn't the BBC. You're allowed to name it if you want.</strong></p>\n<p>JAMES LAVERACK: Yes, we went and bought one of these nice Dyson vacuum cleaners, and the first time I've gotten one so expensive. On the one hand, I feel a little bit bad spending a lot of money on a vacuum cleaner. On the other hand, it's so much easier.</p>\n<p><strong>CRAIG BOX: Is it one of those handheld ones, like a giant Dust-Buster with a long leg?</strong></p>\n<p>JAMES LAVERACK: No, I got one of the corded floor ones, because the problem was, of course, I lost the charger for the last one, so I didn't want that to happen again. So I got a wall plug-in one.</p>\n<p><strong>CRAIG BOX: I must say, going from a standard <a href=\"https://www.myhenry.com/\">Henry Hoover</a> to ‚Äî the place we're staying at the moment has what I'll call a knock-off Dyson portable vacuum cleaner ‚Äî having something that you can just pick up and carry around with you, and not have to worry about the cord, actually does encourage me to keep the place tidier.</strong></p>\n<p>JAMES LAVERACK: Really? I think our last one was corded, but it didn't encourage us to use it anymore, just because it was so useless.</p>\n<hr>\n<p><em><a href=\"https://twitter.com/jameslaverack\">James Laverack</a> is a Staff Solutions Engineer at Jetstack, and was the release team lead for Kubernetes 1.24.</em></p>\n<p><em>You can find the <a href=\"http://www.kubernetespodcast.com/\">Kubernetes Podcast from Google</a> at <a href=\"https://twitter.com/KubernetesPod\">@KubernetesPod</a> on Twitter, and you can <a href=\"https://kubernetespodcast.com/subscribe/\">subscribe</a> so you never miss an episode.</em></p>","PublishedAt":"2022-08-18 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/","SourceName":"Kubernetes"}},{"node":{"ID":1390,"Title":"Blog: Meet Our Contributors - APAC (China region)","Description":"<p><strong>Authors &amp; Interviewers:</strong> <a href=\"https://github.com/AvineshTripathi\">Avinesh Tripathi</a>, <a href=\"https://github.com/Debanitrkl\">Debabrata Panigrahi</a>, <a href=\"https://github.com/jayesh-srivastava\">Jayesh Srivastava</a>, <a href=\"https://github.com/Priyankasaggu11929/\">Priyanka Saggu</a>, <a href=\"https://github.com/PurneswarPrasad\">Purneswar Prasad</a>, <a href=\"https://github.com/vedant-kakde\">Vedant Kakde</a></p>\n<hr>\n<p>Hello, everyone üëã</p>\n<p>Welcome back to the third edition of the &quot;Meet Our Contributors&quot; blog post series for APAC.</p>\n<p>This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project.</p>\n<p>So, without further ado, let's get straight to the article.</p>\n<h2 id=\"andy-zhang-https-github-com-andyzhangx\"><a href=\"https://github.com/andyzhangx\">Andy Zhang</a></h2>\n<p>Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago.</p>\n<p>He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code.</p>\n<p>His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects.</p>\n<p>Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.</p>\n<h2 id=\"shiming-zhang-https-github-com-wzshiming\"><a href=\"https://github.com/wzshiming\">Shiming Zhang</a></h2>\n<p>Shiming Zhang is a Software Engineer working on Kubernetes for DaoCloud in Shanghai, China.</p>\n<p>He has mostly been involved with SIG Node as a reviewer. His major contributions have mainly been bug fixes and feature improvements in an ongoing <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown\">KEP</a>, all revolving around SIG Node.</p>\n<p>Some of his major PRs are <a href=\"https://github.com/kubernetes/kubernetes/pull/100326\">fixing watchForLockfileContention memory leak</a>, <a href=\"https://github.com/kubernetes/kubernetes/pull/101093\">fixing startupProbe behaviour</a>, <a href=\"https://github.com/kubernetes/enhancements/pull/2661\">adding Field status.hostIPs for Pod</a>.</p>\n<h2 id=\"paco-xu-https-github-com-pacoxu\"><a href=\"https://github.com/pacoxu\">Paco Xu</a></h2>\n<p>Paco Xu works at DaoCloud, a Shanghai-based cloud-native firm. He works with the infra and the open source team, focusing on enterprise cloud native platforms based on Kubernetes.</p>\n<p>He started with Kubernetes in early 2017 and his first contribution was in March 2018. He started with a bug that he found, but his solution was not that graceful, hence wasn't accepted. He then started with some good first issues, which helped him to a great extent. In addition to this, from 2016 to 2017, he made some minor contributions to Docker.</p>\n<p>Currently, Paco is a reviewer for <code>kubeadm</code> (a SIG Cluster Lifecycle product), and for SIG Node.</p>\n<p>Paco says that you should contribute to open source projects you use. For him, an open source project is like a book to learn, getting inspired through discussions with the project maintainers.</p>\n<blockquote>\n<p>In my opinion, the best way for me is learning how owners work on the project.</p>\n</blockquote>\n<h2 id=\"jintao-zhang-https-github-com-tao12345666333\"><a href=\"https://github.com/tao12345666333\">Jintao Zhang</a></h2>\n<p>Jintao Zhang is presently employed at API7, where he focuses on ingress and service mesh.</p>\n<p>In 2017, he encountered an issue which led to a community discussion and his contributions to Kubernetes started. Before contributing to Kubernetes, Jintao was a long-time contributor to Docker-related open source projects.</p>\n<p>Currently Jintao is a reviewer for the <a href=\"https://kubernetes.github.io/ingress-nginx/\">ingress-nginx</a> project.</p>\n<p>He suggests keeping track of job opportunities at open source companies so that you can find one that allows you to contribute full time. For new contributors Jintao says that if anyone wants to make a significant contribution to an open source project, then they should choose the project based on their interests and should generously invest time.</p>\n<hr>\n<p>If you have any recommendations/suggestions for who we should interview next, please let us know in the <a href=\"https://kubernetes.slack.com/archives/C1TU9EB9S\">#sig-contribex channel</a> channel on the Kubernetes Slack. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.</p>\n<p>We'll see you all in the next one. Everyone, till then, have a happy contributing! üëã</p>","PublishedAt":"2022-08-15 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/","SourceName":"Kubernetes"}},{"node":{"ID":1380,"Title":"Blog: Enhancing Kubernetes one KEP at a Time","Description":"<p><strong>Author:</strong> Ryler Hockenbury (Mastercard)</p>\n<p>Did you know that Kubernetes v1.24 has <a href=\"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/\">46 enhancements</a>? That's a lot of new functionality packed into a 4-month release cycle. The Kubernetes release team coordinates the logistics of the release, from remediating test flakes to publishing updated docs. It's a ton of work, but they always deliver.</p>\n<p>The release team comprises around 30 people across six subteams - Bug Triage, CI Signal, Enhancements, Release Notes, Communications, and Docs.¬† Each of these subteams manages a component of the release. This post will focus on the role of the enhancements subteam and how you can get involved.</p>\n<h2 id=\"what-s-the-enhancements-subteam\">What's the enhancements subteam?</h2>\n<p>Great question. We'll get to that in a second but first, let's talk about how features are managed in Kubernetes.</p>\n<p>Each new feature requires a <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/README.md\">Kubernetes Enhancement Proposal</a> - KEP for short. KEPs are small structured design documents that provide a way to propose and coordinate new features. The KEP author describes the motivation, design (and alternatives), risks, and tests - then community members provide feedback to build consensus.</p>\n<p>KEPs are submitted and updated through a pull request (PR) workflow on the <a href=\"https://github.com/kubernetes/enhancements\">k/enhancements repo</a>. Features start in alpha and move through a graduation process to beta and stable as they mature. For example, here's a cool KEP about <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/1981-windows-privileged-container-support/kep.yaml\">privileged container support on Windows Server</a>.¬† It was introduced as alpha in Kubernetes v1.22 and graduated to beta in v1.23.</p>\n<p>Now getting back to the question - the enhancements subteam coordinates the lifecycle tracking of the KEPs for each release. Each KEP is required to meet a set of requirements to be cleared for inclusion in a release. The enhancements subteam verifies each requirement for each KEP and tracks the status.</p>\n<p>At the start of a release, <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Kubernetes Special Interest Groups</a> (SIGs) submit their enhancements to opt into a release. A typical release might have from 60 to 90 enhancements at the beginning.¬† During the release, many enhancements will drop out. Some do not quite meet the KEP requirements, and others do not complete their implementation in code. About 60%-70% of the opted-in KEPs will make it into the final release.</p>\n<h2 id=\"what-does-the-enhancements-subteam-do\">What does the enhancements subteam do?</h2>\n<p>Another great question, keep them coming! The enhancements team is involved in two crucial milestones during each release: enhancements freeze and code freeze.</p>\n<h4 id=\"enhancements-freeze\">Enhancements Freeze</h4>\n<p>Enhancements freeze is the deadline for a KEP to be complete in order for the enhancement to be included in a release. It's a quality gate to enforce alignment around maintaining and updating KEPs. The most notable requirements are a (1) <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md\">production readiness review </a>(PRR) and a (2) <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/NNNN-kep-template\">KEP file</a> with a complete test plan and graduation criteria.</p>\n<p>The enhancements subteam communicates to each KEP author through comments on the KEP issue on Github. As a first step, they'll verify the status and check if it meets the requirements.¬† The KEP gets marked as tracked after satisfying the requirements; otherwise, it's considered at risk. If a KEP is still at risk when enhancement freeze is in effect, the KEP is removed from the release.</p>\n<p>This part of the cycle is typically the busiest for the enhancements subteam because of the large number of KEPs to groom, and each KEP might need to be visited multiple times to verify whether it meets requirements.</p>\n<h4 id=\"code-freeze\">Code Freeze</h4>\n<p>Code freeze is the implementation deadline for all enhancements. The code must be implemented, reviewed, and merged by this point if a code change or update is needed for the enhancement. The latter third of the release is focused on stabilizing the codebase - fixing flaky tests, resolving various regressions, and preparing docs - and all the code needs to be in place before those steps can happen.</p>\n<p>The enhancements subteam verifies that all PRs for an enhancement are merged into the <a href=\"https://github.com/kubernetes/kubernetes\">Kubernetes codebase</a> (k/k). During this period, the subteam reaches out to KEP authors to understand what PRs are part of the KEP, verifies that those PRs get merged, and then updates the status of the KEP. The enhancement is removed from the release if the code isn't all merged before the code freeze deadline.</p>\n<h2 id=\"how-can-i-get-involved-with-the-release-team\">How can I get involved with the release team?</h2>\n<p>I'm glad you asked. The most direct way is to apply to be a <a href=\"https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md\">release team shadow</a>. The shadow role is a hands-on apprenticeship intended to prepare individuals for leadership positions on the release team. Many shadow roles are non-technical and do not require prior contributions to the Kubernetes codebase.</p>\n<p>With 3 Kubernetes releases every year and roughly 25 shadows per release, the release team is always in need of individuals wanting to contribute. Before each release cycle, the release team opens the application for the shadow program. When the application goes live, it's posted in the <a href=\"https://groups.google.com/a/kubernetes.io/g/dev\">Kubernetes Dev Mailing List</a>.¬† You can subscribe to notifications from that list (or check it regularly!) to watch when the application opens. The announcement will typically go out in mid-April, mid-July, and mid-December - or roughly a month before the start of each release.</p>\n<h2 id=\"how-can-i-find-out-more\">How can I find out more?</h2>\n<p>Check out the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks\">role handbooks</a> if you're curious about the specifics of all the Kubernetes release subteams. The handbooks capture the logistics of each subteam, including a week-by-week breakdown of the subteam activities.¬† It's an excellent reference for getting to know each team better.</p>\n<p>You can also check out the release-related Kubernetes slack channels - particularly #release, #sig-release, and #sig-arch. These channels have discussions and updates surrounding many aspects of the release.</p>","PublishedAt":"2022-08-11 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/","SourceName":"Kubernetes"}},{"node":{"ID":1326,"Title":"Blog: Kubernetes Removals and Major Changes In 1.25","Description":"<p><strong>Authors</strong>: Kat Cosgrove, Frederico Mu√±oz, Debabrata Panigrahi</p>\n<p>As Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements for the health of the project. Kubernetes v1.25 includes several major changes and one major removal.</p>\n<h2 id=\"the-kubernetes-api-removal-and-deprecation-process\">The Kubernetes API Removal and Deprecation process</h2>\n<p>The Kubernetes project has a well-documented <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">deprecation policy</a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</p>\n<ul>\n<li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</li>\n<li>Beta or pre-release API versions must be supported for 3 releases after deprecation.</li>\n<li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</li>\n</ul>\n<p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.</p>\n<h2 id=\"a-note-about-podsecuritypolicy\">A Note About PodSecurityPolicy</h2>\n<p>In Kubernetes v1.25, we will be removing PodSecurityPolicy <a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">after its deprecation in v1.21</a>. PodSecurityPolicy has served us honorably, but its complex and often confusing usage necessitated changes, which unfortunately would have been breaking changes. To address this, it is being removed in favor of a replacement, Pod Security Admission, which is graduating to stable in this release as well. If you are currently relying on PodSecurityPolicy, follow the instructions for <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/\">migration to Pod Security Admission</a>.</p>\n<h2 id=\"major-changes-for-kubernetes-v1-25\">Major Changes for Kubernetes v1.25</h2>\n<p>Kubernetes v1.25 includes several major changes, in addition to the removal of PodSecurityPolicy.</p>\n<h3 id=\"csi-migration-https-github-com-kubernetes-enhancements-issues-625\"><a href=\"https://github.com/kubernetes/enhancements/issues/625\">CSI Migration</a></h3>\n<p>The effort to move the in-tree volume plugins to out-of-tree CSI drivers continues, with the core CSI Migration feature going GA in v1.25. This is an important step towards removing the in-tree volume plugins entirely.</p>\n<h3 id=\"volume-plugin-deprecations-and-removals\">Volume Plugin Deprecations and Removals</h3>\n<p>Several volume are being deprecated or removed.</p>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/3446\">GlusterFS will be deprecated in v1.25</a>. While a CSI driver was built for it, it has not been maintained. The possibility of migration to a compatible CSI driver <a href=\"https://github.com/kubernetes/kubernetes/issues/100897\">was discussed</a>, but a decision was ultimately made to begin the deprecation of the GlusterFS plugin from in-tree drivers. The <a href=\"https://github.com/kubernetes/enhancements/issues/2589\">Portworx in-tree volume plugin</a> is also being deprecated with this release. The Flocker, Quobyte, and StorageOS in-tree volume plugins are being removed.</p>\n<h3 id=\"declare-unsupported-vsphere-versions-https-github-com-kubernetes-kubernetes-pull-111255\"><a href=\"https://github.com/kubernetes/kubernetes/pull/111255\">Declare Unsupported vSphere Versions</a></h3>\n<p>From Kubernetes v1.25, the in-tree vSphere volume driver will not support any vSphere release before 7.0u2. Check the v1.25 detailed release notes for more advice on how to handle this.</p>\n<h3 id=\"signing-release-artifacts-https-github-com-kubernetes-enhancements-issues-3031\"><a href=\"https://github.com/kubernetes/enhancements/issues/3031\">Signing Release Artifacts</a></h3>\n<p>An additional step in improving the security posture of the release process, the signing of Kubernetes release artifacts will graduate to Beta in this release. This is in line with the proposed enhancement of targeting SLSA Level 3 compliance for the Kubernetes release process.</p>\n<h3 id=\"support-for-cgroup-v2-graduating-to-stable-https-github-com-kubernetes-enhancements-issues-2254\"><a href=\"https://github.com/kubernetes/enhancements/issues/2254\">Support for cgroup v2 Graduating to Stable</a></h3>\n<p>The new kernel cgroups v2 API was declared stable more than two years ago, and in this release we're taking solid steps towards full adoption of it. While cgroup v1 will continue to be supported, this change makes us ready to deal with the eventual deprecation of cgroup v1 and its replacement by cgroup v2.</p>\n<h3 id=\"cleaning-up-iptables-chain-ownership-https-github-com-kubernetes-enhancements-issues-3178\"><a href=\"https://github.com/kubernetes/enhancements/issues/3178\">Cleaning up IPTables Chain Ownership</a></h3>\n<p>From the Kubernetes 1.25 release, the iptables chains created by Kubernetes will only support for internal Kubernetes use cases. Starting with v1.25, the Kubelet will gradually move towards not creating the following iptables chains in the <code>nat</code> table:</p>\n<ul>\n<li><code>KUBE-MARK-DROP</code></li>\n<li><code>KUBE-MARK-MASQ</code></li>\n<li><code>KUBE-POSTROUTING</code></li>\n</ul>\n<p>This change will be phased in via the <code>IPTablesCleanup</code> feature gate.</p>\n<h2 id=\"looking-ahead\">Looking ahead</h2>\n<p>The official <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26\">list of API removals planned for Kubernetes 1.26</a> is:</p>\n<ul>\n<li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)</li>\n<li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)</li>\n</ul>\n<h3 id=\"want-to-know-more\">Want to know more?</h3>\n<p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation\">Kubernetes 1.21</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation\">Kubernetes 1.22</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation\">Kubernetes 1.23</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation\">Kubernetes 1.24</a></li>\n<li>We will formally announce the deprecations that come with <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation\">Kubernetes 1.25</a> as part of the CHANGELOG for that release.</li>\n</ul>\n<p>For information on the process of deprecation and removal, check out the official Kubernetes <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api\">deprecation policy</a> document.</p>","PublishedAt":"2022-08-04 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/","SourceName":"Kubernetes"}},{"node":{"ID":1291,"Title":"Blog: Spotlight on SIG Docs","Description":"<p><strong>Author:</strong> Purneswar Prasad</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>The official documentation is the go-to source for any open source project. For Kubernetes,\nit's an ever-evolving Special Interest Group (SIG) with people constantly putting in their efforts\nto make details about the project easier to consume for new contributors and users. SIG Docs publishes\nthe official documentation on <a href=\"https://kubernetes.io\">kubernetes.io</a> which includes,\nbut is not limited to, documentation of the core APIs, core architectural details, and CLI tools\nshipped with the Kubernetes release.</p>\n<p>To learn more about the work of SIG Docs and its future ahead in shaping the community, I have summarised\nmy conversation with the co-chairs, <a href=\"https://twitter.com/Divya_Mohan02\">Divya Mohan</a> (DM),\n<a href=\"https://twitter.com/reylejano\">Rey Lejano</a> (RL) and Natali Vlatko (NV), who ran through the\nSIG's goals and how fellow contributors can help.</p>\n<h2 id=\"a-summary-of-the-conversation\">A summary of the conversation</h2>\n<h3 id=\"could-you-tell-us-a-little-bit-about-what-sig-docs-does\">Could you tell us a little bit about what SIG Docs does?</h3>\n<p>SIG Docs is the special interest group for documentation for the Kubernetes project on kubernetes.io,\ngenerating reference guides for the Kubernetes API, kubeadm and kubectl as well as maintaining the official\nwebsite‚Äôs infrastructure and analytics. The remit of their work also extends to docs releases, translation of docs,\nimprovement and adding new features to existing documentation, pushing and reviewing content for the official\nKubernetes blog and engaging with the Release Team for each cycle to get docs and blogs reviewed.</p>\n<h3 id=\"there-are-2-subprojects-under-docs-blogs-and-localization-how-has-the-community-benefited-from-it-and-are-there-some-interesting-contributions-by-those-teams-you-want-to-highlight\">There are 2 subprojects under Docs: blogs and localization. How has the community benefited from it and are there some interesting contributions by those teams you want to highlight?</h3>\n<p><strong>Blogs</strong>: This subproject highlights new or graduated Kubernetes enhancements, community reports, SIG updates\nor any relevant news to the Kubernetes community such as thought leadership, tutorials and project updates,\nsuch as the Dockershim removal and removal of PodSecurityPolicy, which is upcoming in the 1.25 release.\nTim Bannister, one of the SIG Docs tech leads, does awesome work and is a major force when pushing contributions\nthrough to the docs and blogs.</p>\n<p><strong>Localization</strong>: With this subproject, the Kubernetes community has been able to achieve greater inclusivity\nand diversity among both users and contributors. This has also helped the project gain more contributors,\nespecially students, since a couple of years ago.\nOne of the major highlights and up-and-coming localizations are Hindi and Bengali. The efforts for Hindi\nlocalization are currently being spearheaded by students in India.</p>\n<p>In addition to that, there are two other subprojects: <a href=\"https://github.com/kubernetes-sigs/reference-docs\">reference-docs</a> and the <a href=\"https://github.com/kubernetes/website\">website</a>, which is built with Hugo and is an important ownership area.</p>\n<h3 id=\"dockershim-removal\">Recently there has been a lot of buzz around the Kubernetes ecosystem as well as the industry regarding the removal of dockershim in the latest 1.24 release. How has SIG Docs helped the project to ensure a smooth change among the end-users?</h3>\n<p>Documenting the removal of Dockershim was a mammoth task, requiring the revamping of existing documentation\nand communicating to the various stakeholders regarding the deprecation efforts. It needed a community effort,\nso ahead of the 1.24 release, SIG Docs partnered with Docs and Comms verticals, the Release Lead from the\nRelease Team, and also the CNCF to help put the word out. Weekly meetings and a GitHub project board were\nset up to track progress, review issues and approve PRs and keep the Kubernetes website updated. This has\nalso helped new contributors know about the depreciation, so that if any good-first-issue pops up, they could chip in.\nA dedicated Slack channel was used to communicate meeting updates, invite feedback or to solicit help on\noutstanding issues and PRs. The weekly meeting also continued for a month after the 1.24 release to review related issues and fix them.\nA huge shoutout to <a href=\"https://twitter.com/celeste_horgan\">Celeste Horgan</a>, who kept the ball rolling on this\nconversation throughout the deprecation process.</p>\n<h3 id=\"why-should-new-and-existing-contributors-consider-joining-this-sig\">Why should new and existing contributors consider joining this SIG?</h3>\n<p>Kubernetes is a vast project and can be intimidating at first for a lot of folks to find a place to start.\nAny open source project is defined by its quality of documentation and SIG Docs aims to be a welcoming,\nhelpful place for new contributors to get onboard. One gets the perks of working with the project docs\nas well as learning by reading it. They can also bring their own, new perspective to create and improve\nthe documentation. In the long run if they stick to SIG Docs, they can rise up the ladder to be maintainers.\nThis will help make a big project like Kubernetes easier to parse and navigate.</p>\n<h3 id=\"how-do-you-help-new-contributors-get-started-are-there-any-prerequisites-to-join\">How do you help new contributors get started? Are there any prerequisites to join?</h3>\n<p>There are no such prerequisites to get started with contributing to Docs. But there is certainly a fantastic\nContribution to Docs guide which is always kept as updated and relevant as possible and new contributors\nare urged to read it and keep it handy. Also, there are a lot of useful pins and bookmarks in the\ncommunity Slack channel <a href=\"https://kubernetes.slack.com/archives/C1J0BPD2M\">#sig-docs</a>. GitHub issues with\nthe good-first-issue labels in the kubernetes/website repo is a great place to create your first PR.\nNow, SIG Docs has a monthly New Contributor Meet and Greet on the first Tuesday of the month with the\nfirst occupant of the New Contributor Ambassador role, <a href=\"https://twitter.com/RinkiyaKeDad\">Arsh Sharma</a>.\nThis has helped in making a more accessible point of contact within the SIG for new contributors.</p>\n<h3 id=\"any-sig-related-accomplishment-that-you-re-really-proud-of\">Any SIG related accomplishment that you‚Äôre really proud of?</h3>\n<p><strong>DM &amp; RL</strong> : The formalization of the localization subproject in the last few months has been a big win\nfor SIG Docs, given all the great work put in by contributors from different countries. Earlier the\nlocalization efforts didn‚Äôt have any streamlined process and focus was given to provide a structure by\ndrafting a KEP over the past couple of months for localization to be formalized as a subproject, which\nis planned to be pushed through by the end of third quarter.</p>\n<p><strong>DM</strong> : Another area where there has been a lot of success is the New Contributor Ambassador role,\nwhich has helped in making a more accessible point of contact for the onboarding of new contributors into the project.</p>\n<p><strong>NV</strong> : For each release cycle, SIG Docs have to review release docs and feature blogs highlighting\nrelease updates within a short window. This is always a big effort for the docs and blogs reviewers.</p>\n<h3 id=\"is-there-something-exciting-coming-up-for-the-future-of-sig-docs-that-you-want-the-community-to-know\">Is there something exciting coming up for the future of SIG Docs that you want the community to know?</h3>\n<p>SIG Docs is now looking forward to establishing a roadmap, having a steady pipeline of folks being able\nto push improvements to the documentation and streamlining community involvement in triaging issues and\nreviewing PRs being filed. To build one such contributor and reviewership base, a mentorship program is\nbeing set up to help current contributors become reviewers. This definitely is a space to watch out for more!</p>\n<h2 id=\"wrap-up\">Wrap Up</h2>\n<p>SIG Docs hosted a <a href=\"https://www.youtube.com/watch?v=GDfcBF5et3Q\">deep dive talk</a>\nduring on KubeCon + CloudNativeCon North America 2021, covering their awesome SIG.\nThey are very welcoming and have been the starting ground into Kubernetes\nfor a lot of new folks who want to contribute to the project.\nJoin the <a href=\"https://github.com/kubernetes/community/blob/master/sig-docs/README.md\">SIG's meetings</a> to find out\nabout the most recent research results, their plans for the forthcoming year, and how to get involved in the upstream Docs team as a contributor!</p>","PublishedAt":"2022-08-02 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/08/02/sig-docs-spotlight-2022/","SourceName":"Kubernetes"}},{"node":{"ID":1212,"Title":"Blog: Kubernetes Gateway API Graduates to Beta","Description":"<p><strong>Authors:</strong> Shane Utt (Kong), Rob Scott (Google), Nick Young (VMware), Jeff Apple (HashiCorp)</p>\n<p>We are excited to announce the v0.5.0 release of Gateway API. For the first\ntime, several of our most important Gateway API resources are graduating to\nbeta. Additionally, we are starting a new initiative to explore how Gateway API\ncan be used for mesh and introducing new experimental concepts such as URL\nrewrites. We'll cover all of this and more below.</p>\n<h2 id=\"what-is-gateway-api\">What is Gateway API?</h2>\n<p>Gateway API is a collection of resources centered around <a href=\"https://gateway-api.sigs.k8s.io/api-types/gateway/\">Gateway</a> resources\n(which represent the underlying network gateways / proxy servers) to enable\nrobust Kubernetes service networking through expressive, extensible and\nrole-oriented interfaces that are implemented by many vendors and have broad\nindustry support.</p>\n<p>Originally conceived as a successor to the well known <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/\">Ingress</a> API, the\nbenefits of Gateway API include (but are not limited to) explicit support for\nmany commonly used networking protocols (e.g. <code>HTTP</code>, <code>TLS</code>, <code>TCP</code>, <code>UDP</code>) as\nwell as tightly integrated support for Transport Layer Security (TLS). The\n<code>Gateway</code> resource in particular enables implementations to manage the lifecycle\nof network gateways as a Kubernetes API.</p>\n<p>If you're an end-user interested in some of the benefits of Gateway API we\ninvite you to jump in and find an implementation that suits you. At the time of\nthis release there are over a dozen <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">implementations</a> for popular API\ngateways and service meshes and guides are available to start exploring quickly.</p>\n<h3 id=\"getting-started\">Getting started</h3>\n<p>Gateway API is an official Kubernetes API like\n<a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a>.\nGateway API represents a superset of Ingress functionality, enabling more\nadvanced concepts. Similar to Ingress, there is no default implementation of\nGateway API built into Kubernetes. Instead, there are many different\n<a href=\"https://gateway-api.sigs.k8s.io/implementations/\">implementations</a> available, providing significant choice in terms of underlying\ntechnologies while providing a consistent and portable experience.</p>\n<p>Take a look at the <a href=\"https://gateway-api.sigs.k8s.io/concepts/api-overview/\">API concepts documentation</a> and check out some of\nthe <a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/\">Guides</a> to start familiarizing yourself with the APIs and how they\nwork. When you're ready for a practical application open the <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">implementations\npage</a> and select an implementation that belongs to an existing technology\nyou may already be familiar with or the one your cluster provider uses as a\ndefault (if applicable). Gateway API is a <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/\">Custom Resource Definition\n(CRD)</a> based API so you'll need to <a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/#install-the-crds\">install the CRDs</a> onto a\ncluster to use the API.</p>\n<p>If you're specifically interested in helping to contribute to Gateway API, we\nwould love to have you! Please feel free to <a href=\"https://github.com/kubernetes-sigs/gateway-api/issues/new/choose\">open a new issue</a> on the\nrepository, or join in the <a href=\"https://github.com/kubernetes-sigs/gateway-api/discussions\">discussions</a>. Also check out the <a href=\"https://gateway-api.sigs.k8s.io/contributing/community/\">community\npage</a> which includes links to the Slack channel and community meetings.</p>\n<h2 id=\"release-highlights\">Release highlights</h2>\n<h3 id=\"graduation-to-beta\">Graduation to beta</h3>\n<p>The <code>v0.5.0</code> release is particularly historic because it marks the growth in\nmaturity to a beta API version (<code>v1beta1</code>) release for some of the key APIs:</p>\n<ul>\n<li><a href=\"https://gateway-api.sigs.k8s.io/api-types/gatewayclass/\">GatewayClass</a></li>\n<li><a href=\"https://gateway-api.sigs.k8s.io/api-types/gateway/\">Gateway</a></li>\n<li><a href=\"https://gateway-api.sigs.k8s.io/api-types/httproute/\">HTTPRoute</a></li>\n</ul>\n<p>This achievement was marked by the completion of several graduation criteria:</p>\n<ul>\n<li>API has been <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">widely implemented</a>.</li>\n<li>Conformance tests provide basic coverage for all resources and have multiple implementations passing tests.</li>\n<li>Most of the API surface is actively being used.</li>\n<li>Kubernetes SIG Network API reviewers have approved graduation to beta.</li>\n</ul>\n<p>For more information on Gateway API versioning, refer to the <a href=\"https://gateway-api.sigs.k8s.io/concepts/versioning/\">official\ndocumentation</a>. To see\nwhat's in store for future releases check out the <a href=\"#next-steps\">next steps</a>\nsection.</p>\n<h3 id=\"release-channels\">Release channels</h3>\n<p>This release introduces the <code>experimental</code> and <code>standard</code> <a href=\"https://gateway-api.sigs.k8s.io/concepts/versioning/#release-channels-eg-experimental-standard\">release channels</a>\nwhich enable a better balance of maintaining stability while still enabling\nexperimentation and iterative development.</p>\n<p>The <code>standard</code> release channel includes:</p>\n<ul>\n<li>resources that have graduated to beta</li>\n<li>fields that have graduated to standard (no longer considered experimental)</li>\n</ul>\n<p>The <code>experimental</code> release channel includes everything in the <code>standard</code> release\nchannel, plus:</p>\n<ul>\n<li><code>alpha</code> API resources</li>\n<li>fields that are considered experimental and have not graduated to <code>standard</code> channel</li>\n</ul>\n<p>Release channels are used internally to enable iterative development with\nquick turnaround, and externally to indicate feature stability to implementors\nand end-users.</p>\n<p>For this release we've added the following experimental features:</p>\n<ul>\n<li><a href=\"https://gateway-api.sigs.k8s.io/geps/gep-957/\">Routes can attach to Gateways by specifying port numbers</a></li>\n<li><a href=\"https://gateway-api.sigs.k8s.io/geps/gep-726/\">URL rewrites and path redirects</a></li>\n</ul>\n<h3 id=\"other-improvements\">Other improvements</h3>\n<p>For an exhaustive list of changes included in the <code>v0.5.0</code> release, please see\nthe <a href=\"https://github.com/kubernetes-sigs/gateway-api/releases/tag/v0.5.0\">v0.5.0 release notes</a>.</p>\n<h2 id=\"gateway-api-for-service-mesh-the-gamma-initiative\">Gateway API for service mesh: the GAMMA Initiative</h2>\n<p>Some service mesh projects have <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">already implemented support for the Gateway\nAPI</a>. Significant overlap\nbetween the Service Mesh Interface (SMI) APIs and the Gateway API has <a href=\"https://github.com/servicemeshinterface/smi-spec/issues/249\">inspired\ndiscussion in the SMI\ncommunity</a> about\npossible integration.</p>\n<p>We are pleased to announce that the service mesh community, including\nrepresentatives from Cilium Service Mesh, Consul, Istio, Kuma, Linkerd, NGINX\nService Mesh and Open Service Mesh, is coming together to form the <a href=\"https://gateway-api.sigs.k8s.io/contributing/gamma/\">GAMMA\nInitiative</a>, a dedicated\nworkstream within the Gateway API subproject focused on Gateway API for Mesh\nManagement and Administration.</p>\n<p>This group will deliver <a href=\"https://gateway-api.sigs.k8s.io/v1beta1/contributing/gep/\">enhancement\nproposals</a> consisting\nof resources, additions, and modifications to the Gateway API specification for\nmesh and mesh-adjacent use-cases.</p>\n<p>This work has begun with <a href=\"https://docs.google.com/document/d/1T_DtMQoq2tccLAtJTpo3c0ohjm25vRS35MsestSL9QU/edit#heading=h.jt37re3yi6k5\">an exploration of using Gateway API for\nservice-to-service\ntraffic</a>\nand will continue with enhancement in areas such as authentication and\nauthorization policy.</p>\n<h2 id=\"next-steps\">Next steps</h2>\n<p>As we continue to mature the API for production use cases, here are some of the highlights of what we'll be working on for the next Gateway API releases:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes-sigs/gateway-api/blob/master/site-src/geps/gep-1016.md\">GRPCRoute</a> for <a href=\"https://grpc.io/\">gRPC</a> traffic routing</li>\n<li><a href=\"https://github.com/kubernetes-sigs/gateway-api/pull/1085\">Route delegation</a></li>\n<li>Layer 4 API maturity: Graduating <a href=\"https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tcproute_types.go\">TCPRoute</a>, <a href=\"https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/udproute_types.go\">UDPRoute</a> and\n<a href=\"https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tlsroute_types.go\">TLSRoute</a> to beta</li>\n<li><a href=\"https://gateway-api.sigs.k8s.io/contributing/gamma/\">GAMMA Initiative</a> - Gateway API for Service Mesh</li>\n</ul>\n<p>If there's something on this list you want to get involved in, or there's\nsomething not on this list that you want to advocate for to get on the roadmap\nplease join us in the #sig-network-gateway-api channel on Kubernetes Slack or our weekly <a href=\"https://gateway-api.sigs.k8s.io/contributing/community/#meetings\">community calls</a>.</p>","PublishedAt":"2022-07-13 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/07/13/gateway-api-graduates-to-beta/","SourceName":"Kubernetes"}},{"node":{"ID":1213,"Title":"Blog: Annual Report Summary 2021","Description":"<p><strong>Author:</strong> Paris Pittman (Steering Committee)</p>\n<p>Last year, we published our first <a href=\"https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/\">Annual Report Summary</a> for 2020 and it's already time for our second edition!</p>\n<p><a href=\"https://www.cncf.io/reports/kubernetes-annual-report-2021/\">2021 Annual Report Summary</a></p>\n<p>This summary reflects the work that has been done in 2021 and the initiatives on deck for the rest of 2022. Please forward to organizations and indidviduals participating in upstream activities, planning cloud native strategies, and/or those looking to help out. To find a specific community group's complete report, go to the <a href=\"https://github.com/kubernetes/community\">kubernetes/community repo</a> under the groups folder. Example: <a href=\"https://github.com/kubernetes/community/blob/master/sig-api-machinery/annual-report-2021.md\">sig-api-machinery/annual-report-2021.md</a></p>\n<p>You‚Äôll see that this report summary is a growth area in itself. It takes us roughly 6 months to prepare and execute, which isn‚Äôt helpful or valuable to anyone as a fast moving project with short and long term needs. How can we make this better? Provide your feedback here: <a href=\"https://github.com/kubernetes/steering/issues/242\">https://github.com/kubernetes/steering/issues/242</a></p>\n<p>Reference:\n<a href=\"https://github.com/kubernetes/community/blob/master/committee-steering/governance/annual-reports.md\">Annual Report Documentation</a></p>","PublishedAt":"2022-06-01 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/06/01/annual-report-summary-2021/","SourceName":"Kubernetes"}},{"node":{"ID":1214,"Title":"Blog: Kubernetes 1.24: Maximum Unavailable Replicas for StatefulSet","Description":"<p><strong>Author:</strong> Mayank Kumar (Salesforce)</p>\n<p>Kubernetes <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a>, since their introduction in\n1.5 and becoming stable in 1.9, have been widely used to run stateful applications. They provide stable pod identity, persistent\nper pod storage and ordered graceful deployment, scaling and rolling updates. You can think of StatefulSet as the atomic building\nblock for running complex stateful applications. As the use of Kubernetes has grown, so has the number of scenarios requiring\nStatefulSets. Many of these scenarios, require faster rolling updates than the currently supported one-pod-at-a-time updates, in the\ncase where you're using the <code>OrderedReady</code> Pod management policy for a StatefulSet.</p>\n<p>Here are some examples:</p>\n<ul>\n<li>\n<p>I am using a StatefulSet to orchestrate a multi-instance, cache based application where the size of the cache is large. The cache\nstarts cold and requires some siginificant amount of time before the container can start. There could be more initial startup tasks\nthat are required. A RollingUpdate on this StatefulSet would take a lot of time before the application is fully updated. If the\nStatefulSet supported updating more than one pod at a time, it would result in a much faster update.</p>\n</li>\n<li>\n<p>My stateful application is composed of leaders and followers or one writer and multiple readers. I have multiple readers or\nfollowers and my application can tolerate multiple pods going down at the same time. I want to update this application more than\none pod at a time so that i get the new updates rolled out quickly, especially if the number of instances of my application are\nlarge. Note that my application still requires unique identity per pod.</p>\n</li>\n</ul>\n<p>In order to support such scenarios, Kubernetes 1.24 includes a new alpha feature to help. Before you can use the new feature you must\nenable the <code>MaxUnavailableStatefulSet</code> feature flag. Once you enable that, you can specify a new field called <code>maxUnavailable</code>, part\nof the <code>spec</code> for a StatefulSet. For example:</p>\n<pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: web\nnamespace: default\nspec:\npodManagementPolicy: OrderedReady # you must set OrderedReady\nreplicas: 5\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- image: k8s.gcr.io/nginx-slim:0.8\nimagePullPolicy: IfNotPresent\nname: nginx\nupdateStrategy:\nrollingUpdate:\nmaxUnavailable: 2 # this is the new alpha field, whose default value is 1\npartition: 0\ntype: RollingUpdate\n</code></pre><p>If you enable the new feature and you don't specify a value for <code>maxUnavailable</code> in a StatefulSet, Kubernetes applies a default\n<code>maxUnavailable: 1</code>. This matches the behavior you would see if you don't enable the new feature.</p>\n<p>I'll run through a scenario based on that example manifest to demonstrate how this feature works. I will deploy a StatefulSet that\nhas 5 replicas, with <code>maxUnavailable</code> set to 2 and <code>partition</code> set to 0.</p>\n<p>I can trigger a rolling update by changing the image to <code>k8s.gcr.io/nginx-slim:0.9</code>. Once I initiate the rolling update, I can\nwatch the pods update 2 at a time as the current value of maxUnavailable is 2. The below output shows a span of time and is not\ncomplete. The maxUnavailable can be an absolute number (for example, 2) or a percentage of desired Pods (for example, 10%). The\nabsolute number is calculated from percentage by rounding down.</p>\n<pre tabindex=\"0\"><code>kubectl get pods --watch\n</code></pre><pre tabindex=\"0\"><code>NAME READY STATUS RESTARTS AGE\nweb-0 1/1 Running 0 85s\nweb-1 1/1 Running 0 2m6s\nweb-2 1/1 Running 0 106s\nweb-3 1/1 Running 0 2m47s\nweb-4 1/1 Running 0 2m27s\nweb-4 1/1 Terminating 0 5m43s ----&gt; start terminating 4\nweb-3 1/1 Terminating 0 6m3s ----&gt; start terminating 3\nweb-3 0/1 Terminating 0 6m7s\nweb-3 0/1 Pending 0 0s\nweb-3 0/1 Pending 0 0s\nweb-4 0/1 Terminating 0 5m48s\nweb-4 0/1 Terminating 0 5m48s\nweb-3 0/1 ContainerCreating 0 2s\nweb-3 1/1 Running 0 2s\nweb-4 0/1 Pending 0 0s\nweb-4 0/1 Pending 0 0s\nweb-4 0/1 ContainerCreating 0 0s\nweb-4 1/1 Running 0 1s\nweb-2 1/1 Terminating 0 5m46s ----&gt; start terminating 2 (only after both 4 and 3 are running)\nweb-1 1/1 Terminating 0 6m6s ----&gt; start terminating 1\nweb-2 0/1 Terminating 0 5m47s\nweb-1 0/1 Terminating 0 6m7s\nweb-1 0/1 Pending 0 0s\nweb-1 0/1 Pending 0 0s\nweb-1 0/1 ContainerCreating 0 1s\nweb-1 1/1 Running 0 2s\nweb-2 0/1 Pending 0 0s\nweb-2 0/1 Pending 0 0s\nweb-2 0/1 ContainerCreating 0 0s\nweb-2 1/1 Running 0 1s\nweb-0 1/1 Terminating 0 6m6s ----&gt; start terminating 0 (only after 2 and 1 are running)\nweb-0 0/1 Terminating 0 6m7s\nweb-0 0/1 Pending 0 0s\nweb-0 0/1 Pending 0 0s\nweb-0 0/1 ContainerCreating 0 0s\nweb-0 1/1 Running 0 1s\n</code></pre><p>Note that as soon as the rolling update starts, both 4 and 3 (the two highest ordinal pods) start terminating at the same time. Pods\nwith ordinal 4 and 3 may become ready at their own pace. As soon as both pods 4 and 3 are ready, pods 2 and 1 start terminating at the\nsame time. When pods 2 and 1 are both running and ready, pod 0 starts terminating.</p>\n<p>In Kubernetes, updates to StatefulSets follow a strict ordering when updating Pods. In this example, the update starts at replica 4, then\nreplica 3, then replica 2, and so on, one pod at a time. When going one pod at a time, its not possible for 3 to be running and ready\nbefore 4. When <code>maxUnavailable</code> is more than 1 (in the example scenario I set <code>maxUnavailable</code> to 2), it is possible that replica 3 becomes\nready and running before replica 4 is ready‚Äîand that is ok. If you're a developer and you set <code>maxUnavailable</code> to more than 1, you should\nknow that this outcome is possible and you must ensure that your application is able to handle such ordering issues that occur\nif any. When you set <code>maxUnavailable</code> greater than 1, the ordering is guaranteed in between each batch of pods being updated. That guarantee\nmeans that pods in update batch 2 (replicas 2 and 1) cannot start updating until the pods from batch 0 (replicas 4 and 3) are ready.</p>\n<p>Although Kubernetes refers to these as <em>replicas</em>, your stateful application may have a different view and each pod of the StatefulSet may\nbe holding completely different data than other pods. The important thing here is that updates to StatefulSets happen in batches, and you can\nnow have a batch size larger than 1 (as an alpha feature).</p>\n<p>Also note, that the above behavior is with <code>podManagementPolicy: OrderedReady</code>. If you defined a StatefulSet as <code>podManagementPolicy: Parallel</code>,\nnot only <code>maxUnavailable</code> number of replicas are terminated at the same time; <code>maxUnavailable</code> number of replicas start in <code>ContainerCreating</code>\nphase at the same time as well. This is called bursting.</p>\n<p>So, now you may have a lot of questions about:-</p>\n<ul>\n<li>What is the behavior when you set <code>podManagementPolicy: Parallel</code>?</li>\n<li>What is the behavior when <code>partition</code> to a value other than <code>0</code>?</li>\n</ul>\n<p>It might be better to try and see it for yourself. This is an alpha feature, and the Kubernetes contributors are looking for feedback on this feature. Did\nthis help you achieve your stateful scenarios Did you find a bug or do you think the behavior as implemented is not intuitive or can\nbreak applications or catch them by surprise? Please <a href=\"https://github.com/kubernetes/kubernetes/issues\">open an issue</a> to let us know.</p>\n<h2 id=\"next-steps\">Further reading and next steps</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#maximum-unavailable-pods\">Maximum unavailable Pods</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/961-maxunavailable-for-statefulset\">KEP for MaxUnavailable for StatefulSet</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/82162/files\">Implementation</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/961\">Enhancement Tracking Issue</a></li>\n</ul>","PublishedAt":"2022-05-27 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/27/maxunavailable-for-statefulset/","SourceName":"Kubernetes"}},{"node":{"ID":1215,"Title":"Blog: Contextual Logging in Kubernetes 1.24","Description":"<p><strong>Authors:</strong> Patrick Ohly (Intel)</p>\n<p>The <a href=\"https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md\">Structured Logging Working\nGroup</a>\nhas added new capabilities to the logging infrastructure in Kubernetes\n1.24. This blog post explains how developers can take advantage of those to\nmake log output more useful and how they can get involved with improving Kubernetes.</p>\n<h2 id=\"structured-logging\">Structured logging</h2>\n<p>The goal of <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/1602-structured-logging/README.md\">structured\nlogging</a>\nis to replace C-style formatting and the resulting opaque log strings with log\nentries that have a well-defined syntax for storing message and parameters\nseparately, for example as a JSON struct.</p>\n<p>When using the traditional klog text output format for structured log calls,\nstrings were originally printed with <code>\\n</code> escape sequences, except when\nembedded inside a struct. For structs, log entries could still span multiple\nlines, with no clean way to split the log stream into individual entries:</p>\n<pre tabindex=\"0\"><code>I1112 14:06:35.783529 328441 structured_logging.go:51] &#34;using InfoS&#34; longData={Name:long Data:Multiple\nlines\nwith quite a bit\nof text. internal:0}\nI1112 14:06:35.783549 328441 structured_logging.go:52] &#34;using InfoS with\\nthe message across multiple lines&#34; int=1 stringData=&#34;long: Multiple\\nlines\\nwith quite a bit\\nof text.&#34; str=&#34;another value&#34;\n</code></pre><p>Now, the <code>&lt;</code> and <code>&gt;</code> markers along with indentation are used to ensure that splitting at a\nklog header at the start of a line is reliable and the resulting output is human-readable:</p>\n<pre tabindex=\"0\"><code>I1126 10:31:50.378204 121736 structured_logging.go:59] &#34;using InfoS&#34; longData=&lt;\n{Name:long Data:Multiple\nlines\nwith quite a bit\nof text. internal:0}\n&gt;\nI1126 10:31:50.378228 121736 structured_logging.go:60] &#34;using InfoS with\\nthe message across multiple lines&#34; int=1 stringData=&lt;\nlong: Multiple\nlines\nwith quite a bit\nof text.\n&gt; str=&#34;another value&#34;\n</code></pre><p>Note that the log message itself is printed with quoting. It is meant to be a\nfixed string that identifies a log entry, so newlines should be avoided there.</p>\n<p>Before Kubernetes 1.24, some log calls in kube-scheduler still used <code>klog.Info</code>\nfor multi-line strings to avoid the unreadable output. Now all log calls have\nbeen updated to support structured logging.</p>\n<h2 id=\"contextual-logging\">Contextual logging</h2>\n<p><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md\">Contextual logging</a>\nis based on the <a href=\"https://github.com/go-logr/logr#a-minimal-logging-api-for-go\">go-logr API</a>. The key\nidea is that libraries are passed a logger instance by their caller and use\nthat for logging instead of accessing a global logger. The binary decides about\nthe logging implementation, not the libraries. The go-logr API is designed\naround structured logging and supports attaching additional information to a\nlogger.</p>\n<p>This enables additional use cases:</p>\n<ul>\n<li>\n<p>The caller can attach additional information to a logger:</p>\n<ul>\n<li><a href=\"https://pkg.go.dev/github.com/go-logr/logr#Logger.WithName\"><code>WithName</code></a> adds a prefix</li>\n<li><a href=\"https://pkg.go.dev/github.com/go-logr/logr#Logger.WithValues\"><code>WithValues</code></a> adds key/value pairs</li>\n</ul>\n<p>When passing this extended logger into a function and a function uses it\ninstead of the global logger, the additional information is\nthen included in all log entries, without having to modify the code that\ngenerates the log entries. This is useful in highly parallel applications\nwhere it can become hard to identify all log entries for a certain operation\nbecause the output from different operations gets interleaved.</p>\n</li>\n<li>\n<p>When running unit tests, log output can be associated with the current test.\nThen when a test fails, only the log output of the failed test gets shown\nby <code>go test</code>. That output can also be more verbose by default because it\nwill not get shown for successful tests. Tests can be run in parallel\nwithout interleaving their output.</p>\n</li>\n</ul>\n<p>One of the design decisions for contextual logging was to allow attaching a\nlogger as value to a <code>context.Context</code>. Since the logger encapsulates all\naspects of the intended logging for the call, it is <em>part</em> of the context and\nnot just <em>using</em> it. A practical advantage is that many APIs already have a\n<code>ctx</code> parameter or adding one has additional advantages, like being able to get\nrid of <code>context.TODO()</code> calls inside the functions.</p>\n<p>Another decision was to not break compatibility with klog v2:</p>\n<ul>\n<li>\n<p>Libraries that use the traditional klog logging calls in a binary that has\nset up contextual logging will work and log through the logging backend\nchosen by the binary. However, such log output will not include the\nadditional information and will not work well in unit tests, so libraries\nshould be modified to support contextual logging. The <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md\">migration guide</a>\nfor structured logging has been extended to also cover contextual logging.</p>\n</li>\n<li>\n<p>When a library supports contextual logging and retrieves a logger from its\ncontext, it will still work in a binary that does not initialize contextual\nlogging because it will get a logger that logs through klog.</p>\n</li>\n</ul>\n<p>In Kubernetes 1.24, contextual logging is a new alpha feature with\n<code>ContextualLogging</code> as feature gate. When disabled (the default), the new klog\nAPI calls for contextual logging (see below) become no-ops to avoid performance\nor functional regressions.</p>\n<p>No Kubernetes component has been converted yet. An <a href=\"https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go\">example program</a>\nin the Kubernetes repository demonstrates how to enable contextual logging in a\nbinary and how the output depends on the binary's parameters:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> <span style=\"color:#a2f\">cd</span> <span style=\"color:#b8860b\">$GOPATH</span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">$</span> go run . --help\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> --feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> AllAlpha=true|false (ALPHA - default=false)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> AllBeta=true|false (BETA - default=false)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"> ContextualLogging=true|false (ALPHA - default=false)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"color:#000080;font-weight:bold\">$</span> go run . --feature-gates <span style=\"color:#b8860b\">ContextualLogging</span><span style=\"color:#666\">=</span><span style=\"color:#a2f\">true</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">...\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">I0404 18:00:02.916429 451895 logger.go:94] &#34;example/myname: runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">I0404 18:00:02.916447 451895 logger.go:95] &#34;example: another runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34;\n</span></span></span></code></pre></div><p>The <code>example</code> prefix and <code>foo=&quot;bar&quot;</code> were added by the caller of the function\nwhich logs the <code>runtime</code> message and <code>duration=&quot;1m0s&quot;</code> value.</p>\n<p>The sample code for klog includes an\n<a href=\"https://github.com/kubernetes/klog/blob/v2.60.1/ktesting/example/example_test.go\">example</a>\nfor a unit test with per-test output.</p>\n<h2 id=\"klog-enhancements\">klog enhancements</h2>\n<h3 id=\"contextual-logging-api\">Contextual logging API</h3>\n<p>The following calls manage the lookup of a logger:</p>\n<dl>\n<dt><a href=\"https://pkg.go.dev/k8s.io/klog/v2#FromContext\"><code>FromContext</code></a></dt>\n<dd>from a <code>context</code> parameter, with fallback to the global logger</dd>\n<dt><a href=\"https://pkg.go.dev/k8s.io/klog/v2#Background\"><code>Background</code></a></dt>\n<dd>the global fallback, with no intention to support contextual logging</dd>\n<dt><a href=\"https://pkg.go.dev/k8s.io/klog/v2#TODO\"><code>TODO</code></a></dt>\n<dd>the global fallback, but only as a temporary solution until the function gets extended to accept\na logger through its parameters</dd>\n<dt><a href=\"https://pkg.go.dev/k8s.io/klog/v2#SetLoggerWithOptions\"><code>SetLoggerWithOptions</code></a></dt>\n<dd>changes the fallback logger; when called with <a href=\"https://pkg.go.dev/k8s.io/klog/v2#ContextualLogger\"><code>ContextualLogger(true)</code></a>,\nthe logger is ready to be called directly, in which case logging will be done\nwithout going through klog</dd>\n</dl>\n<p>To support the feature gate mechanism in Kubernetes, klog has wrapper calls for\nthe corresponding go-logr calls and a global boolean controlling their behavior:</p>\n<ul>\n<li><a href=\"https://pkg.go.dev/k8s.io/klog/v2#LoggerWithName\"><code>LoggerWithName</code></a></li>\n<li><a href=\"https://pkg.go.dev/k8s.io/klog/v2#LoggerWithValues\"><code>LoggerWithValues</code></a></li>\n<li><a href=\"https://pkg.go.dev/k8s.io/klog/v2#NewContext\"><code>NewContext</code></a></li>\n<li><a href=\"https://pkg.go.dev/k8s.io/klog/v2#EnableContextualLogging\"><code>EnableContextualLogging</code></a></li>\n</ul>\n<p>Usage of those functions in Kubernetes code is enforced with a linter\ncheck. The klog default for contextual logging is to enable the functionality\nbecause it is considered stable in klog. It is only in Kubernetes binaries\nwhere that default gets overridden and (in some binaries) controlled via the\n<code>--feature-gate</code> parameter.</p>\n<h3 id=\"ktesting-logger\">ktesting logger</h3>\n<p>The new <a href=\"https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting\">ktesting</a> package\nimplements logging through <code>testing.T</code> using klog's text output format. It has\na <a href=\"https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting#NewTestContext\">single API call</a> for\ninstrumenting a test case and <a href=\"https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting/init\">support for command line flags</a>.</p>\n<h3 id=\"klogr\">klogr</h3>\n<p><a href=\"https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr\"><code>klog/klogr</code></a> continues to be\nsupported and it's default behavior is unchanged: it formats structured log\nentries using its own, custom format and prints the result via klog.</p>\n<p>However, this usage is discouraged because that format is neither\nmachine-readable (in contrast to real JSON output as produced by zapr, the\ngo-logr implementation used by Kubernetes) nor human-friendly (in contrast to\nthe klog text format).</p>\n<p>Instead, a klogr instance should be created with\n<a href=\"https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr#WithFormat\"><code>WithFormat(FormatKlog)</code></a>\nwhich chooses the klog text format. A simpler construction method with the same\nresult is the new\n<a href=\"https://pkg.go.dev/k8s.io/klog/v2#NewKlogr\"><code>klog.NewKlogr</code></a>. That is the\nlogger that klog returns as fallback when nothing else is configured.</p>\n<h3 id=\"reusable-output-test\">Reusable output test</h3>\n<p>A lot of go-logr implementations have very similar unit tests where they check\nthe result of certain log calls. If a developer didn't know about certain\ncaveats like for example a <code>String</code> function that panics when called, then it\nis likely that both the handling of such caveats and the unit test are missing.</p>\n<p><a href=\"https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/test\"><code>klog.test</code></a> is a reusable set\nof test cases that can be applied to a go-logr implementation.</p>\n<h3 id=\"output-flushing\">Output flushing</h3>\n<p>klog used to start a goroutine unconditionally during <code>init</code> which flushed\nbuffered data at a hard-coded interval. Now that goroutine is only started on\ndemand (i.e. when writing to files with buffering) and can be controlled with\n<a href=\"https://pkg.go.dev/k8s.io/klog/v2#StopFlushDaemon\"><code>StopFlushDaemon</code></a> and\n<a href=\"https://pkg.go.dev/k8s.io/klog/v2#StartFlushDaemon\"><code>StartFlushDaemon</code></a>.</p>\n<p>When a go-logr implementation buffers data, flushing that data can be\nintegrated into <a href=\"https://pkg.go.dev/k8s.io/klog/v2#Flush\"><code>klog.Flush</code></a> by\nregistering the logger with the\n<a href=\"https://pkg.go.dev/k8s.io/klog/v2#FlushLogger\"><code>FlushLogger</code></a> option.</p>\n<h3 id=\"various-other-changes\">Various other changes</h3>\n<p>For a description of all other enhancements see in the <a href=\"https://github.com/kubernetes/klog/releases\">release notes</a>.</p>\n<h2 id=\"logcheck\">logcheck</h2>\n<p>Originally designed as a linter for structured log calls, the\n<a href=\"https://github.com/kubernetes/klog/tree/788efcdee1e9be0bfbe5b076343d447314f2377e/hack/tools/logcheck\"><code>logcheck</code></a>\ntool has been enhanced to support also contextual logging and traditional klog\nlog calls. These enhanced checks already found bugs in Kubernetes, like calling\n<code>klog.Info</code> instead of <code>klog.Infof</code> with a format string and parameters.</p>\n<p>It can be included as a plugin in a <code>golangci-lint</code> invocation, which is how\n<a href=\"https://github.com/kubernetes/kubernetes/commit/17e3c555c5115f8c9176bae10ba45baa04d23a7b\">Kubernetes uses it now</a>,\nor get invoked stand-alone.</p>\n<p>We are in the process of <a href=\"https://github.com/kubernetes/klog/issues/312\">moving the tool</a> into a new repository because it isn't\nreally related to klog and its releases should be tracked and tagged properly.</p>\n<h2 id=\"next-steps\">Next steps</h2>\n<p>The <a href=\"https://github.com/kubernetes/community/tree/master/wg-structured-logging\">Structured Logging WG</a>\nis always looking for new contributors. The migration\naway from C-style logging is now going to target structured, contextual logging\nin one step to reduce the overall code churn and number of PRs. Changing log\ncalls is good first contribution to Kubernetes and an opportunity to get to\nknow code in various different areas.</p>","PublishedAt":"2022-05-25 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/25/contextual-logging/","SourceName":"Kubernetes"}},{"node":{"ID":1216,"Title":"Blog: Kubernetes 1.24: Avoid Collisions Assigning IP Addresses to Services","Description":"<p><strong>Author:</strong> Antonio Ojea (Red Hat)</p>\n<p>In Kubernetes, <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Services</a> are an abstract way to expose\nan application running on a set of Pods. Services\ncan have a cluster-scoped virtual IP address (using a Service of <code>type: ClusterIP</code>).\nClients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that\nService across the different backing Pods.</p>\n<h2 id=\"how-service-clusterips-are-allocated\">How Service ClusterIPs are allocated?</h2>\n<p>A Service <code>ClusterIP</code> can be assigned:</p>\n<dl>\n<dt><em>dynamically</em></dt>\n<dd>the cluster's control plane automatically picks a free IP address from within the configured IP range for <code>type: ClusterIP</code> Services.</dd>\n<dt><em>statically</em></dt>\n<dd>you specify an IP address of your choice, from within the configured IP range for Services.</dd>\n</dl>\n<p>Across your whole cluster, every Service <code>ClusterIP</code> must be unique.\nTrying to create a Service with a specific <code>ClusterIP</code> that has already\nbeen allocated will return an error.</p>\n<h2 id=\"why-do-you-need-to-reserve-service-cluster-ips\">Why do you need to reserve Service Cluster IPs?</h2>\n<p>Sometimes you may want to have Services running in well-known IP addresses, so other components and\nusers in the cluster can use them.</p>\n<p>The best example is the DNS Service for the cluster. Some Kubernetes installers assign the 10th address from\nthe Service IP range to the DNS service. Assuming you configured your cluster with Service IP range\n10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like\nthis:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">labels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">k8s-app</span>:<span style=\"color:#bbb\"> </span>kube-dns<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kubernetes.io/cluster-service</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;true&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kubernetes.io/name</span>:<span style=\"color:#bbb\"> </span>CoreDNS<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>kube-dns<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>kube-system<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">clusterIP</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">10.96.0.10</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ports</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>dns<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">53</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">protocol</span>:<span style=\"color:#bbb\"> </span>UDP<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">targetPort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">53</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>dns-tcp<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">53</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">protocol</span>:<span style=\"color:#bbb\"> </span>TCP<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">targetPort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">53</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">k8s-app</span>:<span style=\"color:#bbb\"> </span>kube-dns<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>ClusterIP<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>but as I explained before, the IP address 10.96.0.10 has not been reserved; if other Services are created\nbefore or in parallel with dynamic allocation, there is a chance they can allocate this IP, hence,\nyou will not be able to create the DNS Service because it will fail with a conflict error.</p>\n<h2 id=\"avoid-ClusterIP-conflict\">How can you avoid Service ClusterIP conflicts?</h2>\n<p>In Kubernetes 1.24, you can enable a new feature gate <code>ServiceIPStaticSubrange</code>.\nTurning this on allows you to use a different IP\nallocation strategy for Services, reducing the risk of collision.</p>\n<p>The <code>ClusterIP</code> range will be divided, based on the formula <code>min(max(16, cidrSize / 16), 256)</code>,\ndescribed as <em>never less than 16 or more than 256 with a graduated step between them</em>.</p>\n<p>Dynamic IP assignment will use the upper band by default, once this has been exhausted it will\nuse the lower range. This will allow users to use static allocations on the lower band with a low\nrisk of collision.</p>\n<p>Examples:</p>\n<h4 id=\"service-ip-cidr-block-10-96-0-0-24\">Service IP CIDR block: 10.96.0.0/24</h4>\n<p>Range Size: 2<sup>8</sup> - 2 = 254<br>\nBand Offset: <code>min(max(16, 256/16), 256)</code> = <code>min(16, 256)</code> = 16<br>\nStatic band start: 10.96.0.1<br>\nStatic band end: 10.96.0.16<br>\nRange end: 10.96.0.254</p>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 10.96.0.0/24\n\"Static\" : 16\n\"Dynamic\" : 238\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h4 id=\"service-ip-cidr-block-10-96-0-0-20\">Service IP CIDR block: 10.96.0.0/20</h4>\n<p>Range Size: 2<sup>12</sup> - 2 = 4094<br>\nBand Offset: <code>min(max(16, 4096/16), 256)</code> = <code>min(256, 256)</code> = 256<br>\nStatic band start: 10.96.0.1<br>\nStatic band end: 10.96.1.0<br>\nRange end: 10.96.15.254</p>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 10.96.0.0/20\n\"Static\" : 256\n\"Dynamic\" : 3838\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h4 id=\"service-ip-cidr-block-10-96-0-0-16\">Service IP CIDR block: 10.96.0.0/16</h4>\n<p>Range Size: 2<sup>16</sup> - 2 = 65534<br>\nBand Offset: <code>min(max(16, 65536/16), 256)</code> = <code>min(4096, 256)</code> = 256<br>\nStatic band start: 10.96.0.1<br>\nStatic band ends: 10.96.1.0<br>\nRange end: 10.96.255.254</p>\n<figure>\n<div class=\"mermaid\">\npie showData\ntitle 10.96.0.0/16\n\"Static\" : 256\n\"Dynamic\" : 65278\n</div>\n</figure>\n<noscript>\n<div class=\"alert alert-secondary callout\" role=\"alert\">\n<em class=\"javascript-required\">JavaScript must be <a href=\"https://www.enable-javascript.com/\">enabled</a> to view this content</em>\n</div>\n</noscript>\n<h2 id=\"get-involved-with-sig-network\">Get involved with SIG Network</h2>\n<p>The current SIG-Network <a href=\"https://github.com/orgs/kubernetes/projects/10\">KEPs</a> and <a href=\"https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork\">issues</a> on GitHub illustrate the SIG‚Äôs areas of emphasis.</p>\n<p><a href=\"https://github.com/kubernetes/community/tree/master/sig-network\">SIG Network meetings</a> are a friendly, welcoming venue for you to connect with the community and share your ideas.\nLooking forward to hearing from you!</p>","PublishedAt":"2022-05-23 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/23/service-ip-dynamic-and-static-allocation/","SourceName":"Kubernetes"}},{"node":{"ID":1217,"Title":"Blog: Kubernetes 1.24: Introducing Non-Graceful Node Shutdown Alpha","Description":"<p><strong>Authors</strong> Xing Yang and Yassine Tijani (VMware)</p>\n<p>Kubernetes v1.24 introduces alpha support for <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown\">Non-Graceful Node Shutdown</a>. This feature allows stateful workloads to failover to a different node after the original node is shutdown or in a non-recoverable state such as hardware failure or broken OS.</p>\n<h2 id=\"how-is-this-different-from-graceful-node-shutdown\">How is this different from Graceful Node Shutdown</h2>\n<p>You might have heard about the <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown\">Graceful Node Shutdown</a> capability of Kubernetes,\nand are wondering how the Non-Graceful Node Shutdown feature is different from that. Graceful Node Shutdown\nallows Kubernetes to detect when a node is shutting down cleanly, and handles that situation appropriately.\nA Node Shutdown can be &quot;graceful&quot; only if the node shutdown action can be detected by the kubelet ahead\nof the actual shutdown. However, there are cases where a node shutdown action may not be detected by\nthe kubelet. This could happen either because the shutdown command does not trigger the systemd inhibitor\nlocks mechanism that kubelet relies upon, or because of a configuration error\n(the <code>ShutdownGracePeriod</code> and <code>ShutdownGracePeriodCriticalPods</code> are not configured properly).</p>\n<p>Graceful node shutdown relies on Linux-specific support. The kubelet does not watch for upcoming\nshutdowns on Windows nodes (this may change in a future Kubernetes release).</p>\n<p>When a node is shutdown but without the kubelet detecting it, pods on that node\nalso shut down ungracefully. For stateless apps, that's often not a problem (a ReplicaSet adds a new pod once\nthe cluster detects that the affected node or pod has failed). For stateful apps, the story is more complicated.\nIf you use a StatefulSet and have a pod from that StatefulSet on a node that fails uncleanly, that affected pod\nwill be marked as terminating; the StatefulSet cannot create a replacement pod because the pod\nstill exists in the cluster.\nAs a result, the application running on the StatefulSet may be degraded or even offline. If the original, shut\ndown node comes up again, the kubelet on that original node reports in, deletes the existing pods, and\nthe control plane makes a replacement pod for that StatefulSet on a different running node.\nIf the original node has failed and does not come up, those stateful pods would be stuck in a\nterminating status on that failed node indefinitely.</p>\n<pre tabindex=\"0\"><code>$ kubectl get pod -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\nweb-0 1/1 Running 0 100m 10.244.2.4 k8s-node-876-1639279816 &lt;none&gt; &lt;none&gt;\nweb-1 1/1 Terminating 0 100m 10.244.1.3 k8s-node-433-1639279804 &lt;none&gt; &lt;none&gt;\n</code></pre><h2 id=\"try-out-the-new-non-graceful-shutdown-handling\">Try out the new non-graceful shutdown handling</h2>\n<p>To use the non-graceful node shutdown handling, you must enable the <code>NodeOutOfServiceVolumeDetach</code>\n<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> for the <code>kube-controller-manager</code>\ncomponent.</p>\n<p>In the case of a node shutdown, you can manually taint that node as out of service. You should make certain that\nthe node is truly shutdown (not in the middle of restarting) before you add that taint. You could add that\ntaint following a shutdown that the kubelet did not detect and handle in advance; another case where you\ncan use that taint is when the node is in a non-recoverable state due to a hardware failure or a broken OS.\nThe values you set for that taint can be <code>node.kubernetes.io/out-of-service=nodeshutdown: &quot;NoExecute&quot;</code>\nor <code>node.kubernetes.io/out-of-service=nodeshutdown:&quot; NoSchedule&quot;</code>.\nProvided you have enabled the feature gate mentioned earlier, setting the out-of-service taint on a Node\nmeans that pods on the node will be deleted unless if there are matching tolerations on the pods.\nPersistent volumes attached to the shutdown node will be detached, and for StatefulSets, replacement pods will\nbe created successfully on a different running node.</p>\n<pre tabindex=\"0\"><code>$ kubectl taint nodes &lt;node-name&gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute\n$ kubectl get pod -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\nweb-0 1/1 Running 0 150m 10.244.2.4 k8s-node-876-1639279816 &lt;none&gt; &lt;none&gt;\nweb-1 1/1 Running 0 10m 10.244.1.7 k8s-node-433-1639279804 &lt;none&gt; &lt;none&gt;\n</code></pre><p>Note: Before applying the out-of-service taint, you <strong>must</strong> verify that a node is already in shutdown or power off state (not in the middle of restarting), either because the user intentionally shut it down or the node is down due to hardware failures, OS issues, etc.</p>\n<p>Once all the workload pods that are linked to the out-of-service node are moved to a new running node, and the shutdown node has been recovered, you should remove\nthat taint on the affected node after the node is recovered.\nIf you know that the node will not return to service, you could instead delete the node from the cluster.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>Depending on feedback and adoption, the Kubernetes team plans to push the Non-Graceful Node Shutdown implementation to Beta in either 1.25 or 1.26.</p>\n<p>This feature requires a user to manually add a taint to the node to trigger workloads failover and remove the taint after the node is recovered. In the future, we plan to find ways to automatically detect and fence nodes that are shutdown/failed and automatically failover workloads to another node.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>Check out the <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown\">documentation</a>\nfor non-graceful node shutdown.</p>\n<h2 id=\"how-to-get-involved\">How to get involved?</h2>\n<p>This feature has a long story. Yassine Tijani (<a href=\"https://github.com/yastij\">yastij</a>) started the KEP more than two years ago. Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>) continued to drive the effort. There were many discussions among SIG Storage, SIG Node, and API reviewers to nail down the design details. Ashutosh Kumar (<a href=\"https://github.com/sonasingh46\">sonasingh46</a>) did most of the implementation and brought it to Alpha in Kubernetes 1.24.</p>\n<p>We want to thank the following people for their insightful reviews: Tim Hockin (<a href=\"https://github.com/thockin\">thockin</a>) for his guidance on the design, Jing Xu (<a href=\"https://github.com/jingxu97\">jingxu97</a>), Hemant Kumar (<a href=\"https://github.com/gnufied\">gnufied</a>), and Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>) for reviews from SIG Storage side, and Mrunal Patel (<a href=\"https://github.com/mrunalp\">mrunalp</a>), David Porter (<a href=\"https://github.com/bobbypage\">bobbypage</a>), Derek Carr (<a href=\"https://github.com/derekwaynecarr\">derekwaynecarr</a>), and Danielle Endocrimes (<a href=\"https://github.com/endocrimes\">endocrimes</a>) for reviews from SIG Node side.</p>\n<p>There are many people who have helped review the design and implementation along the way. We want to thank everyone who has contributed to this effort including the about 30 people who have reviewed the <a href=\"https://github.com/kubernetes/enhancements/pull/1116\">KEP</a> and implementation over the last couple of years.</p>\n<p>This feature is a collaboration between SIG Storage and SIG Node. For those interested in getting involved with the design and development of any part of the Kubernetes Storage system, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group</a> (SIG). For those interested in getting involved with the design and development of the components that support the controlled interactions between pods and host resources, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-node\">Kubernetes Node SIG</a>.</p>","PublishedAt":"2022-05-20 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":1218,"Title":"Blog: Kubernetes 1.24: Prevent unauthorised volume mode conversion","Description":"<p><strong>Author:</strong> Raunak Pradip Shah (Mirantis)</p>\n<p>Kubernetes v1.24 introduces a new alpha-level feature that prevents unauthorised users\nfrom modifying the volume mode of a <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\"><code>PersistentVolumeClaim</code></a> created from an\nexisting <a href=\"https://kubernetes.io/docs/concepts/storage/volume-snapshots/\"><code>VolumeSnapshot</code></a> in the Kubernetes cluster.</p>\n<h3 id=\"the-problem\">The problem</h3>\n<p>The <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode\">Volume Mode</a> determines whether a volume\nis formatted into a filesystem or presented as a raw block device.</p>\n<p>Users can leverage the <code>VolumeSnapshot</code> feature, which has been stable since Kubernetes v1.20,\nto create a <code>PersistentVolumeClaim</code> (shortened as PVC) from an existing <code>VolumeSnapshot</code> in\nthe Kubernetes cluster. The PVC spec includes a <code>dataSource</code> field, which can point to an\nexisting <code>VolumeSnapshot</code> instance.\nVisit <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#create-persistent-volume-claim-from-volume-snapshot\">Create a PersistentVolumeClaim from a Volume Snapshot</a> for more details.</p>\n<p>When leveraging the above capability, there is no logic that validates whether the mode of the\noriginal volume, whose snapshot was taken, matches the mode of the newly created volume.</p>\n<p>This presents a security gap that allows malicious users to potentially exploit an\nas-yet-unknown vulnerability in the host operating system.</p>\n<p>Many popular storage backup vendors convert the volume mode during the course of a\nbackup operation, for efficiency purposes, which prevents Kubernetes from blocking\nthe operation completely and presents a challenge in distinguishing trusted\nusers from malicious ones.</p>\n<h3 id=\"preventing-unauthorised-users-from-converting-the-volume-mode\">Preventing unauthorised users from converting the volume mode</h3>\n<p>In this context, an authorised user is one who has access rights to perform <code>Update</code>\nor <code>Patch</code> operations on <code>VolumeSnapshotContents</code>, which is a cluster-level resource.<br>\nIt is upto the cluster administrator to provide these rights only to trusted users\nor applications, like backup vendors.</p>\n<p>If the alpha feature is <a href=\"https://kubernetes-csi.github.io/docs/\">enabled</a> in\n<code>snapshot-controller</code>, <code>snapshot-validation-webhook</code> and <code>external-provisioner</code>,\nthen unauthorised users will not be allowed to modify the volume mode of a PVC\nwhen it is being created from a <code>VolumeSnapshot</code>.</p>\n<p>To convert the volume mode, an authorised user must do the following:</p>\n<ol>\n<li>Identify the <code>VolumeSnapshot</code> that is to be used as the data source for a newly\ncreated PVC in the given namespace.</li>\n<li>Identify the <code>VolumeSnapshotContent</code> bound to the above <code>VolumeSnapshot</code>.</li>\n</ol>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl get volumesnapshot -n &lt;namespace&gt;\n</span></span></code></pre></div><ol start=\"3\">\n<li>\n<p>Add the annotation <a href=\"https://kubernetes.io/docs/reference/labels-annotations-taints/#snapshot-storage-kubernetes-io-allowvolumemodechange\"><code>snapshot.storage.kubernetes.io/allowVolumeModeChange</code></a>\nto the <code>VolumeSnapshotContent</code>.</p>\n</li>\n<li>\n<p>This annotation can be added either via software or manually by the authorised\nuser. The <code>VolumeSnapshotContent</code> annotation must look like following manifest fragment:</p>\n</li>\n</ol>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshotContent<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">snapshot.storage.kubernetes.io/allowVolumeModeChange</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;true&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#00f;font-weight:bold\">...</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p><strong>Note</strong>: For pre-provisioned <code>VolumeSnapshotContents</code>, you must take an extra\nstep of setting <code>spec.sourceVolumeMode</code> field to either <code>Filesystem</code> or <code>Block</code>,\ndepending on the mode of the volume from which this snapshot was taken.</p>\n<p>An example is shown below:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshotContent<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">snapshot.storage.kubernetes.io/allowVolumeModeChange</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;true&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new-snapshot-content-test<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">deletionPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>hostpath.csi.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">snapshotHandle</span>:<span style=\"color:#bbb\"> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">sourceVolumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeSnapshotRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>new-snapshot-test<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>default<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Repeat steps 1 to 3 for all <code>VolumeSnapshotContents</code> whose volume mode needs to be\nconverted during a backup or restore operation.</p>\n<p>If the annotation shown in step 4 above is present on a <code>VolumeSnapshotContent</code>\nobject, Kubernetes will not prevent the volume mode from being converted.\nUsers should keep this in mind before they attempt to add the annotation\nto any <code>VolumeSnapshotContent</code>.</p>\n<h3 id=\"what-s-next\">What's next</h3>\n<p><a href=\"https://kubernetes-csi.github.io/docs/\">Enable this feature</a> and let us know\nwhat you think!</p>\n<p>We hope this feature causes no disruption to existing workflows while preventing\nmalicious users from exploiting security vulnerabilities in their clusters.</p>\n<p>For any queries or issues, join <a href=\"https://slack.k8s.io/\">Kubernetes on Slack</a> and\ncreate a thread in the #sig-storage channel. Alternately, create an issue in the\nCSI external-snapshotter <a href=\"https://github.com/kubernetes-csi/external-snapshotter\">repository</a>.</p>","PublishedAt":"2022-05-18 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":1219,"Title":"Blog: Kubernetes 1.24: Volume Populators Graduate to Beta","Description":"<p><strong>Author:</strong>\nBen Swartzlander (NetApp)</p>\n<p>The volume populators feature is now two releases old and entering beta! The <code>AnyVolumeDataSource</code> feature\ngate defaults to enabled in Kubernetes v1.24, which means that users can specify any custom resource\nas the data source of a PVC.</p>\n<p>An <a href=\"https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/\">earlier blog article</a> detailed how the\nvolume populators feature works. In short, a cluster administrator can install a CRD and\nassociated populator controller in the cluster, and any user who can create instances of\nthe CR can create pre-populated volumes by taking advantage of the populator.</p>\n<p>Multiple populators can be installed side by side for different purposes. The SIG storage\ncommunity is already seeing some implementations in public, and more prototypes should\nappear soon.</p>\n<p>Cluster administrations are <strong>strongly encouraged</strong> to install the\nvolume-data-source-validator controller and associated <code>VolumePopulator</code> CRD before installing\nany populators so that users can get feedback about invalid PVC data sources.</p>\n<h2 id=\"new-features\">New Features</h2>\n<p>The <a href=\"https://github.com/kubernetes-csi/lib-volume-populator\">lib-volume-populator</a> library\non which populators are built now includes metrics to help operators monitor and detect\nproblems. This library is now beta and latest release is v1.0.1.</p>\n<p>The <a href=\"https://github.com/kubernetes-csi/volume-data-source-validator\">volume data source validator</a>\ncontroller also has metrics support added, and is in beta. The <code>VolumePopulator</code> CRD is\nbeta and the latest release is v1.0.1.</p>\n<h2 id=\"trying-it-out\">Trying it out</h2>\n<p>To see how this works, you can install the sample &quot;hello&quot; populator and try it\nout.</p>\n<p>First install the volume-data-source-validator controller.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/rbac-data-source-validator.yaml\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/setup-data-source-validator.yaml\n</span></span></code></pre></div><p>Next install the example populator.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/v1.0.1/example/hello-populator/crd.yaml\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/87a47467b86052819e9ad13d15036d65b9a32fbb/example/hello-populator/deploy.yaml\n</span></span></code></pre></div><p>Your cluster now has a new CustomResourceDefinition that provides a test API named Hello.\nCreate an instance of the <code>Hello</code> custom resource, with some text:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>hello.example.com/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fileName</span>:<span style=\"color:#bbb\"> </span>example.txt<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fileContents</span>:<span style=\"color:#bbb\"> </span>Hello, world!<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Create a PVC that refers to that CR as its data source.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>10Mi<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSourceRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>hello.example.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Next, run a Job that reads the file in the PVC.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>batch/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-container<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>busybox:latest<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- cat<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- /mnt/example.txt<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMounts</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>vol<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">mountPath</span>:<span style=\"color:#bbb\"> </span>/mnt<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">restartPolicy</span>:<span style=\"color:#bbb\"> </span>Never<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>vol<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">persistentVolumeClaim</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claimName</span>:<span style=\"color:#bbb\"> </span>example-pvc<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Wait for the job to complete (including all of its dependencies).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl <span style=\"color:#a2f\">wait</span> --for<span style=\"color:#666\">=</span><span style=\"color:#b8860b\">condition</span><span style=\"color:#666\">=</span>Complete job/example-job\n</span></span></code></pre></div><p>And last examine the log from the job.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl logs job/example-job\n</span></span></code></pre></div><p>The output should be:</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">Hello, world!\n</code></pre><p>Note that the volume already contained a text file with the string contents from\nthe CR. This is only the simplest example. Actual populators can set up the volume\nto contain arbitrary contents.</p>\n<h2 id=\"how-to-write-your-own-volume-populator\">How to write your own volume populator</h2>\n<p>Developers interested in writing new poplators are encouraged to use the\n<a href=\"https://github.com/kubernetes-csi/lib-volume-populator\">lib-volume-populator</a> library\nand to only supply a small controller wrapper around the library, and a pod image\ncapable of attaching to volumes and writing the appropriate data to the volume.</p>\n<p>Individual populators can be extremely generic such that they work with every type\nof PVC, or they can do vendor specific things to rapidly fill a volume with data\nif the volume was provisioned by a specific CSI driver from the same vendor, for\nexample, by communicating directly with the storage for that volume.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>The enhancement proposal,\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators\">Volume Populators</a>, includes lots of detail about the history and technical implementation\nof this feature.</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources\">Volume populators and data sources</a>, within the documentation topic about persistent volumes,\nexplains how to use this feature in your cluster.</p>\n<p>Please get involved by joining the Kubernetes storage SIG to help us enhance this\nfeature. There are a lot of good ideas already and we'd be thrilled to have more!</p>","PublishedAt":"2022-05-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/16/volume-populators-beta/","SourceName":"Kubernetes"}},{"node":{"ID":1220,"Title":"Blog: Kubernetes 1.24: gRPC container probes in beta","Description":"<p><strong>Author</strong>: Sergey Kanzhelev (Google)</p>\n<p>With Kubernetes 1.24 the gRPC probes functionality entered beta and is available by default.\nNow you can configure startup, liveness, and readiness probes for your gRPC app\nwithout exposing any HTTP endpoint, nor do you need an executable. Kubernetes can natively connect to your your workload via gRPC and query its status.</p>\n<h2 id=\"some-history\">Some history</h2>\n<p>It's useful to let the system managing your workload check that the app is\nhealthy, has started OK, and whether the app considers itself good to accept\ntraffic. Before the gRPC support was added, Kubernetes already allowed you to\ncheck for health based on running an executable from inside the container image,\nby making an HTTP request, or by checking whether a TCP connection succeeded.</p>\n<p>For most apps, those checks are enough. If your app provides a gRPC endpoint\nfor a health (or readiness) check, it is easy\nto repurpose the <code>exec</code> probe to use it for gRPC health checking.\nIn the blog article <a href=\"https://kubernetes.io/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/\">Health checking gRPC servers on Kubernetes</a>,\nAhmet Alp Balkan described how you can do that ‚Äî a mechanism that still works today.</p>\n<p>There is a commonly used tool to enable this that was <a href=\"https://github.com/grpc-ecosystem/grpc-health-probe/commit/2df4478982e95c9a57d5fe3f555667f4365c025d\">created</a>\non August 21, 2018, and with\nthe first release at <a href=\"https://github.com/grpc-ecosystem/grpc-health-probe/releases/tag/v0.1.0-alpha.1\">Sep 19, 2018</a>.</p>\n<p>This approach for gRPC apps health checking is very popular. There are <a href=\"https://github.com/search?l=Dockerfile&amp;q=grpc_health_probe&amp;type=code\">3,626 Dockerfiles</a>\nwith the <code>grpc_health_probe</code> and <a href=\"https://github.com/search?l=YAML&amp;q=grpc_health_probe&amp;type=Code\">6,621 yaml</a> files that are discovered with the\nbasic search on GitHub (at the moment of writing). This is good indication of the tool popularity\nand the need to support this natively.</p>\n<p>Kubernetes v1.23 introduced an alpha-quality implementation of native support for\nquerying a workload status using gRPC. Because it was an alpha feature,\nthis was disabled by default for the v1.23 release.</p>\n<h2 id=\"using-the-feature\">Using the feature</h2>\n<p>We built gRPC health checking in similar way with other probes and believe\nit will be <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe\">easy to use</a>\nif you are familiar with other probe types in Kubernetes.\nThe natively supported health probe has many benefits over the workaround involving <code>grpc_health_probe</code> executable.</p>\n<p>With the native gRPC support you don't need to download and carry <code>10MB</code> of an additional executable with your image.\nExec probes are generally slower than a gRPC call as they require instantiating a new process to run an executable.\nIt also makes the checks less sensible for edge cases when the pod is running at maximum resources and has troubles\ninstantiating new processes.</p>\n<p>There are a few limitations though. Since configuring a client certificate for probes is hard,\nservices that require client authentication are not supported. The built-in probes are also\nnot checking the server certificates and ignore related problems.</p>\n<p>Built-in checks also cannot be configured to ignore certain types of errors\n(<code>grpc_health_probe</code> returns different exit codes for different errors),\nand cannot be &quot;chained&quot; to run the health check on multiple services in a single probe.</p>\n<p>But all these limitations are quite standard for gRPC and there are easy workarounds\nfor those.</p>\n<h2 id=\"try-it-for-yourself\">Try it for yourself</h2>\n<h3 id=\"cluster-level-setup\">Cluster-level setup</h3>\n<p>You can try this feature today. To try native gRPC probes, you can spin up a Kubernetes cluster\nyourself with the <code>GRPCContainerProbe</code> feature gate enabled, there are many <a href=\"https://kubernetes.io/docs/tasks/tools/\">tools available</a>.</p>\n<p>Since the feature gate <code>GRPCContainerProbe</code> is enabled by default in 1.24,\nmany vendors will have this functionality working out of the box.\nSo you may just create an 1.24 cluster on platform of your choice. Some vendors\nallow to enable alpha features on 1.23 clusters.</p>\n<p>For example, at the moment of writing, you can spin up the test cluster on GKE for a quick test.\nOther vendors may also have similar capabilities, especially if you\nare reading this blog post long after the Kubernetes 1.24 release.</p>\n<p>On GKE use the following command (note, version is <code>1.23</code> and <code>enable-kubernetes-alpha</code> are specified).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>gcloud container clusters create test-grpc <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --enable-kubernetes-alpha <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --no-enable-autorepair <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --no-enable-autoupgrade <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --release-channel<span style=\"color:#666\">=</span>rapid <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> --cluster-version<span style=\"color:#666\">=</span>1.23\n</span></span></code></pre></div><p>You will also need to configure <code>kubectl</code> to access the cluster:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>gcloud container clusters get-credentials test-grpc\n</span></span></code></pre></div><h3 id=\"trying-the-feature-out\">Trying the feature out</h3>\n<p>Let's create the pod to test how gRPC probes work. For this test we will use the <code>agnhost</code> image.\nThis is a k8s maintained image with that can be used for all sorts of workload testing.\nFor example, it has a useful <a href=\"https://github.com/kubernetes/kubernetes/blob/b2c5bd2a278288b5ef19e25bf7413ecb872577a4/test/images/agnhost/README.md#grpc-health-checking\">grpc-health-checking</a> module\nthat exposes two ports - one is serving health checking service,\nanother - http port to react on commands <code>make-serving</code> and <code>make-not-serving</code>.</p>\n<p>Here is an example pod definition. It starts the <code>grpc-health-checking</code> module,\nexposes ports <code>5000</code> and <code>8080</code>, and configures gRPC readiness probe:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-grpc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>agnhost<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>k8s.gcr.io/e2e-test-images/agnhost:2.35<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;/agnhost&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;grpc-health-checking&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ports</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">containerPort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">containerPort</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">8080</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">readinessProbe</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">grpc</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">port</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5000</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>If the file called <code>test.yaml</code>, you can create the pod and check it's status.\nThe pod will be in ready state as indicated by the snippet of the output.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl apply -f test.yaml\n</span></span><span style=\"display:flex;\"><span>kubectl describe test-grpc\n</span></span></code></pre></div><p>The output will contain something like this:</p>\n<pre tabindex=\"0\"><code>Conditions:\nType Status\nInitialized True\nReady True\nContainersReady True\nPodScheduled True\n</code></pre><p>Now let's change the health checking endpoint status to NOT_SERVING.\nIn order to call the http port of the Pod, let's create a port forward:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl port-forward test-grpc 8080:8080\n</span></span></code></pre></div><p>You can <code>curl</code> to call the command...</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>curl http://localhost:8080/make-not-serving\n</span></span></code></pre></div><p>... and in a few seconds the port status will switch to not ready.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl describe pod test-grpc\n</span></span></code></pre></div><p>The output now will have:</p>\n<pre tabindex=\"0\"><code>Conditions:\nType Status\nInitialized True\nReady False\nContainersReady False\nPodScheduled True\n...\nWarning Unhealthy 2s (x6 over 42s) kubelet Readiness probe failed: service unhealthy (responded with &#34;NOT_SERVING&#34;)\n</code></pre><p>Once it is switched back, in about one second the Pod will get back to ready status:</p>\n<pre tabindex=\"0\"><code class=\"language-bsh\" data-lang=\"bsh\">curl http://localhost:8080/make-serving\nkubectl describe test-grpc\n</code></pre><p>The output indicates that the Pod went back to being <code>Ready</code>:</p>\n<pre tabindex=\"0\"><code>Conditions:\nType Status\nInitialized True\nReady True\nContainersReady True\nPodScheduled True\n</code></pre><p>This new built-in gRPC health probing on Kubernetes makes implementing a health-check via gRPC\nmuch easier than the older approach that relied on using a separate <code>exec</code> probe. Read through\nthe official\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe\">documentation</a>\nto learn more and provide feedback before the feature will be promoted to GA.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>Kubernetes is a popular workload orchestration platform and we add features based on feedback and demand.\nFeatures like gRPC probes support is a minor improvement that will make life of many app developers\neasier and apps more resilient. Try it today and give feedback, before the feature went into GA.</p>","PublishedAt":"2022-05-13 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/13/grpc-probes-now-in-beta/","SourceName":"Kubernetes"}},{"node":{"ID":1221,"Title":"Blog: Kubernetes 1.24: Storage Capacity Tracking Now Generally Available","Description":"<p><strong>Authors:</strong> Patrick Ohly (Intel)</p>\n<p>The v1.24 release of Kubernetes brings <a href=\"https://kubernetes.io/docs/concepts/storage/storage-capacity/\">storage capacity</a>\ntracking as a generally available feature.</p>\n<h2 id=\"problems-we-have-solved\">Problems we have solved</h2>\n<p>As explained in more detail in the <a href=\"https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/\">previous blog post about this\nfeature</a>, storage capacity\ntracking allows a CSI driver to publish information about remaining\ncapacity. The kube-scheduler then uses that information to pick suitable nodes\nfor a Pod when that Pod has volumes that still need to be provisioned.</p>\n<p>Without this information, a Pod may get stuck without ever being scheduled onto\na suitable node because kube-scheduler has to choose blindly and always ends up\npicking a node for which the volume cannot be provisioned because the\nunderlying storage system managed by the CSI driver does not have sufficient\ncapacity left.</p>\n<p>Because CSI drivers publish storage capacity information that gets used at a\nlater time when it might not be up-to-date anymore, it can still happen that a\nnode is picked that doesn't work out after all. Volume provisioning recovers\nfrom that by informing the scheduler that it needs to try again with a\ndifferent node.</p>\n<p><a href=\"https://github.com/kubernetes-csi/csi-driver-host-path/blob/master/docs/storage-capacity-tracking.md\">Load\ntests</a>\nthat were done again for promotion to GA confirmed that all storage in a\ncluster can be consumed by Pods with storage capacity tracking whereas Pods got\nstuck without it.</p>\n<h2 id=\"problems-we-have-not-solved\">Problems we have <em>not</em> solved</h2>\n<p>Recovery from a failed volume provisioning attempt has one known limitation: if a Pod\nuses two volumes and only one of them could be provisioned, then all future\nscheduling decisions are limited by the already provisioned volume. If that\nvolume is local to a node and the other volume cannot be provisioned there, the\nPod is stuck. This problem pre-dates storage capacity tracking and while the\nadditional information makes it less likely to occur, it cannot be avoided in\nall cases, except of course by only using one volume per Pod.</p>\n<p>An idea for solving this was proposed in a <a href=\"https://github.com/kubernetes/enhancements/pull/1703\">KEP\ndraft</a>: volumes that were\nprovisioned and haven't been used yet cannot have any valuable data and\ntherefore could be freed and provisioned again elsewhere. SIG Storage is\nlooking for interested developers who want to continue working on this.</p>\n<p>Also not solved is support in Cluster Autoscaler for Pods with volumes. For CSI\ndrivers with storage capacity tracking, a prototype was developed and discussed\nin <a href=\"https://github.com/kubernetes/autoscaler/pull/3887\">a PR</a>. It was meant to\nwork with arbitrary CSI drivers, but that flexibility made it hard to configure\nand slowed down scale up operations: because autoscaler was unable to simulate\nvolume provisioning, it only scaled the cluster by one node at a time, which\nwas seen as insufficient.</p>\n<p>Therefore that PR was not merged and a different approach with tighter coupling\nbetween autoscaler and CSI driver will be needed. For this a better\nunderstanding is needed about which local storage CSI drivers are used in\ncombination with cluster autoscaling. Should this lead to a new KEP, then users\nwill have to try out an implementation in practice before it can move to beta\nor GA. So please reach out to SIG Storage if you have an interest in this\ntopic.</p>\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n<p>Thanks a lot to the members of the community who have contributed to this\nfeature or given feedback including members of <a href=\"https://github.com/kubernetes/community/tree/master/sig-scheduling\">SIG\nScheduling</a>,\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-autoscaling\">SIG\nAutoscaling</a>,\nand of course <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">SIG\nStorage</a>!</p>","PublishedAt":"2022-05-06 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/","SourceName":"Kubernetes"}},{"node":{"ID":1222,"Title":"Blog: Kubernetes 1.24: Volume Expansion Now A Stable Feature","Description":"<p><strong>Author:</strong> Hemant Kumar (Red Hat)</p>\n<p>Volume expansion was introduced as a alpha feature in Kubernetes 1.8 and it went beta in 1.11 and with Kubernetes 1.24 we are excited to announce general availability(GA)\nof volume expansion.</p>\n<p>This feature allows Kubernetes users to simply edit their <code>PersistentVolumeClaim</code> objects and specify new size in PVC Spec and Kubernetes will automatically expand the volume\nusing storage backend and also expand the underlying file system in-use by the Pod without requiring any downtime at all if possible.</p>\n<h3 id=\"how-to-use-volume-expansion\">How to use volume expansion</h3>\n<p>You can trigger expansion for a PersistentVolume by editing the <code>spec</code> field of a PVC, specifying a different\n(and larger) storage request. For example, given following PVC:</p>\n<pre tabindex=\"0\"><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\nname: myclaim\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 1Gi # specify new size here\n</code></pre><p>You can request expansion of the underlying PersistentVolume by specifying a new value instead of old <code>1Gi</code> size.\nOnce you've changed the requested size, watch the <code>status.conditions</code> field of the PVC to see if the\nresize has completed.</p>\n<p>When Kubernetes starts expanding the volume - it will add <code>Resizing</code> condition to the PVC, which will be removed once expansion completes. More information about progress of\nexpansion operation can also be obtained by monitoring events associated with the PVC:</p>\n<pre tabindex=\"0\"><code>kubectl describe pvc &lt;pvc&gt;\n</code></pre><h3 id=\"storage-driver-support\">Storage driver support</h3>\n<p>Not every volume type however is expandable by default. Some volume types such as - intree hostpath volumes are not expandable at all. For CSI volumes - the CSI driver\nmust have capability <code>EXPAND_VOLUME</code> in controller or node service (or both if appropriate). Please refer to documentation of your CSI driver, to find out\nif it supports volume expansion.</p>\n<p>Please refer to volume expansion documentation for intree volume types which support volume expansion - <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims\">Expanding Persistent Volumes</a>.</p>\n<p>In general to provide some degree of control over volumes that can be expanded, only dynamically provisioned PVCs whose storage class has <code>allowVolumeExpansion</code> parameter set to <code>true</code> are expandable.</p>\n<p>A Kubernetes cluster administrator must edit the appropriate StorageClass object and set\nthe <code>allowVolumeExpansion</code> field to <code>true</code>. For example:</p>\n<pre tabindex=\"0\"><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: gp2-default\nprovisioner: kubernetes.io/aws-ebs\nparameters:\nsecretNamespace: &#34;&#34;\nsecretName: &#34;&#34;\nallowVolumeExpansion: true\n</code></pre><h3 id=\"online-expansion-compared-to-offline-expansion\">Online expansion compared to offline expansion</h3>\n<p>By default, Kubernetes attempts to expand volumes immediately after user requests a resize.\nIf one or more Pods are using the volume, Kubernetes tries to expands the volume using an online resize;\nas a result volume expansion usually requires no application downtime.\nFilesystem expansion on the node is also performed online and hence does not require shutting\ndown any Pod that was using the PVC.</p>\n<p>If you expand a PersistentVolume that is not in use, Kubernetes does an offline resize (and,\nbecause the volume isn't in use, there is again no workload disruption).</p>\n<p>In some cases though - if underlying Storage Driver can only support offline expansion, users of the PVC must take down their Pod before expansion can succeed. Please refer to documentation of your storage\nprovider to find out - what mode of volume expansion it supports.</p>\n<p>When volume expansion was introduced as an alpha feature, Kubernetes only supported offline filesystem\nexpansion on the node and hence required users to restart their pods for file system resizing to finish.\nHis behaviour has been changed and Kubernetes tries its best to fulfil any resize request regardless\nof whether the underlying PersistentVolume volume is online or offline. If your storage provider supports\nonline expansion then no Pod restart should be necessary for volume expansion to finish.</p>\n<h2 id=\"next-steps\">Next steps</h2>\n<p>Although volume expansion is now stable as part of the recent v1.24 release,\nSIG Storage are working to make it even simpler for users of Kubernetes to expand their persistent storage.\nKubernetes 1.23 introduced features for triggering recovery from failed volume expansion, allowing users\nto attempt self-service healing after a failed resize.\nSee <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes\">Recovering from volume expansion failure</a> for more details.</p>\n<p>The Kubernetes contributor community is also discussing the potential for StatefulSet-driven storage expansion. This proposed\nfeature would let you trigger expansion for all underlying PVs that are providing storage to a StatefulSet,\nby directly editing the StatefulSet object.\nSee the <a href=\"https://github.com/kubernetes/enhancements/issues/661\">Support Volume Expansion Through StatefulSets</a> enhancement proposal for more details.</p>","PublishedAt":"2022-05-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/","SourceName":"Kubernetes"}},{"node":{"ID":1223,"Title":"Blog: Dockershim: The Historical Context","Description":"<p><strong>Author:</strong> Kat Cosgrove</p>\n<p>Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, we‚Äôve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, it‚Äôs been in the works for so long that many of today‚Äôs users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.</p>\n<p>So what is the dockershim, and why is it going away?</p>\n<p>In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there weren‚Äôt really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.</p>\n<p>The <a href=\"https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/\">Container Runtime Interface</a> (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engine‚Äôs use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.</p>\n<p>However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim\">KEP-2221 was introduced</a>, proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.</p>\n<p>We didn‚Äôt do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released <a href=\"https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/\">a blog</a> and <a href=\"https://kubernetes.io/blog/2020/12/02/dockershim-faq/\">accompanying FAQ</a> to allay the community‚Äôs fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the community‚Äôs concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of <a href=\"https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/\">cri-dockerd</a>, allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/\">migration documentation was written</a>.</p>\n<p>We later <a href=\"https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/\">surveyed the community</a> and <a href=\"https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim\">discovered that there are still many users with questions and concerns</a>. In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.</p>\n<p>Docker is not going away, either as a tool or as a company. It‚Äôs an important part of the cloud native community and the history of the Kubernetes project. We wouldn‚Äôt be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and we‚Äôre glad to be doing so with the help of Docker and the community.</p>","PublishedAt":"2022-05-03 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/","SourceName":"Kubernetes"}},{"node":{"ID":1224,"Title":"Blog: Kubernetes 1.24: Stargazer","Description":"<p><strong>Authors</strong>: <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.24/release-team.md\">Kubernetes 1.24 Release Team</a></p>\n<p>We are excited to announce the release of Kubernetes 1.24, the first release of 2022!</p>\n<p>This release consists of 46 enhancements: fourteen enhancements have graduated to stable,\nfifteen enhancements are moving to beta, and thirteen enhancements are entering alpha.\nAlso, two features have been deprecated, and two features have been removed.</p>\n<h2 id=\"major-themes\">Major Themes</h2>\n<h3 id=\"dockershim-removed-from-kubelet\">Dockershim Removed from kubelet</h3>\n<p>After its deprecation in v1.20, the dockershim component has been removed from the kubelet in Kubernetes v1.24.\nFrom v1.24 onwards, you will need to either use one of the other <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes/\">supported runtimes</a> (such as containerd or CRI-O)\nor use cri-dockerd if you are relying on Docker Engine as your container runtime.\nFor more information about ensuring your cluster is ready for this removal, please\nsee <a href=\"https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/\">this guide</a>.</p>\n<h3 id=\"beta-apis-off-by-default\">Beta APIs Off by Default</h3>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/3136\">New beta APIs will not be enabled in clusters by default</a>.\nExisting beta APIs and new versions of existing beta APIs will continue to be enabled by default.</p>\n<h3 id=\"signing-release-artifacts\">Signing Release Artifacts</h3>\n<p>Release artifacts are <a href=\"https://github.com/kubernetes/enhancements/issues/3031\">signed</a> using <a href=\"https://github.com/sigstore/cosign\">cosign</a>\nsignatures,\nand there is experimental support for <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-images/\">verifying image signatures</a>.\nSigning and verification of release artifacts is part of <a href=\"https://github.com/kubernetes/enhancements/issues/3027\">increasing software supply chain security for the Kubernetes release process</a>.</p>\n<h3 id=\"openapi-v3\">OpenAPI v3</h3>\n<p>Kubernetes 1.24 offers beta support for publishing its APIs in the <a href=\"https://github.com/kubernetes/enhancements/issues/2896\">OpenAPI v3 format</a>.</p>\n<h3 id=\"storage-capacity-and-volume-expansion-are-generally-available\">Storage Capacity and Volume Expansion Are Generally Available</h3>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/1472\">Storage capacity tracking</a>\nsupports exposing currently available storage capacity via <a href=\"https://kubernetes.io/docs/concepts/storage/storage-capacity/#api\">CSIStorageCapacity objects</a>\nand enhances scheduling of pods that use CSI volumes with late binding.</p>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/284\">Volume expansion</a> adds support\nfor resizing existing persistent volumes.</p>\n<h3 id=\"nonpreemptingpriority-to-stable\">NonPreemptingPriority to Stable</h3>\n<p>This feature adds <a href=\"https://github.com/kubernetes/enhancements/issues/902\">a new option to PriorityClasses</a>,\nwhich can enable or disable pod preemption.</p>\n<h3 id=\"storage-plugin-migration\">Storage Plugin Migration</h3>\n<p>Work is underway to <a href=\"https://github.com/kubernetes/enhancements/issues/625\">migrate the internals of in-tree storage plugins</a> to call out to CSI Plugins\nwhile maintaining the original API.\nThe <a href=\"https://github.com/kubernetes/enhancements/issues/1490\">Azure Disk</a>\nand <a href=\"https://github.com/kubernetes/enhancements/issues/1489\">OpenStack Cinder</a> plugins\nhave both been migrated.</p>\n<h3 id=\"grpc-probes-graduate-to-beta\">gRPC Probes Graduate to Beta</h3>\n<p>With Kubernetes 1.24, the <a href=\"https://github.com/kubernetes/enhancements/issues/2727\">gRPC probes functionality</a>\nhas entered beta and is available by default. You can now <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\">configure startup, liveness, and readiness probes</a> for your gRPC app\nnatively within Kubernetes without exposing an HTTP endpoint or\nusing an extra executable.</p>\n<h3 id=\"kubelet-credential-provider-graduates-to-beta\">Kubelet Credential Provider Graduates to Beta</h3>\n<p>Originally released as Alpha in Kubernetes 1.20, the kubelet's support for\n<a href=\"https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/\">image credential providers</a>\nhas now graduated to Beta.\nThis allows the kubelet to dynamically retrieve credentials for a container image registry\nusing exec plugins rather than storing credentials on the node's filesystem.</p>\n<h3 id=\"contextual-logging-in-alpha\">Contextual Logging in Alpha</h3>\n<p>Kubernetes 1.24 has introduced <a href=\"https://github.com/kubernetes/enhancements/issues/3077\">contextual logging</a>\nthat enables the caller of a function to control all aspects of logging (output formatting, verbosity, additional values, and names).</p>\n<h3 id=\"avoiding-collisions-in-ip-allocation-to-services\">Avoiding Collisions in IP allocation to Services</h3>\n<p>Kubernetes 1.24 introduces a new opt-in feature that allows you to\n<a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#service-ip-static-sub-range\">soft-reserve a range for static IP address assignments</a>\nto Services.\nWith the manual enablement of this feature, the cluster will prefer automatic assignment from\nthe pool of Service IP addresses, thereby reducing the risk of collision.</p>\n<p>A Service <code>ClusterIP</code> can be assigned:</p>\n<ul>\n<li>dynamically, which means the cluster will automatically pick a free IP within the configured Service IP range.</li>\n<li>statically, which means the user will set one IP within the configured Service IP range.</li>\n</ul>\n<p>Service <code>ClusterIP</code> are unique; hence, trying to create a Service with a <code>ClusterIP</code> that has already been allocated will return an error.</p>\n<h3 id=\"dynamic-kubelet-configuration-is-removed-from-the-kubelet\">Dynamic Kubelet Configuration is Removed from the Kubelet</h3>\n<p>After being deprecated in Kubernetes 1.22, Dynamic Kubelet Configuration has been removed from the kubelet. The feature will be removed from the API server in Kubernetes 1.26.</p>\n<h2 id=\"cni-version-related-breaking-change\">CNI Version-Related Breaking Change</h2>\n<p>Before you upgrade to Kubernetes 1.24, please verify that you are using/upgrading to a container\nruntime that has been tested to work correctly with this release.</p>\n<p>For example, the following container runtimes are being prepared, or have already been prepared, for Kubernetes:</p>\n<ul>\n<li>containerd v1.6.4 and later, v1.5.11 and later</li>\n<li>CRI-O 1.24 and later</li>\n</ul>\n<p>Service issues exist for pod CNI network setup and tear down in containerd\nv1.6.0‚Äìv1.6.3 when the CNI plugins have not been upgraded and/or the CNI config\nversion is not declared in the CNI config files. The containerd team reports, &quot;these issues are resolved in containerd v1.6.4.&quot;</p>\n<p>With containerd v1.6.0‚Äìv1.6.3, if you do not upgrade the CNI plugins and/or\ndeclare the CNI config version, you might encounter the following &quot;Incompatible\nCNI versions&quot; or &quot;Failed to destroy network for sandbox&quot; error conditions.</p>\n<h2 id=\"csi-snapshot\">CSI Snapshot</h2>\n<p><em>This information was added after initial publication.</em></p>\n<p><a href=\"https://github.com/kubernetes/enhancements/issues/177\">VolumeSnapshot v1beta1 CRD has been removed</a>.\nVolume snapshot and restore functionality for Kubernetes and the Container Storage Interface (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and is now unsupported. Refer to <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot\">KEP-177: CSI Snapshot</a> and <a href=\"https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/\">Volume Snapshot GA blog</a> for more information.</p>\n<h2 id=\"other-updates\">Other Updates</h2>\n<h3 id=\"graduations-to-stable\">Graduations to Stable</h3>\n<p>This release saw fourteen enhancements promoted to stable:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/284\">Container Storage Interface (CSI) Volume Expansion</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/688\">Pod Overhead</a>: Account for resources tied to the pod sandbox but not specific containers.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/902\">Add non-preempting option to PriorityClasses</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1472\">Storage Capacity Tracking</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1489\">OpenStack Cinder In-Tree to CSI Driver Migration</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1490\">Azure Disk In-Tree to CSI Driver Migration</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1904\">Efficient Watch Resumption</a>: Watch can be efficiently resumed after kube-apiserver reboot.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1959\">Service Type=LoadBalancer Class Field</a>: Introduce a new Service annotation <code>service.kubernetes.io/load-balancer-class</code> that allows multiple implementations of <code>type: LoadBalancer</code> Services in the same cluster.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2214\">Indexed Job</a>: Add a completion index to Pods of Jobs with a fixed completion count.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2232\">Add Suspend Field to Jobs API</a>: Add a suspend field to the Jobs API to allow orchestrators to create jobs with more control over when pods are created.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2249\">Pod Affinity NamespaceSelector</a>: Add a <code>namespaceSelector</code> field for to pod affinity/anti-affinity spec.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2436\">Leader Migration for Controller Managers</a>: kube-controller-manager and cloud-controller-manager can apply new controller-to-controller-manager assignment in HA control plane without downtime.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2784\">CSR Duration</a>: Extend the CertificateSigningRequest API with a mechanism to allow clients to request a specific duration for the issued certificate.</li>\n</ul>\n<h3 id=\"major-changes\">Major Changes</h3>\n<p>This release saw two major changes:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2221\">Dockershim Removal</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/3136\">Beta APIs are off by Default</a></li>\n</ul>\n<h3 id=\"release-notes\">Release Notes</h3>\n<p>Check out the full details of the Kubernetes 1.24 release in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md\">release notes</a>.</p>\n<h3 id=\"availability\">Availability</h3>\n<p>Kubernetes 1.24 is available for download on <a href=\"https://github.com/kubernetes/kubernetes/releases/tag/v1.24.0\">GitHub</a>.\nTo get started with Kubernetes, check out these <a href=\"https://kubernetes.io/docs/tutorials/\">interactive tutorials</a> or run local\nKubernetes clusters using containers as ‚Äúnodes‚Äù, with <a href=\"https://kind.sigs.k8s.io/\">kind</a>.\nYou can also easily install 1.24 using <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h3 id=\"release-team\">Release Team</h3>\n<p>This release would not have been possible without the combined efforts of committed individuals\ncomprising the Kubernetes 1.24 release team. This team came together to deliver all of the components\nthat go into each Kubernetes release, including code, documentation, release notes, and more.</p>\n<p>Special thanks to James Laverack, our release lead, for guiding us through a successful release cycle,\nand to all of the release team members for the time and effort they put in to deliver the v1.24\nrelease for the Kubernetes community.</p>\n<h3 id=\"release-theme-and-logo\">Release Theme and Logo</h3>\n<p><strong>Kubernetes 1.24: Stargazer</strong></p>\n<figure class=\"release-logo\">\n<img src=\"https://kubernetes.io/images/blog/2022-05-03-kubernetes-release-1.24/kubernetes-1.24.png\"/>\n</figure>\n<p>The theme for Kubernetes 1.24 is <em>Stargazer</em>.</p>\n<p>Generations of people have looked to the stars in awe and wonder, from ancient astronomers to the\nscientists who built the James Webb Space Telescope. The stars have inspired us, set our imagination\nalight, and guided us through long nights on difficult seas.</p>\n<p>With this release we gaze upwards, to what is possible when our community comes together. Kubernetes\nis the work of hundreds of contributors across the globe and thousands of end-users supporting\napplications that serve millions. Every one is a star in our sky, helping us chart our course.</p>\n<p>The release logo is made by <a href=\"https://www.instagram.com/artsyfie/\">Britnee Laverack</a>, and depicts a telescope set upon starry skies and the\n<a href=\"https://en.wikipedia.org/wiki/Pleiades\">Pleiades</a>, often known in mythology as the ‚ÄúSeven Sisters‚Äù. The number seven is especially auspicious\nfor the Kubernetes project, and is a reference back to our original ‚ÄúProject Seven‚Äù name.</p>\n<p>This release of Kubernetes is named for those that would look towards the night sky and wonder ‚Äî for\nall the stargazers out there. ‚ú®</p>\n<h3 id=\"user-highlights\">User Highlights</h3>\n<ul>\n<li>Check out how leading retail e-commerce company <a href=\"https://www.cncf.io/case-studies/la-redoute/\">La Redoute used Kubernetes, alongside other CNCF projects, to transform and streamline its software delivery lifecycle</a> - from development to operations.</li>\n<li>Trying to ensure no change to an API call would cause any breaks, <a href=\"https://www.cncf.io/case-studies/salt-security/\">Salt Security built its microservices entirely on Kubernetes, and it communicates via gRPC while Linkerd ensures messages are encrypted</a>.</li>\n<li>In their effort to migrate from private to public cloud, <a href=\"https://www.cncf.io/case-studies/allianz/\">Allainz Direct engineers redesigned its CI/CD pipeline in just three months while managing to condense 200 workflows down to 10-15</a>.</li>\n<li>Check out how <a href=\"https://www.cncf.io/case-studies/bink/\">Bink, a UK based fintech company, updated its in-house Kubernetes distribution with Linkerd to build a cloud-agnostic platform that scales as needed whilst allowing them to keep a close eye on performance and stability</a>.</li>\n<li>Using Kubernetes, the Dutch organization <a href=\"http://www.stichtingopennederland.nl/\">Stichting Open Nederland</a> created a testing portal in just one-and-a-half months to help safely reopen events in the Netherlands. The <a href=\"https://www.testenvoortoegang.org/\">Testing for Entry (Testen voor Toegang)</a> platform <a href=\"https://www.cncf.io/case-studies/true/\">leveraged the performance and scalability of Kubernetes to help individuals book over 400,000 COVID-19 testing appointments per day. </a></li>\n<li>Working alongside SparkFabrik and utilizing Backstage, <a href=\"https://www.cncf.io/case-studies/santagostino/\">Santagostino created the developer platform Samaritan to centralize services and documentation, manage the entire lifecycle of services, and simplify the work of Santagostino developers</a>.</li>\n</ul>\n<h3 id=\"ecosystem-updates\">Ecosystem Updates</h3>\n<ul>\n<li>KubeCon + CloudNativeCon Europe 2022 will take place in Valencia, Spain, from 16 ‚Äì 20 May 2022! You can find more information about the conference and registration on the <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/\">event site</a>.</li>\n<li>In the <a href=\"https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/\">2021 Cloud Native Survey</a>, the CNCF saw record Kubernetes and container adoption. Take a look at the <a href=\"https://www.cncf.io/reports/cncf-annual-survey-2021/\">results of the survey</a>.</li>\n<li>The <a href=\"https://www.linuxfoundation.org/\">Linux Foundation</a> and <a href=\"https://www.cncf.io/\">The Cloud Native Computing Foundation</a> (CNCF) announced the availability of a new <a href=\"https://training.linuxfoundation.org/training/cloudnativedev-bootcamp/?utm_source=lftraining&amp;utm_medium=pr&amp;utm_campaign=clouddevbc0322\">Cloud Native Developer Bootcamp</a> to provide participants with the knowledge and skills to design, build, and deploy cloud native applications. Check out the <a href=\"https://www.cncf.io/announcements/2022/03/15/new-cloud-native-developer-bootcamp-provides-a-clear-path-to-cloud-native-careers/\">announcement</a> to learn more.</li>\n</ul>\n<h3 id=\"project-velocity\">Project Velocity</h3>\n<p>The <a href=\"https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m\">CNCF K8s DevStats</a> project\naggregates a number of interesting data points related to the velocity of Kubernetes and various\nsub-projects. This includes everything from individual contributions to the number of companies that\nare contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>\n<p>In the v1.24 release cycle, which <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.24\">ran for 17 weeks</a> (January 10 to May 3), we saw contributions from <a href=\"https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;var-metric=contributions\">1029 companies</a> and <a href=\"https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes\">1179 individuals</a>.</p>\n<h2 id=\"upcoming-release-webinar\">Upcoming Release Webinar</h2>\n<p>Join members of the Kubernetes 1.24 release team on Tue May 24, 2022 9:45am ‚Äì 11am PT to learn about\nthe major features of this release, as well as deprecations and removals to help plan for upgrades.\nFor more information and registration, visit the <a href=\"https://community.cncf.io/e/mck3kd/\">event page</a>\non the CNCF Online Programs site.</p>\n<h2 id=\"get-involved\">Get Involved</h2>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests.\nHave something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below:</p>\n<ul>\n<li>Find out more about contributing to Kubernetes at the <a href=\"https://www.kubernetes.dev/\">Kubernetes Contributors</a> website</li>\n<li>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for the latest updates</li>\n<li>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a></li>\n<li>Join the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Post questions (or answer questions) on <a href=\"https://serverfault.com/questions/tagged/kubernetes\">Server Fault</a>.</li>\n<li>Share your Kubernetes <a href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">story</a></li>\n<li>Read more about what‚Äôs happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a></li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a></li>\n</ul>","PublishedAt":"2022-05-03 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/","SourceName":"Kubernetes"}},{"node":{"ID":1225,"Title":"Blog: Frontiers, fsGroups and frogs: the Kubernetes 1.23 release interview","Description":"<p><strong>Author</strong>: Craig Box (Google)</p>\n<p>One of the highlights of hosting the weekly <a href=\"https://kubernetespodcast.com/\">Kubernetes Podcast from Google</a> is talking to the release managers for each new Kubernetes version. The release team is constantly refreshing. Many working their way from small documentation fixes, step up to shadow roles, and then eventually lead a release.</p>\n<p>As we prepare for the 1.24 release next week, <a href=\"https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog\">in accordance with long-standing tradition</a>, I'm pleased to bring you a look back at the story of 1.23. The release was led by <a href=\"https://twitter.com/reylejano\">Rey Lejano</a>, a Field Engineer at SUSE. <a href=\"https://kubernetespodcast.com/episode/167-kubernetes-1.23/\">I spoke to Rey</a> in December, as he was awaiting the birth of his first child.</p>\n<p>Make sure you <a href=\"https://kubernetespodcast.com/subscribe/\">subscribe, wherever you get your podcasts</a>, so you hear all our stories from the Cloud Native community, including the story of 1.24 next week.</p>\n<p><em>This transcript has been lightly edited and condensed for clarity.</em></p>\n<hr>\n<p><strong>CRAIG BOX: I'd like to start with what is, of course, on top of everyone's mind at the moment. Let's talk African clawed frogs!</strong></p>\n<p>REY LEJANO: [CHUCKLES] Oh, you mean <a href=\"https://en.wikipedia.org/wiki/African_clawed_frog\">Xenopus lavis</a>, the scientific name for the African clawed frog?</p>\n<p><strong>CRAIG BOX: Of course.</strong></p>\n<p>REY LEJANO: Not many people know, but my background and my degree is actually in microbiology, from the University of California Davis. I did some research for about four years in biochemistry, in a biochemistry lab, and I <a href=\"https://www.sciencedirect.com/science/article/pii/\">do have a research paper published</a>. It's actually on glycoproteins, particularly something called &quot;cortical granule lectin&quot;. We used frogs, because they generate lots and lots of eggs, from which we can extract the protein. That protein prevents polyspermy. When the sperm goes into the egg, the egg releases a glycoprotein, cortical granule lectin, to the membrane, and prevents any other sperm from going inside the egg.</p>\n<p><strong>CRAIG BOX: Were you able to take anything from the testing that we did on frogs and generalize that to higher-order mammals, perhaps?</strong></p>\n<p>REY LEJANO: Yes. Since mammals also have cortical granule lectin, we were able to analyze both the convergence and the evolutionary pattern, not just from multiple species of frogs, but also into mammals as well.</p>\n<p><strong>CRAIG BOX: Now, there's a couple of different threads to unravel here. When you were young, what led you into the fields of biology, and perhaps more the technical side of it?</strong></p>\n<p>REY LEJANO: I think it was mostly from family, since I do have a family history in the medical field that goes back generations. So I kind of felt like that was the natural path going into college.</p>\n<p><strong>CRAIG BOX: Now, of course, you're working in a more abstract tech field. What led you out of microbiology?</strong></p>\n<p>REY LEJANO: [CHUCKLES] Well, I've always been interested in tech. Taught myself a little programming when I was younger, before high school, did some web dev stuff. Just kind of got burnt out being in a lab. I was literally in the basement. I had a great opportunity to join a consultancy that specialized in <a href=\"https://www.axelos.com/certifications/itil-service-management/what-is-itil\">ITIL</a>. I actually started off with application performance management, went into monitoring, went into operation management and also ITIL, which is aligning your IT asset management and service managements with business services. Did that for a good number of years, actually.</p>\n<p><strong>CRAIG BOX: It's very interesting, as people describe the things that they went through and perhaps the technologies that they worked on, you can pretty much pinpoint how old they might be. There's a lot of people who come into tech these days that have never heard of ITIL. They have no idea what it is. It's basically just SRE with more process.</strong></p>\n<p>REY LEJANO: Yes, absolutely. It's not very cloud native. [CHUCKLES]</p>\n<p><strong>CRAIG BOX: Not at all.</strong></p>\n<p>REY LEJANO: You don't really hear about it in the cloud native landscape. Definitely, you can tell someone's been in the field for a little bit, if they specialize or have worked with ITIL before.</p>\n<p><strong>CRAIG BOX: You mentioned that you wanted to get out of the basement. That is quite often where people put the programmers. Did they just give you a bit of light in the new basement?</strong></p>\n<p>REY LEJANO: [LAUGHS] They did give us much better lighting. Able to get some vitamin D sometimes, as well.</p>\n<p><strong>CRAIG BOX: To wrap up the discussion about your previous career ‚Äî over the course of the last year, with all of the things that have happened in the world, I could imagine that microbiology skills may be more in demand than perhaps they were when you studied them?</strong></p>\n<p>REY LEJANO: Oh, absolutely. I could definitely see a big increase of numbers of people going into the field. Also, reading what's going on with the world currently kind of brings back all the education I've learned in the past, as well.</p>\n<p><strong>CRAIG BOX: Do you keep in touch with people you went through school with?</strong></p>\n<p>REY LEJANO: Just some close friends, but not in the microbiology field.</p>\n<p><strong>CRAIG BOX: One thing that I think will probably happen as a result of the pandemic is a renewed interest in some of these STEM fields. It will be interesting to see what impact that has on society at large.</strong></p>\n<p>REY LEJANO: Yeah. I think that'll be great.</p>\n<p><strong>CRAIG BOX: You mentioned working at a consultancy doing IT management, application performance monitoring, and so on. When did Kubernetes come into your professional life?</strong></p>\n<p>REY LEJANO: One of my good friends at the company I worked at, left in mid-2015. He went on to a company that was pretty heavily into Docker. He taught me a little bit. I did my first &quot;docker run&quot; around 2015, maybe 2016. Then, one of the applications we were using for the ITIL framework was containerized around 2018 or so, also in Kubernetes. At that time, it was pretty buggy. That was my initial introduction to Kubernetes and containerised applications.</p>\n<p>Then I left that company, and I actually joined my friend over at <a href=\"https://rx-m.com/\">RX-M</a>, which is a cloud native consultancy and training firm. They specialize in Docker and Kubernetes. I was able to get my feet wet. I got my CKD, got my CKA as well. And they were really, really great at encouraging us to learn more about Kubernetes and also to be involved in the community.</p>\n<p><strong>CRAIG BOX: You will have seen, then, the life cycle of people adopting Kubernetes and containerization at large, through your own initial journey and then through helping customers. How would you characterize how that journey has changed from the early days to perhaps today?</strong></p>\n<p>REY LEJANO: I think the early days, there was a lot of questions of, why do I have to containerize? Why can't I just stay with virtual machines?</p>\n<p><strong>CRAIG BOX: It's a line item on your CV.</strong></p>\n<p>REY LEJANO: [CHUCKLES] It is. And nowadays, I think people know the value of using containers, of orchestrating containers with Kubernetes. I don't want to say &quot;jumping on the bandwagon&quot;, but it's become the de-facto standard to orchestrate containers.</p>\n<p><strong>CRAIG BOX: It's not something that a consultancy needs to go out and pitch to customers that they should be doing. They're just taking it as, that will happen, and starting a bit further down the path, perhaps.</strong></p>\n<p>REY LEJANO: Absolutely.</p>\n<p><strong>CRAIG BOX: Working at a consultancy like that, how much time do you get to work on improving process, perhaps for multiple customers, and then looking at how you can upstream that work, versus paid work that you do for just an individual customer at a time?</strong></p>\n<p>REY LEJANO: Back then, it would vary. They helped me introduce myself, and I learned a lot about the cloud native landscape and Kubernetes itself. They helped educate me as to how the cloud native landscape, and the tools around it, can be used together. My boss at that company, Randy, he actually encouraged us to start contributing upstream, and encouraged me to join the release team. He just said, this is a great opportunity. Definitely helped me with starting with the contributions early on.</p>\n<p><strong>CRAIG BOX: Was the release team the way that you got involved with upstream Kubernetes contribution?</strong></p>\n<p>REY LEJANO: Actually, no. My first contribution was with SIG Docs. I met Taylor Dolezal ‚Äî he was the release team lead for 1.19, but he is involved with SIG Docs as well. I met him at KubeCon 2019, I sat at his table during a luncheon. I remember Paris Pittman was hosting this luncheon at the Marriott. Taylor says he was involved with SIG Docs. He encouraged me to join. I started joining into meetings, started doing a few drive-by PRs. That's what we call them ‚Äî drive-by ‚Äî little typo fixes. Then did a little bit more, started to send better or higher quality pull requests, and also reviewing PRs.</p>\n<p><strong>CRAIG BOX: When did you first formally take your release team role?</strong></p>\n<p>REY LEJANO: That was in <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md\">1.18</a>, in December. My boss at the time encouraged me to apply. I did, was lucky enough to get accepted for the release notes shadow. Then from there, stayed in with release notes for a few cycles, then went into Docs, naturally then led Docs, then went to Enhancements, and now I'm the release lead for 1.23.</p>\n<p><strong>CRAIG BOX: I don't know that a lot of people think about what goes into a good release note. What would you say does?</strong></p>\n<p>REY LEJANO: [CHUCKLES] You have to tell the end user what has changed or what effect that they might see in the release notes. It doesn't have to be highly technical. It could just be a few lines, and just saying what has changed, what they have to do if they have to do anything as well.</p>\n<p><strong>CRAIG BOX: As you moved through the process of shadowing, how did you learn from the people who were leading those roles?</strong></p>\n<p>REY LEJANO: I said this a few times when I was the release lead for this cycle. You get out of the release team as much as you put in, or it directly aligns to how much you put in. I learned a lot. I went into the release team having that mindset of learning from the role leads, learning from the other shadows, as well. That's actually a saying that my first role lead told me. I still carry it to heart, and that was back in 1.18. That was Eddie, in the very first meeting we had, and I still carry it to heart.</p>\n<p><strong>CRAIG BOX: You, of course, were <a href=\"https://github.com/kubernetes/sig-release/tree/master/releases/release-1.23\">the release lead for 1.23</a>. First of all, congratulations on the release.</strong></p>\n<p>REY LEJANO: Thank you very much.</p>\n<p><strong>CRAIG BOX: The theme for this release is <a href=\"https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/\">The Next Frontier</a>. Tell me the story of how we came to the theme and then the logo.</strong></p>\n<p>REY LEJANO: The Next Frontier represents a few things. It not only represents the next enhancements in this release, but Kubernetes itself also has a history of Star Trek references. The original codename for Kubernetes was Project Seven, a reference to Seven of Nine, originally from Star Trek Voyager. Also the seven spokes in the helm in the logo of Kubernetes as well. And, of course, Borg, the predecessor to Kubernetes.</p>\n<p>The Next Frontier continues that Star Trek reference. It's a fusion of two titles in the Star Trek universe. One is <a href=\"https://en.wikipedia.org/wiki/Star_Trek_V:_The_Final_Frontier\">Star Trek V, the Final Frontier</a>, and the Star Trek: The Next Generation.</p>\n<p><strong>CRAIG BOX: Do you have any opinion on the fact that Star Trek V was an odd-numbered movie, and they are <a href=\"https://screenrant.com/star-trek-movies-odd-number-curse-explained/\">canonically referred to as being lesser than the even-numbered ones</a>?</strong></p>\n<p>REY LEJANO: I can't say, because I am such a sci-fi nerd that I love all of them even though they're bad. Even the post-Next Generation movies, after the series, I still liked all of them, even though I know some weren't that great.</p>\n<p><strong>CRAIG BOX: Am I right in remembering that Star Trek V was the one directed by William Shatner?</strong></p>\n<p>REY LEJANO: Yes, that is correct.</p>\n<p><strong>CRAIG BOX: I think that says it all.</strong></p>\n<p>REY LEJANO: [CHUCKLES] Yes.</p>\n<p><strong>CRAIG BOX: Now, I understand that the theme comes from a part of the <a href=\"https://github.com/kubernetes/community/blob/master/sig-release/charter.md\">SIG Release charter</a>?</strong></p>\n<p>REY LEJANO: Yes. There's a line in the SIG Release charter, &quot;ensure there is a consistent group of community members in place to support the release process across time.&quot; With the release team, we have new shadows that join every single release cycle. With this, we're growing with this community. We're growing the release team members. We're growing SIG Release. We're growing the Kubernetes community itself. For a lot of people, this is their first time contributing to open source, so that's why I say it's their new open source frontier.</p>\n<p><strong>CRAIG BOX: And the logo is obviously very Star Trek-inspired. It sort of surprised me that it took that long for someone to go this route.</strong></p>\n<p>REY LEJANO: I was very surprised as well. I had to relearn Adobe Illustrator to create the logo.</p>\n<p><strong>CRAIG BOX: This your own work, is it?</strong></p>\n<p>REY LEJANO: This is my own work.</p>\n<p><strong>CRAIG BOX: It's very nice.</strong></p>\n<p>REY LEJANO: Thank you very much. Funny, the galaxy actually took me the longest time versus the ship. Took me a few days to get that correct. I'm always fine-tuning it, so there might be a final change when this is actually released.</p>\n<p><strong>CRAIG BOX: No frontier is ever truly final.</strong></p>\n<p>REY LEJANO: True, very true.</p>\n<p><strong>CRAIG BOX: Moving now from the theme of the release to the substance, perhaps, what is new in 1.23?</strong></p>\n<p>REY LEJANO: We have 47 enhancements. I'm going to run through most of the stable ones, if not all of them, some of the key Beta ones, and a few of the Alpha enhancements for 1.23.</p>\n<p>One of the key enhancements is <a href=\"https://github.com/kubernetes/enhancements/issues/563\">dual-stack IPv4/IPv6</a>, which went GA in 1.23.</p>\n<p>Some background info: dual-stack was introduced as Alpha in 1.15. You probably saw a keynote at KubeCon 2019. Back then, the way dual-stack worked was that you needed two services ‚Äî you needed a service per IP family. You would need a service for IPv4 and a service for IPv6. It was refactored in 1.20. In 1.21, it was in Beta; clusters were enabled to be dual-stack by default.</p>\n<p>And then in 1.23 we did remove the IPv6 dual-stack feature flag. It's not mandatory to use dual-stack. It's actually not &quot;default&quot; still. The pods, the services still default to single-stack. There are some requirements to be able to use dual-stack. The nodes have to be routable on IPv4 and IPv6 network interfaces. You need a CNI plugin that supports dual-stack. The pods themselves have to be configured to be dual-stack. And the services need the ipFamilyPolicy field to specify prefer dual-stack, or require dual-stack.</p>\n<p><strong>CRAIG BOX: This sounds like there's an implication in this that v4 is still required. Do you see a world where we can actually move to v6-only clusters?</strong></p>\n<p>REY LEJANO: I think we'll be talking about IPv4 and IPv6 for many, many years to come. I remember a long time ago, they kept saying &quot;it's going to be all IPv6&quot;, and that was decades ago.</p>\n<p><strong>CRAIG BOX: I think I may have mentioned on the show before, but there was <a href=\"https://www.youtube.com/watch?v=AEaJtZVimqs\">a meeting in London that Vint Cerf attended</a>, and he gave a public presentation at the time to say, now is the time of v6. And that was 10 years ago at least. It's still not the time of v6, and my desktop still doesn't have Linux on it. One day.</strong></p>\n<p>REY LEJANO: [LAUGHS] In my opinion, that's one of the big key features that went stable for 1.23.</p>\n<p>One of the other highlights of 1.23 is <a href=\"https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/\">pod security admission going to Beta</a>. I know this feature is going to Beta, but I highlight this because as some people might know, PodSecurityPolicy, which was deprecated in 1.21, is targeted to be removed in 1.25. Pod security admission replaces pod security policy. It's an admission controller. It evaluates the pods against a predefined set of pod security standards to either admit or deny the pod for running.</p>\n<p>There's three levels of pod security standards. Privileged, that's totally open. Baseline, known privileges escalations are minimized. Or Restricted, which is hardened. And you could set pod security standards either to run in three modes, which is enforce: reject any pods that are in violation; to audit: pods are allowed to be created, but the violations are recorded; or warn: it will send a warning message to the user, and the pod is allowed.</p>\n<p><strong>CRAIG BOX: You mentioned there that PodSecurityPolicy is due to be deprecated in two releases' time. Are we lining up these features so that pod security admission will be GA at that time?</strong></p>\n<p>REY LEJANO: Yes. Absolutely. I'll talk about that for another feature in a little bit as well. There's also another feature that went to GA. It was an API that went to GA, and therefore the Beta API is now deprecated. I'll talk about that a little bit.</p>\n<p><strong>CRAIG BOX: All right. Let's talk about what's next on the list.</strong></p>\n<p>REY LEJANO: Let's move on to more stable enhancements. One is the <a href=\"https://github.com/kubernetes/enhancements/issues/592\">TTL controller</a>. This cleans up jobs and pods after the jobs are finished. There is a TTL timer that starts when the job or pod is finished. This TTL controller watches all the jobs, and ttlSecondsAfterFinished needs to be set. The controller will see if the ttlSecondsAfterFinished, combined with the last transition time, if it's greater than now. If it is, then it will delete the job and the pods of that job.</p>\n<p><strong>CRAIG BOX: Loosely, it could be called a garbage collector?</strong></p>\n<p>REY LEJANO: Yes. Garbage collector for pods and jobs, or jobs and pods.</p>\n<p><strong>CRAIG BOX: If Kubernetes is truly becoming a programming language, it of course has to have a garbage collector implemented.</strong></p>\n<p>REY LEJANO: Yeah. There's another one, too, coming in Alpha. [CHUCKLES]</p>\n<p><strong>CRAIG BOX: Tell me about that.</strong></p>\n<p>REY LEJANO: That one is coming in in Alpha. It's actually one of my favorite features, because there's only a few that I'm going to highlight today. <a href=\"https://github.com/kubernetes/enhancements/issues/1847\">PVCs for StafeulSet will be cleaned up</a>. It will auto-delete PVCs created by StatefulSets, when you delete that StatefulSet.</p>\n<p><strong>CRAIG BOX: What's next on our tour of stable features?</strong></p>\n<p>REY LEJANO: Next one is, <a href=\"https://github.com/kubernetes/enhancements/issues/695\">skip volume ownership change goes to stable</a>. This is from SIG Storage. There are times when you're running a stateful application, like many databases, they're sensitive to permission bits changing underneath. Currently, when a volume is bind mounted inside the container, the permissions of that volume will change recursively. It might take a really long time.</p>\n<p>Now, there's a field, the fsGroupChangePolicy, which allows you, as a user, to tell Kubernetes how you want the permission and ownership change for that volume to happen. You can set it to always, to always change permissions, or just on mismatch, to only do it when the permission ownership changes at the top level is different from what is expected.</p>\n<p><strong>CRAIG BOX: It does feel like a lot of these enhancements came from a very particular use case where someone said, &quot;hey, this didn't work for me and I've plumbed in a feature that works with exactly the thing I need to have&quot;.</strong></p>\n<p>REY LEJANO: Absolutely. People create issues for these, then create Kubernetes enhancement proposals, and then get targeted for releases.</p>\n<p><strong>CRAIG BOX: Another GA feature in this release ‚Äî ephemeral volumes.</strong></p>\n<p>REY LEJANO: We've always been able to use empty dir for ephemeral volumes, but now we could actually have <a href=\"https://github.com/kubernetes/enhancements/issues/1698\">ephemeral inline volumes</a>, meaning that you could take your standard CSI driver and be able to use ephemeral volumes with it.</p>\n<p><strong>CRAIG BOX: And, a long time coming, <a href=\"https://github.com/kubernetes/enhancements/issues/19\">CronJobs</a>.</strong></p>\n<p>REY LEJANO: CronJobs is a funny one, because it was stable before 1.23. For 1.23, it was still tracked,but it was just cleaning up some of the old controller. With CronJobs, there's a v2 controller. What was cleaned up in 1.23 is just the old v1 controller.</p>\n<p><strong>CRAIG BOX: Were there any other duplications or major cleanups of note in this release?</strong></p>\n<p>REY LEJANO: Yeah. There were a few you might see in the major themes. One's a little tricky, around FlexVolumes. This is one of the efforts from SIG Storage. They have an effort to migrate in-tree plugins to CSI drivers. This is a little tricky, because FlexVolumes were actually deprecated in November 2020. We're <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors\">formally announcing it in 1.23</a>.</p>\n<p><strong>CRAIG BOX: FlexVolumes, in my mind, predate CSI as a concept. So it's about time to get rid of them.</strong></p>\n<p>REY LEJANO: Yes, it is. There's another deprecation, just some <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog\">klog specific flags</a>, but other than that, there are no other big deprecations in 1.23.</p>\n<p><strong>CRAIG BOX: The buzzword of the last KubeCon, and in some ways the theme of the last 12 months, has been secure software supply chain. What work is Kubernetes doing to improve in this area?</strong></p>\n<p>REY LEJANO: For 1.23, Kubernetes is now SLSA compliant at Level 1, which means that provenance attestation files that describe the staging and release phases of the release process are satisfactory for the SLSA framework.</p>\n<p><strong>CRAIG BOX: What needs to happen to step up to further levels?</strong></p>\n<p>REY LEJANO: Level 1 means a few things ‚Äî that the build is scripted; that the provenance is available, meaning that the artifacts are verified and they're handed over from one phase to the next; and describes how the artifact is produced. Level 2 means that the source is version-controlled, which it is, provenance is authenticated, provenance is service-generated, and there is a build service. There are four levels of SLSA compliance.</p>\n<p><strong>CRAIG BOX: It does seem like the levels were largely influenced by what it takes to build a big, secure project like this. It doesn't seem like it will take a lot of extra work to move up to verifiable provenance, for example. There's probably just a few lines of script required to meet many of those requirements.</strong></p>\n<p>REY LEJANO: Absolutely. I feel like we're almost there; we'll see what will come out of 1.24. And I do want to give a big shout-out to SIG Release and Release Engineering, primarily to Adolfo Garc√≠a Veytia, who is aka Puerco on GitHub and on Slack. He's been driving this forward.</p>\n<p><strong>CRAIG BOX: You've mentioned some APIs that are being graduated in time to replace their deprecated version. Tell me about the new HPA API.</strong></p>\n<p>REY LEJANO: The <a href=\"https://github.com/kubernetes/enhancements/issues/2702\">horizontal pod autoscaler v2 API</a>, is now stable, which means that the v2beta2 API is deprecated. Just for everyone's knowledge, the v1 API is not being deprecated. The difference is that v2 adds support for multiple and custom metrics to be used for HPA.</p>\n<p><strong>CRAIG BOX: There's also now a facility to validate my CRDs with an expression language.</strong></p>\n<p>REY LEJANO: Yeah. You can use the <a href=\"https://github.com/google/cel-spec\">Common Expression Language, or CEL</a>, to validate your CRDs, so you no longer need to use webhooks. This also makes the CRDs more self-contained and declarative, because the rules are now kept within the CRD object definition.</p>\n<p><strong>CRAIG BOX: What new features, perhaps coming in Alpha or Beta, have taken your interest?</strong></p>\n<p>REY LEJANO: Aside from pod security policies, I really love <a href=\"https://github.com/kubernetes/enhancements/issues/277\">ephemeral containers</a> supporting kubectl debug. It launches an ephemeral container and a running pod, shares those pod namespaces, and you can do all your troubleshooting with just running kubectl debug.</p>\n<p><strong>CRAIG BOX: There's also been some interesting changes in the way that events are handled with kubectl.</strong></p>\n<p>REY LEJANO: Yeah. kubectl events has always had some issues, like how things weren't sorted. <a href=\"https://github.com/kubernetes/enhancements/issues/1440\">kubectl events improved</a> that so now you can do <code>--watch</code>, and it will also sort with the <code>--watch</code> option as well. That is something new. You can actually combine fields and custom columns. And also, you can list events in the timeline with doing the last N number of minutes. And you can also sort events using other criteria as well.</p>\n<p><strong>CRAIG BOX: You are a field engineer at SUSE. Are there any things that are coming in that your individual customers that you deal with are looking out for?</strong></p>\n<p>REY LEJANO: More of what I look out for to help the customers.</p>\n<p><strong>CRAIG BOX: Right.</strong></p>\n<p>REY LEJANO: I really love kubectl events. Really love the PVCs being cleaned up with StatefulSets. Most of it's for selfish reasons that it will improve troubleshooting efforts. [CHUCKLES]</p>\n<p><strong>CRAIG BOX: I have always hoped that a release team lead would say to me, &quot;yes, I have selfish reasons. And I finally got something I wanted in.&quot;</strong></p>\n<p>REY LEJANO: [LAUGHS]</p>\n<p><strong>CRAIG BOX: Perhaps I should run to be release team lead, just so I can finally get init containers fixed once and for all.</strong></p>\n<p>REY LEJANO: Oh, init containers, I've been looking for that for a while. I've actually created animated GIFs on how init containers will be run with that Kubernetes enhancement proposal, but it's halted currently.</p>\n<p><strong>CRAIG BOX: One day.</strong></p>\n<p>REY LEJANO: One day. Maybe I shouldn't stay halted.</p>\n<p><strong>CRAIG BOX: You mentioned there are obviously the things you look out for. Are there any things that are coming down the line, perhaps Alpha features or maybe even just proposals you've seen lately, that you're personally really looking forward to seeing which way they go?</strong></p>\n<p>REY LEJANO: Yeah. Oone is a very interesting one, it affects the whole community, so it's not just for personal reasons. As you may have known, Dockershim is deprecated. And we did release a blog that it will be removed in 1.24.</p>\n<p><strong>CRAIG BOX: Scared a bunch of people.</strong></p>\n<p>REY LEJANO: Scared a bunch of people. From a survey, we saw that a lot of people are still using Docker and Dockershim. One of the enhancements for 1.23 is, <a href=\"https://github.com/kubernetes/enhancements/issues/2040\">kubelet CRI goes to Beta</a>. This promotes the CRI API, which is required. This had to be in Beta for Dockershim to be removed in 1.24.</p>\n<p><strong>CRAIG BOX: Now, in the last release team lead interview, <a href=\"https://kubernetespodcast.com/episode/157-kubernetes-1.22/\">we spoke with Savitha Raghunathan</a>, and she talked about what she would advise you as her successor. It was to look out for the mental health of the team members. How were you able to take that advice on board?</strong></p>\n<p>REY LEJANO: That was great advice from Savitha. A few things I've made note of with each release team meeting. After each release team meeting, I stop the recording, because we do record all the meetings and post them on YouTube. And I open up the floor to anyone who wants to say anything that's not recorded, that's not going to be on the agenda. Also, I tell people not to work on weekends. I broke this rule once, but other than that, I told people it could wait. Just be mindful of your mental health.</p>\n<p><strong>CRAIG BOX: It's just been announced that <a href=\"https://twitter.com/JamesLaverack/status/1466834312993644551\">James Laverack from Jetstack</a> will be the release team lead for 1.24. James and I shared an interesting Mexican dinner at the last KubeCon in San Diego.</strong></p>\n<p>REY LEJANO: Oh, nice. I didn't know you knew James.</p>\n<p><strong>CRAIG BOX: The British tech scene. We're a very small world. What will your advice to James be?</strong></p>\n<p>REY LEJANO: What I would tell James for 1.24 is use teachable moments in the release team meetings. When you're a shadow for the first time, it's very daunting. It's very difficult, because you don't know the repos. You don't know the release process. Everyone around you seems like they know the release process, and very familiar with what the release process is. But as a first-time shadow, you don't know all the vernacular for the community. I just advise to use teachable moments. Take a few minutes in the release team meetings to make it a little easier for new shadows to ramp up and to be familiar with the release process.</p>\n<p><strong>CRAIG BOX: Has there been major evolution in the process in the time that you've been involved? Or do you think that it's effectively doing what it needs to do?</strong></p>\n<p>REY LEJANO: It's always evolving. I remember my first time in release notes, 1.18, we said that our goal was to automate and program our way out so that we don't have a release notes team anymore. That's changed [CHUCKLES] quite a bit. Although there's been significant advancements in the release notes process by Adolfo and also James, they've created a subcommand in krel to generate release notes.</p>\n<p>But nowadays, all their release notes are richer. Still not there at the automation process yet. Every release cycle, there is something a little bit different. For this release cycle, we had a production readiness review deadline. It was a soft deadline. A production readiness review is a review by several people in the community. It's actually been required since 1.21, and it ensures that the enhancements are observable, scalable, supportable, and it's safe to operate in production, and could also be disabled or rolled back. In 1.23, we had a deadline to have the production readiness review completed by a specific date.</p>\n<p><strong>CRAIG BOX: How have you found the change of schedule to three releases per year rather than four?</strong></p>\n<p>REY LEJANO: Moving to three releases a year from four, in my opinion, has been an improvement, because we support the last three releases, and now we can actually support the last releases in a calendar year instead of having 9 months out of 12 months of the year.</p>\n<p><strong>CRAIG BOX: The next event on the calendar is a <a href=\"https://www.kubernetes.dev/events/kcc2021/\">Kubernetes contributor celebration</a> starting next Monday. What can we expect from that event?</strong></p>\n<p>REY LEJANO: This is our second time running this virtual event. It's a virtual celebration to recognize the whole community and all of our accomplishments of the year, and also contributors. There's a number of events during this week of celebration. It starts the week of December 13.</p>\n<p>There's events like the Kubernetes Contributor Awards, where SIGs honor and recognize the hard work of the community and contributors. There's also a DevOps party game as well. There is a cloud native bake-off. I do highly suggest people to go to <a href=\"https://www.kubernetes.dev/events/past-events/2021/kcc2021/\">kubernetes.dev/celebration</a> to learn more.</p>\n<p><strong>CRAIG BOX: How exactly does one judge a virtual bake-off?</strong></p>\n<p>REY LEJANO: That I don't know. [CHUCKLES]</p>\n<p><strong>CRAIG BOX: I tasted my scones. I think they're the best. I rate them 10 out of 10.</strong></p>\n<p>REY LEJANO: Yeah. That is very difficult to do virtually. I would have to say, probably what the dish is, how closely it is tied with Kubernetes or open source or to CNCF. There's a few judges. I know Josh Berkus and Rin Oliver are a few of the judges running the bake-off.</p>\n<p><strong>CRAIG BOX: Yes. We spoke with Josh about his love of the kitchen, and so he seems like a perfect fit for that role.</strong></p>\n<p>REY LEJANO: He is.</p>\n<p><strong>CRAIG BOX: Finally, your wife and yourself are expecting your first child in January. Have you had a production readiness review for that?</strong></p>\n<p>REY LEJANO: I think we failed that review. [CHUCKLES]</p>\n<p><strong>CRAIG BOX: There's still time.</strong></p>\n<p>REY LEJANO: We are working on refactoring. We're going to refactor a little bit in December, and <code>--apply</code> again.</p>\n<hr>\n<p><em><a href=\"https://twitter.com/reylejano\">Rey Lejano</a> is a field engineer at SUSE, by way of Rancher Labs, and was the release team lead for Kubernetes 1.23. He is now also a co-chair for SIG Docs. His son Liam is now 3 and a half months old.</em></p>\n<p><em>You can find the <a href=\"http://www.kubernetespodcast.com/\">Kubernetes Podcast from Google</a> at <a href=\"https://twitter.com/KubernetesPod\">@KubernetesPod</a> on Twitter, and you can <a href=\"https://kubernetespodcast.com/subscribe/\">subscribe</a> so you never miss an episode.</em></p>","PublishedAt":"2022-04-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/","SourceName":"Kubernetes"}},{"node":{"ID":1226,"Title":"Blog: Increasing the security bar in Ingress-NGINX v1.2.0","Description":"<p><strong>Authors:</strong> Ricardo Katz (VMware), James Strong (Chainguard)</p>\n<p>The <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> may be one of the most targeted components\nof Kubernetes. An Ingress typically defines an HTTP reverse proxy, exposed to the Internet, containing\nmultiple websites, and with some privileged access to Kubernetes API (such as to read Secrets relating to\nTLS certificates and their private keys).</p>\n<p>While it is a risky component in your architecture, it is still the most popular way to properly expose your services.</p>\n<p>Ingress-NGINX has been part of security assessments that figured out we have a big problem: we don't\ndo all proper sanitization before turning the configuration into an <code>nginx.conf</code> file, which may lead to information\ndisclosure risks.</p>\n<p>While we understand this risk and the real need to fix this, it's not an easy process to do, so we took another approach to reduce (but not remove!) this risk in the current (v1.2.0) release.</p>\n<h2 id=\"meet-ingress-nginx-v1-2-0-and-the-chrooted-nginx-process\">Meet Ingress NGINX v1.2.0 and the chrooted NGINX process</h2>\n<p>One of the main challenges is that Ingress-NGINX runs the web proxy server (NGINX) alongside the Ingress\ncontroller (the component that has access to Kubernetes API that and that creates the <code>nginx.conf</code> file).</p>\n<p>So, NGINX does have the same access to the filesystem of the controller (and Kubernetes service account token, and other configurations from the container). While splitting those components is our end goal, the project needed a fast response; that lead us to the idea of using <code>chroot()</code>.</p>\n<p>Let's take a look into what an Ingress-NGINX container looked like before this change:</p>\n<p><img src=\"ingress-pre-chroot.png\" alt=\"Ingress NGINX pre chroot\"></p>\n<p>As we can see, the same container (not the Pod, the container!) that provides HTTP Proxy is the one that watches Ingress objects and writes the Container Volume</p>\n<p>Now, meet the new architecture:</p>\n<p><img src=\"ingress-post-chroot.png\" alt=\"Ingress NGINX post chroot\"></p>\n<p>What does all of this mean? A basic summary is: that we are isolating the NGINX service as a container inside the\ncontroller container.</p>\n<p>While this is not strictly true, to understand what was done here, it's good to understand how\nLinux containers (and underlying mechanisms such as kernel namespaces) work.\nYou can read about cgroups in the Kubernetes glossary: <a href=\"https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-cgroup\"><code>cgroup</code></a> and learn more about cgroups interact with namespaces in the NGINX project article\n<a href=\"https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/\">What Are Namespaces and cgroups, and How Do They Work?</a>.\n(As you read that, bear in mind that Linux kernel namespaces are a different thing from\n<a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\">Kubernetes namespaces</a>).</p>\n<h2 id=\"skip-the-talk-what-do-i-need-to-use-this-new-approach\">Skip the talk, what do I need to use this new approach?</h2>\n<p>While this increases the security, we made this feature an opt-in in this release so you can have\ntime to make the right adjustments in your environment(s). This new feature is only available from\nrelease v1.2.0 of the Ingress-NGINX controller.</p>\n<p>There are two required changes in your deployments to use this feature:</p>\n<ul>\n<li>Append the suffix &quot;-chroot&quot; to the container image name. For example: <code>gcr.io/k8s-staging-ingress-nginx/controller-chroot:v1.2.0</code></li>\n<li>In your Pod template for the Ingress controller, find where you add the capability <code>NET_BIND_SERVICE</code> and add the capability <code>SYS_CHROOT</code>. After you edit the manifest, you'll see a snippet like:</li>\n</ul>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">capabilities</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">drop</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ALL<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">add</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- NET_BIND_SERVICE<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- SYS_CHROOT<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>If you deploy the controller using the official Helm chart then change the following setting in\n<code>values.yaml</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">controller</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">chroot</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Ingress controllers are normally set up cluster-wide (the IngressClass API is cluster scoped). If you manage the\nIngress-NGINX controller but you're not the overall cluster operator, then check with your cluster admin about\nwhether you can use the <code>SYS_CHROOT</code> capability, <strong>before</strong> you enable it in your deployment.</p>\n<h2 id=\"ok-but-how-does-this-increase-the-security-of-my-ingress-controller\">OK, but how does this increase the security of my Ingress controller?</h2>\n<p>Take the following configuration snippet and imagine, for some reason it was added to your <code>nginx.conf</code>:</p>\n<pre tabindex=\"0\"><code>location /randomthing/ {\nalias /;\nautoindex on;\n}\n</code></pre><p>If you deploy this configuration, someone can call <code>http://website.example/randomthing</code> and get some listing (and access) to the whole filesystem of the Ingress controller.</p>\n<p>Now, can you spot the difference between chrooted and non chrooted Nginx on the listings below?</p>\n<table>\n<thead>\n<tr>\n<th>Without extra <code>chroot()</code></th>\n<th>With extra <code>chroot()</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>bin</code></td>\n<td><code>bin</code></td>\n</tr>\n<tr>\n<td><code>dev</code></td>\n<td><code>dev</code></td>\n</tr>\n<tr>\n<td><code>etc</code></td>\n<td><code>etc</code></td>\n</tr>\n<tr>\n<td><code>home</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>lib</code></td>\n<td><code>lib</code></td>\n</tr>\n<tr>\n<td><code>media</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>mnt</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>opt</code></td>\n<td><code>opt</code></td>\n</tr>\n<tr>\n<td><code>proc</code></td>\n<td><code>proc</code></td>\n</tr>\n<tr>\n<td><code>root</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>run</code></td>\n<td><code>run</code></td>\n</tr>\n<tr>\n<td><code>sbin</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>srv</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>sys</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>tmp</code></td>\n<td><code>tmp</code></td>\n</tr>\n<tr>\n<td><code>usr</code></td>\n<td><code>usr</code></td>\n</tr>\n<tr>\n<td><code>var</code></td>\n<td><code>var</code></td>\n</tr>\n<tr>\n<td><code>dbg</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>nginx-ingress-controller</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>wait-shutdown</code></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>The one in left side is not chrooted. So NGINX has full access to the filesystem. The one in right side is chrooted, so a new filesystem with only the required files to make NGINX work is created.</p>\n<h2 id=\"what-about-other-security-improvements-in-this-release\">What about other security improvements in this release?</h2>\n<p>We know that the new <code>chroot()</code> mechanism helps address some portion of the risk, but still, someone\ncan try to inject commands to read, for example, the <code>nginx.conf</code> file and extract sensitive information.</p>\n<p>So, another change in this release (this is opt-out!) is the <em>deep inspector</em>.\nWe know that some directives or regular expressions may be dangerous to NGINX, so the deep inspector\nchecks all fields from an Ingress object (during its reconciliation, and also with a\n<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook\">validating admission webhook</a>)\nto verify if any fields contains these dangerous directives.</p>\n<p>The ingress controller already does this for annotations, and our goal is to move this existing validation to happen inside\ndeep inspection as part of a future release.</p>\n<p>You can take a look into the existing rules in <a href=\"https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go\">https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go</a>.</p>\n<p>Due to the nature of inspecting and matching all strings within relevant Ingress objects, this new feature may consume a bit more CPU. You can disable it by running the ingress controller with the command line argument <code>--deep-inspect=false</code>.</p>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>This is not our final goal. Our final goal is to split the control plane and the data plane processes.\nIn fact, doing so will help us also achieve a <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway</a> API implementation,\nas we may have a different controller as soon as it &quot;knows&quot; what to provide to the data plane\n(we need some help here!!)</p>\n<p>Some other projects in Kubernetes already take this approach\n(like <a href=\"https://github.com/kubernetes-sigs/kpng\">KPNG</a>, the proposed replacement for <code>kube-proxy</code>),\nand we plan to align with them and get the same experience for Ingress-NGINX.</p>\n<h2 id=\"further-reading\">Further reading</h2>\n<p>If you want to take a look into how chrooting was done in Ingress NGINX, take a look\ninto <a href=\"https://github.com/kubernetes/ingress-nginx/pull/8337\">https://github.com/kubernetes/ingress-nginx/pull/8337</a>\nThe release v1.2.0 containing all the changes can be found at\n<a href=\"https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0\">https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0</a></p>","PublishedAt":"2022-04-28 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/","SourceName":"Kubernetes"}},{"node":{"ID":1227,"Title":"Blog: Kubernetes Removals and Deprecations In 1.24","Description":"<p><strong>Author</strong>: Mickey Boxell (Oracle)</p>\n<p>As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer\nan alternative or improved approach to solving existing problems, motivating the team to remove the\nold approach.</p>\n<p>We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will\n<strong>deprecate</strong> several (beta) APIs in favor of stable versions of the same APIs. The major change coming\nin the Kubernetes 1.24 release is the\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim\">removal of Dockershim</a>.\nThis is discussed below and will be explored in more depth at release time. For an early look at the\nchanges coming in Kubernetes 1.24, take a look at the in-progress\n<a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md\">CHANGELOG</a>.</p>\n<h2 id=\"a-note-about-dockershim\">A note about Dockershim</h2>\n<p>It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24\nis Dockershim. Dockershim was deprecated in v1.20. As noted in the <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation\">Kubernetes 1.20 changelog</a>:\n&quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet\nuses a module called &quot;dockershim&quot; which implements CRI support for Docker and it has seen maintenance\nissues in the Kubernetes community.&quot; With the upcoming release of Kubernetes 1.24, the Dockershim will\nfinally be removed.</p>\n<p>In the article <a href=\"https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/\">Don't Panic: Kubernetes and Docker</a>,\nthe authors succinctly captured the change's impact and encouraged users to remain calm:</p>\n<blockquote>\n<p>Docker as an underlying runtime is being deprecated in favor of runtimes that use the\nContainer Runtime Interface (CRI) created for Kubernetes. Docker-produced images\nwill continue to work in your cluster with all runtimes, as they always have.</p>\n</blockquote>\n<p>Several guides have been created with helpful information about migrating from dockershim\nto container runtimes that are directly compatible with Kubernetes. You can find them on the\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/\">Migrating from dockershim</a>\npage in the Kubernetes documentation.</p>\n<p>For more information about why Kubernetes is moving away from dockershim, check out the aptly\nnamed: <a href=\"https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/\">Kubernetes is Moving on From Dockershim</a>\nand the <a href=\"https://kubernetes.io/blog/2022/02/17/dockershim-faq/\">updated dockershim removal FAQ</a>.</p>\n<p>Take a look at the <a href=\"https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/\">Is Your Cluster Ready for v1.24?</a> post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.</p>\n<h2 id=\"the-kubernetes-api-removal-and-deprecation-process\">The Kubernetes API removal and deprecation process</h2>\n<p>Kubernetes contains a large number of components that evolve over time. In some cases, this\nevolution results in APIs, flags, or entire features, being removed. To prevent users from facing\nbreaking changes, Kubernetes contributors adopted a feature <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">deprecation policy</a>.\nThis policy ensures that stable APIs may only be deprecated when a newer stable version of that\nsame API is available and that APIs have a minimum lifetime as indicated by the following stability levels:</p>\n<ul>\n<li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</li>\n<li>Beta or pre-release API versions must be supported for 3 releases after deprecation.</li>\n<li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</li>\n</ul>\n<p>Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature\ngraduating to stable or because that API was not proven to be successful. Kubernetes will continue to make\nsure migration options are documented whenever APIs are removed.</p>\n<p><strong>Deprecated</strong> APIs are those that have been marked for removal in a future Kubernetes release. <strong>Removed</strong>\nAPIs are those that are no longer available for use in current, supported Kubernetes versions after having\nbeen deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.</p>\n<h2 id=\"api-removals-deprecations-and-other-changes-for-kubernetes-1-24\">API removals, deprecations, and other changes for Kubernetes 1.24</h2>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/281\">Dynamic kubelet configuration</a>: <code>DynamicKubeletConfig</code> is used to enable the dynamic configuration of the kubelet. The <code>DynamicKubeletConfig</code> flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\">Reconfigure kubelet</a>. Refer to the <a href=\"https://github.com/kubernetes/enhancements/issues/281\">&quot;Dynamic kubelet config is removed&quot; KEP</a> for more information.</li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/107207\">Dynamic log sanitization</a>: The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation\">KEP-1753: Kubernetes system components logs sanitization</a> for more information and an <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=\">alternative approach</a>.</li>\n<li>In-tree provisioner to CSI driver migration: This applies to a number of in-tree plugins, including <a href=\"https://github.com/kubernetes/enhancements/issues/2589\">Portworx</a>. Refer to the <a href=\"https://git.k8s.io/design-proposals-archive/storage/csi-migration.md#background-and-motivations\">In-tree Storage Plugin to CSI Migration Design Doc</a> for more information.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2221\">Removing Dockershim from kubelet</a>: the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on <a href=\"https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/\">what you need to do be ready for v1.24</a>.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1472\">Storage capacity tracking for pod scheduling</a>: The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking\">Storage Capacity Constraints for Pod Scheduling KEP</a> for more information.</li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/pull/107533\">The <code>master</code> label is no longer present on kubeadm control plane nodes</a>. For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint\">KEP-2067: Rename the kubeadm &quot;master&quot; label and taint</a>.</li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/177\">VolumeSnapshot v1beta1 CRD will be removed</a>. Volume snapshot and restore functionality for Kubernetes and the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md\">Container Storage Interface</a> (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and will become unsupported with the v1.24 release. Refer to <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot\">KEP-177: CSI Snapshot</a> and the <a href=\"https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/\">Volume Snapshot GA blog</a> blog article for more information.</li>\n</ul>\n<h2 id=\"what-to-do\">What to do</h2>\n<h3 id=\"dockershim-removal\">Dockershim removal</h3>\n<p>As stated earlier, there are several guides about\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/\">Migrating from dockershim</a>.\nYou can start with <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/\">Finding what container runtime are on your nodes</a>.\nIf your nodes are using dockershim, there are other possible Docker Engine dependencies such as\nPods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/\">Check whether Dockershim removal affects you</a> guide to review possible\nDocker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/\">Migrate Docker Engine nodes from dockershim to cri-dockerd</a> or migrate to a CRI-compatible runtime. Here's a guide to\n<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/\">change the container runtime on a node from Docker Engine to containerd</a>.</p>\n<h3 id=\"kubectl-convert\"><code>kubectl convert</code></h3>\n<p>The <a href=\"https://kubernetes.io/docs/tasks/tools/included/kubectl-convert-overview/\"><code>kubectl convert</code></a> plugin for <code>kubectl</code>\ncan be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of\nmanifests between different API versions, for example, from a deprecated to a non-deprecated API\nversion. More general information about the API migration process can be found in the <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/\">Deprecated API Migration Guide</a>.\nFollow the <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin\">install <code>kubectl convert</code> plugin</a>\ndocumentation to download and install the <code>kubectl-convert</code> binary.</p>\n<h3 id=\"looking-ahead\">Looking ahead</h3>\n<p>The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions\nof several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy,\nwhich was deprecated with Kubernetes 1.21 and will not graduate to stable. See <a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">PodSecurityPolicy\nDeprecation: Past, Present, and Future</a> for more information.</p>\n<p>The official <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25\">list of API removals planned for Kubernetes 1.25</a> is:</p>\n<ul>\n<li>The beta CronJob API (batch/v1beta1)</li>\n<li>The beta EndpointSlice API (discovery.k8s.io/v1beta1)</li>\n<li>The beta Event API (events.k8s.io/v1beta1)</li>\n<li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta1)</li>\n<li>The beta PodDisruptionBudget API (policy/v1beta1)</li>\n<li>The beta PodSecurityPolicy API (policy/v1beta1)</li>\n<li>The beta RuntimeClass API (node.k8s.io/v1beta1)</li>\n</ul>\n<p>The official <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26\">list of API removals planned for Kubernetes 1.26</a> is:</p>\n<ul>\n<li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)</li>\n<li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)</li>\n</ul>\n<h3 id=\"want-to-know-more\">Want to know more?</h3>\n<p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation\">Kubernetes 1.21</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation\">Kubernetes 1.22</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation\">Kubernetes 1.23</a></li>\n<li>We will formally announce the deprecations that come with <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation\">Kubernetes 1.24</a> as part of the CHANGELOG for that release.</li>\n</ul>\n<p>For information on the process of deprecation and removal, check out the official Kubernetes <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api\">deprecation policy</a> document.</p>","PublishedAt":"2022-04-07 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/","SourceName":"Kubernetes"}},{"node":{"ID":1228,"Title":"Blog: Is Your Cluster Ready for v1.24?","Description":"<p><strong>Author:</strong> Kat Cosgrove</p>\n<p>Way back in December of 2020, Kubernetes announced the <a href=\"https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/\">deprecation of Dockershim</a>. In Kubernetes, dockershim is a software shim that allows you to use the entire Docker engine as your container runtime within Kubernetes. In the upcoming v1.24 release, we are removing Dockershim - the delay between deprecation and removal in line with the <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">project‚Äôs policy</a> of supporting features for at least one year after deprecation. If you are a cluster operator, this guide includes the practical realities of what you need to know going into this release. Also, what do you need to do to ensure your cluster doesn‚Äôt fall over!</p>\n<h2 id=\"first-does-this-even-affect-you\">First, does this even affect you?</h2>\n<p>If you are rolling your own cluster or are otherwise unsure whether or not this removal affects you, stay on the safe side and <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/\">check to see if you have any dependencies on Docker Engine</a>. Please note that using Docker Desktop to build your application containers is not a Docker dependency for your cluster. Container images created by Docker are compliant with the <a href=\"https://opencontainers.org/\">Open Container Initiative (OCI)</a>, a Linux Foundation governance structure that defines industry standards around container formats and runtimes. They will work just fine on any container runtime supported by Kubernetes.</p>\n<p>If you are using a managed Kubernetes service from a cloud provider, and you haven‚Äôt explicitly changed the container runtime, there may be nothing else for you to do. Amazon EKS, Azure AKS, and Google GKE all default to containerd now, though you should make sure they do not need updating if you have any node customizations. To check the runtime of your nodes, follow <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/\">Find Out What Container Runtime is Used on a Node</a>.</p>\n<p>Regardless of whether you are rolling your own cluster or using a managed Kubernetes service from a cloud provider, you may need to <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/\">migrate telemetry or security agents that rely on Docker Engine</a>.</p>\n<h2 id=\"i-have-a-docker-dependency-what-now\">I have a Docker dependency. What now?</h2>\n<p>If your Kubernetes cluster depends on Docker Engine and you intend to upgrade to Kubernetes v1.24 (which you should eventually do for security and similar reasons), you will need to change your container runtime from Docker Engine to something else or use <a href=\"https://github.com/Mirantis/cri-dockerd\">cri-dockerd</a>. Since <a href=\"https://containerd.io/\">containerd</a> is a graduated CNCF project and the runtime within Docker itself, it‚Äôs a safe bet as an alternative container runtime. Fortunately, the Kubernetes project has already documented the process of <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/\">changing a node‚Äôs container runtime</a>, using containerd as an example. Instructions are similar for switching to one of the other supported runtimes.</p>\n<h2 id=\"i-want-to-upgrade-kubernetes-and-i-need-to-maintain-compatibility-with-docker-as-a-runtime-what-are-my-options\">I want to upgrade Kubernetes, and I need to maintain compatibility with Docker as a runtime. What are my options?</h2>\n<p>Fear not, you aren‚Äôt being left out in the cold and you don‚Äôt have to take the security risk of staying on an old version of Kubernetes. Mirantis and Docker have jointly released, and are maintaining, a replacement for dockershim. That replacement is called <a href=\"https://github.com/Mirantis/cri-dockerd\">cri-dockerd</a>. If you do need to maintain compatibility with Docker as a runtime, install cri-dockerd following the instructions in the project‚Äôs documentation.</p>\n<h2 id=\"is-that-it\">Is that it?</h2>\n<p>Yes. As long as you go into this release aware of the changes being made and the details of your own clusters, and you make sure to communicate clearly with your development teams, it will be minimally dramatic. You may have some changes to make to your cluster, application code, or scripts, but all of these requirements are documented. Switching from using Docker Engine as your runtime to using <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes/\">one of the other supported container runtimes</a> effectively means removing the middleman, since the purpose of dockershim is to access the container runtime used by Docker itself. From a practical perspective, this removal is better both for you and for Kubernetes maintainers in the long-run.</p>\n<p>If you still have questions, please first check the <a href=\"https://kubernetes.io/blog/2022/02/17/dockershim-faq/\">Dockershim Removal FAQ</a>.</p>","PublishedAt":"2022-03-31 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/","SourceName":"Kubernetes"}},{"node":{"ID":1229,"Title":"Blog: Meet Our Contributors - APAC (Aus-NZ region)","Description":"<p><strong>Authors &amp; Interviewers:</strong> <a href=\"https://github.com/anubha-v-ardhan\">Anubhav Vardhan</a>, <a href=\"https://github.com/Atharva-Shinde\">Atharva Shinde</a>, <a href=\"https://github.com/AvineshTripathi\">Avinesh Tripathi</a>, <a href=\"https://github.com/bradmccoydev\">Brad McCoy</a>, <a href=\"https://github.com/Debanitrkl\">Debabrata Panigrahi</a>, <a href=\"https://github.com/jayesh-srivastava\">Jayesh Srivastava</a>, <a href=\"https://github.com/verma-kunal\">Kunal Verma</a>, <a href=\"https://github.com/PranshuSrivastava\">Pranshu Srivastava</a>, <a href=\"github.com/Priyankasaggu11929/\">Priyanka Saggu</a>, <a href=\"https://github.com/PurneswarPrasad\">Purneswar Prasad</a>, <a href=\"https://github.com/vedant-kakde\">Vedant Kakde</a></p>\n<hr>\n<p>Good day, everyone üëã</p>\n<p>Welcome back to the second episode of the &quot;Meet Our Contributors&quot; blog post series for APAC.</p>\n<p>This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.</p>\n<p>So, without further ado, let's get straight to the blog.</p>\n<h2 id=\"caleb-woodbine-https-github-com-bobymcbobs\"><a href=\"https://github.com/BobyMCbobs\">Caleb Woodbine</a></h2>\n<p>Caleb Woodbine is currently a member of the ii.nz organisation.</p>\n<p>He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from <a href=\"https://github.com/hh\">Hippie Hacker</a>, a fellow contributor from New Zealand.</p>\n<p>He has made major contributions to Kubernetes project since then through <code>SIG k8s-infra</code> and <code>k8s-conformance</code> working group.</p>\n<p>Caleb is also a co-organizer of the <a href=\"https://www.meetup.com/cloudnative-nz/\">CloudNative NZ</a> community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.</p>\n<blockquote>\n<p><em>There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally.</em></p>\n</blockquote>\n<h2 id=\"dylan-graham-https-github-com-dylangraham\"><a href=\"https://github.com/DylanGraham\">Dylan Graham</a></h2>\n<p>Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.</p>\n<p>He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.</p>\n<p>He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.</p>\n<p>He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.</p>\n<blockquote>\n<p><em>The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)</em></p>\n</blockquote>\n<h2 id=\"hippie-hacker-https-github-com-hh\"><a href=\"https://github.com/hh\">Hippie Hacker</a></h2>\n<p>Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp; CNCF projects.</p>\n<p>He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated <a href=\"https://ii.nz/post/bringing-the-cloud-to-your-community/\">network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers</a>.</p>\n<p>He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.</p>\n<p>He recommends that new contributors use pair programming.</p>\n<blockquote>\n<p><em>The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford.</em></p>\n</blockquote>\n<h2 id=\"nick-young-https-github-com-youngnick\"><a href=\"https://github.com/youngnick\">Nick Young</a></h2>\n<p>Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.</p>\n<p>His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.</p>\n<p>He asserts the best thing a new contributor can do is to &quot;start contributing&quot;. Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.</p>\n<blockquote>\n<p><em>Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert.</em></p>\n</blockquote>\n<hr>\n<p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.</p>\n<p>We'll see you all in the next one. Everyone, till then, have a happy contributing! üëã</p>","PublishedAt":"2022-03-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/","SourceName":"Kubernetes"}},{"node":{"ID":1230,"Title":"Blog: Updated: Dockershim Removal FAQ","Description":"<p><strong>This supersedes the original\n<a href=\"https://kubernetes.io/blog/2020/12/02/dockershim-faq/\">Dockershim Deprecation FAQ</a> article,\npublished in late 2020. The article includes updates from the v1.24\nrelease of Kubernetes.</strong></p>\n<hr>\n<p>This document goes over some frequently asked questions regarding the\nremoval of <em>dockershim</em> from Kubernetes. The removal was originally\n<a href=\"https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/\">announced</a>\nas a part of the Kubernetes v1.20 release. The Kubernetes\n<a href=\"https://kubernetes.io/releases/#release-v1-24\">v1.24 release</a> actually removed the dockershim\nfrom Kubernetes.</p>\n<p>For more on what that means, check out the blog post\n<a href=\"https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/\">Don't Panic: Kubernetes and Docker</a>.</p>\n<p>To determine the impact that the removal of dockershim would have for you or your organization,\nyou can read <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/\">Check whether dockershim removal affects you</a>.</p>\n<p>In the months and days leading up to the Kubernetes 1.24 release, Kubernetes contributors worked hard to try to make this a smooth transition.</p>\n<ul>\n<li>A blog post detailing our <a href=\"https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/\">commitment and next steps</a>.</li>\n<li>Checking if there were major blockers to migration to <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes\">other container runtimes</a>.</li>\n<li>Adding a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/\">migrating from dockershim</a> guide.</li>\n<li>Creating a list of\n<a href=\"https://kubernetes.io/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/\">articles on dockershim removal and on using CRI-compatible runtimes</a>.\nThat list includes some of the already mentioned docs, and also covers selected external sources\n(including vendor guides).</li>\n</ul>\n<h3 id=\"why-was-the-dockershim-removed-from-kubernetes\">Why was the dockershim removed from Kubernetes?</h3>\n<p>Early versions of Kubernetes only worked with a specific container runtime:\nDocker Engine. Later, Kubernetes added support for working with other container runtimes.\nThe CRI standard was <a href=\"https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/\">created</a> to\nenable interoperability between orchestrators (like Kubernetes) and many different container\nruntimes.\nDocker Engine doesn't implement that interface (CRI), so the Kubernetes project created\nspecial code to help with the transition, and made that <em>dockershim</em> code part of Kubernetes\nitself.</p>\n<p>The dockershim code was always intended to be a temporary solution (hence the name: shim).\nYou can read more about the community discussion and planning in the\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim\">Dockershim Removal Kubernetes Enhancement Proposal</a>.\nIn fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.</p>\n<p>Additionally, features that were largely incompatible with the dockershim, such\nas cgroups v2 and user namespaces are being implemented in these newer CRI\nruntimes. Removing the dockershim from Kubernetes allows further development in those areas.</p>\n<h3 id=\"are-docker-and-containers-the-same-thing\">Are Docker and containers the same thing?</h3>\n<p>Docker popularized the Linux containers pattern and has been instrumental in\ndeveloping the underlying technology, however containers in Linux have existed\nfor a long time. The container ecosystem has grown to be much broader than just\nDocker. Standards like OCI and CRI have helped many tools grow and thrive in our\necosystem, some replacing aspects of Docker while others enhance existing\nfunctionality.</p>\n<h3 id=\"will-my-existing-container-images-still-work\">Will my existing container images still work?</h3>\n<p>Yes, the images produced from <code>docker build</code> will work with all CRI implementations.\nAll your existing images will still work exactly the same.</p>\n<h4 id=\"what-about-private-images\">What about private images?</h4>\n<p>Yes. All CRI runtimes support the same pull secrets configuration used in\nKubernetes, either via the PodSpec or ServiceAccount.</p>\n<h3 id=\"can-i-still-use-docker-engine-in-kubernetes-1-23\">Can I still use Docker Engine in Kubernetes 1.23?</h3>\n<p>Yes, the only thing changed in 1.20 is a single warning log printed at <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/\">kubelet</a>\nstartup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurred\nin Kubernetes 1.24.</p>\n<p>If you're running Kubernetes v1.24 or later, see <a href=\"#can-i-still-use-docker-engine-as-my-container-runtime\">Can I still use Docker Engine as my container runtime?</a>.\n(Remember, you can switch away from the dockershim if you're using any supported Kubernetes release; from release v1.24, you\n<strong>must</strong> switch as Kubernetes no longer includes the dockershim).</p>\n<h3 id=\"which-cri-implementation-should-i-use\">Which CRI implementation should I use?</h3>\n<p>That‚Äôs a complex question and it depends on a lot of factors. If Docker Engine is\nworking for you, moving to containerd should be a relatively easy swap and\nwill have strictly better performance and less overhead. However, we encourage you\nto explore all the options from the <a href=\"https://landscape.cncf.io/card-mode?category=container-runtime&amp;grouping=category\">CNCF landscape</a> in case another would be an\neven better fit for your environment.</p>\n<h4 id=\"can-i-still-use-docker-engine-as-my-container-runtime\">Can I still use Docker Engine as my container runtime?</h4>\n<p>First off, if you use Docker on your own PC to develop or test containers: nothing changes.\nYou can still use Docker locally no matter what container runtime(s) you use for your\nKubernetes clusters. Containers make this kind of interoperability possible.</p>\n<p>Mirantis and Docker have <a href=\"https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/\">committed</a> to maintaining a replacement adapter for\nDocker Engine, and to maintain that adapter even after the in-tree dockershim is removed\nfrom Kubernetes. The replacement adapter is named <a href=\"https://github.com/Mirantis/cri-dockerd\"><code>cri-dockerd</code></a>.</p>\n<p>You can install <code>cri-dockerd</code> and use it to connect the kubelet to Docker Engine. Read <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/\">Migrate Docker Engine nodes from dockershim to cri-dockerd</a> to learn more.</p>\n<h3 id=\"are-there-examples-of-folks-using-other-runtimes-in-production-today\">Are there examples of folks using other runtimes in production today?</h3>\n<p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated\nwith each release.</p>\n<p>Additionally, the <a href=\"https://kind.sigs.k8s.io/\">kind</a> project has been using containerd for some time and has\nseen an improvement in stability for its use case. Kind and containerd are leveraged\nmultiple times every day to validate any changes to the Kubernetes codebase. Other\nrelated projects follow a similar pattern as well, demonstrating the stability and\nusability of other container runtimes. As an example, OpenShift 4.x has been\nusing the <a href=\"https://cri-o.io/\">CRI-O</a> runtime in production since June 2019.</p>\n<p>For other examples and references you can look at the adopters of containerd and\nCRI-O, two container runtimes under the Cloud Native Computing Foundation (<a href=\"https://cncf.io\">CNCF</a>).</p>\n<ul>\n<li><a href=\"https://github.com/containerd/containerd/blob/master/ADOPTERS.md\">containerd</a></li>\n<li><a href=\"https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md\">CRI-O</a></li>\n</ul>\n<h3 id=\"people-keep-referencing-oci-what-is-that\">People keep referencing OCI, what is that?</h3>\n<p>OCI stands for the <a href=\"https://opencontainers.org/about/overview/\">Open Container Initiative</a>, which standardized many of the\ninterfaces between container tools and technologies. They maintain a standard\nspecification for packaging container images (OCI image-spec) and running containers\n(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec\nin the form of <a href=\"https://github.com/opencontainers/runc\">runc</a>, which is the underlying default runtime for both\n<a href=\"https://containerd.io/\">containerd</a> and <a href=\"https://cri-o.io/\">CRI-O</a>. The CRI builds on these low-level specifications to\nprovide an end-to-end standard for managing containers.</p>\n<h3 id=\"what-should-i-look-out-for-when-changing-cri-implementations\">What should I look out for when changing CRI implementations?</h3>\n<p>While the underlying containerization code is the same between Docker and most\nCRIs (including containerd), there are a few differences around the edges. Some\ncommon things to consider when migrating are:</p>\n<ul>\n<li>Logging configuration</li>\n<li>Runtime resource limitations</li>\n<li>Node provisioning scripts that call docker or use Docker Engine via its control socket</li>\n<li>Plugins for <code>kubectl</code> that require the <code>docker</code> CLI or the Docker Engine control socket</li>\n<li>Tools from the Kubernetes project that require direct access to Docker Engine\n(for example: the deprecated <code>kube-imagepuller</code> tool)</li>\n<li>Configuration of functionality like <code>registry-mirrors</code> and insecure registries</li>\n<li>Other support scripts or daemons that expect Docker Engine to be available and are run\noutside of Kubernetes (for example, monitoring or security agents)</li>\n<li>GPUs or special hardware and how they integrate with your runtime and Kubernetes</li>\n</ul>\n<p>If you use Kubernetes resource requests/limits or file-based log collection\nDaemonSets then they will continue to work the same, but if you've customized\nyour <code>dockerd</code> configuration, you‚Äôll need to adapt that for your new container\nruntime where possible.</p>\n<p>Another thing to look out for is anything expecting to run for system maintenance\nor nested inside a container when building images will no longer work. For the\nformer, you can use the <a href=\"https://github.com/kubernetes-sigs/cri-tools\"><code>crictl</code></a> tool as a drop-in replacement (see\n<a href=\"https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl\">mapping from docker cli to crictl</a>)\nand for the latter you can use newer container build options like <a href=\"https://github.com/genuinetools/img\">img</a>, <a href=\"https://github.com/containers/buildah\">buildah</a>,\n<a href=\"https://github.com/GoogleContainerTools/kaniko\">kaniko</a>, or <a href=\"https://github.com/vmware-tanzu/buildkit-cli-for-kubectl\">buildkit-cli-for-kubectl</a> that don‚Äôt require Docker.</p>\n<p>For containerd, you can start with their <a href=\"https://github.com/containerd/cri/blob/master/docs/registry.md\">documentation</a> to see what configuration\noptions are available as you migrate things over.</p>\n<p>For instructions on how to use containerd and CRI-O with Kubernetes, see the\nKubernetes documentation on <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes/\">Container Runtimes</a>.</p>\n<h3 id=\"what-if-i-have-more-questions\">What if I have more questions?</h3>\n<p>If you use a vendor-supported Kubernetes distribution, you can ask them about\nupgrade plans for their products. For end-user questions, please post them\nto our end user community forum: <a href=\"https://discuss.kubernetes.io/\">https://discuss.kubernetes.io/</a>.</p>\n<p>You can discuss the decision to remove dockershim via a dedicated\n<a href=\"https://github.com/kubernetes/kubernetes/issues/106917\">GitHub issue</a>.</p>\n<p>You can also check out the excellent blog post\n<a href=\"https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m\">Wait, Docker is deprecated in Kubernetes now?</a> a more in-depth technical\ndiscussion of the changes.</p>\n<h3 id=\"is-there-any-tooling-that-can-help-me-find-dockershim-in-use\">Is there any tooling that can help me find dockershim in use?</h3>\n<p>Yes! The <a href=\"https://github.com/aws-containers/kubectl-detector-for-docker-socket\">Detector for Docker Socket (DDS)</a> is a kubectl plugin that you can\ninstall and then use to check your cluster. DDS can detect if active Kubernetes workloads\nare mounting the Docker Engine socket (<code>docker.sock</code>) as a volume.\nFind more details and usage patterns in the DDS project's <a href=\"https://github.com/aws-containers/kubectl-detector-for-docker-socket\">README</a>.</p>\n<h3 id=\"can-i-have-a-hug\">Can I have a hug?</h3>\n<p>Yes, we're still giving hugs as requested. ü§óü§óü§ó</p>","PublishedAt":"2022-02-17 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/02/17/dockershim-faq/","SourceName":"Kubernetes"}},{"node":{"ID":1231,"Title":"Blog: SIG Node CI Subproject Celebrates Two Years of Test Improvements","Description":"<p><strong>Authors:</strong> Sergey Kanzhelev (Google), Elana Hashman (Red Hat)</p>\n<p>Ensuring the reliability of SIG Node upstream code is a continuous effort\nthat takes a lot of behind-the-scenes effort from many contributors.\nThere are frequent releases of Kubernetes, base operating systems,\ncontainer runtimes, and test infrastructure that result in a complex matrix that\nrequires attention and steady investment to &quot;keep the lights on.&quot;\nIn May 2020, the Kubernetes node special interest group (&quot;SIG Node&quot;) organized a new\nsubproject for continuous integration (CI) for node-related code and tests. Since its\ninauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour\nis often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all\nrelated ongoing work within the subgroup.</p>\n<p>Over the past two years, we've fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors' pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent &gt;90% job pass rate. We've closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we've helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers.</p>\n<p>The Node CI subproject is an approachable first stop to help new contributors\nget started with SIG Node. There is a low barrier to entry for new contributors\nto address high-impact bugs and test fixes, although there is a long\nroad before contributors can climb the entire contributor ladder:\nit took over a year to establish two new approvers for the group.\nThe complexity of all the different components that power Kubernetes nodes\nand its test infrastructure requires a sustained investment over a long period\nfor developers to deeply understand the entire system,\nboth at high and low levels of detail.</p>\n<p>We have several regular contributors at our meetings, however; our reviewers\nand approvers pool is still small. It is our goal to continue to grow\ncontributors to ensure a sustainable distribution of work\nthat does not just fall to a few key approvers.</p>\n<p>It's not always obvious how subprojects within SIGs are formed, operate,\nand work. Each is unique to its sponsoring SIG and tailored to the projects\nthat the group is intended to support. As a group that has welcomed many\nfirst-time SIG Node contributors, we'd like to share some of the details and\naccomplishments over the past two years,\nhelping to demystify our inner workings and celebrate the hard work\nof all our dedicated contributors!</p>\n<h2 id=\"timeline\">Timeline</h2>\n<p><em><strong>May 2020.</strong></em> SIG Node CI group was formed on May 11, 2020, with more than\n<a href=\"https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib\">30 volunteers</a>\nsigned up, to improve SIG Node CI signal and overall observability.\nVictor Pickard focused on getting\n<a href=\"https://testgrid.k8s.io/sig-node\">testgrid jobs</a> passing\nwhen Ning Liao suggested forming a group around this effort and came up with\nthe <a href=\"https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf\">original group charter document</a>.\nThe SIG Node chairs sponsored group creation with Victor as a subproject lead.\nSergey Kanzhelev joined Victor shortly after as a co-lead.</p>\n<p>At the kick-off meeting, we discussed which tests to concentrate on fixing first\nand discussed merge-blocking and release-blocking tests, many of which were failing due\nto infrastructure issues or buggy test code.</p>\n<p>The subproject launched weekly hour-long meetings to discuss ongoing work\ndiscussion and triage.</p>\n<p><em><strong>June 2020.</strong></em> Morgan Bauer, Karan Goel, and Jorge Alarcon Ochoa were\nrecognized as reviewers for the SIG Node CI group for their contributions,\nhelping significantly with the early stages of the subproject.\nDavid Porter and Roy Yang also joined the SIG test failures GitHub team.</p>\n<p><em><strong>August 2020.</strong></em> All merge-blocking and release-blocking tests were passing,\nwith some flakes. However, only 42% of all SIG Node test jobs were green, as there\nwere many flakes and failing tests.</p>\n<p><em><strong>October 2020.</strong></em> Amim Knabben becomes a Kubernetes org member for his\ncontributions to the subproject.</p>\n<p><em><strong>January 2021.</strong></em> With healthy presubmit and critical periodic jobs passing,\nthe subproject discussed its goal for cleaning up the rest of periodic tests\nand ensuring they passed without flakes.</p>\n<p>Elana Hashman joined the subproject, stepping up to help lead it after\nVictor's departure.</p>\n<p><em><strong>February 2021.</strong></em> Artyom Lukianov becomes a Kubernetes org member for his\ncontributions to the subproject.</p>\n<p><em><strong>August 2021.</strong></em> After SIG Node successfully ran a <a href=\"https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ\">bug scrub</a>\nto clean up its bug backlog, the scope of the meeting was extended to\ninclude bug triage to increase overall reliability, anticipating issues\nbefore they affect the CI signal.</p>\n<p>Subproject leads Elana Hashman and Sergey Kanzhelev are both recognized as\napprovers on all node test code, supported by SIG Node and SIG Testing.</p>\n<p><em><strong>September 2021.</strong></em> After significant deflaking progress with serial tests in\nthe 1.22 release spearheaded by Francesco Romani, the subproject set a goal\nfor getting the serial job fully passing by the 1.23 release date.</p>\n<p>Mike Miranda becomes a Kubernetes org member for his contributions\nto the subproject.</p>\n<p><em><strong>November 2021.</strong></em> Throughout 2021, SIG Node had no merge or\nrelease-blocking test failures. Many flaky tests from past releases are removed\nfrom release-blocking dashboards as they had been fully cleaned up.</p>\n<p>Danielle Lancashire was recognized as a reviewer for SIG Node's subgroup, test code.</p>\n<p>The final node serial tests were completely fixed. The serial tests consist of\nmany disruptive and slow tests which tend to be flakey and are hard\nto troubleshoot. By the 1.23 release freeze, the last serial tests were\nfixed and the job was passing without flakes.</p>\n<p><a href=\"https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900\"><img src=\"serial-tests-green.png\" alt=\"Slack announcement that Serial tests are green\"></a></p>\n<p>The 1.23 release got a special shout out for the tests quality and CI signal.\nThe SIG Node CI subproject was proud to have helped contribute to such\na high-quality release, in part due to our efforts in identifying\nand fixing flakes in Node and beyond.</p>\n<p><a href=\"https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200\"><img src=\"release-mostly-green.png\" alt=\"Slack shoutout that release was mostly green\"></a></p>\n<p><em><strong>December 2021.</strong></em> An estimated 90% of test jobs were passing at the time of\nthe 1.23 release (up from 42% in August 2020).</p>\n<p>Dockershim code was removed from Kubernetes. This affected nearly half of SIG Node's\ntest jobs and the SIG Node CI subproject reacted quickly and retargeted all the\ntests. SIG Node was the first SIG to complete test migrations off dockershim,\nproviding examples for other affected SIGs. The vast majority of new jobs passed\nat the time of introduction without further fixes required. The <a href=\"https://k8s.io/dockershim\">effort of\nremoving dockershim</a>) from Kubernetes is ongoing.\nThere are still some wrinkles from the dockershim removal as we uncover more\ndependencies on dockershim, but we plan to stabilize all test jobs\nby the 1.24 release.</p>\n<h2 id=\"statistics\">Statistics</h2>\n<p>Our regular meeting attendees and subproject participants for the past few months:</p>\n<ul>\n<li>Aditi Sharma</li>\n<li>Artyom Lukianov</li>\n<li>Arnaud Meukam</li>\n<li>Danielle Lancashire</li>\n<li>David Porter</li>\n<li>Davanum Srinivas</li>\n<li>Elana Hashman</li>\n<li>Francesco Romani</li>\n<li>Matthias Bertschy</li>\n<li>Mike Miranda</li>\n<li>Paco Xu</li>\n<li>Peter Hunt</li>\n<li>Ruiwen Zhao</li>\n<li>Ryan Phillips</li>\n<li>Sergey Kanzhelev</li>\n<li>Skyler Clark</li>\n<li>Swati Sehgal</li>\n<li>Wenjun Wu</li>\n</ul>\n<p>The <a href=\"https://github.com/kubernetes/test-infra/\">kubernetes/test-infra</a> source code repository contains test definitions. The number of\nNode PRs just in that repository:</p>\n<ul>\n<li>2020 PRs (since May): <a href=\"https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+\">183</a></li>\n<li>2021 PRs: <a href=\"https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+\">264</a></li>\n</ul>\n<p>Triaged issues and PRs on CI board (including triaging away from the subgroup scope):</p>\n<ul>\n<li>2020 (since May): <a href=\"https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31\">132</a></li>\n<li>2021: <a href=\"https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+\">532</a></li>\n</ul>\n<h2 id=\"future\">Future</h2>\n<p>Just &quot;keeping the lights on&quot; is a bold task and we are committed to improving this experience.\nWe are working to simplify the triage and review processes for SIG Node.</p>\n<p>Specifically, we are working on better test organization, naming,\nand tracking:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/pull/3042\">https://github.com/kubernetes/enhancements/pull/3042</a></li>\n<li><a href=\"https://github.com/kubernetes/test-infra/issues/24641\">https://github.com/kubernetes/test-infra/issues/24641</a></li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0\">Kubernetes SIG-Node CI Testgrid Tracker</a></li>\n</ul>\n<p>We are also constantly making progress on improved tests debuggability and de-flaking.</p>\n<p>If any of this interests you, we'd love for you to join us!\nThere's plenty to learn in debugging test failures, and it will help you gain\nfamiliarity with the code that SIG Node maintains.</p>\n<p>You can always find information about the group on the\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-node\">SIG Node</a> page.\nWe give group updates at our maintainer track sessions, such as\n<a href=\"https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google\">KubeCon + CloudNativeCon Europe 2021</a> and\n<a href=\"https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;w=100%25&amp;sidebar=yes&amp;bg=no\">KubeCon + CloudNative North America 2021</a>.\nJoin us in our mission to keep the kubelet and other SIG Node components reliable and ensure smooth and uneventful releases!</p>","PublishedAt":"2022-02-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/02/16/sig-node-ci-subproject-celebrates/","SourceName":"Kubernetes"}},{"node":{"ID":1232,"Title":"Blog: Spotlight on SIG Multicluster","Description":"<p><strong>Authors:</strong> Dewan Ahmed (Aiven) and Chris Short (AWS)</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p><a href=\"https://github.com/kubernetes/community/tree/master/sig-multicluster\">SIG Multicluster</a> is the SIG focused on how Kubernetes concepts are expanded and used beyond the cluster boundary. Historically, Kubernetes resources only interacted within that boundary - KRU or Kubernetes Resource Universe (not an actual Kubernetes concept). Kubernetes clusters, even now, don't really know anything about themselves or, about other clusters. Absence of cluster identifiers is a case in point. With the growing adoption of multicloud and multicluster deployments, the work SIG Multicluster doing is gaining a lot of attention. In this blog, <a href=\"https://twitter.com/jeremyot\">Jeremy Olmsted-Thompson, Google</a> and <a href=\"https://twitter.com/ChrisShort\">Chris Short, AWS</a> discuss the interesting problems SIG Multicluster is solving and how you can get involved. Their initials <strong>JOT</strong> and <strong>CS</strong> will be used for brevity.</p>\n<h2 id=\"a-summary-of-their-conversation\">A summary of their conversation</h2>\n<p><strong>CS</strong>: How long has the SIG Multicluster existed and how was the SIG in its infancy? How long have you been with this SIG?</p>\n<p><strong>JOT</strong>: I've been around for almost two years in the SIG Multicluster. All I know about the infancy years is from the lore but even in the early days, it was always about solving this same problem. Early efforts have been things like <a href=\"https://github.com/kubernetes-sigs/kubefed\">KubeFed</a>. I think there are still folks using KubeFed but it's a smaller slice. Back then, I think people out there deploying large numbers of Kubernetes clusters were really not at a point where we had a ton of real concrete use cases. Projects like KubeFed and <a href=\"https://github.com/kubernetes-retired/cluster-registry\">Cluster Registry</a> were developed around that time and the need back then can be associated to these projects. The motivation for these projects were how do we solve the problems that we think people are <strong>going to have</strong>, when they start expanding to multiple clusters. Honestly, in some ways, it was trying to do too much at that time.</p>\n<p><strong>CS</strong>: How does KubeFed differ from the current state of SIG Multicluster? How does the <strong>lore</strong> differ from the <strong>now</strong>?</p>\n<p><strong>JOT</strong>: Yeah, it was like trying to get ahead of potential problems instead of addressing specific problems. I think towards the end of 2019, there was a slow down in SIG multicluster work and we kind of picked it back up with one of the most active recent projects that is the <a href=\"https://github.com/kubernetes-sigs/mcs-api\">SIG Multicluster services (MCS)</a>.</p>\n<p>Now this is the shift to solving real specific problems. For example,</p>\n<blockquote>\n<p>I've got workloads that are spread across multiple clusters and I need them to talk to each other.</p>\n</blockquote>\n<p>Okay, that's very straightforward and we know that we need to solve that. To get started, let's make sure that these projects can work together on a common API so you get the same kind of portability that you get with Kubernetes.</p>\n<p>There's a few implementations of the MCS API out there and more are being developed. But, we didn't build an implementation because depending on how you're deploying things there could be hundreds of implementations. As long as you only need the basic Multicluster service functionality, it'll just work on whatever background you want, whether it's Submariner, GKE, or a service mesh.</p>\n<p>My favorite example of &quot;then vs. now&quot; is cluster ID. A few years ago, there was an effort to define a cluster ID. A lot of really good thought went into this concept, for example, how do we make a cluster ID is unique across multiple clusters. How do we make this ID globally unique so it'll work in every contact? Let's say, there's an acquisition or merger of teams - does the cluster IDs still remain unique for those teams?</p>\n<p>With Multicluster services, we found the need for an actual cluster ID, and it has a very specific need. To address this specific need, we're no longer considering every single Kubernetes cluster out there rather the ClusterSets - a grouping of clusters that work together in some kind of bounds. That's a much narrower scope than considering clusters everywhere in time and space. It also leaves flexibility for an implementer to define the boundary (a ClusterSet) beyond which this cluster ID will no longer be unique.</p>\n<p><strong>CS</strong>: How do you feel about the current state of SIG Multicluster versus where you're hoping to be in future?</p>\n<p><strong>JOT</strong>: There's a few projects that are kind of getting started, for example, Work API. In the future, I think that some common practices around how do we deploy things across clusters are going to develop.</p>\n<blockquote>\n<p>If I have clusters deployed in a bunch of different regions; what's the best way to actually do that?</p>\n</blockquote>\n<p>The answer is, almost always, &quot;it depends&quot;. Why are you doing this? Is it because there's some kind of compliance that makes you care about locality? Is it performance? Is it availability?</p>\n<p>I think revisiting registry patterns will probably be a natural step after we have cluster IDs, that is, how do you actually associate these clusters together? Maybe you've got a distributed deployment that you run in your own data centers all over the world. I imagine that expanding the API in that space is going to be important as more multi cluster features develop. It really depends on what the community starts doing with these tools.</p>\n<p><strong>CS</strong>: In the early days of Kubernetes, we used to have a few large Kubernetes clusters and now we're dealing with many small Kubernetes clusters - even multiple clusters for our own dev environments. How has this shift from a few large clusters to many small clusters affected the SIG? Has it accelerated the work or make it challenging in any way?</p>\n<p><strong>JOT</strong>: I think that it has created a lot of ambiguity that needs solving. Originally, you'd have a dev cluster, a staging cluster, and a prod cluster. When the multi region thing came in, we started needing dev/staging/prod clusters, per region. And then, sometimes clusters really need more isolation due to compliance or some regulations issues. Thus, we're ending up with a lot of clusters. I think figuring out the right balance on how many clusters should you actually have is important. The power of Kubernetes is being able to deploy a lot of things managed by a single control plane. So, it's not like every single workload that gets deployed should be in its own cluster. But I think it's pretty clear that we can't put every single workload in a single cluster.</p>\n<p><strong>CS</strong>: What are some of your favorite things about this SIG?</p>\n<p><strong>JOT</strong>: The complexity of the problems, the people and the newness of the space. We don't have right answers and we have to figure this out. At the beginning, we couldn't even think about multi clusters because there was no way to connect services across clusters. Now there is and we're starting to go tackle those problems, I think that this is a really fun place to be in because I expect that the SIG is going to get a lot busier the next couple of years. It's a very collaborative group and we definitely would like more people to come join us, get involved, raise their problems and bring their ideas.</p>\n<p><strong>CS</strong>: What do you think keeps people in this group? How has the pandemic affected you?</p>\n<p><strong>JOT</strong>: I think it definitely got a little bit quieter during the pandemic. But for the most part; it's a very distributed group so whether you're calling in to our weekly meetings from a conference room or from your home, it doesn't make that huge of a difference. During the pandemic, a lot of people had time to focus on what's next for their scale and growth. I think that's what keeps people in the group - we have real problems that need to be solved which are very new in this space. And it's fun :)</p>\n<h2 id=\"wrap-up\">Wrap up</h2>\n<p><strong>CS</strong>: That's all we have for today. Thanks Jeremy for your time.</p>\n<p><strong>JOT</strong>: Thanks Chris. Everybody is welcome at our <a href=\"https://github.com/kubernetes/community/tree/master/sig-multicluster#meetings\">bi-weekly meetings</a>. We love as many people to come as possible and welcome all questions and all ideas. It's a new space and it'd be great to grow the community.</p>","PublishedAt":"2022-02-07 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/02/07/sig-multicluster-spotlight-2022/","SourceName":"Kubernetes"}},{"node":{"ID":1233,"Title":"Blog: Securing Admission Controllers","Description":"<p><strong>Author:</strong> Rory McCune (Aqua Security)</p>\n<p><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">Admission control</a> is a key part of Kubernetes security, alongside authentication and authorization. Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organization‚Äôs security requirements.</p>\n<p>However, as with any additional component added to a cluster, security risks can present themselves. A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately, the <a href=\"https://github.com/kubernetes/community/tree/master/sig-security#security-docs\">security documentation</a> subgroup of SIG Security has spent some time developing a <a href=\"https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control\">threat model for admission controllers</a>. This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.</p>\n<p>From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.</p>\n<h2 id=\"admission-controllers-and-good-practices-for-security\">Admission controllers and good practices for security</h2>\n<p>From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.</p>\n<h3 id=\"secure-webhook-configuration\">Secure webhook configuration</h3>\n<p>It‚Äôs important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers</p>\n<ul>\n<li><strong>Correctly configured TLS for all webhook traffic</strong>. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities</li>\n<li><strong>Only authenticated access allowed</strong>. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.</li>\n<li><strong>Admission controller fails closed</strong>. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the cluster‚Äôs threat model. If an admission controller fails closed, when the API server can‚Äôt get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the cluster‚Äôs operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.</li>\n<li><strong>Regular reviews of webhook configuration</strong>. Configuration mistakes can lead to security issues, so it‚Äôs important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.</li>\n</ul>\n<h3 id=\"secure-cluster-configuration-for-admission-control\">Secure cluster configuration for admission control</h3>\n<p>In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, it‚Äôs important to ensure that Kubernetes' security features that could impact its operation are well configured.</p>\n<ul>\n<li><strong>Restrict <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\">RBAC</a> rights</strong>. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So it‚Äôs important to make sure that only cluster administrators have those rights.</li>\n<li><strong>Prevent privileged workloads</strong>. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster they‚Äôre protecting, it‚Äôs important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.</li>\n<li><strong>Strictly control external system access</strong>. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, <a href=\"https://kubernetes.io/docs/concepts/services-networking/network-policies/\">network policies</a> should be used to restrict the admission controller services access to external networks.</li>\n<li><strong>Each cluster has a dedicated webhook</strong>. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where it‚Äôs shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.</li>\n</ul>\n<h3 id=\"admission-controller-rules\">Admission controller rules</h3>\n<p>A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.</p>\n<ul>\n<li><strong>Regularly test and review rules</strong>. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.</li>\n</ul>","PublishedAt":"2022-01-19 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/","SourceName":"Kubernetes"}},{"node":{"ID":1234,"Title":"Blog: Meet Our Contributors - APAC (India region)","Description":"<p><strong>Authors &amp; Interviewers:</strong> <a href=\"https://github.com/anubha-v-ardhan\">Anubhav Vardhan</a>, <a href=\"https://github.com/Atharva-Shinde\">Atharva Shinde</a>, <a href=\"https://github.com/AvineshTripathi\">Avinesh Tripathi</a>, <a href=\"https://github.com/Debanitrkl\">Debabrata Panigrahi</a>, <a href=\"https://github.com/verma-kunal\">Kunal Verma</a>, <a href=\"https://github.com/PranshuSrivastava\">Pranshu Srivastava</a>, <a href=\"https://github.com/CIPHERTron\">Pritish Samal</a>, <a href=\"https://github.com/PurneswarPrasad\">Purneswar Prasad</a>, <a href=\"https://github.com/vedant-kakde\">Vedant Kakde</a></p>\n<p><strong>Editor:</strong> <a href=\"https://psaggu.com\">Priyanka Saggu</a></p>\n<hr>\n<p>Good day, everyone üëã</p>\n<p>Welcome to the first episode of the APAC edition of the &quot;Meet Our Contributors&quot; blog post series.</p>\n<p>In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.</p>\n<p>üí´ <em>Let's get started, so without further ado‚Ä¶</em></p>\n<h2 id=\"arsh-sharma-https-github-com-rinkiyakedad\"><a href=\"https://github.com/RinkiyaKeDad\">Arsh Sharma</a></h2>\n<p>Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.</p>\n<p>He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the <a href=\"https://github.com/cert-manager/infrastructure\">cert-manager</a> tools development work that is being done under the aegis of SIG Architecture.</p>\n<p>To the newcomers, Arsh helps plan their early contributions sustainably.</p>\n<blockquote>\n<p><em>I would encourage folks to contribute in a way that's sustainable. What I mean by that\nis that it's easy to be very enthusiastic early on and take up more stuff than one can\nactually handle. This can often lead to burnout in later stages. It's much more sustainable\nto work on things iteratively.</em></p>\n</blockquote>\n<h2 id=\"kunal-kushwaha-https-github-com-kunal-kushwaha\"><a href=\"https://github.com/kunal-kushwaha\">Kunal Kushwaha</a></h2>\n<p>Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the <a href=\"https://community.cncf.io/cloud-native-students/\">CNCF Students Program</a>.. He also served as a Communications role shadow during the 1.22 release cycle.</p>\n<p>At the end of his first year, Kunal began contributing to the <a href=\"https://github.com/fabric8io/kubernetes-client\">fabric8io kubernetes-client</a> project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.</p>\n<p>As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.</p>\n<blockquote>\n<p><em>I believe if you find yourself in a place where you do not know much about the\nproject, that's a good thing because now you can learn while contributing and the\ncommunity is there to help you. It has helped me a lot in gaining skills, meeting\npeople from around the world and also helping them. You can learn on the go,\nyou don't have to be an expert. Make sure to also check out no code contributions\nbecause being a beginner is a skill and you can bring new perspectives to the\norganisation.</em></p>\n</blockquote>\n<h2 id=\"madhav-jivarajani-https-github-com-madhavjivrajani\"><a href=\"https://github.com/MadhavJivrajani\">Madhav Jivarajani</a></h2>\n<p>Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).</p>\n<p>Among several significant contributions are his recent efforts toward the Archival of <a href=\"https://github.com/kubernetes/community/issues/6055\">design proposals</a>, refactoring the <a href=\"https://github.com/kubernetes/k8s.io/pull/2713\">&quot;groups&quot; codebase</a> under k8s-infra repository to make it mockable and testable, and improving the functionality of the <a href=\"https://github.com/kubernetes/test-infra/issues/23129\">GitHub k8s bot</a>.</p>\n<p>In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly &quot;KEP reading club&quot; sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing <a href=\"https://github.com/kubernetes-sigs/contributor-katacoda\">Katacoda scenarios</a> to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several <a href=\"https://www.youtube.com/watch?v=FgsXbHBRYIc\">new contributors workshops (NCW)</a>.</p>\n<blockquote>\n<p><em>I initially did not know much about Kubernetes. I joined because the community was\nsuper friendly. But what made me stay was not just the people, but the project itself.\nMy solution to not feeling overwhelmed in the community was to gain as much context\nand knowledge into the topics that I was interested in and were being discussed. And\nas a result I continued to dig deeper into Kubernetes and the design of it.\nI am a systems nut &amp; thus Kubernetes was an absolute goldmine for me.</em></p>\n</blockquote>\n<h2 id=\"rajas-kakodkar-https-github-com-rajaskakodkar\"><a href=\"https://github.com/rajaskakodkar\">Rajas Kakodkar</a></h2>\n<p>Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.</p>\n<p>He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the <a href=\"https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/\">NetworkPolicy++</a> and <a href=\"https://github.com/kubernetes-sigs/kpng\"><code>kpng</code></a> sub-projects.</p>\n<p>One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.</p>\n<blockquote>\n<p><em>I enjoy contributing to Kubernetes not just because I get to work on\ncutting edge tech but more importantly because I get to work with\nawesome people and help in solving real world problems.</em></p>\n</blockquote>\n<h2 id=\"rajula-vineet-reddy-https-github-com-rajula96reddy\"><a href=\"https://github.com/rajula96reddy\">Rajula Vineet Reddy</a></h2>\n<p>Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.</p>\n<p>He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.</p>\n<p>According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.</p>\n<blockquote>\n<p><em>I find the community very helpful and it's always</em>\n‚Äúyou get back as much as you contribute‚Äù.\n<em>The more involved you are, the more you will understand, get to learn and\ncontribute new things.</em></p>\n<p><em>The first step to</em> ‚Äúcome forward and start‚Äù <em>is hard. But it's all gonna be\nsmooth after that. Just take that jump.</em></p>\n</blockquote>\n<hr>\n<p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.</p>\n<p>We'll see you all in the next one. Everyone, till then, have a happy contributing! üëã</p>","PublishedAt":"2022-01-10 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/01/10/meet-our-contributors-india-ep-01/","SourceName":"Kubernetes"}},{"node":{"ID":1235,"Title":"Blog: Kubernetes is Moving on From Dockershim: Commitments and Next Steps","Description":"<p><strong>Authors:</strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)</p>\n<p>Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited\nto reaffirm our community values by supporting open source container runtimes,\nenabling a smaller kubelet, and increasing engineering velocity for teams using\nKubernetes. If you <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/\">use Docker Engine as a container runtime</a>\nfor your Kubernetes cluster, get ready to migrate in 1.24! To check if you're\naffected, refer to <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/\">Check whether dockershim removal affects you</a>.</p>\n<h2 id=\"why-we-re-moving-away-from-dockershim\">Why we‚Äôre moving away from dockershim</h2>\n<p>Docker was the first container runtime used by Kubernetes. This is one of the\nreasons why Docker is so familiar to many Kubernetes users and enthusiasts.\nDocker support was hardcoded into Kubernetes ‚Äì a component the project refers to\nas dockershim.\nAs containerization became an industry standard, the Kubernetes project added support\nfor additional runtimes. This culminated in the implementation of the\ncontainer runtime interface (CRI), letting system components (like the kubelet)\ntalk to container runtimes in a standardized way. As a result, dockershim became\nan anomaly in the Kubernetes project.\nDependencies on Docker and dockershim have crept into various tools\nand projects in the CNCF ecosystem ecosystem, resulting in fragile code.</p>\n<p>By removing the\ndockershim CRI, we're embracing the first value of CNCF: &quot;<a href=\"https://github.com/cncf/foundation/blob/master/charter.md#3-values\">Fast is better than\nslow</a>&quot;.\nStay tuned for future communications on the topic!</p>\n<h2 id=\"deprecation-timeline\">Deprecation timeline</h2>\n<p>We <a href=\"https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/\">formally announced</a> the dockershim deprecation in December 2020. Full removal is targeted\nin Kubernetes 1.24, in April 2022. This timeline\naligns with our <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior\">deprecation policy</a>,\nwhich states that deprecated behaviors must function for at least 1 year\nafter their announced deprecation.</p>\n<p>We'll support Kubernetes version 1.23, which includes\ndockershim, for another year in the Kubernetes project. For managed\nKubernetes providers, vendor support is likely to last even longer, but this is\ndependent on the companies themselves. Regardless, we're confident all cluster operations will have\ntime to migrate. If you have more questions about the dockershim removal, refer\nto the <a href=\"https://kubernetes.io/dockershim\">Dockershim Deprecation FAQ</a>.</p>\n<p>We asked you whether you feel prepared for the migration from dockershim in this\nsurvey: <a href=\"https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/\">Are you ready for Dockershim removal</a>.\nWe had over 600 responses. To everybody who took time filling out the survey,\nthank you.</p>\n<p>The results show that we still have a lot of ground to cover to help you to\nmigrate smoothly. Other container runtimes exist, and have been promoted\nextensively. However, many users told us they still rely on dockershim,\nand sometimes have dependencies that need to be re-worked. Some of these\ndependencies are outside of your control. Based on your feedback, here are some\nof the steps we are taking to help.</p>\n<h2 id=\"our-next-steps\">Our next steps</h2>\n<p>Based on the feedback you provided:</p>\n<ul>\n<li>CNCF and the 1.24 release team are committed to delivering documentation in\ntime for the 1.24 release. This includes more informative blog posts like this\none, updating existing code samples, tutorials, and tasks, and producing a\nmigration guide for cluster operators.</li>\n<li>We are reaching out to the rest of the CNCF community to help prepare them for\nthis change.</li>\n</ul>\n<p>If you're part of a project with dependencies on dockershim, or if you're\ninterested in helping with the migration effort, please join us! There's always\nroom for more contributors, whether to our transition tools or to our\ndocumentation. To get started, say hello in the\n<a href=\"https://kubernetes.slack.com/archives/C0BP8PW9G\">#sig-node</a>\nchannel on <a href=\"https://slack.kubernetes.io/\">Kubernetes Slack</a>!</p>\n<h2 id=\"final-thoughts\">Final thoughts</h2>\n<p>As a project, we've already seen cluster operators increasingly adopt other\ncontainer runtimes through 2021.\nWe believe there are no major blockers to migration. The steps we're taking to\nimprove the migration experience will light the path more clearly for you.</p>\n<p>We understand that migration from dockershim is yet another action you may need to\ndo to keep your Kubernetes infrastructure up to date. For most of you, this step\nwill be straightforward and transparent. In some cases, you will encounter\nhiccups or issues. The community has discussed at length whether postponing the\ndockershim removal would be helpful. For example, we recently talked about it in\nthe <a href=\"https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid\">SIG Node discussion on November 11th</a>\nand in the <a href=\"https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx\">Kubernetes Steering committee meeting held on December 6th</a>.\nWe already <a href=\"https://github.com/kubernetes/enhancements/pull/2481/\">postponed</a> it\nonce in 2021 because the adoption rate of other\nruntimes was lower than we wanted, which also gave us more time to identify\npotential blocking issues.</p>\n<p>At this point, we believe that the value that you (and Kubernetes) gain from\ndockershim removal makes up for the migration effort you'll have. Start planning\nnow to avoid surprises. We'll have more updates and guides before Kubernetes\n1.24 is released.</p>","PublishedAt":"2022-01-07 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/","SourceName":"Kubernetes"}},{"node":{"ID":1236,"Title":"Blog: Kubernetes-in-Kubernetes and the WEDOS PXE bootable server farm","Description":"<p><strong>Author</strong>: Andrei Kvapil (WEDOS)</p>\n<p>When you own two data centers, thousands of physical servers, virtual machines and hosting for hundreds of thousands sites, Kubernetes can actually simplify the management of all these things. As practice has shown, by using Kubernetes, you can declaratively describe and manage not only applications, but also the infrastructure itself. I work for the largest Czech hosting provider <strong>WEDOS Internet a.s</strong> and today I'll show you two of my projects ‚Äî <a href=\"https://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> and <a href=\"https://github.com/kvaps/kubefarm\">Kubefarm</a>.</p>\n<p>With their help you can deploy a fully working Kubernetes cluster inside another Kubernetes using Helm in just a couple of commands. How and why?</p>\n<p>Let me introduce you to how our infrastructure works. All our physical servers can be divided into two groups: <strong>control-plane</strong> and <strong>compute</strong> nodes. Control plane nodes are usually set up manually, have a stable OS installed, and designed to run all cluster services including Kubernetes control-plane. The main task of these nodes is to ensure the smooth operation of the cluster itself. Compute nodes do not have any operating system installed by default, instead they are booting the OS image over the network directly from the control plane nodes. Their work is to carry out the workload.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme01.svg\"\nalt=\"Kubernetes cluster layout\"/>\n</figure>\n<p>Once nodes have downloaded their image, they can continue to work without keeping connection to the PXE server. That is, a PXE server is just keeping rootfs image and does not hold any other complex logic. After our nodes have booted, we can safely restart the PXE server, nothing critical will happen to them.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme02.svg\"\nalt=\"Kubernetes cluster after bootstrapping\"/>\n</figure>\n<p>After booting, the first thing our nodes do is join to the existing Kubernetes cluster, namely, execute the <strong>kubeadm join</strong> command so that kube-scheduler could schedule some pods on them and launch various workloads afterwards. From the beginning we used the scheme when nodes were joined into the same cluster used for the control-plane nodes.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme03.svg\"\nalt=\"Kubernetes scheduling containers to the compute nodes\"/>\n</figure>\n<p>This scheme worked stably for over two years. However later we decided to add containerized Kubernetes to it. And now we can spawn new Kubernetes-clusters very easily right on our control-plane nodes which are now member special admin-clusters. Now, compute nodes can be joined directly to their own clusters - depending on the configuration.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme04.svg\"\nalt=\"Multiple clusters are running in single Kubernetes, compute nodes joined to them\"/>\n</figure>\n<h2 id=\"kubefarm\">Kubefarm</h2>\n<p>This project came with the goal of enabling anyone to deploy such an infrastructure in just a couple of commands using Helm and get about the same in the end.</p>\n<p>At this time, we moved away from the idea of a monocluster. Because it turned out to be not very convenient for managing work of several development teams in the same cluster. The fact is that Kubernetes was never designed as a multi-tenant solution and at the moment it does not provide sufficient means of isolation between projects. Therefore, running separate clusters for each team turned out to be a good idea. However, there should not be too many clusters, to let them be convenient to manage. Nor is it too small to have sufficient independence between development teams.</p>\n<p>The scalability of our clusters became noticeably better after that change. The more clusters you have per number of nodes, the smaller the failure domain and the more stable they work. And as a bonus, we got a fully declaratively described infrastructure. Thus, now you can deploy a new Kubernetes cluster in the same way as deploying any other application in Kubernetes.</p>\n<p>It uses <a href=\"http://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> as a basis, <a href=\"https://github.com/ltsp/ltsp/\">LTSP</a> as PXE-server from which the nodes are booted, and automates the DHCP server configuration using <a href=\"https://github.com/kvaps/dnsmasq-controller\">dnsmasq-controller</a>:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/kubefarm.png\"\nalt=\"Kubefarm\"/>\n</figure>\n<h2 id=\"how-it-works\">How it works</h2>\n<p>Now let's see how it works. In general, if you look at Kubernetes as from an application perspective, you can note that it follows all the principles of <a href=\"https://12factor.net/\">The Twelve-Factor App</a>, and is actually written very well. Thus, it means running Kubernetes as an app in a different Kubernetes shouldn't be a big deal.</p>\n<h3 id=\"running-kubernetes-in-kubernetes\">Running Kubernetes in Kubernetes</h3>\n<p>Now let's take a look at the <a href=\"https://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> project, which provides a ready-made Helm chart for running Kubernetes in Kubernetes.</p>\n<p>Here is the parameters that you can pass to Helm in the values file:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubernetes-in-kubernetes/tree/v0.13.1/deploy/helm/kubernetes\"><strong>kubernetes/values.yaml</strong></a></li>\n</ul>\n<img alt=\"Kubernetes is just five binaries\" style=\"float: right; max-height: 280px;\" src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/5binaries.png\">\n<p>Beside <strong>persistence</strong> (storage parameters for the cluster), the Kubernetes control-plane components are described here: namely: <strong>etcd cluster</strong>, <strong>apiserver</strong>, <strong>controller-manager</strong> and <strong>scheduler</strong>. These are pretty much standard Kubernetes components. There is a light-hearted saying that ‚ÄúKubernetes is just five binaries‚Äù. So here is where the configuration for these binaries is located.</p>\n<p>If you ever tried to bootstrap a cluster using kubeadm, then this config will remind you it's configuration. But in addition to Kubernetes entities, you also have an admin container. In fact, it is a container which holds two binaries inside: <strong>kubectl</strong> and <strong>kubeadm</strong>. They are used to generate kubeconfig for the above components and to perform the initial configuration for the cluster. Also, in an emergency, you can always exec into it to check and manage your cluster.</p>\n<p>After the release <a href=\"https://asciinema.org/a/407280\">has been deployed</a>, you can see a list of pods: <strong>admin-container</strong>, <strong>apiserver</strong> in two replicas, <strong>controller-manager</strong>, <strong>etcd-cluster</strong>, <strong>scheduller</strong> and the initial job that initializes the cluster. In the end you have a command, which allows you to get shell into the admin container, you can use it to see what is happening inside:</p>\n<p><a href=\"https://asciinema.org/a/407280?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot01.svg\" alt=\"\"></a></p>\n<p>Also, let's take look at the certificates. If you've ever installed Kubernetes, then you know that it has a <em>scary</em> directory <code>/etc/kubernetes/pki</code> with a bunch of some certificates. In case of Kubernetes-in-Kubernetes, you have fully automated management of them with cert-manager. Thus, it is enough to pass all certificates parameters to Helm during installation, and all the certificates will automatically be generated for your cluster.</p>\n<p><a href=\"https://asciinema.org/a/407280?t=15&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot02.svg\" alt=\"\"></a></p>\n<p>Looking at one of the certificates, eg. apiserver, you can see that it has a list of DNS names and IP addresses. If you want to make this cluster accessible outside, then just describe the additional DNS names in the values file and update the release. This will update the certificate resource, and cert-manager will regenerate the certificate. You'll no longer need to think about this. If kubeadm certificates need to be renewed at least once a year, here the cert-manager will take care and automatically renew them.</p>\n<p><a href=\"https://asciinema.org/a/407280?t=25&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot03.svg\" alt=\"\"></a></p>\n<p>Now let's log into the admin container and look at the cluster and nodes. Of course, there are no nodes, yet, because at the moment you have deployed just the blank control-plane for Kubernetes. But in kube-system namespace you can see some coredns pods waiting for scheduling and configmaps already appeared. That is, you can conclude that the cluster is working:</p>\n<p><a href=\"https://asciinema.org/a/407280?t=30&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot04.svg\" alt=\"\"></a></p>\n<p>Here is the <a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html\">diagram of the deployed cluster</a>. You can see services for all Kubernetes components: <strong>apiserver</strong>, <strong>controller-manager</strong>, <strong>etcd-cluster</strong> and <strong>scheduler</strong>. And the pods on right side to which they forward traffic.</p>\n<p><a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd01.png\" alt=\"\"></a></p>\n<p><em>By the way, the diagram, is drawn in <a href=\"https://argoproj.github.io/argo-cd/\">ArgoCD</a> ‚Äî the GitOps tool we use to manage our clusters, and cool diagrams are one of its features.</em></p>\n<h3 id=\"orchestrating-physical-servers\">Orchestrating physical servers</h3>\n<p>OK, now you can see the way how is our Kubernetes control-plane deployed, but what about worker nodes, how are we adding them? As I already said, all our servers are bare metal. We do not use virtualization to run Kubernetes, but we orchestrate all physical servers by ourselves.</p>\n<p>Also, we do use Linux network boot feature very actively. Moreover, this is exactly the booting, not some kind of automation of the installation. When the nodes are booting, they just run a ready-made system image for them. That is, to update any node, we just need to reboot it - and it will download a new image. It is very easy, simple and convenient.</p>\n<p>For this, the <a href=\"https://github.com/kvaps/kubefarm\">Kubefarm</a> project was created, which allows you to automate this. The most commonly used examples can be found in the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples\">examples</a> directory. The most standard of them named <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic\">generic</a>. Let's take a look at values.yaml:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/generic/values.yaml\"><strong>generic/values.yaml</strong></a></li>\n</ul>\n<p>Here you can specify the parameters which are passed into the upstream Kubernetes-in-Kubernetes chart. In order for you control-plane to be accessible from the outside, it is enough to specify the IP address here, but if you wish, you can specify some DNS name here.</p>\n<p>In the PXE server configuration you can specify a timezone. You can also add an SSH key for logging in without a password (but you can also specify a password), as well as kernel modules and parameters that should be applied during booting the system.</p>\n<p>Next comes the <strong>nodePools</strong> configuration, i.e. the nodes themselves. If you've ever used a terraform module for gke, then this logic will remind you of it. Here you statically describe all nodes with a set of parameters:</p>\n<ul>\n<li>\n<p><strong>Name</strong> (hostname);</p>\n</li>\n<li>\n<p><strong>MAC-addresses</strong> ‚Äî we have nodes with two network cards, and each one can boot from any of the MAC addresses specified here.</p>\n</li>\n<li>\n<p><strong>IP-address</strong>, which the DHCP server should issue to this node.</p>\n</li>\n</ul>\n<p>In this example, you have two pools: the first has five nodes, the second has only one, the second pool has also two tags assigned. Tags are the way to describe configuration for specific nodes. For example, you can add specific DHCP options for some pools, options for the PXE server for booting (e.g. here is debug option enabled) and set of <strong>kubernetesLabels</strong> and <strong>kubernetesTaints</strong> options. What does that mean?</p>\n<p>For example, in this configuration you have a second nodePool with one node. The pool has <strong>debug</strong> and <strong>foo</strong> tags assigned. Now see the options for <strong>foo</strong> tag in <strong>kubernetesLabels</strong>. This means that the m1c43 node will boot with these two labels and taint assigned. Everything seems to be simple. Now <a href=\"https://asciinema.org/a/407282\">let's try</a> this in practice.</p>\n<h3 id=\"demo\">Demo</h3>\n<p>Go to <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples\">examples</a> and update previously deployed chart to Kubefarm. Just use the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic\">generic</a> parameters and look at the pods. You can see that a PXE server and one more job were added. This job essentially goes to the deployed Kubernetes cluster and creates a new token. Now it will run repeatedly every 12 hours to generate a new token, so that the nodes can connect to your cluster.</p>\n<p><a href=\"https://asciinema.org/a/407282?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot05.svg\" alt=\"\"></a></p>\n<p>In a <a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html\">graphical representation</a>, it looks about the same, but now apiserver started to be exposed outside.</p>\n<p><a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd02.png\" alt=\"\"></a></p>\n<p>In the diagram, the IP is highlighted in green, the PXE server can be reached through it. At the moment, Kubernetes does not allow creating a single LoadBalancer service for TCP and UDP protocols by default, so you have to create two different services with the same IP address. One is for TFTP, and the second for HTTP, through which the system image is downloaded.</p>\n<p>But this simple example is not always enough, sometimes you might need to modify the logic at boot. For example, here is a directory <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network\">advanced_network</a>, inside which there is a <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network\">values file</a> with a simple shell script. Let's call it <code>network.sh</code>:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/advanced_network/values.yaml#L14-L78\"><strong>network.sh</strong></a></li>\n</ul>\n<p>All this script does is take environment variables at boot time, and generates a network configuration based on them. It creates a directory and puts the netplan config inside. For example, a bonding interface is created here. Basically, this script can contain everything you need. It can hold the network configuration or generate the system services, add some hooks or describe any other logic. Anything that can be described in bash or shell languages will work here, and it will be executed at boot time.</p>\n<p>Let's see how it can be <a href=\"https://asciinema.org/a/407284\">deployed</a>. Let's pass the generic values file as the first parameter, and an additional values file as the second parameter. This is a standard Helm feature. This way you can also pass the secrets, but in this case, the configuration is just expanded by the second file:</p>\n<p><a href=\"https://asciinema.org/a/407284?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot06.svg\" alt=\"\"></a></p>\n<p>Let's look at the configmap <strong>foo-kubernetes-ltsp</strong> for the netboot server and make sure that <code>network.sh</code> script is really there. These commands used to configure the network at boot time:</p>\n<p><a href=\"https://asciinema.org/a/407284?t=15&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot07.svg\" alt=\"\"></a></p>\n<p><a href=\"https://asciinema.org/a/407286\">Here</a> you can see how it works in principle. The chassis interface (we use HPE Moonshots 1500) have the nodes, you can enter <code>show node list</code> command to get a list of all the nodes. Now you can see the booting process.</p>\n<p><a href=\"https://asciinema.org/a/407286?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot08.svg\" alt=\"\"></a></p>\n<p>You can also get their MAC addresses by <code>show node macaddr all</code> command. We have a clever operator that collects MAC-addresses from chassis automatically and passes them to the DHCP server. Actually, it's just creating custom configuration resources for dnsmasq-controller which is running in same admin Kubernetes cluster. Also, trough this interface you can control the nodes themselves, e.g. turn them on and off.</p>\n<p>If you have no such opportunity to enter the chassis through iLO and collect a list of MAC addresses for your nodes, you can consider using <a href=\"https://asciinema.org/a/407287\">catchall cluster</a> pattern. Purely speaking, it is just a cluster with a dynamic DHCP pool. Thus, all nodes that are not described in the configuration to other clusters will automatically join to this cluster.</p>\n<p><a href=\"https://asciinema.org/a/407287?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot09.svg\" alt=\"\"></a></p>\n<p>For example, you can see a special cluster with some nodes. They are joined to the cluster with an auto-generated name based on their MAC address. Starting from this point you can connect to them and see what happens there. Here you can somehow prepare them, for example, set up the file system and then rejoin them to another cluster.</p>\n<p>Now let's try connecting to the node terminal and see how it is booting. After the BIOS, the network card is configured, here it sends a request to the DHCP server from a specific MAC address, which redirects it to a specific PXE server. Later the kernel and initrd image are downloaded from the server using the standard HTTP protocol:</p>\n<p><a href=\"https://asciinema.org/a/407286?t=28&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot10.svg\" alt=\"\"></a></p>\n<p>After loading the kernel, the node downloads the rootfs image and transfers control to systemd. Then the booting proceeds as usual, and after that the node joins Kubernetes:</p>\n<p><a href=\"https://asciinema.org/a/407286?t=80&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot11.svg\" alt=\"\"></a></p>\n<p>If you take a look at <strong>fstab</strong>, you can see only two entries there: <strong>/var/lib/docker</strong> and <strong>/var/lib/kubelet</strong>, they are mounted as <strong>tmpfs</strong> (in fact, from RAM). At the same time, the root partition is mounted as <strong>overlayfs</strong>, so all changes that you make here on the system will be lost on the next reboot.</p>\n<p>Looking into the block devices on the node, you can see some nvme disk, but it has not yet been mounted anywhere. There is also a loop device - this is the exact rootfs image downloaded from the server. At the moment it is located in RAM, occupies 653 MB and mounted with the <strong>loop</strong> option.</p>\n<p>If you look in <strong>/etc/ltsp</strong>, you find the <code>network.sh</code> file that was executed at boot. From containers, you can see running <code>kube-proxy</code> and <code>pause</code> container for it.</p>\n<p><a href=\"https://asciinema.org/a/407286?t=100&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot12.svg\" alt=\"\"></a></p>\n<h2 id=\"details\">Details</h2>\n<h3 id=\"network-boot-image\">Network Boot Image</h3>\n<p>But where does the main image come from? There is a little trick here. The image for the nodes is built through the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/build/ltsp\">Dockerfile</a> along with the server. The <a href=\"https://docs.docker.com/develop/develop-images/multistage-build/\">Docker multi-stage build</a> feature allows you to easily add any packages and kernel modules exactly at the stage of the image build. It looks like this:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/build/ltsp/Dockerfile\"><strong>Dockerfile</strong></a></li>\n</ul>\n<p>What's going on here? First, we take a regular Ubuntu 20.04 and install all the packages we need. First of all we install the <strong>kernel</strong>, <strong>lvm</strong>, <strong>systemd</strong>, <strong>ssh</strong>. In general, everything that you want to see on the final node should be described here. Here we also install <code>docker</code> with <code>kubelet</code> and <code>kubeadm</code>, which are used to join the node to the cluster.</p>\n<p>And then we perform an additional configuration. In the last stage, we simply install <code>tftp</code> and <code>nginx</code> (which serves our image to clients), <strong>grub</strong> (bootloader). Then root of the previous stages copied into the final image and generate squashed image from it. That is, in fact, we get a docker image, which has both the server and the boot image for our nodes. At the same time, it can be easily updated by changing the Dockerfile.</p>\n<h3 id=\"webhooks-and-api-aggregation-layer\">Webhooks and API aggregation layer</h3>\n<p>I want to pay special attention to the problem of webhooks and aggregation layer. In general, webhooks is a Kubernetes feature that allows you to respond to the creation or modification of any resources. Thus, you can add a handler so that when resources are applied, Kubernetes must send request to some pod and check if configuration of this resource is correct, or make additional changes to it.</p>\n<p>But the point is, in order for the webhooks to work, the apiserver must have direct access to the cluster for which it is running. And if it is started in a separate cluster, like our case, or even separately from any cluster, then Konnectivity service can help us here. Konnectivity is one of the optional but officially supported Kubernetes components.</p>\n<p>Let's take cluster of four nodes for example, each of them is running a <code>kubelet</code> and we have other Kubernetes components running outside: <code>kube-apiserver</code>, <code>kube-scheduler</code> and <code>kube-controller-manager</code>. By default, all these components interact with the apiserver directly - this is the most known part of the Kubernetes logic. But in fact, there is also a reverse connection. For example, when you want to view the logs or run a <code>kubectl exec command</code>, the API server establishes a connection to the specific kubelet independently:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity01.svg\"\nalt=\"Kubernetes apiserver reaching kubelet\"/>\n</figure>\n<p>But the problem is that if we have a webhook, then it usually runs as a standard pod with a service in our cluster. And when apiserver tries to reach it, it will fail because it will try to access an in-cluster service named <strong>webhook.namespace.svc</strong> being outside of the cluster where it is actually running:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity02.svg\"\nalt=\"Kubernetes apiserver can&#39;t reach webhook\"/>\n</figure>\n<p>And here Konnectivity comes to our rescue. Konnectivity is a tricky proxy server developed especially for Kubernetes. It can be deployed as a server next to the apiserver. And Konnectivity-agent is deployed in several replicas directly in the cluster you want to access. The agent establishes a connection to the server and sets up a stable channel to make apiserver able to access all webhooks and all kubelets in the cluster. Thus, now all communication with the cluster will take place through the Konnectivity-server:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity03.svg\"\nalt=\"Kubernetes apiserver reaching webhook via konnectivity\"/>\n</figure>\n<h2 id=\"our-plans\">Our plans</h2>\n<p>Of course, we are not going to stop at this stage. People interested in the project often write to me. And if there will be a sufficient number of interested people, I hope to move Kubernetes-in-Kubernetes project under <a href=\"https://github.com/kubernetes-sigs\">Kubernetes SIGs</a>, by representing it in form of the official Kubernetes Helm chart. Perhaps, by making this project independent we'll gather an even larger community.</p>\n<p>I am also thinking of integrating it with the Machine Controller Manager, which would allow creating worker nodes, not only of physical servers, but also, for example, for creating virtual machines using kubevirt and running them in the same Kubernetes cluster. By the way, it also allows to spawn virtual machines in the clouds, and have a control-plane deployed locally.</p>\n<p>I am also considering the option of integrating with the Cluster-API so that you can create physical Kubefarm clusters directly through the Kubernetes environment. But at the moment I'm not completely sure about this idea. If you have any thoughts on this matter, I'll be happy to listen to them.</p>","PublishedAt":"2021-12-22 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/22/kubernetes-in-kubernetes-and-pxe-bootable-server-farm/","SourceName":"Kubernetes"}},{"node":{"ID":1237,"Title":"Blog: Using Admission Controllers to Detect Container Drift at Runtime","Description":"<p><strong>Author:</strong> Saifuding Diliyaer (Box)\n<figure>\n<img src=\"https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/intro-illustration.png\"\nalt=\"Introductory illustration\"/> <figcaption>\n<p>Illustration by Munire Aireti</p>\n</figcaption>\n</figure>\n</p>\n<p>At Box, we use Kubernetes (K8s) to manage hundreds of micro-services that enable Box to stream data at a petabyte scale. When it comes to the deployment process, we run <a href=\"https://github.com/box/kube-applier\">kube-applier</a> as part of the GitOps workflows with declarative configuration and automated deployment. Developers declare their K8s apps manifest into a Git repository that requires code reviews and automatic checks to pass, before any changes can get merged and applied inside our K8s clusters. With <code>kubectl exec</code> and other similar commands, however, developers are able to directly interact with running containers and alter them from their deployed state. This interaction could then subvert the change control and code review processes that are enforced in our CI/CD pipelines. Further, it allows such impacted containers to continue receiving traffic long-term in production.</p>\n<p>To solve this problem, we developed our own K8s component called <a href=\"https://github.com/box/kube-exec-controller\">kube-exec-controller</a> along with its corresponding <a href=\"https://github.com/box/kube-exec-controller#kubectl-pi\">kubectl plugin</a>. They function together in detecting and terminating potentially mutated containers (caused by interactive kubectl commands), as well as revealing the interaction events directly to the target Pods for better visibility.</p>\n<h2 id=\"admission-control-for-interactive-kubectl-commands\">Admission control for interactive kubectl commands</h2>\n<p>Once a request is sent to K8s, it needs to be authenticated and authorized by the API server to proceed. Additionally, K8s has a separate layer of protection called <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">admission controllers</a>, which can intercept the request before an object is persisted in <em>etcd</em>. There are various predefined admission controls compiled into the API server binary (e.g. ResourceQuota to enforce hard resource usage limits per namespace). Besides, there are two dynamic admission controls named <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook\">MutatingAdmissionWebhook</a> and <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook\">ValidatingAdmissionWebhook</a>, used for mutating or validating K8s requests respectively. The latter is what we adopted to detect container drift at runtime caused by interactive kubectl commands. This whole process can be divided into three steps as explained in detail below.</p>\n<h3 id=\"1-admit-interactive-kubectl-command-requests\">1. Admit interactive kubectl command requests</h3>\n<p>First of all, we needed to enable a validating webhook that sends qualified requests to <em>kube-exec-controller</em>. To add the new validation mechanism applying to interactive kubectl commands specifically, we configured the webhook‚Äôs rules with resources as <code>[pods/exec, pods/attach]</code>, and operations as <code>CONNECT</code>. These rules tell the cluster's API server that all <code>exec</code> and <code>attach</code> requests should be subject to our admission control webhook. In the ValidatingAdmissionWebhook that we configured, we specified a <code>service</code> reference (could also be replaced with <code>url</code> that gives the location of the webhook) and <code>caBundle</code> to allow validating its X.509 certificate, both under the <code>clientConfig</code> stanza.</p>\n<p>Here is a short example of what our ValidatingWebhookConfiguration object looks like:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingWebhookConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-validating-webhook-config<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">webhooks</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>validate-pod-interaction.example.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">sideEffects</span>:<span style=\"color:#bbb\"> </span>None<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;*&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;*&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CONNECT&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;pods/exec&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;pods/attach&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">failurePolicy</span>:<span style=\"color:#bbb\"> </span>Fail<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">clientConfig</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">service</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># reference to kube-exec-controller service deployed inside the K8s cluster</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>kube-exec-controller<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">path</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;/admit-pod-interaction&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">caBundle</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;{{VALUE}}&#34;</span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># PEM encoded CA bundle to validate kube-exec-controller&#39;s certificate</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">admissionReviewVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;v1beta1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"2-label-the-target-pod-with-potentially-mutated-containers\">2. Label the target Pod with potentially mutated containers</h3>\n<p>Once a request of <code>kubectl exec</code> comes in, <em>kube-exec-controller</em> makes an internal note to label the associated Pod. The added labels mean that we can not only query all the affected Pods, but also enable the security mechanism to retrieve previously identified Pods, in case the controller service itself gets restarted.</p>\n<p>The admission control process cannot directly modify the targeted in its admission response. This is because the <code>pods/exec</code> request is against a subresource of the Pod API, and the API kind for that subresource is <code>PodExecOptions</code>. As a result, there is a separate process in <em>kube-exec-controller</em> that patches the labels asynchronously. The admission control always permits the <code>exec</code> request, then acts as a client of the K8s API to label the target Pod and to log related events. Developers can check whether their Pods are affected or not using <code>kubectl</code> or similar tools. For example:</p>\n<pre tabindex=\"0\"><code>$ kubectl get pod --show-labels\nNAME READY STATUS RESTARTS AGE LABELS\ntest-pod 1/1 Running 0 2s box.com/podInitialInteractionTimestamp=1632524400,box.com/podInteractorUsername=username-1,box.com/podTTLDuration=1h0m0s\n$ kubectl describe pod test-pod\n...\nEvents:\nType Reason Age¬†¬†¬† From¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† Message\n----¬†¬†¬†¬†¬†¬†¬†------¬†¬†¬†¬†¬†¬† ----¬†¬† ----¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†-------\nWarning PodInteraction 5s admission-controller-service Pod was interacted with &#39;kubectl exec&#39; command by user &#39;username-1&#39; initially at time 2021-09-24 16:00:00 -0800 PST\nWarning PodInteraction 5s admission-controller-service Pod will be evicted at time 2021-09-24 17:00:00 -0800 PST (in about 1h0m0s).\n</code></pre><h3 id=\"3-evict-the-target-pod-after-a-predefined-period\">3. Evict the target Pod after a predefined period</h3>\n<p>As you can see in the above event messages, the affected Pod is not evicted immediately. At times, developers might have to get into their running containers necessarily for debugging some live issues. Therefore, we define a time to live (TTL) of affected Pods based on the environment of clusters they are running. In particular, we allow a longer time in our dev clusters as it is more common to run <code>kubectl exec</code> or other interactive commands for active development.</p>\n<p>For our production clusters, we specify a lower time limit so as to avoid the impacted Pods serving traffic abidingly. The <em>kube-exec-controller</em> internally sets and tracks a timer for each Pod that matches the associated TTL. Once the timer is up, the controller evicts that Pod using K8s API. The eviction (rather than deletion) is to ensure service availability, since the cluster respects any configured <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\">PodDisruptionBudget</a> (PDB). Let's say if a user has defined <em>x</em> number of Pods as critical in their PDB, the eviction (as requested by <em>kube-exec-controller</em>) does not continue when the target workload has fewer than <em>x</em> Pods running.</p>\n<p>Here comes a sequence diagram of the entire workflow mentioned above:</p>\n<!-- Mermaid Live Editor link - https://mermaid-js.github.io/mermaid-live-editor/edit/#pako:eNp9kjFPAzEMhf-KlalIbWd0QpUQdGJB3JrFTUyJmjhHzncFof53nGtpqYTYEuu958-Wv4zLnkxjenofiB09BtwWTJbRSS6QCLCHu01ZPdJIMXdUYNZTGYOjRd4zlRvLHRYJLnTIArvbtozV83TbAnZhUcVUrkXo04OU2I6uKu99Cn0fMsNDZik5Rm3SHntYTrRYrabUBl4GBmt2w4acRKAPcrBcLq0Bl1NC9pYnoRouHZopX9RX9aotddJeADaf4DDGwFuQN4IRY_Ao9bunzVvOO13COeYCcR9j3k-OCQDP9KfgC8TJsFbZIHSxnGljzp1lgKs2v9HXugMBwe2WPHTZ94CvottB6Ap5eg2s9cBaUnrLVEP_Yp5ynrOf3fxPV2V1lBOhmZtEJWHweiFfldQa1SWyptGnAuAQxRrLB5UOna6P1j7o4ZhGykBzg4Pk9pPdz_-oOR3ZsXj4BjrP5rU-->\n<p><img src=\"https://kubernetes.io/images/sequence_diagram.svg\" alt=\"Sequence Diagram\"></p>\n<h2 id=\"a-new-kubectl-plugin-for-better-user-experience\">A new kubectl plugin for better user experience</h2>\n<p>Our admission controller component works great for solving the container drift issue we had on the platform. It is also able to submit all related Events to the target Pod that has been affected. However, K8s clusters don't retain Events very long (the default retention period is one hour). We need to provide other ways for developers to get their Pod interaction activity. A <a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\">kubectl plugin</a> is a perfect choice for us to expose this information. We named our plugin <code>kubectl pi</code> (short for <code>pod-interaction</code>) and provide two subcommands: <code>get</code> and <code>extend</code>.</p>\n<p>When the <code>get</code> subcommand is called, the plugin checks the metadata attached by our admission controller and transfers it to human-readable information. Here is an example output from running <code>kubectl pi get</code>:</p>\n<pre tabindex=\"0\"><code>$ kubectl pi get test-pod\nPOD-NAME INTERACTOR POD-TTL EXTENSION EXTENSION-REQUESTER EVICTION-TIME\ntest-pod¬†¬†username-1¬†¬†1h0m0s¬†¬†¬†/¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†/¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†2021-09-24 17:00:00 -0800 PST\n</code></pre><p>The plugin can also be used to extend the TTL for a Pod that is marked for future eviction. This is useful in case developers need extra time to debug ongoing issues. To achieve this, a developer uses the <code>kubectl pi extend</code> subcommand, where the plugin patches the relevant <em>annotations</em> for the given Pod. These <em>annotations</em> include the duration and username who made the extension request for transparency (displayed in the table returned from the <code>kubectl pi get</code> command).</p>\n<p>Correspondingly, there is another webhook defined in <em>kube-exec-controller</em> which admits valid annotation updates. Once admitted, those updates reset the eviction timer of the target Pod as requested. An example of requesting the extension from the developer side would be:</p>\n<pre tabindex=\"0\"><code>$ kubectl pi extend test-pod --duration=30m\nSuccessfully extended the termination time of pod/test-pod with a duration=30m\n¬†\n$ kubectl pi get test-pod\nPOD-NAME¬†¬†INTERACTOR¬†¬†POD-TTL¬†¬†EXTENSION¬†¬†EXTENSION-REQUESTER¬†¬†EVICTION-TIME\ntest-pod¬†¬†username-1¬†¬†1h0m0s¬†¬†¬†30m¬†¬†¬†¬†¬†¬†¬†¬†username-2¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† 2021-09-24 17:30:00 -0800 PST\n</code></pre><h2 id=\"future-improvement\">Future improvement</h2>\n<p>Although our admission controller service works great in handling interactive requests to a Pod, it could as well evict the Pod while the actual commands are no-op in these requests. For instance, developers sometimes run <code>kubectl exec</code> merely to check their service logs stored on hosts. Nevertheless, the target Pods would still get bounced despite the state of their containers not changing at all. One of the improvements here could be adding the ability to distinguish the commands that are passed to the interactive requests, so that no-op commands should not always force a Pod eviction. However, this becomes challenging when developers get a shell to a running container and execute commands inside the shell, since they will no longer be visible to our admission controller service.</p>\n<p>Another item worth pointing out here is the choice of using K8s <em>labels</em> and <em>annotations</em>. In our design, we decided to have all immutable metadata attached as <em>labels</em> for better enforcing the immutability in our admission control. Yet some of these metadata could fit better as <em>annotations</em>. For instance, we had a label with the key <code>box.com/podInitialInteractionTimestamp</code> used to list all affected Pods in <em>kube-exec-controller</em> code, although its value would be unlikely to query for. As a more ideal design in the K8s world, a single <em>label</em> could be preferable in our case for identification with other metadata applied as <em>annotations</em> instead.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>With the power of admission controllers, we are able to secure our K8s clusters by detecting potentially mutated containers at runtime, and evicting their Pods without affecting service availability. We also utilize kubectl plugins to provide flexibility of the eviction time and hence, bringing a better and more self-independent experience to service owners. We are proud to announce that we have open-sourced the whole project for the community to leverage in their own K8s clusters. Any contribution is more than welcomed and appreciated. You can find this project hosted on GitHub at <a href=\"https://github.com/box/kube-exec-controller\">https://github.com/box/kube-exec-controller</a></p>\n<p><em>Special thanks to Ayush Sobti and Ethan Goldblum for their technical guidance on this project.</em></p>","PublishedAt":"2021-12-21 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/","SourceName":"Kubernetes"}},{"node":{"ID":1238,"Title":"Blog: What's new in Security Profiles Operator v0.4.0","Description":"<p><strong>Authors:</strong> Jakub Hrozek, Juan Antonio Osorio, Paulo Gomes, Sascha Grunert</p>\n<hr>\n<p>The <a href=\"https://sigs.k8s.io/security-profiles-operator\">Security Profiles Operator (SPO)</a>\nis an out-of-tree Kubernetes enhancement to make the management of\n<a href=\"https://en.wikipedia.org/wiki/Seccomp\">seccomp</a>,\n<a href=\"https://en.wikipedia.org/wiki/Security-Enhanced_Linux\">SELinux</a> and\n<a href=\"https://en.wikipedia.org/wiki/AppArmor\">AppArmor</a> profiles easier and more\nconvenient. We're happy to announce that we recently <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.4.0\">released\nv0.4.0</a>\nof the operator, which contains a ton of new features, fixes and usability\nimprovements.</p>\n<h2 id=\"what-s-new\">What's new</h2>\n<p>It has been a while since the last\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.3.0\">v0.3.0</a>\nrelease of the operator. We added new features, fine-tuned existing ones and\nreworked our documentation in 290 commits over the past half year.</p>\n<p>One of the highlights is that we're now able to record seccomp and SELinux\nprofiles using the operators <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#log-enricher-based-recording\">log enricher</a>.\nThis allows us to reduce the dependencies required for profile recording to have\n<a href=\"https://linux.die.net/man/8/auditd\">auditd</a> or\n<a href=\"https://en.wikipedia.org/wiki/Syslog\">syslog</a> (as fallback) running on the\nnodes. All profile recordings in the operator work in the same way by using the\n<code>ProfileRecording</code> CRD as well as their corresponding <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\">label\nselectors</a>. The log\nenricher itself can be also used to gather meaningful insights about seccomp and\nSELinux messages of a node. Checkout the <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-the-log-enricher\">official\ndocumentation</a>\nto learn more about it.</p>\n<h3 id=\"seccomp-related-improvements\">seccomp related improvements</h3>\n<p>Beside the log enricher based recording we now offer an alternative to record\nseccomp profiles by utilizing <a href=\"https://ebpf.io\">ebpf</a>. This optional feature can\nbe enabled by setting <code>enableBpfRecorder</code> to <code>true</code>. This results in running a\ndedicated container, which ships a custom bpf module on every node to collect\nthe syscalls for containers. It even supports older Kernel versions which do not\nexpose the <a href=\"https://www.kernel.org/doc/html/latest/bpf/btf.html\">BPF Type Format (BTF)</a> per\ndefault as well as the <code>amd64</code> and <code>arm64</code> architectures. Checkout\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#ebpf-based-recording\">our documentation</a>\nto see it in action. By the way, we now add the seccomp profile architecture of\nthe recorder host to the recorded profile as well.</p>\n<p>We also graduated the seccomp profile API from <code>v1alpha1</code> to <code>v1beta1</code>. This\naligns with our overall goal to stabilize the CRD APIs over time. The only thing\nwhich has changed is that the seccomp profile type <code>Architectures</code> now points to\n<code>[]Arch</code> instead of <code>[]*Arch</code>.</p>\n<h3 id=\"selinux-enhancements\">SELinux enhancements</h3>\n<p>Managing SELinux policies (an equivalent to using <code>semodule</code> that\nyou would normally call on a single server) is not done by SPO\nitself, but by another container called selinuxd to provide better\nisolation. This release switched to using selinuxd containers from\na personal repository to images located under <a href=\"https://quay.io/organization/security-profiles-operator\">our team's quay.io\nrepository</a>.\nThe selinuxd repository has moved as well to <a href=\"https://github.com/containers/selinuxd\">the containers GitHub\norganization</a>.</p>\n<p>Please note that selinuxd links dynamically to <code>libsemanage</code> and mounts the\nSELinux directories from the nodes, which means that the selinuxd container\nmust be running the same distribution as the cluster nodes. SPO defaults\nto using CentOS-8 based containers, but we also build Fedora based ones.\nIf you are using another distribution and would like us to add support for\nit, please file <a href=\"https://github.com/containers/selinuxd/issues\">an issue against selinuxd</a>.</p>\n<h4 id=\"profile-recording\">Profile Recording</h4>\n<p>This release adds support for recording of SELinux profiles.\nThe recording itself is managed via an instance of a <code>ProfileRecording</code> Custom\nResource as seen in an\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/profilerecording-selinux-logs.yaml\">example</a>\nin our repository. From the user's point of view it works pretty much the same\nas recording of seccomp profiles.</p>\n<p>Under the hood, to know what the workload is doing SPO installs a special\npermissive policy called <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/profiles/selinuxrecording.cil\">selinuxrecording</a>\non startup which allows everything and logs all AVCs to <code>audit.log</code>.\nThese AVC messages are scraped by the log enricher component and when\nthe recorded workload exits, the policy is created.</p>\n<h4 id=\"selinuxprofile-crd-graduation\"><code>SELinuxProfile</code> CRD graduation</h4>\n<p>An <code>v1alpha2</code> version of the <code>SelinuxProfile</code> object has been introduced. This\nremoves the raw Common Intermediate Language (CIL) from the object itself and\ninstead adds a simple policy language to ease the writing and parsing\nexperience.</p>\n<p>Alongside, a <code>RawSelinuxProfile</code> object was also introduced. This contains a\nwrapped and raw representation of the policy. This was intended for folks to be\nable to take their existing policies into use as soon as possible. However, on\nvalidations are done here.</p>\n<h3 id=\"apparmor-support\">AppArmor support</h3>\n<p>This version introduces the initial support for AppArmor, allowing users to load and\nunload AppArmor profiles into cluster nodes by using the new <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/crds/apparmorprofile.yaml\">AppArmorProfile</a> CRD.</p>\n<p>To enable AppArmor support use the <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/config.yaml#L10\">enableAppArmor feature gate</a> switch of your SPO configuration.\nThen use our <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/apparmorprofile.yaml\">apparmor example</a> to deploy your first profile across your cluster.</p>\n<h3 id=\"metrics\">Metrics</h3>\n<p>The operator now exposes metrics, which are described in detail in\nour new <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-metrics\">metrics documentation</a>.\nWe decided to secure the metrics retrieval process by using\n<a href=\"https://github.com/brancz/kube-rbac-proxy\">kube-rbac-proxy</a>, while we ship an\nadditional <code>spo-metrics-client</code> cluster role (and binding) to retrieve the\nmetrics from within the cluster. If you're using\n<a href=\"https://www.redhat.com/en/technologies/cloud-computing/openshift\">OpenShift</a>,\nthen we provide an out of the box working\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#automatic-servicemonitor-deployment\"><code>ServiceMonitor</code></a>\nto access the metrics.</p>\n<h4 id=\"debuggability-and-robustness\">Debuggability and robustness</h4>\n<p>Beside all those new features, we decided to restructure parts of the Security\nProfiles Operator internally to make it better to debug and more robust. For\nexample, we now maintain an internal <a href=\"https://grpc.io\">gRPC</a> API to communicate\nwithin the operator across different features. We also improved the performance\nof the log enricher, which now caches results for faster retrieval of the log\ndata. The operator can be put into a more <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#set-logging-verbosity\">verbose log mode</a>\nby setting <code>verbosity</code> from <code>0</code> to <code>1</code>.</p>\n<p>We also print the used <code>libseccomp</code> and <code>libbpf</code> versions on startup, as well as\nexpose CPU and memory profiling endpoints for each container via the\n<a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#enable-cpu-and-memory-profiling\"><code>enableProfiling</code> option</a>.\nDedicated liveness and startup probes inside of the operator daemon will now\nadditionally improve the life cycle of the operator.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Thank you for reading this update. We're looking forward to future enhancements\nof the operator and would love to get your feedback about the latest release.\nFeel free to reach out to us via the Kubernetes slack\n<a href=\"https://kubernetes.slack.com/messages/security-profiles-operator\">#security-profiles-operator</a>\nfor any feedback or question.</p>","PublishedAt":"2021-12-17 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/17/security-profiles-operator/","SourceName":"Kubernetes"}},{"node":{"ID":1239,"Title":"Blog: Kubernetes 1.23: StatefulSet PVC Auto-Deletion (alpha)","Description":"<p><strong>Author:</strong> Matthew Cary (Google)</p>\n<p>Kubernetes v1.23 introduced a new, alpha-level policy for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a> that controls the lifetime of\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolumeClaims</a> (PVCs) generated from the\nStatefulSet spec template for cases when they should be deleted automatically when the StatefulSet\nis deleted or pods in the StatefulSet are scaled down.</p>\n<h2 id=\"what-problem-does-this-solve\">What problem does this solve?</h2>\n<p>A StatefulSet spec can include Pod and PVC templates. When a replica is first created, the\nKubernetes control plane creates a PVC for that replica if one does not already exist. The behavior\nbefore Kubernetes v1.23 was that the control plane never cleaned up the PVCs created for\nStatefulSets - this was left up to the cluster administrator, or to some add-on automation that\nyou‚Äôd have to find, check suitability, and deploy. The common pattern for managing PVCs, either\nmanually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,\nwith explicit lifecycle. Workflows that use StatefulSets must determine on their own what PVCs are\ncreated by a StatefulSet and what their lifecycle should be.</p>\n<p>Before this new feature, when a StatefulSet-managed replica disappears, either because the\nStatefulSet is reducing its replica count, or because its StatefulSet is deleted, the PVC and its\nbacking volume remains and must be manually deleted. While this behavior is appropriate when the\ndata is critical, in many cases the persistent data in these PVCs is either temporary, or can be\nreconstructed from another source. In those cases, PVCs and their backing volumes remaining after\ntheir StatefulSet or replicas have been deleted are not necessary, incur cost, and require manual\ncleanup.</p>\n<h2 id=\"the-new-statefulset-pvc-retention-policy\">The new StatefulSet PVC retention policy</h2>\n<p>If you enable the alpha feature, a StatefulSet spec includes a PersistentVolumeClaim retention\npolicy. This is used to control if and when PVCs created from a StatefulSet‚Äôs <code>volumeClaimTemplate</code>\nare deleted. This first iteration of the retention policy contains two situations where PVCs may be\ndeleted.</p>\n<p>The first situation is when the StatefulSet resource is deleted (which implies that all replicas are\nalso deleted). This is controlled by the <code>whenDeleted</code> policy. The second situation, controlled by\n<code>whenScaled</code> is when the StatefulSet is scaled down, which removes some but not all of the replicas\nin a StatefulSet. In both cases the policy can either be <code>Retain</code>, where the corresponding PVCs are\nnot touched, or <code>Delete</code>, which means that PVCs are deleted. The deletion is done with a normal\n<a href=\"https://kubernetes.io/docs/concepts/architecture/garbage-collection/\">object deletion</a>, so that, for example, all\nretention policies for the underlying PV are respected.</p>\n<p>This policy forms a matrix with four cases. I‚Äôll walk through and give an example for each one.</p>\n<ul>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Retain</code>.</strong> This matches the existing behavior for\nStatefulSets, where no PVCs are deleted. This is also the default retention policy. It‚Äôs\nappropriate to use when data on StatefulSet volumes may be irreplaceable and should only be\ndeleted manually.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Delete</code> and <code>whenScaled</code> is <code>Retain</code>.</strong> In this case, PVCs are deleted only when\nthe entire StatefulSet is deleted. If the StatefulSet is scaled down, PVCs are not touched,\nmeaning they are available to be reattached if a scale-up occurs with any data from the previous\nreplica. This might be used for a temporary StatefulSet, such as in a CI instance or ETL\npipeline, where the data on the StatefulSet is needed only during the lifetime of the\nStatefulSet lifetime, but while the task is running the data is not easily reconstructible. Any\nretained state is needed for any replicas that scale down and then up.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> and <code>whenScaled</code> are both <code>Delete</code>.</strong> PVCs are deleted immediately when their\nreplica is no longer needed. Note this does not include when a Pod is deleted and a new version\nrescheduled, for example when a node is drained and Pods need to migrate elsewhere. The PVC is\ndeleted only when the replica is no longer needed as signified by a scale-down or StatefulSet\ndeletion. This use case is for when data does not need to live beyond the life of its\nreplica. Perhaps the data is easily reconstructable and the cost savings of deleting unused PVCs\nis more important than quick scale-up, or perhaps that when a new replica is created, any data\nfrom a previous replica is not usable and must be reconstructed anyway.</p>\n</li>\n<li>\n<p><strong><code>whenDeleted</code> is <code>Retain</code> and <code>whenScaled</code> is <code>Delete</code>.</strong> This is similar to the previous case,\nwhen there is little benefit to keeping PVCs for fast reuse during scale-up. An example of a\nsituation where you might use this is an Elasticsearch cluster. Typically you would scale that\nworkload up and down to match demand, whilst ensuring a minimum number of replicas (for example:\n3). When scaling down, data is migrated away from removed replicas and there is no benefit to\nretaining those PVCs. However, it can be useful to bring the entire Elasticsearch cluster down\ntemporarily for maintenance. If you need to take the Elasticsearch system offline, you can do\nthis by temporarily deleting the StatefulSet, and then bringing the Elasticsearch cluster back\nby recreating the StatefulSet. The PVCs holding the Elasticsearch data will still exist and the\nnew replicas will automatically use them.</p>\n</li>\n</ul>\n<p>Visit the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies\">documentation</a> to\nsee all the details.</p>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>Enable the feature and try it out! Enable the <code>StatefulSetAutoDeletePVC</code> feature gate on a cluster,\nthen create a StatefulSet using the new policy. Test it out and tell us what you think!</p>\n<p>I'm very curious to see if this owner reference mechanism works well in practice. For example, we\nrealized there is no mechanism in Kubernetes for knowing who set a reference, so it‚Äôs possible that\nthe StatefulSet controller may fight with custom controllers that set their own\nreferences. Fortunately, maintaining the existing retention behavior does not involve any new owner\nreferences, so default behavior will be compatible.</p>\n<p>Please tag any issues you report with the label <code>sig/apps</code> and assign them to Matthew Cary\n(<a href=\"https://github.com/mattcary\">@mattcary</a> at GitHub).</p>\n<p>Enjoy!</p>","PublishedAt":"2021-12-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/","SourceName":"Kubernetes"}},{"node":{"ID":1240,"Title":"Blog: Kubernetes 1.23: Prevent PersistentVolume leaks when deleting out of order","Description":"<p><strong>Author:</strong> Deepak Kinni (VMware)</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolume</a> (or PVs for short) are\nassociated with <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaim-policy\">Reclaim Policy</a>.\nThe Reclaim Policy is used to determine the actions that need to be taken by the storage\nbackend on deletion of the PV.\nWhere the reclaim policy is <code>Delete</code>, the expectation is that the storage backend\nreleases the storage resource that was allocated for the PV. In essence, the reclaim\npolicy needs to honored on PV deletion.</p>\n<p>With the recent Kubernetes v1.23 release, an alpha feature lets you configure your\ncluster to behave that way and honor the configured reclaim policy.</p>\n<h2 id=\"how-did-reclaim-work-in-previous-kubernetes-releases\">How did reclaim work in previous Kubernetes releases?</h2>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#Introduction\">PersistentVolumeClaim</a> (or PVC for short) is\na request for storage by a user. A PV and PVC are considered <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#Binding\">Bound</a>\nif there is a newly created PV or a matching PV is found. The PVs themselves are\nbacked by a volume allocated by the storage backend.</p>\n<p>Normally, if the volume is to be deleted, then the expectation is to delete the\nPVC for a bound PV-PVC pair. However, there are no restrictions to delete a PV\nprior to deleting a PVC.</p>\n<p>First, I'll demonstrate the behavior for clusters that are running an older version of Kubernetes.</p>\n<h4 id=\"retrieve-an-pvc-that-is-bound-to-a-pv\">Retrieve an PVC that is bound to a PV</h4>\n<p>Retrieve an existing PVC <code>example-vanilla-block-pvc</code></p>\n<pre tabindex=\"0\"><code>kubectl get pvc example-vanilla-block-pvc\n</code></pre><p>The following output shows the PVC and it's <code>Bound</code> PV, the PV is shown under the <code>VOLUME</code> column:</p>\n<pre tabindex=\"0\"><code>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\nexample-vanilla-block-pvc Bound pvc-6791fdd4-5fad-438e-a7fb-16410363e3da 5Gi RWO example-vanilla-block-sc 19s\n</code></pre><h4 id=\"delete-pv\">Delete PV</h4>\n<p>When I try to delete a bound PV, the cluster blocks and the <code>kubectl</code> tool does\nnot return back control to the shell; for example:</p>\n<pre tabindex=\"0\"><code>kubectl delete pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da\n</code></pre><pre tabindex=\"0\"><code>persistentvolume &#34;pvc-6791fdd4-5fad-438e-a7fb-16410363e3da&#34; deleted\n^C\n</code></pre><p>Retrieving the PV:</p>\n<pre tabindex=\"0\"><code>kubectl get pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da\n</code></pre><p>It can be observed that the PV is in <code>Terminating</code> state</p>\n<pre tabindex=\"0\"><code>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\npvc-6791fdd4-5fad-438e-a7fb-16410363e3da 5Gi RWO Delete Terminating default/example-vanilla-block-pvc example-vanilla-block-sc 2m23s\n</code></pre><h4 id=\"delete-pvc\">Delete PVC</h4>\n<pre tabindex=\"0\"><code>kubectl delete pvc example-vanilla-block-pvc\n</code></pre><p>The following output is seen if the PVC gets successfully deleted:</p>\n<pre tabindex=\"0\"><code>persistentvolumeclaim &#34;example-vanilla-block-pvc&#34; deleted\n</code></pre><p>The PV object from the cluster also gets deleted. When attempting to retrieve the PV\nit will be observed that the PV is no longer found:</p>\n<pre tabindex=\"0\"><code>kubectl get pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da\n</code></pre><pre tabindex=\"0\"><code>Error from server (NotFound): persistentvolumes &#34;pvc-6791fdd4-5fad-438e-a7fb-16410363e3da&#34; not found\n</code></pre><p>Although the PV is deleted the underlying storage resource is not deleted, and\nneeds to be removed manually.</p>\n<p>To sum it up, the reclaim policy associated with the Persistent Volume is currently\nignored under certain circumstance. For a <code>Bound</code> PV-PVC pair the ordering of PV-PVC\ndeletion determines whether the PV reclaim policy is honored. The reclaim policy\nis honored if the PVC is deleted first, however, if the PV is deleted prior to\ndeleting the PVC then the reclaim policy is not exercised. As a result of this behavior,\nthe associated storage asset in the external infrastructure is not removed.</p>\n<h2 id=\"pv-reclaim-policy-with-kubernetes-v1-23\">PV reclaim policy with Kubernetes v1.23</h2>\n<p>The new behavior ensures that the underlying storage object is deleted from the backend when users attempt to delete a PV manually.</p>\n<h4 id=\"how-to-enable-new-behavior\">How to enable new behavior?</h4>\n<p>To make use of the new behavior, you must have upgraded your cluster to the v1.23 release of Kubernetes.\nYou need to make sure that you are running the CSI <a href=\"https://github.com/kubernetes-csi/external-provisioner\"><code>external-provisioner</code></a> version <code>4.0.0</code>, or later.\nYou must also enable the <code>HonorPVReclaimPolicy</code> <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature gate</a> for the\n<code>external-provisioner</code> and for the <code>kube-controller-manager</code>.</p>\n<p>If you're not using a CSI driver to integrate with your storage backend, the fix isn't\navailable. The Kubernetes project doesn't have a current plan to fix the bug for in-tree\nstorage drivers: the future of those in-tree drivers is deprecation and migration to CSI.</p>\n<h4 id=\"how-does-it-work\">How does it work?</h4>\n<p>The new behavior is achieved by adding a finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code> on new and existing PVs, the finalizer is only removed after the storage from backend is deleted.</p>\n<p>An example of a PV with the finalizer, notice the new finalizer in the finalizers list</p>\n<pre tabindex=\"0\"><code>kubectl get pv pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53 -o yaml\n</code></pre><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolume<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">pv.kubernetes.io/provisioned-by</span>:<span style=\"color:#bbb\"> </span>csi.vsphere.vmware.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">creationTimestamp</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;2021-11-17T19:28:56Z&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">finalizers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- kubernetes.io/pv-protection<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- external-provisioner.volume.kubernetes.io/finalizer<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;194711&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">uid</span>:<span style=\"color:#bbb\"> </span>087f14f2-4157-4e95-8a70-8294b039d30e<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">capacity</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claimRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-vanilla-block-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>default<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resourceVersion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;194677&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">uid</span>:<span style=\"color:#bbb\"> </span>a7b7e3ba-f837-45ba-b243-dec7d8aaed53<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">csi</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>csi.vsphere.vmware.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fsType</span>:<span style=\"color:#bbb\"> </span>ext4<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeAttributes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage.kubernetes.io/csiProvisionerIdentity</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1637110610497-8081</span>-csi.vsphere.vmware.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>vSphere CNS Block Volume<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeHandle</span>:<span style=\"color:#bbb\"> </span>2dacf297-803f-4ccc-afc7-3d3c3f02051e<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">persistentVolumeReclaimPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>example-vanilla-block-sc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">status</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">phase</span>:<span style=\"color:#bbb\"> </span>Bound<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The presence of the finalizer prevents the PV object from being removed from the\ncluster. As stated previously, the finalizer is only removed from the PV object\nafter it is successfully deleted from the storage backend. To learn more about\nfinalizers, please refer to <a href=\"https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/\">Using Finalizers to Control Deletion</a>.</p>\n<h4 id=\"what-about-csi-migrated-volumes\">What about CSI migrated volumes?</h4>\n<p>The fix is applicable to CSI migrated volumes as well. However, when the feature\n<code>HonorPVReclaimPolicy</code> is enabled on 1.23, and CSI Migration is disabled, the finalizer\nis removed from the PV object if it exists.</p>\n<h3 id=\"some-caveats\">Some caveats</h3>\n<ol>\n<li>The fix is applicable only to CSI volumes and migrated volumes. In-tree volumes will exhibit older behavior.</li>\n<li>The fix is introduced as an alpha feature in the <a href=\"https://github.com/kubernetes-csi/external-provisioner\">external-provisioner</a> under the feature gate <code>HonorPVReclaimPolicy</code>. The feature is disabled by default, and needs to be enabled explicitly.</li>\n</ol>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2644-honor-pv-reclaim-policy\">KEP-2644</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-provisioner/issues/546\">Volume leak issue</a></li>\n</ul>\n<h3 id=\"how-do-i-get-involved\">How do I get involved?</h3>\n<p>The Kubernetes Slack channel <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact\">SIG Storage communication channels</a> are great mediums to reach out to the SIG Storage and migration working group teams.</p>\n<p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution:</p>\n<ul>\n<li>Jan ≈†afr√°nek (jsafrane)</li>\n<li>Xing Yang (xing-yang)</li>\n<li>Matthew Wong (wongma7)</li>\n</ul>\n<p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group (SIG)</a>. We‚Äôre rapidly growing and always welcome new contributors.</p>","PublishedAt":"2021-12-15 18:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/15/kubernetes-1-23-prevent-persistentvolume-leaks-when-deleting-out-of-order/","SourceName":"Kubernetes"}},{"node":{"ID":1241,"Title":"Blog: Kubernetes 1.23: Kubernetes In-Tree to CSI Volume Migration Status Update","Description":"<p><strong>Author:</strong> Jiawei Wang (Google)</p>\n<p>The Kubernetes in-tree storage plugin to <a href=\"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/\">Container Storage Interface (CSI)</a> migration infrastructure has already been <a href=\"https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/\">beta</a> since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.</p>\n<p>Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for GA.\nThis article is intended to give a status update to the feature as well as changes between Kubernetes 1.17 and 1.23. In addition, I will also cover the future roadmap for the CSI migration feature GA for each storage plugin.</p>\n<h2 id=\"quick-recap-what-is-csi-migration-and-why-migrate\">Quick recap: What is CSI Migration, and why migrate?</h2>\n<p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.\nKubernetes support for the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md#README\">Container Storage Interface</a> has been\n<a href=\"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/\">generally available</a> since Kubernetes v1.13.\nSupport for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).</p>\n<p>As more CSI Drivers were created and became production ready, SIG Storage group wanted all Kubernetes users to benefit from the CSI model. However, we cannot break API compatibility with the existing storage API types. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.</p>\n<p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as <code>kubernetes.io/gce-pd</code> or <code>kubernetes.io/aws-ebs</code> with a corresponding <a href=\"https://kubernetes-csi.github.io/docs/introduction.html\">CSI driver</a> from the storage backend.\nIf CSI Migration is working properly, Kubernetes end users shouldn‚Äôt notice a difference. Existing <code>StorageClass</code>, <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> objects should continue to work.\nWhen a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.\nHowever, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.</p>\n<p>For example, suppose you are a <code>kubernetes.io/gce-pd</code> user, after CSI migration, you can still use <code>kubernetes.io/gce-pd</code> to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing API/Interface will still function correctly. However, the underlying function calls are all going through the <a href=\"https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver\">GCE PD CSI driver</a> instead of the in-tree Kubernetes function.</p>\n<p>This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.</p>\n<h2 id=\"what-has-been-changed-and-what-s-new\">What has been changed, and what's new?</h2>\n<p>Building on the work done in Kubernetes v1.17 and earlier, the releases since then have\nmade a series of changes:</p>\n<h3 id=\"new-feature-gates\">New feature gates</h3>\n<p>The Kubernetes v1.21 release deprecated the <code>CSIMigration{provider}Complete</code> feature flags, and stopped honoring them. In their place came new feature flags named <code>InTreePlugin{vendor}Unregister</code>, that replace the old feature flag and retain all the functionality that <code>CSIMigration{provider}Complete</code> provided.</p>\n<p><code>CSIMigration{provider}Complete</code> was introduced before as a supplementary feature gate once CSI migration is enabled on all of the nodes. This flag unregisters the in-tree storage plugin you specify with the <code>{provider}</code> part of the flag name.</p>\n<p>When you enable that feature gate, then instead of using the in-tree driver code, your cluster directly selects and uses the relevant CSI driver. This happens without any check for whether CSI migration is enabled on the node, or whether you have in fact deployed that CSI driver.</p>\n<p>While this feature gate is a great helper, SIG Storage (and, I'm sure, lots of cluster operators) also wanted a feature gate that lets you disable an in-tree storage plugin, even without also enabling CSI migration. For example, you might want to disable the EBS storage plugin on a GCE cluster, because EBS volumes are specific to a different vendor's cloud (AWS).</p>\n<p>To make this possible, Kubernetes v1.21 introduced a new feature flag set: <code>InTreePlugin{vendor}Unregister</code>.</p>\n<p><code>InTreePlugin{vendor}Unregister</code> is a standalone feature gate that can be enabled and disabled independently from CSI Migration. When enabled, the component will not register the specific in-tree storage plugin to the supported list. If the cluster operator only enables this flag, end users will get an error from PVC saying it cannot find the plugin when the plugin is used. The cluster operator may want to enable this regardless of CSI Migration if they do not want to support the legacy in-tree APIs and only support CSI moving forward.</p>\n<h3 id=\"observability\">Observability</h3>\n<p>Kubernetes v1.21 introduced <a href=\"https://github.com/kubernetes/kubernetes/issues/98279\">metrics</a> for tracking CSI migration.\nYou can use these metrics to observe how your cluster is using storage services and whether access to that storage is using the legacy in-tree driver or its CSI-based replacement.</p>\n<table>\n<thead>\n<tr>\n<th>Components</th>\n<th>Metrics</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Kube-Controller-Manager</td>\n<td>storage_operation_duration_seconds</td>\n<td>A new label <code>migrated</code> is added to the metric to indicate whether this storage operation is a CSI migration operation(string value <code>true</code> for enabled and <code>false</code> for not enabled).</td>\n</tr>\n<tr>\n<td>Kubelet</td>\n<td>csi_operations_seconds</td>\n<td>The new metric exposes labels including <code>driver_name</code>, <code>method_name</code>, <code>grpc_status_code</code> and <code>migrated</code>. The meaning of these labels is identical to <code>csi_sidecar_operations_seconds</code>.</td>\n</tr>\n<tr>\n<td>CSI Sidecars(provisioner, attacher, resizer)</td>\n<td>csi_sidecar_operations_seconds</td>\n<td>A new label <code>migrated</code> is added to the metric to indicate whether this storage operation is a CSI migration operation(string value <code>true</code> for enabled and <code>false</code> for not enabled).</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"bug-fixes-and-feature-improvement\">Bug fixes and feature improvement</h3>\n<p>We have fixed numerous bugs like dangling attachment, garbage collection, incorrect topology label through the help of our beta testers.</p>\n<h3 id=\"cloud-provider-cluster-lifecycle-collaboration\">Cloud Provider &amp;&amp; Cluster Lifecycle Collaboration</h3>\n<p>SIG Storage has been working closely with SIG Cloud Provider and SIG Cluster Lifecycle on the rollout of CSI migration.</p>\n<p>If you are a user of a managed Kubernetes service, check with your provider if anything needs to be done. In many cases, the provider will manage the migration and no additional work is required.</p>\n<p>If you use a distribution of Kubernetes, check its official documentation for information about support for this feature. For the CSI Migration feature graduation to GA, SIG Storage and SIG Cluster Lifecycle are collaborating towards making the migration mechanisms available in tooling (such as kubeadm) as soon as they're available in Kubernetes itself.</p>\n<h2 id=\"timeline-and-status\">What is the timeline / status?</h2>\n<p>The current and targeted releases for each individual driver is shown in the table below:</p>\n<table>\n<thead>\n<tr>\n<th>Driver</th>\n<th>Alpha</th>\n<th>Beta (in-tree deprecated)</th>\n<th>Beta (on-by-default)</th>\n<th>GA</th>\n<th>Target &quot;in-tree plugin&quot; removal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AWS EBS</td>\n<td>1.14</td>\n<td>1.17</td>\n<td>1.23</td>\n<td>1.24 (Target)</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>GCE PD</td>\n<td>1.14</td>\n<td>1.17</td>\n<td>1.23</td>\n<td>1.24 (Target)</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>OpenStack Cinder</td>\n<td>1.14</td>\n<td>1.18</td>\n<td>1.21</td>\n<td>1.24 (Target)</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>Azure Disk</td>\n<td>1.15</td>\n<td>1.19</td>\n<td>1.23</td>\n<td>1.24 (Target)</td>\n<td>1.26 (Target)</td>\n</tr>\n<tr>\n<td>Azure File</td>\n<td>1.15</td>\n<td>1.21</td>\n<td>1.24 (Target)</td>\n<td>1.25 (Target)</td>\n<td>1.27 (Target)</td>\n</tr>\n<tr>\n<td>vSphere</td>\n<td>1.18</td>\n<td>1.19</td>\n<td>1.24 (Target)</td>\n<td>1.25 (Target)</td>\n<td>1.27 (Target)</td>\n</tr>\n<tr>\n<td>Ceph RBD</td>\n<td>1.23</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Portworx</td>\n<td>1.23</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>The following storage drivers will not have CSI migration support. The ScaleIO driver was already removed; the others are deprecated and will be removed from core Kubernetes.</p>\n<table>\n<thead>\n<tr>\n<th>Driver</th>\n<th>Deprecated</th>\n<th>Code Removal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ScaleIO</td>\n<td>1.16</td>\n<td>1.22</td>\n</tr>\n<tr>\n<td>Flocker</td>\n<td>1.22</td>\n<td>1.25 (Target)</td>\n</tr>\n<tr>\n<td>Quobyte</td>\n<td>1.22</td>\n<td>1.25 (Target)</td>\n</tr>\n<tr>\n<td>StorageOS</td>\n<td>1.22</td>\n<td>1.25 (Target)</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>With more CSI drivers graduating to GA, we hope to soon mark the overall CSI Migration feature as GA. We are expecting cloud provider in-tree storage plugins code removal to happen by Kubernetes v1.26 and v1.27.</p>\n<h2 id=\"what-should-i-do-as-a-user\">What should I do as a user?</h2>\n<p>Note that all new features for the Kubernetes storage system (such as volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the <a href=\"https://kubernetes-csi.github.io/docs/drivers.html\">updated user guides for CSI drivers</a> and use the new CSI APIs.</p>\n<p>However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers. However, if you want to leverage new features like snapshot, it will require a manual migration to re-import an existing intree PV as a CSI PV.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>The Kubernetes Slack channel <a href=\"https://kubernetes.slack.com/messages/csi-migration\">#csi-migration</a> along with any of the standard <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact\">SIG Storage communication channels</a> are great mediums to reach out to the SIG Storage and migration working group teams.</p>\n<p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:</p>\n<ul>\n<li>Michelle Au (msau42)</li>\n<li>Jan ≈†afr√°nek (jsafrane)</li>\n<li>Hemant Kumar (gnufied)</li>\n</ul>\n<p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:</p>\n<ul>\n<li>Andy Zhang (andyzhangz)</li>\n<li>Divyen Patel (divyenpatel)</li>\n<li>Deep Debroy (ddebroy)</li>\n<li>Humble Devassy Chirammal (humblec)</li>\n<li>Jing Xu (jingxu97)</li>\n<li>Jordan Liggitt (liggitt)</li>\n<li>Matthew Cary (mattcary)</li>\n<li>Matthew Wong (wongma7)</li>\n<li>Neha Arora (nearora-msft)</li>\n<li>Oksana Naumov (trierra)</li>\n<li>Saad Ali (saad-ali)</li>\n<li>Tim Bannister (sftim)</li>\n<li>Xing Yang (xing-yang)</li>\n</ul>\n<p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group (SIG)</a>. We‚Äôre rapidly growing and always welcome new contributors.</p>","PublishedAt":"2021-12-10 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/","SourceName":"Kubernetes"}},{"node":{"ID":1242,"Title":"Blog: Kubernetes 1.23: Pod Security Graduates to Beta","Description":"<p><strong>Authors:</strong> Jim Angel (Google), Lachlan Evenson (Microsoft)</p>\n<p>With the release of Kubernetes v1.23, <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Pod Security admission</a> has now entered beta. Pod Security is a <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">built-in</a> admission controller that evaluates pod specifications against a predefined set of <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a> and determines whether to <code>admit</code> or <code>deny</code> the pod from running.</p>\n<p>Pod Security is the successor to <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-policy/\">PodSecurityPolicy</a> which was deprecated in the v1.21 release, and will be removed in Kubernetes v1.25. In this article, we cover the key concepts of Pod Security along with how to use it. We hope that cluster administrators and developers alike will use this new mechanism to enforce secure defaults for their workloads.</p>\n<h2 id=\"why-pod-security\">Why Pod Security</h2>\n<p>The overall aim of Pod Security is to let you isolate workloads. You can run a cluster that runs different workloads and, without adding extra third-party tooling, implement controls that require Pods for a workload to restrict their own privileges to a defined bounding set.</p>\n<p>Pod Security overcomes key shortcomings of Kubernetes' existing, but deprecated, PodSecurityPolicy (PSP) mechanism:</p>\n<ul>\n<li>Policy authorization model ‚Äî challenging to deploy with controllers.</li>\n<li>Risks around switching ‚Äî a lack of dry-run/audit capabilities made it hard to enable PodSecurityPolicy.</li>\n<li>Inconsistent and Unbounded API ‚Äî the large configuration surface and evolving constraints led to a complex and confusing API.</li>\n</ul>\n<p>The shortcomings of PSP made it very difficult to use which led the community to reevaluate whether or not a better implementation could achieve the same goals. One of those goals was to provide an out-of-the-box solution to apply security best practices. Pod Security ships with predefined Pod Security levels that a cluster administrator can configure to meet the desired security posture.</p>\n<p>It's important to note that Pod Security doesn't have complete feature parity with the deprecated PodSecurityPolicy. Specifically, it doesn't have the ability to mutate or change Kubernetes resources to auto-remediate a policy violation on behalf of the user. Additionally, it doesn't provide fine-grained control over each allowed field and value within a pod specification or any other Kubernetes resource that you may wish to evaluate. If you need more fine-grained policy control then take a look at these <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#faq\">other</a> projects which support such use cases.</p>\n<p>Pod Security also adheres to Kubernetes best practices of declarative object management by denying resources that violate the policy. This requires resources to be updated in source repositories, and tooling to be updated prior to being deployed to Kubernetes.</p>\n<h2 id=\"how-does-pod-security-work\">How Does Pod Security Work?</h2>\n<p>Pod Security is a built-in <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">admission controller</a> starting with Kubernetes v1.22, but can also be run as a standalone <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/#webhook\">webhook</a>. Admission controllers function by intercepting requests in the Kubernetes API server prior to persistence to storage. They can either <code>admit</code> or <code>deny</code> a request. In the case of Pod Security, pod specifications will be evaluated against a configured policy in the form of a Pod Security Standard. This means that security sensitive fields in a pod specification will only be allowed to have <a href=\"h/docs/concepts/security/pod-security-standards/#profile-details\">specific</a> values.</p>\n<h2 id=\"configuring-pod-security\">Configuring Pod Security</h2>\n<h3 id=\"pod-security-standards\">Pod Security Standards</h3>\n<p>In order to use Pod Security we first need to understand <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a>. These standards define three different policy levels that range from permissive to restrictive. These levels are as follows:</p>\n<ul>\n<li><code>privileged</code> ‚Äî open and unrestricted</li>\n<li><code>baseline</code> ‚Äî Covers known privilege escalations while minimizing restrictions</li>\n<li><code>restricted</code> ‚Äî Highly restricted, hardening against known and unknown privilege escalations. May cause compatibility issues</li>\n</ul>\n<p>Each of these policy levels define which fields are restricted within a pod specification and the allowed values. Some of the fields restricted by these policies include:</p>\n<ul>\n<li><code>spec.securityContext.sysctls</code></li>\n<li><code>spec.hostNetwork</code></li>\n<li><code>spec.volumes[*].hostPath</code></li>\n<li><code>spec.containers[*].securityContext.privileged</code></li>\n</ul>\n<p>Policy levels are applied via labels on Namespace resources, which allows for granular per-namespace policy selection. The AdmissionConfiguration in the API server can also be configured to set cluster-wide default levels and exemptions.</p>\n<h3 id=\"policy-modes\">Policy modes</h3>\n<p>Policies are applied in a specific mode. Multiple modes (with different policy levels) can be set on the same namespace. Here is a list of modes:</p>\n<ul>\n<li><code>enforce</code> ‚Äî Any Pods that violate the policy will be rejected</li>\n<li><code>audit</code> ‚Äî Violations will be recorded as an annotation in the audit logs, but don't affect whether the pod is allowed.</li>\n<li><code>warn</code> ‚Äî Violations will send a warning message back to the user, but don't affect whether the pod is allowed.</li>\n</ul>\n<p>In addition to modes you can also pin the policy to a specific version (for example v1.22). Pinning to a specific version allows the behavior to remain consistent if the policy definition changes in future Kubernetes releases.</p>\n<h2 id=\"hands-on-demo\">Hands on demo</h2>\n<h3 id=\"prerequisites\">Prerequisites</h3>\n<ul>\n<li><a href=\"https://kind.sigs.k8s.io/docs/user/quick-start/#installation\">KinD</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/tools/\">kubectl</a></li>\n<li><a href=\"https://docs.docker.com/get-docker/\">Docker</a> or <a href=\"https://podman.io/getting-started/installation\">Podman</a> container runtime &amp; CLI</li>\n</ul>\n<h3 id=\"deploy-a-kind-cluster\">Deploy a kind cluster</h3>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kind create cluster --image kindest/node:v1.23.0\n</span></span></code></pre></div><p>It might take a while to start and once it's started it might take a minute or so before the node becomes ready.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl cluster-info --context kind-kind\n</span></span></code></pre></div><p>Wait for the node STATUS to become ready.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl get nodes\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>NAME STATUS ROLES AGE VERSION\nkind-control-plane Ready control-plane,master 54m v1.23.0\n</code></pre><h3 id=\"confirm-pod-security-is-enabled\">Confirm Pod Security is enabled</h3>\n<p>The best way to <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default\">confirm the API's default enabled plugins</a> is to check the Kubernetes API container's help arguments.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl -n kube-system <span style=\"color:#a2f\">exec</span> kube-apiserver-kind-control-plane -it -- kube-apiserver -h | grep <span style=\"color:#b44\">&#34;default enabled ones&#34;</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>...\n--enable-admission-plugins strings\nadmission plugins that should be enabled in addition\nto default enabled ones (NamespaceLifecycle, LimitRanger,\nServiceAccount, TaintNodesByCondition, PodSecurity, Priority,\nDefaultTolerationSeconds, DefaultStorageClass,\nStorageObjectInUseProtection, PersistentVolumeClaimResize,\nRuntimeClass, CertificateApproval, CertificateSigning,\nCertificateSubjectRestriction, DefaultIngressClass,\nMutatingAdmissionWebhook, ValidatingAdmissionWebhook,\nResourceQuota).\n...\n</code></pre><p><code>PodSecurity</code> is listed in the group of default enabled admission plugins.</p>\n<p>If using a cloud provider, or if you don't have access to the API server, the best way to check would be to run a quick end-to-end test:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl create namespace verify-pod-security\n</span></span><span style=\"display:flex;\"><span>kubectl label namespace verify-pod-security pod-security.kubernetes.io/enforce<span style=\"color:#666\">=</span>restricted\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># The following command does NOT create a workload (--dry-run=server)</span>\n</span></span><span style=\"display:flex;\"><span>kubectl -n verify-pod-security run <span style=\"color:#a2f\">test</span> --dry-run<span style=\"color:#666\">=</span>server --image<span style=\"color:#666\">=</span>busybox --privileged\n</span></span><span style=\"display:flex;\"><span>kubectl delete namespace verify-pod-security\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Error from server (Forbidden): pods &#34;test&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: privileged (container &#34;test&#34; must not set securityContext.privileged=true), allowPrivilegeEscalation != false (container &#34;test&#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container &#34;test&#34; must set securityContext.capabilities.drop=[&#34;ALL&#34;]), runAsNonRoot != true (pod or container &#34;test&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &#34;test&#34; must set securityContext.seccompProfile.type to &#34;RuntimeDefault&#34; or &#34;Localhost&#34;)\n</code></pre><h3 id=\"configure-pod-security\">Configure Pod Security</h3>\n<p>Policies are applied to a namespace via labels. These labels are as follows:</p>\n<ul>\n<li><code>pod-security.kubernetes.io/&lt;MODE&gt;: &lt;LEVEL&gt;</code> (required to enable pod security)</li>\n<li><code>pod-security.kubernetes.io/&lt;MODE&gt;-version: &lt;VERSION&gt;</code> (<em>optional</em>, defaults to latest)</li>\n</ul>\n<p>A specific version can be supplied for each enforcement mode. The version pins the policy to the version that was shipped as part of the Kubernetes release. Pinning to a specific Kubernetes version allows for deterministic policy behavior while allowing flexibility for future updates to Pod Security Standards. The possible &lt;MODE(S)&gt; are <code>enforce</code>, <code>audit</code> and <code>warn</code>.</p>\n<h3 id=\"when-to-use-warn\">When to use <code>warn</code>?</h3>\n<p>The typical uses for <code>warn</code> are to get ready for a future change where you want to enforce a different policy. The most two common cases would be:</p>\n<ul>\n<li><code>warn</code> at the same level but a different version (e.g. pin <code>enforce</code> to <em>restricted+v1.23</em> and <code>warn</code> at <em>restricted+latest</em>)</li>\n<li><code>warn</code> at a stricter level (e.g. <code>enforce</code> baseline, <code>warn</code> restricted)</li>\n</ul>\n<p>It's not recommended to use <code>warn</code> for the exact same level+version of the policy as <code>enforce</code>. In the admission sequence, if <code>enforce</code> fails, the entire sequence fails before evaluating the <code>warn</code>.</p>\n<p>First, create a namespace called <code>verify-pod-security</code> if not created earlier. For the demo, <code>--overwrite</code> is used when labeling to allow repurposing a single namespace for multiple examples.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl create namespace verify-pod-security\n</span></span></code></pre></div><h3 id=\"deploy-demo-workloads\">Deploy demo workloads</h3>\n<p>Each workload represents a higher level of security that would not pass the profile that comes after it.</p>\n<p>For the following examples, use the <code>busybox</code> container runs a <code>sleep</code> command for 1 million seconds (‚âÖ11 days) or until deleted. Pod Security is not interested in which container image you chose, but rather the Pod level settings and their implications for security.</p>\n<h3 id=\"privileged-level-and-workload\">Privileged level and workload</h3>\n<p>For the privileged pod, use the <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#privileged\">privileged policy</a>. This allows the process inside a container to gain new processes (also known as &quot;privilege escalation&quot;) and can be dangerous if untrusted.</p>\n<p>First, let's apply a restricted Pod Security level for a test.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># enforces a &#34;restricted&#34; security policy and audits on restricted</span>\n</span></span><span style=\"display:flex;\"><span>kubectl label --overwrite ns verify-pod-security <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/enforce<span style=\"color:#666\">=</span>restricted <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/audit<span style=\"color:#666\">=</span>restricted\n</span></span></code></pre></div><p>Next, try to deploy a privileged workload in the namespace.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-privileged\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Error from server (Forbidden): error when creating &#34;STDIN&#34;: pods &#34;busybox-privileged&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: allowPrivilegeEscalation != false (container &#34;busybox&#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container &#34;busybox&#34; must set securityContext.capabilities.drop=[&#34;ALL&#34;]), runAsNonRoot != true (pod or container &#34;busybox&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &#34;busybox&#34; must set securityContext.seccompProfile.type to &#34;RuntimeDefault&#34; or &#34;Localhost&#34;)\n</code></pre><p>Now let's apply the privileged Pod Security level and try again.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># enforces a &#34;privileged&#34; security policy and warns / audits on baseline</span>\n</span></span><span style=\"display:flex;\"><span>kubectl label --overwrite ns verify-pod-security <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/enforce<span style=\"color:#666\">=</span>privileged <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/warn<span style=\"color:#666\">=</span>baseline <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/audit<span style=\"color:#666\">=</span>baseline\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-privileged\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>pod/busybox-privileged created\n</code></pre><p>We can run <code>kubectl -n verify-pod-security get pods</code> to verify it is running. Clean up with:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl -n verify-pod-security delete pod busybox-privileged\n</span></span></code></pre></div><h3 id=\"baseline-level-and-workload\">Baseline level and workload</h3>\n<p>The <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline\">baseline policy</a> demonstrates sensible defaults while preventing common container exploits.</p>\n<p>Let's revert back to a restricted Pod Security level for a quick test.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># enforces a &#34;restricted&#34; security policy and audits on restricted</span>\n</span></span><span style=\"display:flex;\"><span>kubectl label --overwrite ns verify-pod-security <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/enforce<span style=\"color:#666\">=</span>restricted <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/audit<span style=\"color:#666\">=</span>restricted\n</span></span></code></pre></div><p>Apply the workload.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-baseline\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: false\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> capabilities:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> add:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - NET_BIND_SERVICE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - CHOWN\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Error from server (Forbidden): error when creating &#34;STDIN&#34;: pods &#34;busybox-baseline&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: unrestricted capabilities (container &#34;busybox&#34; must set securityContext.capabilities.drop=[&#34;ALL&#34;]; container &#34;busybox&#34; must not include &#34;CHOWN&#34; in securityContext.capabilities.add), runAsNonRoot != true (pod or container &#34;busybox&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &#34;busybox&#34; must set securityContext.seccompProfile.type to &#34;RuntimeDefault&#34; or &#34;Localhost&#34;)\n</code></pre><p>Let's apply the baseline Pod Security level and try again.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># enforces a &#34;baseline&#34; security policy and warns / audits on restricted</span>\n</span></span><span style=\"display:flex;\"><span>kubectl label --overwrite ns verify-pod-security <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/enforce<span style=\"color:#666\">=</span>baseline <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/warn<span style=\"color:#666\">=</span>restricted <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/audit<span style=\"color:#666\">=</span>restricted\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-baseline\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: false\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> capabilities:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> add:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - NET_BIND_SERVICE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - CHOWN\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to the following. Note that the warnings match the error message from the test above, but the pod is still successfully created.</p>\n<pre tabindex=\"0\"><code>Warning: would violate PodSecurity &#34;restricted:latest&#34;: unrestricted capabilities (container &#34;busybox&#34; must set securityContext.capabilities.drop=[&#34;ALL&#34;]; container &#34;busybox&#34; must not include &#34;CHOWN&#34; in securityContext.capabilities.add), runAsNonRoot != true (pod or container &#34;busybox&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &#34;busybox&#34; must set securityContext.seccompProfile.type to &#34;RuntimeDefault&#34; or &#34;Localhost&#34;)\npod/busybox-baseline created\n</code></pre><p>Remember, we set the <code>verify-pod-security</code> namespace to <code>warn</code> based on the restricted profile. We can run <code>kubectl -n verify-pod-security get pods</code> to verify it is running. Clean up with:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl -n verify-pod-security delete pod busybox-baseline\n</span></span></code></pre></div><h3 id=\"restricted-level-and-workload\">Restricted level and workload</h3>\n<p>The <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted\">restricted policy</a> requires rejection of all privileged parameters. It is the most secure with a trade-off for complexity.\nThe restricted policy allows containers to add the <code>NET_BIND_SERVICE</code> capability only.</p>\n<p>While we've already tested restricted as a blocking function, let's try to get something running that meets all the criteria.</p>\n<p>First we need to reapply the restricted profile, for the last time.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># enforces a &#34;restricted&#34; security policy and audits on restricted</span>\n</span></span><span style=\"display:flex;\"><span>kubectl label --overwrite ns verify-pod-security <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/enforce<span style=\"color:#666\">=</span>restricted <span style=\"color:#b62;font-weight:bold\">\\\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b62;font-weight:bold\"></span> pod-security.kubernetes.io/audit<span style=\"color:#666\">=</span>restricted\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-restricted\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: false\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> capabilities:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> add:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - NET_BIND_SERVICE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Error from server (Forbidden): error when creating &#34;STDIN&#34;: pods &#34;busybox-restricted&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: unrestricted capabilities (container &#34;busybox&#34; must set securityContext.capabilities.drop=[&#34;ALL&#34;]), runAsNonRoot != true (pod or container &#34;busybox&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &#34;busybox&#34; must set securityContext.seccompProfile.type to &#34;RuntimeDefault&#34; or &#34;Localhost&#34;)\n</code></pre><p>This is because the restricted profile explicitly requires that certain values are set to the most secure parameters.</p>\n<p>By requiring explicit values, manifests become more declarative and your entire security model can shift left. With the <code>restricted</code> level of enforcement, a company could audit their cluster's compliance based on permitted manifests.</p>\n<p>Let's fix each warning resulting in the following file:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-restricted\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> seccompProfile:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> type: RuntimeDefault\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> runAsNonRoot: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: false\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> capabilities:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> drop:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - ALL\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> add:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - NET_BIND_SERVICE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>pod/busybox-restricted created\n</code></pre><p>Run <code>kubectl -n verify-pod-security get pods</code> to verify it is running. The output is similar to this:</p>\n<pre tabindex=\"0\"><code>NAME READY STATUS RESTARTS AGE\nbusybox-restricted 0/1 CreateContainerConfigError 0 2m26s\n</code></pre><p>Let's figure out why the container is not starting with <code>kubectl -n verify-pod-security describe pod busybox-restricted</code>. The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Events:\nType Reason Age From Message\n---- ------ ---- ---- -------\nWarning Failed 2m29s (x8 over 3m55s) kubelet Error: container has runAsNonRoot and image will run as root (pod: &#34;busybox-restricted_verify-pod-security(a4c6a62d-2166-41a9-b288-20df17cf5c90)&#34;, container: busybox)\n</code></pre><p>To solve this, set the effective UID (<code>runAsUser</code>) to a non-zero (root) value or use the <code>nobody</code> UID (65534).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># delete the original pod</span>\n</span></span><span style=\"display:flex;\"><span>kubectl -n verify-pod-security delete pod busybox-restricted\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># create the pod again with new runAsUser</span>\n</span></span><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n verify-pod-security apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-restricted\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> runAsUser: 65534\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> seccompProfile:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> type: RuntimeDefault\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> runAsNonRoot: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: false\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> capabilities:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> drop:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - ALL\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> add:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - NET_BIND_SERVICE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>Run <code>kubectl -n verify-pod-security get pods</code> to verify it is running. The output is similar to this:</p>\n<pre tabindex=\"0\"><code>NAME READY STATUS RESTARTS AGE\nbusybox-restricted 1/1 Running 0 25s\n</code></pre><p>Clean up the demo (restricted pod and namespace) with:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl delete namespace verify-pod-security\n</span></span></code></pre></div><p>At this point, if you wanted to dive deeper into linux permissions or what is permitted for a certain container, exec into the control plane and play around with <code>containerd</code> and <code>crictl inspect</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># if using docker, shell into the control plane</span>\n</span></span><span style=\"display:flex;\"><span>docker <span style=\"color:#a2f\">exec</span> -it kind-control-plane bash\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># list running containers</span>\n</span></span><span style=\"display:flex;\"><span>crictl ps\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># inspect each one by container ID</span>\n</span></span><span style=\"display:flex;\"><span>crictl inspect &lt;CONTAINER ID&gt;\n</span></span></code></pre></div><h3 id=\"applying-a-cluster-wide-policy\">Applying a cluster-wide policy</h3>\n<p>In addition to applying labels to namespaces to configure policy you can also configure cluster-wide policies and exemptions using the AdmissionConfiguration resource.</p>\n<p>Using this resource, policy definitions are applied cluster-wide by default and any policy that is applied via namespace labels will take precedence.</p>\n<p>There is no runtime configurable API for the <code>AdmissionConfiguration</code> configuration file so a cluster administrator would need to specify a path to the file below via the <code>--admission-control-config-file</code> flag on the API server.</p>\n<p>In the following resource we are enforcing the baseline policy and warning and auditing the baseline policy. We are also making the kube-system namespace exempt from this policy.</p>\n<p>It's not recommended to alter control plane / clusters after install, so let's build a new cluster with a default policy on all namespaces.</p>\n<p>First, delete the current cluster.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kind delete cluster\n</span></span></code></pre></div><p>Create a Pod Security configuration that <code>enforce</code> and <code>audit</code> baseline policies while using a restricted profile to <code>warn</code> the end user.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF &gt; pod-security.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: apiserver.config.k8s.io/v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: AdmissionConfiguration\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">plugins:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">- name: PodSecurity\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> configuration:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> apiVersion: pod-security.admission.config.k8s.io/v1beta1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> kind: PodSecurityConfiguration\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> defaults:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> enforce: &#34;baseline&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> enforce-version: &#34;latest&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> audit: &#34;baseline&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> audit-version: &#34;latest&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> warn: &#34;restricted&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> warn-version: &#34;latest&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> exemptions:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> # Array of authenticated usernames to exempt.\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> usernames: []\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> # Array of runtime class names to exempt.\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> runtimeClasses: []\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> # Array of namespaces to exempt.\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> namespaces: [kube-system]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>For additional options, check out the official <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller\"><em>standards admission controller</em></a> docs.</p>\n<p>We now have a default baseline policy. Next pass it to the kind configuration to enable the <code>--admission-control-config-file</code> API server argument and pass the policy file. To pass a file to a kind cluster, use a configuration file to pass additional setup instructions. Kind uses <code>kubeadm</code> to provision the cluster and the configuration file has the ability to pass <code>kubeadmConfigPatches</code> for further customization. In our case, the local file is mounted into the control plane node as <code>/etc/kubernetes/policies/pod-security.yaml</code> which is then mounted into the <code>apiServer</code> container. We also pass the <code>--admission-control-config-file</code> argument pointing to the policy's location.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF &gt; kind-config.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Cluster\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: kind.x-k8s.io/v1alpha4\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">nodes:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">- role: control-plane\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> kubeadmConfigPatches:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - |\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> kind: ClusterConfiguration\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> apiServer:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> # enable admission-control-config flag on the API server\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> extraArgs:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> admission-control-config-file: /etc/kubernetes/policies/pod-security.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> # mount new file / directories on the control plane\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> extraVolumes:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: policies\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> hostPath: /etc/kubernetes/policies\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> mountPath: /etc/kubernetes/policies\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> readOnly: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> pathType: &#34;DirectoryOrCreate&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> # mount the local file on the control plane\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> extraMounts:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - hostPath: ./pod-security.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containerPath: /etc/kubernetes/policies/pod-security.yaml\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> readOnly: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>Create a new cluster using the kind configuration file defined above.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kind create cluster --image kindest/node:v1.23.0 --config kind-config.yaml\n</span></span></code></pre></div><p>Let's look at the default namespace.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl describe namespace default\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Name: default\nLabels: kubernetes.io/metadata.name=default\nAnnotations: &lt;none&gt;\nStatus: Active\nNo resource quota.\nNo LimitRange resource.\n</code></pre><p>Let's create a new namespace and see if the labels apply there.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl create namespace test-defaults\n</span></span><span style=\"display:flex;\"><span>kubectl describe namespace test-defaults\n</span></span></code></pre></div><p>Same.</p>\n<pre tabindex=\"0\"><code>Name: test-defaults\nLabels: kubernetes.io/metadata.name=test-defaults\nAnnotations: &lt;none&gt;\nStatus: Active\nNo resource quota.\nNo LimitRange resource.\n</code></pre><p>Can a privileged workload be deployed?</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n test-defaults apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-privileged\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> securityContext:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> allowPrivilegeEscalation: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>Hmm... yep. The default <code>warn</code> level is working at least.</p>\n<pre tabindex=\"0\"><code>Warning: would violate PodSecurity &#34;restricted:latest&#34;: allowPrivilegeEscalation != false (container &#34;busybox&#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container &#34;busybox&#34; must set securityContext.capabilities.drop=[&#34;ALL&#34;]), runAsNonRoot != true (pod or container &#34;busybox&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &#34;busybox&#34; must set securityContext.seccompProfile.type to &#34;RuntimeDefault&#34; or &#34;Localhost&#34;)\npod/busybox-privileged created\n</code></pre><p>Let's delete the pod with <code>kubectl -n test-defaults delete pod/busybox-privileged</code>.</p>\n<p>Is my config even working?</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># if using docker, shell into the control plane</span>\n</span></span><span style=\"display:flex;\"><span>docker <span style=\"color:#a2f\">exec</span> -it kind-control-plane bash\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># cat out the file we mounted</span>\n</span></span><span style=\"display:flex;\"><span>cat /etc/kubernetes/policies/pod-security.yaml\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># check the api server logs</span>\n</span></span><span style=\"display:flex;\"><span>cat /var/log/containers/kube-apiserver*.log\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># check the api server config</span>\n</span></span><span style=\"display:flex;\"><span>cat /etc/kubernetes/manifests/kube-apiserver.yaml\n</span></span></code></pre></div><p><strong>UPDATE:</strong> The baseline policy permits <code>allowPrivilegeEscalation</code>. While I cannot see the Pod Security default levels of enforcement, they are there. Let's try to provide a manifest that violates the baseline by requesting hostNetwork access.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># delete the original pod</span>\n</span></span><span style=\"display:flex;\"><span>kubectl -n test-defaults delete pod busybox-privileged\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>cat <span style=\"color:#b44\">&lt;&lt;EOF | kubectl -n test-defaults apply -f -\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">apiVersion: v1\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">kind: Pod\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">metadata:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> name: busybox-privileged\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">spec:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> containers:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - name: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> image: busybox\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> args:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - sleep\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> - &#34;1000000&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\"> hostNetwork: true\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#b44\">EOF</span>\n</span></span></code></pre></div><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code>Error from server (Forbidden): error when creating &#34;STDIN&#34;: pods &#34;busybox-privileged&#34; is forbidden: violates PodSecurity &#34;baseline:latest&#34;: host namespaces (hostNetwork=true)\n</code></pre><h4 id=\"it-worked\">Yes!!! It worked! üéâüéâüéâ</h4>\n<p>I later found out, another way to check if things are operating as intended is to check the raw API server metrics endpoint.</p>\n<p>Run the following command:</p>\n<pre tabindex=\"0\"><code>kubectl get --raw /metrics | grep pod_security_evaluations_total\n</code></pre><p>The output is similar to this:</p>\n<pre tabindex=\"0\"><code># HELP pod_security_evaluations_total [ALPHA] Number of policy evaluations that occurred, not counting ignored or exempt requests.\n# TYPE pod_security_evaluations_total counter\npod_security_evaluations_total{decision=&#34;allow&#34;,mode=&#34;enforce&#34;,policy_level=&#34;baseline&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;create&#34;,resource=&#34;pod&#34;,subresource=&#34;&#34;} 2\npod_security_evaluations_total{decision=&#34;allow&#34;,mode=&#34;enforce&#34;,policy_level=&#34;privileged&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;create&#34;,resource=&#34;pod&#34;,subresource=&#34;&#34;} 0\npod_security_evaluations_total{decision=&#34;allow&#34;,mode=&#34;enforce&#34;,policy_level=&#34;privileged&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;update&#34;,resource=&#34;pod&#34;,subresource=&#34;&#34;} 0\npod_security_evaluations_total{decision=&#34;deny&#34;,mode=&#34;audit&#34;,policy_level=&#34;baseline&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;create&#34;,resource=&#34;pod&#34;,subresource=&#34;&#34;} 1\npod_security_evaluations_total{decision=&#34;deny&#34;,mode=&#34;enforce&#34;,policy_level=&#34;baseline&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;create&#34;,resource=&#34;pod&#34;,subresource=&#34;&#34;} 1\npod_security_evaluations_total{decision=&#34;deny&#34;,mode=&#34;warn&#34;,policy_level=&#34;restricted&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;create&#34;,resource=&#34;controller&#34;,subresource=&#34;&#34;} 2\npod_security_evaluations_total{decision=&#34;deny&#34;,mode=&#34;warn&#34;,policy_level=&#34;restricted&#34;,policy_version=&#34;latest&#34;,request_operation=&#34;create&#34;,resource=&#34;pod&#34;,subresource=&#34;&#34;} 2\n</code></pre><p>A monitoring tool could ingest these metrics too for reporting, assessments, or measuring trends.</p>\n<h2 id=\"clean-up\">Clean up</h2>\n<p>When finished, delete the kind cluster.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kind delete cluster\n</span></span></code></pre></div><h2 id=\"auditing\">Auditing</h2>\n<p>Auditing is another way to track what policies are being enforced in your cluster. To set up auditing with kind, review the official docs for <a href=\"https://kind.sigs.k8s.io/docs/user/auditing/\">enabling auditing</a>. As of <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.11.md#sig-auth\">version 1.11</a>, Kubernetes audit logs include two annotations that indicate whether or not a request was authorized (<code>authorization.k8s.io/decision</code>) and the reason for the decision (<code>authorization.k8s.io/reason</code>). Audit events can be streamed to a webhook for monitoring, tracking, or alerting.</p>\n<p>The audit events look similar to the following:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>{<span style=\"color:#b44\">&#34;authorization.k8s.io/decision&#34;</span>:<span style=\"color:#b44\">&#34;allow&#34;</span>,<span style=\"color:#b44\">&#34;authorization.k8s.io/reason&#34;</span>:<span style=\"color:#b44\">&#34;&#34;</span>,<span style=\"color:#b44\">&#34;pod-security.kubernetes.io/audit&#34;</span>:<span style=\"color:#b44\">&#34;allowPrivilegeEscalation != false (container \\&#34;busybox\\&#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \\&#34;busybox\\&#34; must set securityContext.capabilities.drop=[\\&#34;ALL\\&#34;]), runAsNonRoot != true (pod or container \\&#34;busybox\\&#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \\&#34;busybox\\&#34; must set securityContext.seccompProfile.type to \\&#34;RuntimeDefault\\&#34; or \\&#34;Localhost\\&#34;)&#34;</span>}}<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Auditing is also a good first step in evaluating your cluster's current compliance with Pod Security. The Kubernetes Enhancement Proposal (KEP) hints at a future where <code>baseline</code> <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md#rollout-of-baseline-by-default-for-unlabeled-namespaces\">could be the default for unlabeled namespaces</a>.</p>\n<p>Example <code>audit-policy.yaml</code> configuration tuned for Pod Security events:</p>\n<pre tabindex=\"0\"><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: RequestResponse\nresources:\n- group: &#34;&#34; # core API group\nresources: [&#34;pods&#34;, &#34;pods/ephemeralcontainers&#34;, &#34;podtemplates&#34;, &#34;replicationcontrollers&#34;]\n- group: &#34;apps&#34;\nresources: [&#34;daemonsets&#34;, &#34;deployments&#34;, &#34;replicasets&#34;, &#34;statefulsets&#34;]\n- group: &#34;batch&#34;\nresources: [&#34;cronjobs&#34;, &#34;jobs&#34;]\nverbs: [&#34;create&#34;, &#34;update&#34;]\nomitStages:\n- &#34;RequestReceived&#34;\n- &#34;ResponseStarted&#34;\n- &#34;Panic&#34;\n</code></pre><p>Once auditing is enabled, look at the configured local file if using <code>--audit-log-path</code> or the destination of a webhook if using <code>--audit-webhook-config-file</code>.</p>\n<p>If using a file (<code>--audit-log-path</code>), run <code>cat /PATH/TO/API/AUDIT.log | grep &quot;is forbidden:&quot;</code> to see all rejected workloads audited.</p>\n<h2 id=\"psp-migrations\">PSP migrations</h2>\n<p>If you're already using PSP, SIG Auth has created a guide and <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/\">published the steps to migrate off of PSP</a>.</p>\n<p>To summarize the process:</p>\n<ul>\n<li>Update all existing PSPs to be non-mutating</li>\n<li>Apply Pod Security policies in <code>warn</code> or <code>audit</code> mode</li>\n<li>Upgrade Pod Security policies to <code>enforce</code> mode</li>\n<li>Remove <code>PodSecurityPolicy</code> from <code>--enable-admission-plugins</code></li>\n</ul>\n<p>Listed as &quot;optional future extensions&quot; and currently out of scope, SIG Auth has kicked around the idea of providing a tool to assist with migrations. More <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md#automated-psp-migration-tooling\">details in the KEP</a>.</p>\n<h2 id=\"wrap-up\">Wrap up</h2>\n<p>Pod Security is a promising new feature that provides an out-of-the-box way to allow users to improve the security posture of their workloads. Like any new enhancement that has matured to beta, we ask that you try it out, provide feedback, or share your experience via either raising a Github issue or joining SIG Auth community meetings. It's our hope that Pod Security will be deployed on every cluster in our ongoing pursuit as a community to make Kubernetes security a priority.</p>\n<p>For a step by step guide on how to enable &quot;baseline&quot; Pod Security Standards with Pod Security Admission feature please refer to these dedicated <a href=\"https://kubernetes.io/docs/tutorials/security/\">tutorials</a> that cover the configuration needed at cluster level and namespace level.</p>\n<h2 id=\"additional-resources\">Additional resources</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Official Pod Security Docs</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/\">Enforce Pod Security Standards with Namespace Labels</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/\">Enforce Pod Security Standards by Configuring the Built-in Admission Controller</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md\">Official Kubernetes Enhancement Proposal</a> (KEP)</li>\n<li><a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">PodSecurityPolicy Deprecation: Past, Present, and Future</a></li>\n<li><a href=\"https://medium.com/@LachlanEvenson/hands-on-with-kubernetes-pod-security-admission-b6cac495cd11\">Hands on with Kubernetes Pod Security</a></li>\n</ul>","PublishedAt":"2021-12-09 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/","SourceName":"Kubernetes"}},{"node":{"ID":1243,"Title":"Blog: Kubernetes 1.23: Dual-stack IPv4/IPv6 Networking Reaches GA","Description":"<p><strong>Author:</strong> Bridget Kromhout (Microsoft)</p>\n<p>&quot;When will Kubernetes have IPv6?&quot; This question has been asked with increasing frequency ever since alpha support for IPv6 was first added in k8s v1.9. While Kubernetes has supported IPv6-only clusters since v1.18, migration from IPv4 to IPv6 was not yet possible at that point. At long last, <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/\">dual-stack IPv4/IPv6 networking</a> has reached general availability (GA) in Kubernetes v1.23.</p>\n<p>What does dual-stack networking mean for you? Let‚Äôs take a look‚Ä¶</p>\n<h2 id=\"service-api-updates\">Service API updates</h2>\n<p><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Services</a> were single-stack before 1.20, so using both IP families meant creating one Service per IP family. The user experience was simplified in 1.20, when Services were re-implemented to allow both IP families, meaning a single Service can handle both IPv4 and IPv6 workloads. Dual-stack load balancing is possible between services running any combination of IPv4 and IPv6.</p>\n<p>The Service API now has new fields to support dual-stack, replacing the single ipFamily field.</p>\n<ul>\n<li>You can select your choice of IP family by setting <code>ipFamilyPolicy</code> to one of three options: SingleStack, PreferDualStack, or RequireDualStack. A service can be changed between single-stack and dual-stack (within some limits).</li>\n<li>Setting <code>ipFamilies</code> to a list of families assigned allows you to set the order of families used.</li>\n<li><code>clusterIPs</code> is inclusive of the previous <code>clusterIP</code> but allows for multiple entries, so it‚Äôs no longer necessary to run duplicate services, one in each of the two IP families. Instead, you can assign cluster IP addresses in both IP families.</li>\n</ul>\n<p>Note that Pods are also dual-stack. For a given pod, there is no possibility of setting multiple IP addresses in the same family.</p>\n<h2 id=\"default-behavior-remains-single-stack\">Default behavior remains single-stack</h2>\n<p>Starting in 1.20 with the re-implementation of dual-stack services as alpha, the underlying networking for Kubernetes has included dual-stack whether or not a cluster was configured with the feature flag to enable dual-stack.</p>\n<p>Kubernetes 1.23 removed that feature flag as part of graduating the feature to stable. Dual-stack networking is always available if you want to configure it. You can set your cluster network to operate as single-stack IPv4, as single-stack IPv6, or as dual-stack IPv4/IPv6.</p>\n<p>While Services are set according to what you configure, Pods default to whatever the CNI plugin sets. If your CNI plugin assigns single-stack IPs, you will have single-stack unless <code>ipFamilyPolicy</code> specifies PreferDualStack or RequireDualStack. If your CNI plugin assigns dual-stack IPs, <code>pod.status.PodIPs</code> defaults to dual-stack.</p>\n<p>Even though dual-stack is possible, it is not mandatory to use it. Examples in the documentation show the variety possible in <a href=\"https://kubernetes.io/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios\">dual-stack service configurations</a>.</p>\n<h2 id=\"try-dual-stack-right-now\">Try dual-stack right now</h2>\n<p>While upstream Kubernetes now supports <a href=\"https://kubernetes.io/docs/concepts/services-networking/dual-stack/\">dual-stack networking</a> as a GA or stable feature, each provider‚Äôs support of dual-stack Kubernetes may vary. Nodes need to be provisioned with routable IPv4/IPv6 network interfaces. Pods need to be dual-stack. The <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/\">network plugin</a> is what assigns the IP addresses to the Pods, so it's the network plugin being used for the cluster that needs to support dual-stack. Some Container Network Interface (CNI) plugins support dual-stack, as does kubenet.</p>\n<p>Ecosystem support of dual-stack is increasing; you can create <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/dual-stack-support/\">dual-stack clusters with kubeadm</a>, try a <a href=\"https://kind.sigs.k8s.io/docs/user/configuration/#ip-family\">dual-stack cluster locally with KIND</a>, and deploy dual-stack clusters in cloud providers (after checking docs for CNI or kubenet availability).</p>\n<h2 id=\"get-involved-with-sig-network\">Get involved with SIG Network</h2>\n<p>SIG-Network wants to learn from community experiences with dual-stack networking to find out more about evolving needs and your use cases. The <a href=\"https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;index=4\">SIG-network update video from KubeCon NA 2021</a> summarizes the SIG‚Äôs recent updates, including dual-stack going to stable in 1.23.</p>\n<p>The current SIG-Network <a href=\"https://github.com/orgs/kubernetes/projects/10\">KEPs</a> and <a href=\"https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork\">issues</a> on GitHub illustrate the SIG‚Äôs areas of emphasis. The <a href=\"https://github.com/kubernetes/enhancements/issues/2438\">dual-stack API server</a> is one place to consider contributing.</p>\n<p><a href=\"https://github.com/kubernetes/community/tree/master/sig-network#meetings\">SIG-Network meetings</a> are a friendly, welcoming venue for you to connect with the community and share your ideas. Looking forward to hearing from you!</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>The dual-stack networking feature represents the work of many Kubernetes contributors. Thanks to all who contributed code, experience reports, documentation, code reviews, and everything in between. Bridget Kromhout details this community effort in <a href=\"https://containerjournal.com/features/dual-stack-networking-in-kubernetes/\">Dual-Stack Networking in Kubernetes</a>. KubeCon keynotes by Tim Hockin &amp; Khaled (Kal) Henidak in 2019 (<a href=\"https://www.youtube.com/watch?v=o-oMegdZcg4\">The Long Road to IPv4/IPv6 Dual-stack Kubernetes</a>) and by Lachlan Evenson in 2021 (<a href=\"https://www.youtube.com/watch?v=lVrt8F2B9CM\">And Here We Go: Dual-stack Networking in Kubernetes</a>) talk about the dual-stack journey, spanning five years and a great many lines of code.</p>","PublishedAt":"2021-12-08 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/08/dual-stack-networking-ga/","SourceName":"Kubernetes"}},{"node":{"ID":1244,"Title":"Blog: Kubernetes 1.23: The Next Frontier","Description":"<p><strong>Authors:</strong> <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.23/release-team.md\">Kubernetes 1.23 Release Team</a></p>\n<p>We‚Äôre pleased to announce the release of Kubernetes 1.23, the last release of 2021!</p>\n<p>This release consists of 47 enhancements: 11 enhancements have graduated to stable, 17 enhancements are moving to beta, and 19 enhancements are entering alpha. Also, 1 feature has been deprecated.</p>\n<h2 id=\"major-themes\">Major Themes</h2>\n<h3 id=\"deprecation-of-flexvolume\">Deprecation of FlexVolume</h3>\n<p>FlexVolume is deprecated. The out-of-tree CSI driver is the recommended way to write volume drivers in Kubernetes. See <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors\">this doc</a> for more information. Maintainers of FlexVolume drivers should implement a CSI driver and move users of FlexVolume to CSI. Users of FlexVolume should move their workloads to the CSI driver.</p>\n<h3 id=\"deprecation-of-klog-specific-flags\">Deprecation of klog specific flags</h3>\n<p>To simplify the code base, several <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog\">logging flags were marked as deprecated</a> in Kubernetes 1.23. The code which implements them will be removed in a future release, so users of those need to start replacing the deprecated flags with some alternative solutions.</p>\n<h3 id=\"software-supply-chain-slsa-level-1-compliance-in-the-kubernetes-release-process\">Software Supply Chain SLSA Level 1 Compliance in the Kubernetes Release Process</h3>\n<p>Kubernetes releases now generate provenance attestation files describing the staging and release phases of the release process. Artifacts are now verified as they are handed over from one phase to the next. This final piece completes the work needed to comply with Level 1 of the <a href=\"https://slsa.dev/\">SLSA security framework</a> (Supply-chain Levels for Software Artifacts).</p>\n<h3 id=\"ipv4-ipv6-dual-stack-networking-graduates-to-ga\">IPv4/IPv6 Dual-stack Networking graduates to GA</h3>\n<p><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack\">IPv4/IPv6 dual-stack networking</a> graduates to GA. Since 1.21, Kubernetes clusters have been enabled to support dual-stack networking by default. In 1.23, the <code>IPv6DualStack</code> feature gate is removed. The use of dual-stack networking is not mandatory. Although clusters are enabled to support dual-stack networking, Pods and Services continue to default to single-stack. To use dual-stack networking Kubernetes nodes must have routable IPv4/IPv6 network interfaces, a dual-stack capable CNI network plugin must be used, Pods must be configured to be dual-stack and Services must have their <code>.spec.ipFamilyPolicy</code> field set to either <code>PreferDualStack</code> or <code>RequireDualStack</code>.</p>\n<h3 id=\"horizontalpodautoscaler-v2-graduates-to-ga\">HorizontalPodAutoscaler v2 graduates to GA</h3>\n<p>The HorizontalPodAutscaler <code>autoscaling/v2</code> stable API moved to GA in 1.23. The HorizontalPodAutoscaler <code>autoscaling/v2beta2</code> API has been deprecated.</p>\n<h3 id=\"generic-ephemeral-volume-feature-graduates-to-ga\">Generic Ephemeral Volume feature graduates to GA</h3>\n<p>The generic ephemeral volume feature moved to GA in 1.23. This feature allows any existing storage driver that supports dynamic provisioning to be used as an ephemeral volume with the volume‚Äôs lifecycle bound to the Pod. All StorageClass parameters for volume provisioning and all features supported with PersistentVolumeClaims are supported.</p>\n<h3 id=\"skip-volume-ownership-change-graduates-to-ga\">Skip Volume Ownership change graduates to GA</h3>\n<p>The feature to configure volume permission and ownership change policy for Pods moved to GA in 1.23. This allows users to skip recursive permission changes on mount and speeds up the pod start up time.</p>\n<h3 id=\"allow-csi-drivers-to-opt-in-to-volume-ownership-and-permission-change-graduates-to-ga\">Allow CSI drivers to opt-in to volume ownership and permission change graduates to GA</h3>\n<p>The feature to allow CSI Drivers to declare support for fsGroup based permissions graduates to GA in 1.23.</p>\n<h3 id=\"podsecurity-graduates-to-beta\">PodSecurity graduates to Beta</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">PodSecurity</a> moves to Beta. <code>PodSecurity</code> replaces the deprecated <code>PodSecurityPolicy</code> admission controller. <code>PodSecurity</code> is an admission controller that enforces Pod Security Standards on Pods in a Namespace based on specific namespace labels that set the enforcement level. In 1.23, the <code>PodSecurity</code> feature gate is enabled by default.</p>\n<h3 id=\"container-runtime-interface-cri-v1-is-default\">Container Runtime Interface (CRI) v1 is default</h3>\n<p>The Kubelet now supports the CRI <code>v1</code> API, which is now the project-wide default.\nIf a container runtime does not support the <code>v1</code> API, Kubernetes will fall back to the <code>v1alpha2</code> implementation. There is no intermediate action required by end-users, because <code>v1</code> and <code>v1alpha2</code> do not differ in their implementation. It is likely that <code>v1alpha2</code> will be removed in one of the future Kubernetes releases to be able to develop <code>v1</code>.</p>\n<h3 id=\"structured-logging-graduate-to-beta\">Structured logging graduate to Beta</h3>\n<p>Structured logging reached its Beta milestone. Most log messages from kubelet and kube-scheduler have been converted. Users are encouraged to try out JSON output or parsing of the structured text format and provide feedback on possible solutions for the open issues, such as handling of multi-line strings in log values.</p>\n<h3 id=\"simplified-multi-point-plugin-configuration-for-scheduler\">Simplified Multi-point plugin configuration for scheduler</h3>\n<p>The kube-scheduler is adding a new, simplified config field for Plugins to allow multiple extension points to be enabled in one spot. The new <code>multiPoint</code> plugin field is intended to simplify most scheduler setups for administrators. Plugins that are enabled via <code>multiPoint</code> will automatically be registered for each individual extension point that they implement. For example, a plugin that implements Score and Filter extensions can be simultaneously enabled for both. This means entire plugins can be enabled and disabled without having to manually edit individual extension point settings. These extension points can now be abstracted away due to their irrelevance for most users.</p>\n<h3 id=\"csi-migration-updates\">CSI Migration updates</h3>\n<p>CSI Migration enables the replacement of existing in-tree storage plugins such as <code>kubernetes.io/gce-pd</code> or <code>kubernetes.io/aws-ebs</code> with a corresponding CSI driver.\nIf CSI Migration is working properly, Kubernetes end users shouldn‚Äôt notice a difference.\nAfter migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.</p>\n<ul>\n<li>CSI Migration feature is turned on by default but stays in Beta for GCE PD, AWS EBS, and Azure Disk in 1.23.</li>\n<li>CSI Migration is introduced as an Alpha feature for Ceph RBD and Portworx in 1.23.</li>\n</ul>\n<h3 id=\"expression-language-validation-for-crd-is-alpha\">Expression language validation for CRD is alpha</h3>\n<p>Expression language validation for CRD is in alpha starting in 1.23. If the <code>CustomResourceValidationExpressions</code> feature gate is enabled, custom resources will be validated by validation rules using the <a href=\"https://github.com/google/cel-spec\">Common Expression Language (CEL)</a>.</p>\n<h3 id=\"server-side-field-validation-is-alpha\">Server Side Field Validation is Alpha</h3>\n<p>If the <code>ServerSideFieldValidation</code> feature gate is enabled starting 1.23, users will receive warnings from the server when they send Kubernetes objects in the request that contain unknown or duplicate fields. Previously unknown fields and all but the last duplicate fields would be dropped by the server.</p>\n<p>With the feature gate enabled, we also introduce the <code>fieldValidation</code> query parameter so that users can specify the desired behavior of the server on a per request basis. Valid values for the <code>fieldValidation</code> query parameter are:</p>\n<ul>\n<li>Ignore (default when feature gate is disabled, same as pre-1.23 behavior of dropping/ignoring unkonwn fields)</li>\n<li>Warn (default when feature gate is enabled).</li>\n<li>Strict (this will fail the request with an Invalid Request error)</li>\n</ul>\n<h3 id=\"openapi-v3-is-alpha\">OpenAPI v3 is Alpha</h3>\n<p>If the <code>OpenAPIV3</code> feature gate is enabled starting 1.23, users will be able to request the OpenAPI v3.0 spec for all Kubernetes types. OpenAPI v3 aims to be fully transparent and includes support for a set of fields that are dropped when publishing OpenAPI v2: <code>default</code>, <code>nullable</code>, <code>oneOf</code>, <code>anyOf</code>. A separate spec is published per Kubernetes group version (at the <code>$cluster/openapi/v3/apis/&lt;group&gt;/&lt;version&gt;</code> endpoint) for improved performance and discovery, for all group versions can be found at the <code>$cluster/openapi/v3</code> path.</p>\n<h2 id=\"other-updates\">Other Updates</h2>\n<h3 id=\"graduated-to-stable\">Graduated to Stable</h3>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/563\">IPv4/IPv6 Dual-Stack Support</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/695\">Skip Volume Ownership Change</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/592\">TTL After Finished Controller</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1682\">Config FSGroup Policy in CSI Driver object</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1698\">Generic Ephemeral Inline Volumes</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1933\">Defend Against Logging Secrets via Static Analysis</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2365\">Namespace Scoped Ingress Class Parameters</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2420\">Reducing Kubernetes Build Maintenance</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/2702\">Graduate HPA API to GA</a></li>\n</ul>\n<h3 id=\"major-changes\">Major Changes</h3>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/issues/1040\">Priority and Fairness for API Server Requests</a></li>\n</ul>\n<h3 id=\"release-notes\">Release Notes</h3>\n<p>Check out the full details of the Kubernetes 1.23 release in our <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md\">release notes</a>.</p>\n<h3 id=\"availability\">Availability</h3>\n<p>Kubernetes 1.23 is available for download on <a href=\"https://github.com/kubernetes/kubernetes/releases/tag/v1.23.0\">GitHub</a>. To get started with Kubernetes, check out these <a href=\"https://kubernetes.io/docs/tutorials/\">interactive tutorials</a> or run local Kubernetes clusters using Docker container ‚Äúnodes‚Äù with <a href=\"https://kind.sigs.k8s.io/\">kind</a>. You can also easily install 1.23 using <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\">kubeadm</a>.</p>\n<h3 id=\"release-team\">Release Team</h3>\n<p>This release was made possible by a very dedicated group of individuals, who came together as a team to deliver technical content, documentation, code, and a host of other components that go into every Kubernetes release.</p>\n<p>A huge thank you to the release lead Rey Lejano for leading us through a successful release cycle, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.23 release for the community.</p>\n<h3 id=\"release-theme-and-logo\">Release Theme and Logo</h3>\n<p><strong>Kubernetes 1.23: The Next Frontier</strong></p>\n<figure class=\"release-logo\">\n<img src=\"https://kubernetes.io/images/blog/2021-12-07-kubernetes-release-1.23/kubernetes-1.23.png\"/>\n</figure>\n<p>&quot;The Next Frontier&quot; theme represents the new and graduated enhancements in 1.23, Kubernetes' history of Star Trek references, and the growth of community members in the release team.</p>\n<p>Kubernetes has a history of Star Trek references. The original codename for Kubernetes within Google is Project 7, a reference to Seven of Nine from Star Trek Voyager. And of course Borg was the name for the predecessor to Kubernetes. &quot;The Next Frontier&quot; theme continues the Star Trek references. &quot;The Next Frontier&quot; is a fusion of two Star Trek titles, Star Trek V: The Final Frontier and Star Trek the Next Generation.</p>\n<p>&quot;The Next Frontier&quot; represents a line in the SIG Release charter, &quot;Ensure there is a consistent group of community members in place to support the release process across time.&quot; With each release team, we grow the community with new release team members and for many it's their first contribution in their open source frontier.</p>\n<p>Reference: <a href=\"https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/\">https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/</a>\nReference: <a href=\"https://github.com/kubernetes/community/blob/master/sig-release/charter.md\">https://github.com/kubernetes/community/blob/master/sig-release/charter.md</a></p>\n<p>The Kubernetes 1.23 release logo continues with the theme's Star Trek reference. Every star is a helm from the Kubernetes logo. The ship represents the collective teamwork of the release team.</p>\n<p>Rey Lejano designed the logo.</p>\n<h3 id=\"user-highlights\">User Highlights</h3>\n<ul>\n<li><a href=\"https://www.cncf.io/announcements/2021/09/22/cncf-end-user-technology-radar-provides-insights-into-devsecops/\">Findings of the latest CNCF End User Technology Radar</a> were themed around DevSecOps. Check out the <a href=\"https://radar.cncf.io/\">Radar Page</a> for the full details and findings.</li>\n<li>Learn about how <a href=\"https://www.cncf.io/case-studies/aegon-life-india/\">end user Aegon Life India migrated core processes from its traditional monolith to a microservice-based architecture</a> in its effort to transform into a leading digital service company.</li>\n<li>Utilizing multiple cloud native projects, <a href=\"https://www.cncf.io/case-studies/seagate/\">Seagate engineered edgerX to run Real-time Analytics at the Edge</a>.</li>\n<li>Check out how <a href=\"https://www.cncf.io/case-studies/zambon/\">Zambon worked with SparkFabrik to develop 16 websites, with cloud native technologies, to enable stakeholders to easily update content while maintaining a consistent brand identity</a>.</li>\n<li>Using Kubernetes, <a href=\"https://www.cncf.io/case-studies/influxdata/\">InfluxData was able to deliver on the promise of multi-cloud, multi-region service availability</a> by creating a true cloud abstraction layer that allows for the seamless delivery of InfluxDB as a single application to multiple global clusters across three major cloud providers.</li>\n</ul>\n<h3 id=\"ecosystem-updates\">Ecosystem Updates</h3>\n<ul>\n<li><a href=\"https://www.cncf.io/events/kubecon-cloudnativecon-north-america-2021/\">KubeCon + CloudNativeCon NA 2021</a> was held in October 2021, both online and in person. All talks are <a href=\"https://www.youtube.com/playlist?list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP\">now available on-demand</a> for anyone that would like to catch up!</li>\n<li><a href=\"https://www.cncf.io/announcements/2021/11/18/kubernetes-and-cloud-native-essentials-training-and-kcna-certification-now-available/\">Kubernetes and Cloud Native Essentials Training and KCNA Certification are now generally available for enrollment and scheduling</a>. Additionally, a new online training course, <a href=\"https://www.cncf.io/announcements/2021/10/13/entry-level-kubernetes-certification-to-help-advance-cloud-careers/\">Kubernetes and Cloud Native Essentials (LFS250)</a>, has been released to both prepare individuals for entry-level cloud roles and to sit for the KCNA exam.</li>\n<li><a href=\"https://www.cncf.io/announcements/2021/10/13/inclusive-naming-initiative-announces-new-community-resources-for-a-more-inclusive-future/\">New resources are now available from the Inclusive Naming Initiative</a>, including an Inclusive Strategies for Open Source (LFC103) course, Language Evaluation Framework, and Implementation Path.</li>\n</ul>\n<h3 id=\"project-velocity\">Project Velocity</h3>\n<p>The <a href=\"https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m\">CNCF K8s DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p>\n<p>In the v1.23 release cycle, which ran for 16 weeks (August 23 to December 7), we saw contributions from <a href=\"https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.22.0%20-%20now&amp;var-metric=contributions\">1032 companies</a> and <a href=\"https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.22.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes\">1084 individuals</a>.</p>\n<h3 id=\"event-update\">Event Update</h3>\n<ul>\n<li><a href=\"https://www.lfasiallc.com/kubecon-cloudnativecon-open-source-summit-china/\">KubeCon + CloudNativeCon China 2021</a> is happening this month from December 9 - 11. After taking a break last year, the event will be virtual this year and includes 105 sessions. Check out the event schedule <a href=\"https://www.lfasiallc.com/kubecon-cloudnativecon-open-source-summit-china/program/schedule/\">here</a>.</li>\n<li>KubeCon + CloudNativeCon Europe 2022 will take place in Valencia, Spain, May 4 ‚Äì 7, 2022! You can find more information about the conference and registration on the <a href=\"https://events.linuxfoundation.org/archive/2021/kubecon-cloudnativecon-europe/\">event site</a>.</li>\n<li>Kubernetes Community Days has upcoming events scheduled in Pakistan, Brazil, Chengdu, and in Australia.</li>\n</ul>\n<h3 id=\"upcoming-release-webinar\">Upcoming Release Webinar</h3>\n<p>Join members of the Kubernetes 1.23 release team on January 4, 2022 to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the <a href=\"https://community.cncf.io/e/mrey9h/\">event page</a> on the CNCF Online Programs site.</p>\n<h3 id=\"get-involved\">Get Involved</h3>\n<p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests. Have something you‚Äôd like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below:</p>\n<ul>\n<li>Find out more about contributing to Kubernetes at the <a href=\"https://www.kubernetes.dev/\">Kubernetes Contributors</a> website</li>\n<li>Follow us on Twitter <a href=\"https://twitter.com/kubernetesio\">@Kubernetesio</a> for the latest updates</li>\n<li>Join the community discussion on <a href=\"https://discuss.kubernetes.io/\">Discuss</a></li>\n<li>Join the community on <a href=\"http://slack.k8s.io/\">Slack</a></li>\n<li>Post questions (or answer questions) on <a href=\"http://stackoverflow.com/questions/tagged/kubernetes\">Stack Overflow</a></li>\n<li>Share your Kubernetes <a href=\"https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform\">story</a></li>\n<li>Read more about what‚Äôs happening with Kubernetes on the <a href=\"https://kubernetes.io/blog/\">blog</a></li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/sig-release/tree/master/release-team\">Kubernetes Release Team</a></li>\n</ul>","PublishedAt":"2021-12-07 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/","SourceName":"Kubernetes"}},{"node":{"ID":1245,"Title":"Blog: Contribution, containers and cricket: the Kubernetes 1.22 release interview","Description":"<p><strong>Author</strong>: Craig Box (Google)</p>\n<p>The Kubernetes release train rolls on, and we look ahead to the release of 1.23 next week. <a href=\"https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog\">As is our tradition</a>, I'm pleased to bring you a look back at the process that brought us the previous version.</p>\n<p>The release team for 1.22 was led by <a href=\"https://twitter.com/coffeeartgirl\">Savitha Raghunathan</a>, who was, at the time, a Senior Platform Engineer at MathWorks. <a href=\"https://kubernetespodcast.com/episode/157-kubernetes-1.22/\">I spoke to Savitha</a> on the <a href=\"https://kubernetespodcast.com/\">Kubernetes Podcast from Google</a>, the weekly<super>*</super> show covering the Kubernetes and Cloud Native ecosystem.</p>\n<p>Our release conversations shine a light on the team that puts together each Kubernetes release. Make sure you <a href=\"https://kubernetespodcast.com/subscribe/\">subscribe, wherever you get your podcasts</a> so you catch the story of 1.23.</p>\n<p>And in case you're interested in why the show has been on a hiatus the last few weeks, all will be revealed in the next episode!</p>\n<p><em>This transcript has been lightly edited and condensed for clarity.</em></p>\n<hr>\n<p><strong>CRAIG BOX: Welcome to the show, Savitha.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Hey, Craig. Thanks for having me on the show. How are you today?</p>\n<p><strong>CRAIG BOX: I'm very well, thank you. I've interviewed a lot of people on the show, and you're actually the first person who's asked that of me.</strong></p>\n<p>SAVITHA RAGHUNATHAN: I'm glad. It's something that I always do. I just want to make sure the other person is good and happy.</p>\n<p><strong>CRAIG BOX: That's very kind of you. Thank you for kicking off on a wonderful foot there. I want to ask first of all ‚Äî you grew up in Chennai. My association with Chennai is the <a href=\"https://en.wikipedia.org/wiki/Chennai_Super_Kings\">Super Kings cricket team</a>. Was cricket part of your upbringing?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Yeah. Actually, a lot. My mom loves watching cricket. I have a younger brother, and when we were growing up, we used to play cricket on the terrace. Everyone surrounding me, my best friends ‚Äî and even now, my partner ‚Äî loves watching cricket, too. Cricket is a part of my life.</p>\n<p>I stopped watching it a while ago, but I still enjoy a good game.</p>\n<p><strong>CRAIG BOX: It's probably a bit harder in the US. Everything's in a different time zone. I find, with my cricket team being on the other side of the world, that it's a lot easier when they're playing near me, as opposed to trying to keep up with what they're doing when they're playing at 3:00 in the morning.</strong></p>\n<p>SAVITHA RAGHUNATHAN: That is actually one of the things that made me lose touch with cricket. I'm going to give you a piece of interesting information. I never supported Chennai Super Kings. I always supported <a href=\"https://en.wikipedia.org/wiki/Royal_Challengers_Bangalore\">Royal Challengers of Bangalore</a>.</p>\n<p>I once went to the stadium, and it was a match between the Chennai Super Kings and the RCB. I was the only one who was cheering whenever the RCB hit a 6, or when they were scoring. I got the stares of thousands of people looking at me. I'm like, &quot;what are you doing?&quot; My friends are like, &quot;you're going to get us killed! Just stop screaming!&quot;</p>\n<p><strong>CRAIG BOX: I hear you. As a New Zealander in the UK, there are a lot of international cricket matches I've been to where I am one of the few people dressed in the full beige kit. But I have to ask, why an affiliation with a different team?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I'm not sure. When the IPL came out, I really liked Virat Kohli. He was playing for RCB at that time, and I think pretty much that's it.</p>\n<p><strong>CRAIG BOX: Well, what I know about the Chennai Super Kings is that their coach is New Zealand's finest batsmen and <a href=\"https://www.youtube.com/watch?v=vSZAaUCAclw\">air conditioning salesman</a>, <a href=\"https://en.wikipedia.org/wiki/Stephen_Fleming\">Stephen Fleming</a>.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Oh, really?</p>\n<p><strong>CRAIG BOX: Yeah, he's a dead ringer for the guy who played the <a href=\"https://s1.reutersmedia.net/resources/r/?m=02&amp;d=20061130&amp;t=2&amp;i=153531&amp;w=&amp;fh=545px&amp;fw=&amp;ll=&amp;pl=&amp;sq=&amp;r=153531\">yellow Wiggle</a> back in the day.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Oh, interesting. I remember the name, but I cannot put the picture and the name together. I stopped watching cricket once I moved to the States. Then, all my focus was on studies and extracurriculars. I have always been an introvert. The campus ‚Äî it was a new thing for me ‚Äî they had international festivals.</p>\n<p>And every week, they'd have some kind of new thing going on, so I'd go check them out. I wouldn't participate, but I did go out and check them out. That was a big feat for me around that time because a lot of people ‚Äî and still, even now, a lot of people ‚Äî they kind of scare me. I don't know how to make a conversation with everyone.</p>\n<p>I'll just go and say, &quot;hi, how are you? OK, I'm good. I'm just going to move on&quot;. And I'll just go to the next person. And after two hours, I'm out of that place.</p>\n<p><strong>CRAIG BOX: Perhaps a pleasant side effect of the last 12 months ‚Äî a lot fewer gatherings of people.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Could be that, but I'm so excited about KubeCon. But when I think about it, I'm like &quot;oh my God. There's going to be a lot of people. What am I going to do? I'm going to meet all my friends over there&quot;.</p>\n<p>Sometimes I have social anxiety like, what's going to happen?</p>\n<p><strong>CRAIG BOX: What's going to happen is you're going to ask them how they are at the beginning, and they're immediately going to be set at ease.</strong></p>\n<p>SAVITHA RAGHUNATHAN: <em>laughs</em> I hope so.</p>\n<p><strong>CRAIG BOX: Let's talk a little bit, then, about your transition from India to the US. You did your undergraduate degree in computer science at the SSN College of Engineering. How did you end up at Arizona State?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I always wanted to pursue higher studies when I was in India, and I didn't have the opportunity immediately. Once I graduated from my school there, I went and I worked for a couple of years. My aim was always to get out of there and come here, do my graduate studies.</p>\n<p>Eventually, I want to do a PhD. I have an idea of what I want to do. I always wanted to keep studying. If there's an option that I could just keep studying and not do work or anything of that sort, I'd just pick that other one ‚Äî I'll just keep studying.</p>\n<p>But unfortunately, you need money and other things to live and sustain in this world. So I'm like, OK, I'll take a break from studies, and I will work for a while.</p>\n<p><strong>CRAIG BOX: The road to success is littered with dreams of PhDs. I have a lot of friends who thought that that was the path they were going to take, and they've had a beautiful career and probably aren't going to go back to study. Did you use the <a href=\"https://en.wikipedia.org/wiki/MATLAB\">Matlab</a> software at all while you were going through your schooling?</strong></p>\n<p>SAVITHA RAGHUNATHAN: No, unfortunately. That is a question that everyone asks. I have not used Matlab. I haven't used it even now. I don't use it for work. I didn't have any necessity for my school work. I didn't have anything to do with Matlab. I never analysed, or did data processing, or anything, with Matlab. So unfortunately, no.</p>\n<p>Everyone asks me like, you're working at <a href=\"https://en.wikipedia.org/wiki/MathWorks\">MathWorks</a>. Have you used Matlab? I'm like, no.</p>\n<p><strong>CRAIG BOX: Fair enough. Nor have I. But it's been around since the late 1970s, so I imagine there are a lot of people who will have come across it at some point. Do you work with a lot of people who have been working on it that whole time?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Kind of. Not all the time, but I get to meet some folks who work on the product itself. Most of my interactions are with the infrastructure team and platform engineering teams at MathWorks. One other interesting fact is that when I joined the company ‚Äî MathWorks has an extensive internal curriculum for training and learning, which I really love. They have an &quot;Intro to Matlab&quot; course, and that's on my bucket of things to do.</p>\n<p>It was like 500 years ago. I added it, and I never got to it. I'm like, OK, maybe this year at least I want to get to it and I want to learn something new. My partner used Matlab extensively. He misses it right now at his current employer. And he's like, &quot;you have the entire licence! You have access to the entire suite and you haven't used it?&quot; I'm like, &quot;no!&quot;</p>\n<p><strong>CRAIG BOX: Well, I have bad news for the idea of you doing a PhD, I'm sorry.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Another thing is that none of my family knew about the company MathWorks and Matlab. The only person who knew was my younger brother. He was so proud. He was like, &quot;oh my God&quot;.</p>\n<p>When he was 12 years old, he started getting involved in robotics and all that stuff. That's how he got introduced to Matlab. He goes absolutely bananas for the swag. So all the t-shirts, all the hoodies ‚Äî any swag that I get from MathWorks goes to him, without saying.</p>\n<p>Over the five, six years, the things that I've got ‚Äî there was only one sweatshirt that I kept for myself. Everything else I've just given to him. And he cherishes it. He's the only one in my family who knew about Matlab and MathWorks.</p>\n<p>Now, everyone knows, because I'm working there. They were initially like, I don't even know that company name. Is it like Amazon? I'm like, no, we make software that can send people to the moon. And we also make software that can do amazing robotic surgeries and even make a car drive on its own. That's something that I take immense pride in.</p>\n<p>I know I don't directly work on the product, but I'm enabling the people who are creating the product. I'm really, really proud of that.</p>\n<p><strong>CRAIG BOX: I think Jeff Bezos is working on at least two out of three of those disciplines that you mentioned before, so it's maybe a little bit like Amazon. One thing I've always thought about Matlab is that, because it's called Matlab, it solves that whole problem where <a href=\"https://www.grammar.com/math_vs._maths\">Americans call it math, and the rest of the world call it maths</a>. Why do Americans think there's only one math?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Definitely. I had trouble ‚Äî growing up in India, it's always British English. And I had so much trouble when I moved here. So many things changed.</p>\n<p>One of the things is maths. I always got used to writing maths, physics, and everything.</p>\n<p><strong>CRAIG BOX: They don't call it &quot;physic&quot; in the US, do they?</strong></p>\n<p>SAVITHA RAGHUNATHAN: No, no, they don't. Luckily, they don't. That still stays &quot;physics&quot;. But math ‚Äî I had trouble. It's maths. Even when you do the full abbreviations like mathematics and you are still calling it math, I'm like, mm.</p>\n<p><strong>CRAIG BOX: They can do the computer science abbreviation thing and call it math-7-S or whatever the number of letters is.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Just like Kubernetes. K-8-s.</p>\n<p><strong>CRAIG BOX: Your path to Kubernetes is through MathWorks. They started out as a company making software which was distributed in a physical sense ‚Äî boxed copies, if you will. I understand now there is a cloud version. Can I assume that that is where the two worlds intersect?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Kind of. I have interaction with the team that supports Matlab on the cloud, but I don't get to work with them on a day-to-day basis. They use Docker containers, and they are building the platform using Kubernetes. So yeah, a little bit of that.</p>\n<p><strong>CRAIG BOX: So what exactly is the platform that you are engineering day to day?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Providing Kubernetes as a platform, obviously ‚Äî that goes without saying ‚Äî to some of the internal development teams. In the future we might expand it to more teams within the company. That is a focus area right now, so that's what we are doing. In the process, we might even get to work with the people who are deploying Matlab on the cloud, which is exciting.</p>\n<p><strong>CRAIG BOX: Now, your path to contribution to Kubernetes, you've said before, was through <a href=\"https://github.com/kubernetes/website/pull/15588\">fixing a 404 error on the Kubernetes.io website</a>. Do you remember what the page was?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I do. I was going to something for work, and I came across this changelog. In Kubernetes there's a nice page ‚Äî once you got to the release page, there would be a long list of changelogs.</p>\n<p>One of the things that I fixed was, the person who worked on the feature had changed their GitHub handle, and that wasn't reflected on this page. So that was my first. I got curious and clicked on the links. One of the links was the handle, and that went to a 404. And I was like &quot;Yeah, I'll just fix that. They have done all the hard work. They can get the credit that's due&quot;.</p>\n<p>It was easy. It wasn't overwhelming for me to pick it up as my first issue. Before that I logged on around Kubernetes for about six to eight months without doing anything because it was just a lot.</p>\n<p><strong>CRAIG BOX: One of the other things that you said about your initial contribution is that you had to learn how to use Git. As a very powerful tool, I find Git is a high barrier to entry for even contributing code to a project. When you want to contribute a blog post or documentation or a fix like you did before, I find it almost impossible to think how a new user would come along and do that. What was your process? Do you think that there's anything we can do to make that barrier lower for new contributors?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Of course. There are more and more tutorials available these days. There is a new contributor workshop. They actually have a <a href=\"https://www.kubernetes.dev/docs/guide/github-workflow/\">GitHub workflow section</a>, <a href=\"https://www.kubernetes.dev/docs/guide/pull-requests/\">how to do a pull request</a> and stuff like that. I know a couple of folks from SIG Docs that are working on which Git commands that you need, or how to get to writing something small and getting it committed. But more tutorials or more links to intro to Git would definitely help.</p>\n<p>The thing is also, someone like a documentation writer ‚Äî they don't actually want to know the entirety of Git. Honestly, it's an ocean. I don't know how to do it. Most of the time, I still ask for help even though I work with Git on a day to day basis. There are several articles and a lot of help is available already within the community. Maybe we could just add a couple more to <a href=\"https://kubernetes.dev/\">kubernetes.dev</a>. That is an amazing site for all the new contributors and existing contributors who want to build code, who want to write documentation.</p>\n<p>We could just add a tutorial there like, &quot;hey, don't know Git, you are new to Git? You just need to know these main things&quot;.</p>\n<p><strong>CRAIG BOX: I find it a shame, to be honest, that people need to use Git for that, by comparison to Wikipedia where you can come along, and even though it might be written in Markdown or something like it, it seems like the barrier is a lot lower. Similar to you, I always have to look up anything more complicated than the five or six Git commands that I use on a day to day basis. Even to do simple things, I basically just go and follow a recipe which I find on the internet.</strong></p>\n<p>SAVITHA RAGHUNATHAN: This is how I got introduced to one of the amazing mentors in Kubernetes. Everyone knows him by his handle, Dims. It was my second PR to the Kubernetes website, and I made a mistake. I destroyed the Git history. I could not push my reviews and comments ‚Äî I addressed them. I couldn't push them back.</p>\n<p>My immediate thought was to delete it and recreate, do another pull request. But then I was like, &quot;what happens to others who have already put effort into reviewing them?&quot; I asked for help, and Dims was there.</p>\n<p>I would say I just got lucky he was there. And he was like, &quot;OK, let me walk you through&quot;. We did troubleshooting through Slack messages. I copied and pasted all the errors. Every single command that he said, I copied and pasted. And then he was like, &quot;OK, run this one. Try this one. And do this one&quot;.</p>\n<p>Finally, I got it fixed. So you know what I did? I went and I stored the command history somewhere local for the next time when I run into this problem. Luckily, I haven't. But I find the contributors so helpful. They are busy. They have a lot of things to do, but they take moments to stop and help someone who's new.</p>\n<p>That is also another part of the reason why I stay ‚Äî I want to contribute more. It's mainly the community. It's the Kubernetes community. I know you asked me about Git, and I just took the conversation to the Kubernetes community. That's how my brain works.</p>\n<p><strong>CRAIG BOX: A lot of people in the community do that and think that's fantastic, obviously, people like Dims who are just floating around on Slack and seem to have endless time. I don't know how they do it.</strong></p>\n<p>SAVITHA RAGHUNATHAN: I really want to know the secret for endless time. If I only had 48 hours in a day. I would sleep for 16 hours, and I would use the rest of the time for doing the things that I want.</p>\n<p><strong>CRAIG BOX: If I had a chance to sleep up to 48 hours a day, I think it'd be a lot more than 16.</strong></p>\n<p><strong>Now, one of the areas that you've been contributing to Kubernetes is in the release team. In 1.18, you were a shadow for the docs role. You led that role in 1.19. And you were a release lead shadow for versions 1,20 and 1.21 before finally leading this release, 1.22, which we will talk about soon.</strong></p>\n<p><strong>How did you get involved? And how did you decide which roles to take as you went through that process?</strong></p>\n<p>SAVITHA RAGHUNATHAN: That is a topic I love to talk about. This was fresh when I started learning about Kubernetes and using Kubernetes at work. And I got so much help from the community, I got interested in contributing back.</p>\n<p>At the first KubeCon that I attended in 2018, in Seattle, they had a speed mentoring session. Now they call it &quot;pod mentoring&quot;. I went to the session, and said, &quot;hey, I want to contribute. I don't know where to start&quot;. And I got a lot of information on how to get started.</p>\n<p>One of the places was SIG Release and the release team. I came back and diligently attended all the SIG Release meetings for four to six months. And in between, I applied to the Kubernetes release team ‚Äî 1.14 and 1.15. I didn't get through. So I took a little bit of a break, and I focused on doing some documentation work. Then I applied for 1.18.</p>\n<p>Since I was already working on some kinds of ‚Äî not like full fledged &quot;documentation&quot; documentation, I still don't write. I eventually want to write something really nice and full fledged documentation like other awesome folks.</p>\n<p><strong>CRAIG BOX: You'll need a lot more than 48 hours in your day to do that.</strong></p>\n<p>SAVITHA RAGHUNATHAN: <em>laughing</em> That's how I applied for the docs role, because I know a little bit about the website. I've done a few pull requests and commits. That's how I got started. I applied for that one role, and I got selected for the 1.18 team. That's how my journey just took off.</p>\n<p>And the next release, I was leading the documentation team. And as everyone knows, the pandemic hit. It was one of the longest releases. I could lean back on the community. I would just wait for the release team meetings.</p>\n<p>It was my way of coping with the pandemic. It took my mind off. It was actually more than a release team, they were people. They were all people first, and we took care of each other. So it felt good.</p>\n<p>And then, I became a release lead shadow for 1.20 and 1.21 because I wanted to know more. I wanted to learn more. I wasn't ready. I still don't feel ready, but I have led 1.22. So if I could do it, anyone could do it.</p>\n<p><strong>CRAIG BOX: How much of this work is day job?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I am lucky to be blessed with an awesome team. I do most of my work after work, but there have been times where I have to take meetings and attend to immediate urgent stuff. During the time of exception requests and stuff like that, I take a little bit of time from my work.</p>\n<p>My team has been wonderful: they support me in all possible ways, and the management as well. Other than the meetings, I don't do much of the work during the day job. It just takes my focus and attention away too much, and I end up having to spend a lot of time sitting in front of the computer, which I don't like.</p>\n<p>Before the pandemic I had a good work life balance. I'd just go to work at 7:00, 7:30, and I'd be back by 4 o'clock. I never touched my laptop ever again. I left all work behind when I came home. So right now, I'm still learning how to get through.</p>\n<p>I try to limit the amount of open source work that I do during work time. The release lead shadow and the release lead job ‚Äî they require a lot of time, effort. So on average, I'd be spending two to three hours post work time on the release activities.</p>\n<p><strong>CRAIG BOX: Before the pandemic, everyone was worried that if we let people work from home, they wouldn't work enough. I think the opposite has actually happened, is that now we're worried that if we let people work from home, they will just get on the computer in the morning and you'll have to pry it out of their hands at midnight.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Yeah, I think the productivity has increased at least twofold, I would say, for everyone, once they started working from home.</p>\n<p><strong>CRAIG BOX: But at the expense of work-life balance, though, because as you say, when you're sitting in the same chair in front of, perhaps, the same computer doing your MathWorks work and then your open source work, they kind of can blur into one perhaps?</strong></p>\n<p>SAVITHA RAGHUNATHAN: That is a challenge. I face it every day. But so many others are also facing it. I implemented a few little tricks to help me. When I used to come back home from work, the first thing I would do is remove my watch. That was an indication that OK, I'm done.</p>\n<p>That's the thing that I still do. I just remove my watch, and I just keep it right where my workstation is. And I just close the door so that I never look back. Even going past the room, I don't get a glimpse of my work office. I start implementing tiny little things like that to avoid burnout.</p>\n<p>I think I'm still facing a little bit of burnout. I don't know if I have fully recovered from it. I constantly feel like I need a vacation. And I could just take a vacation for like a month or two. If it's possible, I will just do it.</p>\n<p><strong>CRAIG BOX: I do hope that travel opens up for everyone as an opportunity because I know that, for a lot of people, it's not so much they've been working from home but they've been living at work. The idea of taking vacation effectively means, well, I've been stuck in the same place, if I've been under a lockdown. It's hard to justify that. It will be good as things improve worldwide for us to be able to start focusing more on mental health and perhaps getting away from the &quot;everything room,&quot; as I sometimes call it.</strong></p>\n<p>SAVITHA RAGHUNATHAN: I'm totally looking forward to it. I hope that travel opens up and I could go home and I could meet my siblings and my aunt and my parents.</p>\n<p><strong>CRAIG BOX: Catch a cricket match?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Yeah. Probably yes, if I have company and if there is anything interesting happening around the time. I don't mind going back to the Chepauk Stadium and catching a match or two.</p>\n<p><strong>CRAIG BOX: Let's turn now to the recently released <a href=\"https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/\">Kubernetes 1.22</a>. Congratulations on the launch.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Thank you.</p>\n<p><strong>CRAIG BOX: Each launch comes with a theme and a mascot or a logo. What is the theme for this release?</strong></p>\n<p>SAVITHA RAGHUNATHAN: The theme for the release is reaching new peaks. I am fascinated with a lot of space travel and chasing stars, the Milky Way. The best place to do that is over the top of a mountain. So that is the release logo, basically. It's a mountain ‚Äî Mount Rainier. On top of that, there is a Kubernetes flag, and it's overlooking the Milky Way.</p>\n<p>It's also symbolic that with every release, that we are achieving something new, bigger, and better, and we are making the release awesome. So I just wanted to incorporate that into the team as to say, we are achieving new things with every release. That's the &quot;reaching new peaks&quot; theme.</p>\n<p><strong>CRAIG BOX: The last couple of releases have both been incrementally larger ‚Äî as a result, perhaps, of the fact there are now only three releases per year rather than four. There were also changes to the process, where the work has been driven a lot more by the SIGs than by the release team having to go and ask the SIGs what was going on. What can you say about the size and scope of the 1.22 release?</strong></p>\n<p>SAVITHA RAGHUNATHAN: The 1.22 release is the largest release to date. We have 56 enhancements if I'm not wrong, and we have a good amount of features that's graduated as stable. You can now say that Kubernetes as a project has become more mature because you see new features coming in. At the same time, you see the features that weren't used getting deprecated ‚Äî we have like three deprecations in this release.</p>\n<p>Aside from that fact, we also have a big team that's supporting one of the longest releases. This is the first official release cycle after the cadence KEP got approved. Officially, we are at four months, even though 1.19 was six months, and 1.21 was like 3 and 1/2 months, I think, this is the first one after the official KEP approval.</p>\n<p><strong>CRAIG BOX: What changes did you make to the process knowing that you had that extra month?</strong></p>\n<p>SAVITHA RAGHUNATHAN: One of the things the community had asked for is more time for development. We tried to incorporate that in the release schedule. We had about six weeks between the enhancements freeze and the code freeze. That's one.</p>\n<p>It might not be visible to everyone, but one of the things that I wanted to make sure of was the health of the team ‚Äî since it was a long, long release, we had time to plan out, and not have everyone work during the weekends or during their evenings or time off. That actually helped everyone keep their sanity, and also in making good progress and delivering good results at the end of the release. That's one of the process improvements that I'd call out.</p>\n<p>We got better by making a post during the exception request process. Everyone works around the world. People from the UK start a little earlier than the people in the US East Coast. The West Coast starts three hours later than the East Coast. We used to make a post every Friday evening saying &quot;hey, we actually received this many requests. We have addressed a number of them. We are waiting on a couple, or whatever. All the release team members are done for the day. We will see you around on Monday. Have a good weekend.&quot; Something like that.</p>\n<p>We set the expectations from the community as well. We understand things are really important and urgent, but we are done. This gave everyone their time back. They don't have to worry over the weekend thinking like, hey, what's happening? What's happening in the release? They could spend time with their family, or they could do whatever they want to do, like go on a hike, or just sit and watch TV.</p>\n<p>There have been weekends that I just did that. I just binge-watched a series. That's what I did.</p>\n<p><strong>CRAIG BOX: Any recommendations?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I'm a big fan of Marvel, so I have watched the new <a href=\"https://en.wikipedia.org/wiki/Loki_(TV_series)\">Loki</a>, which I really love. Loki is one of my favourite characters in Marvel. And I also liked <a href=\"https://en.wikipedia.org/wiki/WandaVision\">WandaVision</a>. That was good, too.</p>\n<p><strong>CRAIG BOX: I've not seen Loki yet, but I've heard it described as the best series of Doctor Who in the last few years.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Really?</p>\n<p><strong>CRAIG BOX: There must be an element of time-travelling in there if that's how people are describing it.</strong></p>\n<p>SAVITHA RAGHUNATHAN: You should really go and watch it whenever you have time. It's really amazing. I might go back and watch it again because I might have missed bits and pieces. That always happens in Marvel movies and the episodes; you need to watch them a couple of times to catch, &quot;oh, this is how they relate&quot;.</p>\n<p><strong>CRAIG BOX: Yes, the mark of good media that you want to immediately go back and watch it again once you've seen it.</strong></p>\n<p><strong>Let's look now at some of the new features in Kubernetes 1.22. A couple of things that have graduated to general availability ‚Äî server-side apply, external credential providers, a couple of new security features ‚Äî the replacement for pod security policy has been announced, and seccomp is now available by default.</strong></p>\n<p><strong>Do you have any favourite features in 1.22 that you'd like to discuss?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I have a lot of them. All my favourite features are related to security. OK, one of them is not security, but a major theme of my favourite KEPs is security. I'll start with the <a href=\"https://github.com/kubernetes/enhancements/issues/2413\">default seccomp</a>. I think it will help make clusters secure by default, and may assist in preventing more vulnerabilities, which means less headaches for the cluster administrators.</p>\n<p>This is close to my heart because the base of the MathWorks platform is provisioning Kubernetes clusters. Knowing that they are secure by default will definitely provide me with some good sleep. And also, I'm paranoid about security most of the time. I'm super interested in making everything secure. It might get in the way of making the users of the platform angry because it's not usable in any way.</p>\n<p>My next one is <a href=\"https://github.com/kubernetes/enhancements/issues/2033\">rootless Kubelet</a>. That feature's going to enable the cluster admin, the platform developers to deploy Kubernetes components to run in a user namespace. And I think that is also a great addition.</p>\n<p>Like you mention, the most awaited drop in for the PSP replacement is here. It's <a href=\"https://github.com/kubernetes/enhancements/issues/2579\">pod admission control</a>. It lets cluster admins apply the pod security standards. And I think it's just not related to the cluster admins. I might have to go back and check on that. Anyone can probably use it ‚Äî the developers and the admins alike.</p>\n<p>It also supports various modes, which is most welcome. There are times where you don't want to just cut the users off because they are trying to do something which is not securely correct. You just want to warn them, hey, this is what you are doing. This might just cause a security issue later, so you might want to correct it. But you just don't want to cut them off from using the platform, or them trying to attempt to do something ‚Äî deploy their workload and get their day-to-day job done. That is something that I really like, that it also supports a warning mechanism.</p>\n<p>Another one which is not security is <a href=\"https://github.com/kubernetes/enhancements/issues/2400\">node swap support</a>. Kubernetes didn't have support for swap before, but it is taken into consideration now. This is an alpha feature. With this, you can take advantage of the swap, which is provisioned on the Linux VMs.</p>\n<p>Some of the workloads ‚Äî when they are deployed, they might need a lot of swap for the start-up ‚Äî example, like Node and Java applications, which I just took out of their KEP user stories. So if anyone's interested, they can go and look in the KEP. That's useful. And it also increases the node stability and whatnot. So I think it's going to be beneficial for a lot of folks.</p>\n<p>We know how Java and containers work. I think it has gotten better, but five years ago, it was so hard to get a Java application to fit in a small container. It always needed a lot of memory, swap, and everything to start up and run. I think this will help the users and help the admins and keep the cost low, and it will tie into so many other things as well. I'm excited about that feature.</p>\n<p>Another feature that I want to just call out ‚Äî I don't use Windows that much, but I just want to give a shout out to the folks who are doing an amazing job bringing all the Kubernetes features to Windows as well, to give a seamless experience.</p>\n<p>One of the things is <a href=\"https://github.com/kubernetes/enhancements/issues/1981\">Windows privileged containers</a>. I think it went alpha this release. And that is a wonderful addition, if you ask me. It can take advantage of whatever that's happening on the Linux side. And they can also port it over and see, OK, I can now run Windows containers in a privileged mode.</p>\n<p>So whatever they are trying to achieve, they can do it. So that's a noteworthy mention. I need to give a shout out for the folks who work and make things happen in the Windows ecosystem as well.</p>\n<p><strong>CRAIG BOX: One of the things that's great about the release process is the continuity between groups and teams. There's always an emeritus advisor who was a lead from a previous release. One thing that I always ask when I do these interviews is, what is the advice that you give to the next person? When <a href=\"https://kubernetespodcast.com/episode/146-kubernetes-1.21/\">we talked to Nabarun for the 1.21 interview</a>, he said that his advice to you would be &quot;do, delegate, and defer&quot;. Figure out what you can do, figure out what you can ask other people to do, and figure out what doesn't need to be done. Were you able to take that advice on board?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Yeah, you won't believe it. <a href=\"https://twitter.com/KubernetesPod/status/1423188323347177474/photo/3\">I have it right here stuck to my monitor.</a></p>\n<p><strong>CRAIG BOX: Next to your Git cheat sheet?</strong></p>\n<p>SAVITHA RAGHUNATHAN: <em>laughs</em> Absolutely. I just have it stuck there. I just took a look at it.</p>\n<p><strong>CRAIG BOX: Someone that you will have been able to delegate and defer to is Rey Lejano from Rancher Labs and SUSE, who is the release lead to be for 1.23.</strong></p>\n<p>SAVITHA RAGHUNATHAN: I want to tell Rey to beware of the team's mental health. Schedule in such a way that it avoids burnout. Check in, and make sure that everyone is doing good. If they need some kind of help, create a safe space where they can actually ask for help, if they want to step back, if they need someone to cover.</p>\n<p>I think that is most important. The releases are successful based on the thousands and thousands of contributors. But when it comes to a release team, you need to have a healthy team where people feel they are in a good place and they just want to make good contributions, which means they want to be heard. That's one thing that I want to tell Rey.</p>\n<p>Also collaborate and learn from each other. I constantly learn. I think the team was 39 folks, including me. Every day I learned something or the other, even starting from how to interact.</p>\n<p>Sometimes I have learned more leadership skills from my release lead shadows. They are awesome, and they are mature. I constantly learn from them, and I admire them a lot.</p>\n<p>It also helps to have good, strong individuals in the team who can step up and help when needed. For example, unfortunately, we lost one of our teammates after the start of the release cycle. That was tragic. His name was <a href=\"https://github.com/cncf/memorials/blob/main/peeyush-gupta.md\">Peeyush Gupta</a>. He was an awesome and wonderful human ‚Äî very warm.</p>\n<p>I didn't get more of a chance to interact with him. I had exchanged a few Slack messages, but I got his warm personality. I just want to take a couple of seconds to remember him. He was awesome.</p>\n<p>After we lost him, we had this strong person from the team step up and lead the communications, who had never been a part of the release team before at all. He was a shadow for the first time. His name is Jesse Butler. So he stepped up, and he just took it away. He ran the comms show for 1.22.</p>\n<p>That's what the community is about. You take care of team members, and the team will take care of you. So that's one other thing that I want to let Rey know, and maybe whoever ‚Äî I think it's applicable overall.</p>\n<p><strong>CRAIG BOX: There's a link to a <a href=\"https://milaap.org/fundraisers/support-peeyush-gupta-family-education\">family education fund for Peeyush Gupta</a>, which you can find in the show notes.</strong></p>\n<p><strong>Five releases in a row now you've been a member of the release team. Will you be putting your feet up now for 1.23?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I am going to take a break for a while. In the future, I want to be contributing, if not the release team, the SIG Release and the release management effort. But right now, I have been there for five releases. And I feel like, OK, I just need a little bit of fresh air.</p>\n<p>And also the pandemic and the burnout has caught up, so I'm going to take a break from certain contributions. You will see me in the future. I will be around, but I might not be actively participating in the release team activities. I will be around the community. Anyone can reach out to me. They all know my Slack, so they can just reach out to me via Slack or Twitter.</p>\n<p><strong>CRAIG BOX: Yes, your Twitter handle is CoffeeArtGirl. Does that mean that you'll be spending some time working on your lattes?</strong></p>\n<p>SAVITHA RAGHUNATHAN: I am very bad at making lattes. The coffee art means that I used to <a href=\"https://twitter.com/KubernetesPod/status/1423188323347177474/photo/1\">make art with coffee</a>. You get instant coffee powder and just mix it with water. You get the colours, very beautiful brown colours. I used to make art using that.</p>\n<p>And I love coffee. So I just combined all the words together. And I had to come up with it in a span of one hour or so because I was joining this 'meet our contributors' panel. And Paris asked me, &quot;do you have a Twitter handle?&quot; I was planning to create one, but I didn't have the time.</p>\n<p>I'm like, well, let me just think what I could just come up with real quick. So I just came up with that. So that's the story behind my Twitter handle. Everyone's interested in it. You are not the first person you have asked me or mentioned about it. So many others are like, why coffee art?</p>\n<p><strong>CRAIG BOX: And you are also interested in art with perhaps other materials?</strong></p>\n<p>SAVITHA RAGHUNATHAN: Yes. My interests keep changing. I used to do pebble art. It's just collecting pebbles from wherever I go, and I used to paint on them. I used to use watercolour, but I want to come back to watercolour sometime.</p>\n<p>My recent interests are coloured pencils, which came back. When I was very young, I used to do a lot of coloured pencils. And then I switched to watercolours and oil painting. So I just go around in circles.</p>\n<p>One of the hobbies that I picked up during a pandemic is crochet. I made a scarf for Mother's Day. My mum and my dad were here last year. They got stuck because of the pandemic, and they couldn't go back home. So they stayed with me for 10 months. That is the jackpot that I had, that I got to spend so much time with my parents after I moved to the US.</p>\n<p><strong>CRAIG BOX: And they got rewarded with a scarf.</strong></p>\n<p>SAVITHA RAGHUNATHAN: Yeah.</p>\n<p><strong>CRAIG BOX: One to share between them.</strong></p>\n<p>SAVITHA RAGHUNATHAN: I started making a blanket for my dad. And it became so heavy, I might have to just pick up some lighter yarn. I still don't know the differences between different kinds of yarns, but I'm getting better.</p>\n<p>I started out because I wanted to make these little toys. They call them <a href=\"https://en.wikipedia.org/wiki/Amigurumi\">amigurumi</a> in the crochet world. I wanted to make them. That's why I started out. I'm trying. I made <a href=\"https://twitter.com/KubernetesPod/status/1423188323347177474/photo/2\">a little cat</a> which doesn't look like a cat, but it is a cat. I have to tell everyone that it's a cat so that they don't mock me later, but.</p>\n<p><strong>CRAIG BOX: It's an artistic interpretation of a cat.</strong></p>\n<p>SAVITHA RAGHUNATHAN: It definitely is!</p>\n<hr>\n<p><em><a href=\"https://twitter.com/coffeeartgirl\">Savitha Raghunathan</a>, now a Senior Software Engineer at Red Hat, served as the Kubernetes 1.22 release team lead.</em></p>\n<p><em>You can find the <a href=\"http://www.kubernetespodcast.com/\">Kubernetes Podcast from Google</a> at <a href=\"https://twitter.com/KubernetesPod\">@KubernetesPod</a> on Twitter, and you can <a href=\"https://kubernetespodcast.com/subscribe/\">subscribe</a> so you never miss an episode.</em></p>","PublishedAt":"2021-12-01 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/01/contribution-containers-and-cricket-the-kubernetes-1.22-release-interview/","SourceName":"Kubernetes"}},{"node":{"ID":1246,"Title":"Blog: Quality-of-Service for Memory Resources","Description":"<p><strong>Authors:</strong> Tim Xu (Tencent Cloud)</p>\n<p>Kubernetes v1.22, released in August 2021, introduced a new alpha feature that improves how Linux nodes implement memory resource requests and limits.</p>\n<p>In prior releases, Kubernetes did not support memory quality guarantees.\nFor example, if you set container resources as follows:</p>\n<pre tabindex=\"0\"><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: example\nspec:\ncontainers:\n- name: nginx\nresources:\nrequests:\nmemory: &#34;64Mi&#34;\ncpu: &#34;250m&#34;\nlimits:\nmemory: &#34;64Mi&#34;\ncpu: &#34;500m&#34;\n</code></pre><p><code>spec.containers[].resources.requests</code>(e.g. cpu, memory) is designed for scheduling. When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled Containers is less than the capacity of the node.</p>\n<p><code>spec.containers[].resources.limits</code> is passed to the container runtime when the kubelet starts a container. CPU is considered a &quot;compressible&quot; resource. If your app starts hitting your CPU limits, Kubernetes starts throttling your container, giving your app potentially worse performance. However, it won‚Äôt be terminated. That is what &quot;compressible&quot; means.</p>\n<p>In cgroup v1, and prior to this feature, the container runtime never took into account and effectively ignored spec.containers[].resources.requests[&quot;memory&quot;]. This is unlike CPU, in which the container runtime consider both requests and limits. Furthermore, memory actually can't be compressed in cgroup v1. Because there is no way to throttle memory usage, if a container goes past its memory limit it will be terminated by the kernel with an OOM (Out of Memory) kill.</p>\n<p>Fortunately, cgroup v2 brings a new design and implementation to achieve full protection on memory. The new feature relies on cgroups v2 which most current operating system releases for Linux already provide. With this experimental feature, <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/\">quality-of-service for pods and containers</a> extends to cover not just CPU time but memory as well.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes. Memory requests and limits of containers in pod are used to set specific interfaces <code>memory.min</code> and <code>memory.high</code> provided by the memory controller. When <code>memory.min</code> is set to memory requests, memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures the availability of memory for Kubernetes pods. And if memory limits are set in the container, this means that the system needs to limit container memory usage, Memory QoS uses <code>memory.high</code> to throttle workload approaching it's memory limit, ensuring that the system is not overwhelmed by instantaneous memory allocation.</p>\n<p><img src=\"./memory-qos-cal.svg\" alt=\"\"></p>\n<p>The following table details the specific functions of these two parameters and how they correspond to Kubernetes container resources.</p>\n<table>\n<tr>\n<th style=\"text-align:center\">File</th>\n<th style=\"text-align:center\">Description</th>\n</tr>\n<tr>\n<td>memory.min</td>\n<td><code>memory.min</code> specifies a minimum amount of memory the cgroup must always retain, i.e., memory that can never be reclaimed by the system. If the cgroup's memory usage reaches this low limit and can‚Äôt be increased, the system OOM killer will be invoked.\n<br>\n<br>\n<i>We map it to the container's memory request</i>\n</td>\n</tr>\n<tr>\n<td>memory.high</td>\n<td><code>memory.high</code> is the memory usage throttle limit. This is the main mechanism to control a cgroup's memory use. If a cgroup's memory use goes over the high boundary specified here, the cgroup‚Äôs processes are throttled and put under heavy reclaim pressure. The default is max, meaning there is no limit.\n<br>\n<br>\n<i>We use a formula to calculate <code>memory.high</code>, depending on container's memory limit or node allocatable memory (if container's memory limit is empty) and a throttling factor. Please refer to the KEP for more details on the formula.</i>\n</td>\n</tr>\n</table>\n<p>When container memory requests are made, kubelet passes <code>memory.min</code> to the back-end CRI runtime (possibly containerd, cri-o) via the <code>Unified</code> field in CRI during container creation. The <code>memory.min</code> in container level cgroup will be set to:</p>\n<p><img src=\"./container-memory-min.svg\" alt=\"\"><br>\n<sub>i: the i<sup>th</sup> container in one pod</sub></p>\n<p>Since the <code>memory.min</code> interface requires that the ancestor cgroup directories are all set, the pod and node cgroup directories need to be set correctly.</p>\n<p><code>memory.min</code> in pod level cgroup:<br>\n<img src=\"./pod-memory-min.svg\" alt=\"\"><br>\n<sub>i: the i<sup>th</sup> container in one pod</sub></p>\n<p><code>memory.min</code> in node level cgroup:<br>\n<img src=\"./node-memory-min.svg\" alt=\"\"><br>\n<sub>i: the i<sup>th</sup> pod in one node, j: the j<sup>th</sup> container in one pod</sub></p>\n<p>Kubelet will manage the cgroup hierarchy of the pod level and node level cgroups directly using runc libcontainer library, while container cgroup limits are managed by the container runtime.</p>\n<p>For memory limits, in addition to the original way of limiting memory usage, Memory QoS adds an additional feature of throttling memory allocation. A throttling factor is introduced as a multiplier (default is 0.8). If the result of multiplying memory limits by the factor is greater than memory requests, kubelet will set <code>memory.high</code> to the value and use <code>Unified</code> via CRI. And if the container does not specify memory limits, kubelet will use node allocatable memory instead. The <code>memory.high</code> in container level cgroup is set to:</p>\n<p><img src=\"./container-memory-high.svg\" alt=\"\"><br>\n<sub>i: the i<sup>th</sup> container in one pod</sub></p>\n<p>This can can help improve stability when pod memory usage increases, ensuring that memory is throttled as it approaches the memory limit.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>Here are the prerequisites for enabling Memory QoS on your Linux node, some of these are related to <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2\">Kubernetes support for cgroup v2</a>.</p>\n<ol>\n<li>Kubernetes since v1.22</li>\n<li><a href=\"https://github.com/opencontainers/runc\">runc</a> since v1.0.0-rc93; <a href=\"https://containerd.io/\">containerd</a> since 1.4; <a href=\"https://cri-o.io/\">cri-o</a> since 1.20</li>\n<li>Linux kernel minimum version: 4.15, recommended version: 5.2+</li>\n<li>Linux image with cgroupv2 enabled or enabling cgroupv2 unified_cgroup_hierarchy manually</li>\n</ol>\n<p>OCI runtimes such as runc and crun already support cgroups v2 <a href=\"https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#unified\"><code>Unified</code></a>, and Kubernetes CRI has also made the desired changes to support passing <a href=\"https://github.com/kubernetes/kubernetes/pull/102578\"><code>Unified</code></a>. However, CRI Runtime support is required as well. Memory QoS in Alpha phase is designed to support containerd and cri-o. Related PR <a href=\"https://github.com/containerd/containerd/pull/5627\">Feature: containerd-cri support LinuxContainerResources.Unified #5627</a> has been merged and will be released in containerd 1.6. CRI-O <a href=\"https://github.com/cri-o/cri-o/pull/5207\">implement kube alpha features for 1.22 #5207</a> is still in WIP.</p>\n<p>With those prerequisites met, you can enable the memory QoS feature gate (see <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/\">Set kubelet parameters via a config file</a>).</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>You can find more details as follows:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos/#readme\">Support Memory QoS with cgroup v2</a></li>\n<li><a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2/#readme\">cgroup v2</a></li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>You can reach SIG Node by several means:</p>\n<ul>\n<li>Slack: <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/kubernetes-sig-node\">Mailing list</a></li>\n<li><a href=\"https://github.com/kubernetes/community/labels/sig%2Fnode\">Open Community Issues/PRs</a></li>\n</ul>\n<p>You can also contact me directly:</p>\n<ul>\n<li>GitHub / Slack: @xiaoxubeii</li>\n<li>Email: <a href=\"mailto:xiaoxubeii@gmail.com\">xiaoxubeii@gmail.com</a></li>\n</ul>","PublishedAt":"2021-11-26 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/11/26/qos-memory-resources/","SourceName":"Kubernetes"}},{"node":{"ID":1247,"Title":"Blog: Dockershim removal is coming. Are you ready?","Description":"<p><strong>Authors:</strong> Sergey Kanzhelev, Google. With reviews from Davanum Srinivas, Elana Hashman, Noah Kantrowitz, Rey Lejano.</p>\n<div class=\"alert alert-info\" role=\"alert\">\n<h4 class=\"alert-heading\">Poll closed</h4>\nThis poll closed on January 7, 2022.\n</div>\n<p>Last year we <a href=\"https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation\">announced</a>\nthat Kubernetes' dockershim component (which provides a built-in integration for\nDocker Engine) is deprecated.</p>\n<p><em>Update: There's a <a href=\"https://kubernetes.io/blog/2020/12/02/dockershim-faq/\">Dockershim Deprecation FAQ</a>\nwith more information, and you can also discuss the deprecation via a dedicated\n<a href=\"https://github.com/kubernetes/kubernetes/issues/106917\">GitHub issue</a>.</em></p>\n<p>Our current plan is to remove dockershim from the Kubernetes codebase soon.\nWe are looking for feedback from you whether you are ready for dockershim\nremoval and to ensure that you are ready when the time comes.</p>\n<p><del>Please fill out this survey: <a href=\"https://forms.gle/svCJmhvTv78jGdSx8\">https://forms.gle/svCJmhvTv78jGdSx8</a></del></p>\n<p>The dockershim component that enables Docker as a Kubernetes container runtime is\nbeing deprecated in favor of runtimes that directly use the <a href=\"https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/\">Container Runtime Interface</a>\ncreated for Kubernetes. Many Kubernetes users have migrated to\nother container runtimes without problems. However we see that dockershim is\nstill very popular. You may see some public numbers in recent <a href=\"https://www.datadoghq.com/container-report/#8\">Container Report</a> from DataDog.\nSome Kubernetes hosting vendors just recently enabled other runtimes support\n(especially for Windows nodes). And we know that many third party tools vendors\nare still not ready: <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/#telemetry-and-security-agent-vendors\">migrating telemetry and security agents</a>.</p>\n<p>At this point, we believe that there is feature parity between Docker and the\nother runtimes. Many end-users have used our <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/\">migration guide</a>\nand are running production workload using these different runtimes. The plan of\nrecord today is that dockershim will be removed in version 1.24, slated for\nrelease around April of next year. For those developing or running alpha and\nbeta versions, dockershim will be removed in December at the beginning of the\n1.24 release development cycle.</p>\n<p>There is only one month left to give us feedback. We want you to tell us how\nready you are.</p>\n<p><del>We are collecting opinions through this survey: <a href=\"https://forms.gle/svCJmhvTv78jGdSx8\">https://forms.gle/svCJmhvTv78jGdSx8</a></del>\nTo better understand preparedness for the dockershim removal, our survey is\nasking the version of Kubernetes you are currently using, and an estimate of\nwhen you think you will adopt Kubernetes 1.24. All the aggregated information\non dockershim removal readiness will be published.\nFree form comments will be reviewed by SIG Node leadership. If you want to\ndiscuss any details of migrating from dockershim, report bugs or adoption\nblockers, you can use one of the SIG Node contact options any time:\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-node#contact\">https://github.com/kubernetes/community/tree/master/sig-node#contact</a></p>\n<p>Kubernetes is a mature project. This deprecation is another\nstep in the effort to get away from permanent beta features and providing more\nstability and compatibility guarantees. With the migration from dockershim you\nwill get more flexibility and choice of container runtime features as well as\nless dependencies of your apps on specific underlying technology. Please take\ntime to review the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/\">dockershim migration documentation</a>\nand consult your Kubernetes hosting vendor (if you have one) what container runtime options are available for you.\nRead up <a href=\"https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes\">container runtime documentation with instructions on how to use containerd and CRI-O</a>\nto help prepare you when you're ready to upgrade to 1.24. CRI-O, containerd, and\nDocker with <a href=\"https://github.com/Mirantis/cri-dockerd\">Mirantis cri-dockerd</a> are\nnot the only container runtime options, we encourage you to explore the <a href=\"https://landscape.cncf.io/card-mode?category=container-runtime&amp;grouping=category\">CNCF landscape on container runtimes</a>\nin case another suits you better.</p>\n<p>Thank you!</p>","PublishedAt":"2021-11-12 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/","SourceName":"Kubernetes"}},{"node":{"ID":1248,"Title":"Blog: Non-root Containers And Devices","Description":"<p><strong>Author:</strong> Mikko Ylinen (Intel)</p>\n<p>The user/group ID related security settings in Pod's <code>securityContext</code> trigger a problem when users want to\ndeploy containers that use accelerator devices (via <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\">Kubernetes Device Plugins</a>) on Linux. In this blog\npost I talk about the problem and describe the work done so far to address it. It's not meant to be a long story about getting the <a href=\"https://github.com/kubernetes/kubernetes/issues/92211\">k/k issue</a> fixed.</p>\n<p>Instead, this post aims to raise awareness of the issue and to highlight important device use-cases too. This is needed as Kubernetes works on new related features such as support for user namespaces.</p>\n<h2 id=\"why-non-root-containers-can-t-use-devices-and-why-it-matters\">Why non-root containers can't use devices and why it matters</h2>\n<p>One of the key security principles for running containers in Kubernetes is the\nprinciple of least privilege. The Pod/container <code>securityContext</code> specifies the config\noptions to set, e.g., Linux capabilities, MAC policies, and user/group ID values to achieve this.</p>\n<p>Furthermore, the cluster admins are supported with tools like <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-policy/\">PodSecurityPolicy</a> (deprecated) or\n<a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Pod Security Admission</a> (alpha) to enforce the desired security settings for pods that are being deployed in\nthe cluster. These settings could, for instance, require that containers must be <code>runAsNonRoot</code> or\nthat they are forbidden from running with root's group ID in <code>runAsGroup</code> or <code>supplementalGroups</code>.</p>\n<p>In Kubernetes, the kubelet builds the list of <a href=\"https://pkg.go.dev/k8s.io/cri-api@v0.22.1/pkg/apis/runtime/v1#Device\"><code>Device</code></a> resources to be made available to a container\n(based on inputs from the Device Plugins) and the list is included in the CreateContainer CRI message\nsent to the CRI container runtime. Each <code>Device</code> contains little information: host/container device\npaths and the desired devices cgroups permissions.</p>\n<p>The <a href=\"https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md\">OCI Runtime Spec for Linux Container Configuration</a>\nexpects that in addition to the devices cgroup fields, more detailed information about the devices\nmust be provided:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>{<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;type&#34;: </span><span style=\"color:#b44\">&#34;&lt;string&gt;&#34;</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;path&#34;: </span><span style=\"color:#b44\">&#34;&lt;string&gt;&#34;</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;major&#34;: </span>&lt;int64&gt;,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;minor&#34;: </span>&lt;int64&gt;,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;fileMode&#34;: </span>&lt;uint32&gt;,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;uid&#34;: </span>&lt;uint32&gt;,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;gid&#34;: </span>&lt;uint32&gt;<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>},<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The CRI container runtimes (containerd, CRI-O) are responsible for obtaining this information\nfrom the host for each <code>Device</code>. By default, the runtimes copy the host device's user and group IDs:</p>\n<ul>\n<li><code>uid</code> (uint32, OPTIONAL) - id of device owner in the container namespace.</li>\n<li><code>gid</code> (uint32, OPTIONAL) - id of device group in the container namespace.</li>\n</ul>\n<p>Similarly, the runtimes prepare other mandatory <code>config.json</code> sections based on the CRI fields,\nincluding the ones defined in <code>securityContext</code>: <code>runAsUser</code>/<code>runAsGroup</code>, which become part of the POSIX\nplatforms user structure via:</p>\n<ul>\n<li><code>uid</code> (int, REQUIRED) specifies the user ID in the container namespace.</li>\n<li><code>gid</code> (int, REQUIRED) specifies the group ID in the container namespace.</li>\n<li><code>additionalGids</code> (array of ints, OPTIONAL) specifies additional group IDs in the container namespace to be added to the process.</li>\n</ul>\n<p>However, the resulting <code>config.json</code> triggers a problem when trying to run containers with\nboth devices added and with non-root uid/gid set via <code>runAsUser</code>/<code>runAsGroup</code>: the container user process\nhas no permission to use the device even when its group id (gid, copied from host) was permissive to\nnon-root groups. This is because the container user does not belong to that host group (e.g., via <code>additionalGids</code>).</p>\n<p>Being able to run applications that use devices as non-root user is normal and expected to work so that\nthe security principles can be met. Therefore, several alternatives were considered to get the gap filled with what the PodSec/CRI/OCI supports today.</p>\n<h2 id=\"what-was-done-to-solve-the-issue\">What was done to solve the issue?</h2>\n<p>You might have noticed from the problem definition that it would at least be possible to workaround\nthe problem by manually adding the device gid(s) to <code>supplementalGroups</code>, or in\nthe case of just one device, set <code>runAsGroup</code> to the device's group id. However, this is problematic because the device gid(s) may have\ndifferent values depending on the nodes' distro/version in the cluster. For example, with GPUs the following commands for different distros and versions return different gids:</p>\n<p>Fedora 33:</p>\n<pre tabindex=\"0\"><code>$ ls -l /dev/dri/\ntotal 0\ndrwxr-xr-x. 2 root root 80 19.10. 10:21 by-path\ncrw-rw----+ 1 root video 226, 0 19.10. 10:42 card0\ncrw-rw-rw-. 1 root render 226, 128 19.10. 10:21 renderD128\n$ grep -e video -e render /etc/group\nvideo:x:39:\nrender:x:997:\n</code></pre><p>Ubuntu 20.04:</p>\n<pre tabindex=\"0\"><code>$ ls -l /dev/dri/\ntotal 0\ndrwxr-xr-x 2 root root 80 19.10. 17:36 by-path\ncrw-rw---- 1 root video 226, 0 19.10. 17:36 card0\ncrw-rw---- 1 root render 226, 128 19.10. 17:36 renderD128\n$ grep -e video -e render /etc/group\nvideo:x:44:\nrender:x:133:\n</code></pre><p>Which number to choose in your <code>securityContext</code>? Also, what if the <code>runAsGroup</code>/<code>runAsUser</code> values cannot be hard-coded because\nthey are automatically assigned during pod admission time via external security policies?</p>\n<p>Unlike volumes with <code>fsGroup</code>, the devices have no official notion of <code>deviceGroup</code>/<code>deviceUser</code> that the CRI runtimes (or kubelet)\nwould be able to use. We considered using container annotations set by the device plugins (e.g., <code>io.kubernetes.cri.hostDeviceSupplementalGroup/</code>) to get custom OCI <code>config.json</code> uid/gid values.\nThis would have required changes to all existing device plugins which was not ideal.</p>\n<p>Instead, a solution that is <em>seamless</em> to end-users without getting the device plugin vendors involved was preferred. The selected approach was\nto re-use <code>runAsUser</code> and <code>runAsGroup</code> values in <code>config.json</code> for devices:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>{<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;type&#34;: </span><span style=\"color:#b44\">&#34;c&#34;</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;path&#34;: </span><span style=\"color:#b44\">&#34;/dev/foo&#34;</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;major&#34;: </span><span style=\"color:#666\">123</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;minor&#34;: </span><span style=\"color:#666\">4</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;fileMode&#34;: </span><span style=\"color:#666\">438</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;uid&#34;: </span>&lt;runAsUser&gt;,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;gid&#34;: </span>&lt;runAsGroup&gt;<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>},<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>With <code>runc</code> OCI runtime (in non-rootless mode), the device is created (<code>mknod(2)</code>) in\nthe container namespace and the ownership is changed to <code>runAsUser</code>/<code>runAsGroup</code> using <code>chmod(2)</code>.</p>\n<p><div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/\">Rootless mode</a> and devices is not supported.\n</div>\nHaving the ownership updated in the container namespace is justified as the user process is the only one accessing the device. Only <code>runAsUser</code>/<code>runAsGroup</code>\nare taken into account, and, e.g., the <code>USER</code> setting in the container is currently ignored.</p>\n<p>While it is likely that the &quot;faulty&quot; deployments (i.e., non-root <code>securityContext</code> + devices) do not exist, to be absolutely sure no\ndeployments break, an opt-in config entry in both containerd and CRI-O to enable the new behavior was added. The following:</p>\n<p><code>device_ownership_from_security_context (bool)</code></p>\n<p>defaults to <code>false</code> and must be enabled to use the feature.</p>\n<h2 id=\"see-non-root-containers-using-devices-after-the-fix\">See non-root containers using devices after the fix</h2>\n<p>To demonstrate the new behavior, let's use a Data Plane Development Kit (DPDK) application using hardware accelerators, Kubernetes CPU manager, and HugePages as an example. The cluster runs containerd with:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex;\"><span>[plugins]\n</span></span><span style=\"display:flex;\"><span> [plugins.<span style=\"color:#b44\">&#34;io.containerd.grpc.v1.cri&#34;</span>]\n</span></span><span style=\"display:flex;\"><span> device_ownership_from_security_context = <span style=\"color:#a2f;font-weight:bold\">true</span>\n</span></span></code></pre></div><p>or CRI-O with:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex;\"><span>[crio.runtime]\n</span></span><span style=\"display:flex;\"><span>device_ownership_from_security_context = <span style=\"color:#a2f;font-weight:bold\">true</span>\n</span></span></code></pre></div><p>and the <code>Guaranteed</code> QoS Class Pod that runs DPDK's crypto-perf test utility with this YAML:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">...</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>qat-dpdk<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">securityContext</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">runAsUser</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">runAsGroup</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">2000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fsGroup</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">3000</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>crypto-perf<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>intel/crypto-perf:devel<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;3&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;128Mi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">qat.intel.com/generic</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;4&#39;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">hugepages-2Mi</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;128Mi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">limits</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">cpu</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;3&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">memory</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;128Mi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">qat.intel.com/generic</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;4&#39;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">hugepages-2Mi</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;128Mi&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>To verify the results, check the user and group ID that the container runs as:</p>\n<pre tabindex=\"0\"><code>$ kubectl exec -it qat-dpdk -c crypto-perf -- id\n</code></pre><p>They are set to non-zero values as expected:</p>\n<pre tabindex=\"0\"><code>uid=1000 gid=2000 groups=2000,3000\n</code></pre><p>Next, check the device node permissions (<code>qat.intel.com/generic</code> exposes <code>/dev/vfio/</code> devices) are accessible to <code>runAsUser</code>/<code>runAsGroup</code>:</p>\n<pre tabindex=\"0\"><code>$ kubectl exec -it qat-dpdk -c crypto-perf -- ls -la /dev/vfio\ntotal 0\ndrwxr-xr-x 2 root root 140 Sep 7 10:55 .\ndrwxr-xr-x 7 root root 380 Sep 7 10:55 ..\ncrw------- 1 1000 2000 241, 0 Sep 7 10:55 58\ncrw------- 1 1000 2000 241, 2 Sep 7 10:55 60\ncrw------- 1 1000 2000 241, 10 Sep 7 10:55 68\ncrw------- 1 1000 2000 241, 11 Sep 7 10:55 69\ncrw-rw-rw- 1 1000 2000 10, 196 Sep 7 10:55 vfio\n</code></pre><p>Finally, check the non-root container is also allowed to create HugePages:</p>\n<pre tabindex=\"0\"><code>$ kubectl exec -it qat-dpdk -c crypto-perf -- ls -la /dev/hugepages/\n</code></pre><p><code>fsGroup</code> gives a <code>runAsUser</code> writable HugePages emptyDir mountpoint:</p>\n<pre tabindex=\"0\"><code>total 0\ndrwxrwsr-x 2 root 3000 0 Sep 7 10:55 .\ndrwxr-xr-x 7 root root 380 Sep 7 10:55 ..\n</code></pre><h2 id=\"help-us-test-it-and-provide-feedback\">Help us test it and provide feedback!</h2>\n<p>The functionality described here is expected to help with cluster security and the configurability of device permissions. To allow\nnon-root containers to use devices requires cluster admins to opt-in to the functionality by setting\n<code>device_ownership_from_security_context = true</code>. To make it a default setting, please test it and provide your feedback (via SIG-Node meetings or issues)!\nThe flag is available in CRI-O v1.22 release and queued for containerd v1.6.</p>\n<p>More work is needed to get it <em>properly</em> supported. It is known to work with <code>runc</code> but it also needs to be made to function\nwith other OCI runtimes too, where applicable. For instance, Kata Containers supports device passthrough and allows it to make devices\navailable to containers in VM sandboxes too.</p>\n<p>Moreover, the additional challenge comes with support of user names and devices. This problem is still <a href=\"https://github.com/kubernetes/enhancements/pull/2101\">open</a>\nand requires more brainstorming.</p>\n<p>Finally, it needs to be understood whether <code>runAsUser</code>/<code>runAsGroup</code> are enough or if device specific settings similar to <code>fsGroups</code> are needed in PodSpec/CRI v2.</p>\n<h2 id=\"thanks\">Thanks</h2>\n<p>My thanks goes to Mike Brown (IBM, containerd), Peter Hunt (Redhat, CRI-O), and Alexander Kanevskiy (Intel) for providing all the feedback and good conversations.</p>","PublishedAt":"2021-11-09 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/11/09/non-root-containers-and-devices/","SourceName":"Kubernetes"}},{"node":{"ID":1249,"Title":"Blog: Announcing the 2021 Steering Committee Election Results","Description":"<p><strong>Author</strong>: Kaslin Fields</p>\n<p>The <a href=\"https://github.com/kubernetes/community/tree/master/events/elections/2021\">2021 Steering Committee Election</a> is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2021. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.</p>\n<p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee‚Äôs role in their <a href=\"https://github.com/kubernetes/steering/blob/master/charter.md\">charter</a>.</p>\n<h2 id=\"results\">Results</h2>\n<p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):</p>\n<ul>\n<li><strong>Christoph Blecker (<a href=\"https://github.com/cblecker\">@cblecker</a>), Red Hat</strong></li>\n<li><strong>Stephen Augustus (<a href=\"https://github.com/justaugustus\">@justaugustus</a>), Cisco</strong></li>\n<li><strong>Paris Pittman (<a href=\"https://github.com/parispittman\">@parispittman</a>), Apple</strong></li>\n<li><strong>Tim Pepper (<a href=\"https://github.com/tpepper\">@tpepper</a>), VMware</strong></li>\n</ul>\n<p>They join continuing members:</p>\n<ul>\n<li><strong>Davanum Srinivas (<a href=\"https://github.com/dims\">@dims</a>), VMware</strong></li>\n<li><strong>Jordan Liggitt (<a href=\"https://github.com/liggitt\">@liggitt</a>), Google</strong></li>\n<li><strong>Bob Killen (<a href=\"https://github.com/mrbobbytables\">@mrbobbytables</a>), Google</strong></li>\n</ul>\n<p>Paris Pittman and Christoph Blecker are returning Steering Committee Members.</p>\n<h2 id=\"big-thanks\">Big Thanks</h2>\n<p>Thank you and congratulations on a successful election to this round‚Äôs election officers:</p>\n<ul>\n<li>Alison Dowdney, (<a href=\"https://github.com/alisondy\">@alisondy</a>)</li>\n<li>Noah Kantrowitz (<a href=\"https://github.com/coderanger\">@coderanger</a>)</li>\n<li>Josh Berkus (<a href=\"https://github.com/jberkus\">@jberkus</a>)</li>\n</ul>\n<p>Special thanks to Arnaud Meukam (<a href=\"https://github.com/ameukam\">@ameukam</a>), k8s-infra liaison, who enabled our voting software on community-owned infrastructure.</p>\n<p>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:</p>\n<ul>\n<li>Derek Carr (<a href=\"https://github.com/derekwaynecarr\">@derekwaynecarr</a>)</li>\n<li>Nikhita Raghunath (<a href=\"https://github.com/nikhita\">@nikhita</a>)</li>\n</ul>\n<p>And thank you to all the candidates who came forward to run for election.</p>\n<h2 id=\"get-involved-with-the-steering-committee\">Get Involved with the Steering Committee</h2>\n<p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee <a href=\"https://github.com/kubernetes/steering/projects/1\">backlog items</a> and weigh in by filing an issue or creating a PR against their <a href=\"https://github.com/kubernetes/steering\">repo</a>. They have an open meeting on <a href=\"https://github.com/kubernetes/steering\">the first Monday at 9:30am PT of every month</a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list <a href=\"mailto:steering@kubernetes.io\">steering@kubernetes.io</a>.</p>\n<p>You can see what the Steering Committee meetings are all about by watching past meetings on the <a href=\"https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM\">YouTube Playlist</a>.</p>\n<hr>\n<p><em>This post was written by the <a href=\"https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing\">Upstream Marketing Working Group</a>. If you want to write stories about the Kubernetes community, learn more about us.</em></p>","PublishedAt":"2021-11-08 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/11/08/steering-committee-results-2021/","SourceName":"Kubernetes"}},{"node":{"ID":1250,"Title":"Blog: Use KPNG to Write Specialized kube-proxiers","Description":"<p><strong>Author</strong>: Lars Ekman (Ericsson)</p>\n<p>The post will show you how to create a specialized service kube-proxy\nstyle network proxier using Kubernetes Proxy NG\n<a href=\"https://github.com/kubernetes-sigs/kpng\">kpng</a> without interfering\nwith the existing kube-proxy. The kpng project aims at renewing the\nthe default Kubernetes Service implementation, the &quot;kube-proxy&quot;. An\nimportant feature of kpng is that it can be used as a library to\ncreate proxiers outside K8s. While this is useful for CNI-plugins that\nreplaces the kube-proxy it also opens the possibility for anyone to\ncreate a proxier for a special purpose.</p>\n<h2 id=\"define-a-service-that-uses-a-specialized-proxier\">Define a service that uses a specialized proxier</h2>\n<pre tabindex=\"0\"><code>apiVersion: v1\nkind: Service\nmetadata:\nname: kpng-example\nlabels:\nservice.kubernetes.io/service-proxy-name: kpng-example\nspec:\nclusterIP: None\nipFamilyPolicy: RequireDualStack\nexternalIPs:\n- 10.0.0.55\n- 1000::55\nselector:\napp: kpng-alpine\nports:\n- port: 6000\n</code></pre><p>If the <code>service.kubernetes.io/service-proxy-name</code> label is defined the\n<code>kube-proxy</code> will ignore the service. A custom controller can watch\nservices with the label set to it's own name, &quot;kpng-example&quot; in\nthis example, and setup specialized load-balancing.</p>\n<p>The <code>service.kubernetes.io/service-proxy-name</code> label is <a href=\"https://kubernetes.io/docs/reference/labels-annotations-taints/#servicekubernetesioservice-proxy-name\">not\nnew</a>,\nbut so far is has been quite hard to write a specialized proxier.</p>\n<p>The common use for a specialized proxier is assumed to be handling\nexternal traffic for some use-case not supported by K8s. In that\ncase <code>ClusterIP</code> is not needed, so we use a &quot;headless&quot; service in this\nexample.</p>\n<h2 id=\"specialized-proxier-using-kpng\">Specialized proxier using kpng</h2>\n<p>A <a href=\"https://github.com/kubernetes-sigs/kpng\">kpng</a> based proxier\nconsists of the <code>kpng</code> controller handling all the K8s api related\nfunctions, and a &quot;backend&quot; implementing the load-balancing. The\nbackend can be linked with the <code>kpng</code> controller binary or be a\nseparate program communicating with the controller using gRPC.</p>\n<pre tabindex=\"0\"><code>kpng kube --service-proxy-name=kpng-example to-api\n</code></pre><p>This starts the <code>kpng</code> controller and tell it to watch only services\nwith the &quot;kpng-example&quot; service proxy name. The &quot;to-api&quot; parameter\nwill open a gRPC server for backends.</p>\n<p>You can test this yourself outside your cluster. Please see the example\nbelow.</p>\n<p>Now we start a backend that simply prints the updates from the\ncontroller.</p>\n<pre tabindex=\"0\"><code>$ kubectl apply -f kpng-example.yaml\n$ kpng-json | jq # (this is the backend)\n{\n&#34;Service&#34;: {\n&#34;Namespace&#34;: &#34;default&#34;,\n&#34;Name&#34;: &#34;kpng-example&#34;,\n&#34;Type&#34;: &#34;ClusterIP&#34;,\n&#34;IPs&#34;: {\n&#34;ClusterIPs&#34;: {},\n&#34;ExternalIPs&#34;: {\n&#34;V4&#34;: [\n&#34;10.0.0.55&#34;\n],\n&#34;V6&#34;: [\n&#34;1000::55&#34;\n]\n},\n&#34;Headless&#34;: true\n},\n&#34;Ports&#34;: [\n{\n&#34;Protocol&#34;: 1,\n&#34;Port&#34;: 6000,\n&#34;TargetPort&#34;: 6000\n}\n]\n},\n&#34;Endpoints&#34;: [\n{\n&#34;IPs&#34;: {\n&#34;V6&#34;: [\n&#34;1100::202&#34;\n]\n},\n&#34;Local&#34;: true\n},\n{\n&#34;IPs&#34;: {\n&#34;V4&#34;: [\n&#34;11.0.2.2&#34;\n]\n},\n&#34;Local&#34;: true\n},\n{\n&#34;IPs&#34;: {\n&#34;V4&#34;: [\n&#34;11.0.1.2&#34;\n]\n}\n},\n{\n&#34;IPs&#34;: {\n&#34;V6&#34;: [\n&#34;1100::102&#34;\n]\n}\n}\n]\n}\n</code></pre><p>A real backend would use some mechanism to load-balance traffic from\nthe external IPs to the endpoints.</p>\n<h2 id=\"writing-a-backend\">Writing a backend</h2>\n<p>The <code>kpng-json</code> backend looks like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-go\" data-lang=\"go\"><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">package</span> main\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">import</span> (\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#b44\">&#34;os&#34;</span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#b44\">&#34;encoding/json&#34;</span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#b44\">&#34;sigs.k8s.io/kpng/client&#34;</span>\n</span></span><span style=\"display:flex;\"><span>)\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">func</span> <span style=\"color:#00a000\">main</span>() {\n</span></span><span style=\"display:flex;\"><span> client.<span style=\"color:#00a000\">Run</span>(jsonPrint)\n</span></span><span style=\"display:flex;\"><span>}\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f;font-weight:bold\">func</span> <span style=\"color:#00a000\">jsonPrint</span>(items []<span style=\"color:#666\">*</span>client.ServiceEndpoints) {\n</span></span><span style=\"display:flex;\"><span> enc <span style=\"color:#666\">:=</span> json.<span style=\"color:#00a000\">NewEncoder</span>(os.Stdout)\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#a2f;font-weight:bold\">for</span> _, item <span style=\"color:#666\">:=</span> <span style=\"color:#a2f;font-weight:bold\">range</span> items {\n</span></span><span style=\"display:flex;\"><span> _ = enc.<span style=\"color:#00a000\">Encode</span>(item)\n</span></span><span style=\"display:flex;\"><span> }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>(yes, that is the entire program)</p>\n<p>A real backend would of course be much more complex, but this\nillustrates how <code>kpng</code> let you focus on load-balancing.</p>\n<p>You can have several backends connected to a <code>kpng</code> controller, so\nduring development or debug it can be useful to let something like the\n<code>kpng-json</code> backend run in parallel with your real backend.</p>\n<h2 id=\"example\">Example</h2>\n<p>The complete example can be found <a href=\"https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec\">here</a>.</p>\n<p>As an example we implement an &quot;all-ip&quot; backend. It direct all traffic\nfor the externalIPs to a local endpoint, regardless of ports and upper\nlayer protocols. There is a\n<a href=\"https://github.com/kubernetes/enhancements/pull/2611\">KEP</a> for this\nfunction and this example is a much simplified version.</p>\n<p>To direct all traffic from an external address to a local POD <a href=\"https://github.com/kubernetes/enhancements/pull/2611#issuecomment-895061013\">only\none iptables rule is\nneeded</a>,\nfor instance;</p>\n<pre tabindex=\"0\"><code>ip6tables -t nat -A PREROUTING -d 1000::55/128 -j DNAT --to-destination 1100::202\n</code></pre><p>As you can see the addresses are in the call to the backend and all it\nhave to do is:</p>\n<ul>\n<li>Extract the addresses with <code>Local: true</code></li>\n<li>Setup iptables rules for the <code>ExternalIPs</code></li>\n</ul>\n<p>A script doing that may look like:</p>\n<pre tabindex=\"0\"><code>xip=$(cat /tmp/out | jq -r .Service.IPs.ExternalIPs.V6[0])\npodip=$(cat /tmp/out | jq -r &#39;.Endpoints[]|select(.Local == true)|select(.IPs.V6 != null)|.IPs.V6[0]&#39;)\nip6tables -t nat -A PREROUTING -d $xip/128 -j DNAT --to-destination $podip\n</code></pre><p>Assuming the JSON output above is stored in <code>/tmp/out</code> (<a href=\"https://stedolan.github.io/jq/\">jq</a> is an <em>awesome</em> program!).</p>\n<p>As this is an example we make it really simple for ourselves by using\na minor variation of the <code>kpng-json</code> backend above. Instead of just\nprinting, a program is called and the JSON output is passed as <code>stdin</code>\nto that program. The backend can be tested stand-alone:</p>\n<pre tabindex=\"0\"><code>CALLOUT=jq kpng-callout\n</code></pre><p>Where <code>jq</code> can be replaced with your own program or script. A script\nmay look like the example above. For more info and the complete\nexample please see <a href=\"https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec\">https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec</a>.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>While <a href=\"https://github.com/kubernetes-sigs/kpng\">kpng</a> is in early\nstage of development this post wants to show how you may build your\nown specialized K8s proxiers in the future. The only thing your\napplications need to do is to add the\n<code>service.kubernetes.io/service-proxy-name</code> label in the Service\nmanifest.</p>\n<p>It is a tedious process to get new features into the <code>kube-proxy</code> and\nit is not unlikely that they will be rejected, so to write a\nspecialized proxier may be the only option.</p>","PublishedAt":"2021-10-18 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/10/18/use-kpng-to-write-specialized-kube-proxiers/","SourceName":"Kubernetes"}},{"node":{"ID":1251,"Title":"Blog: Introducing ClusterClass and Managed Topologies in Cluster API","Description":"<p><strong>Author:</strong> Fabrizio Pandini (VMware)</p>\n<p>The <a href=\"https://cluster-api.sigs.k8s.io/\">Cluster API community</a> is happy to announce the implementation of <em>ClusterClass and Managed Topologies</em>, a new feature that will greatly simplify how you can provision, upgrade, and operate multiple Kubernetes clusters in a declarative way.</p>\n<h2 id=\"a-little-bit-of-context\">A little bit of context‚Ä¶</h2>\n<p>Before getting into the details, let's take a step back and look at the history of Cluster API.</p>\n<p>The <a href=\"https://github.com/kubernetes-sigs/cluster-api/\">Cluster API project</a> started three years ago, and the first releases focused on extensibility and implementing a declarative API that allows a seamless experience across infrastructure providers. This was a success with many cloud providers: AWS, Azure, Digital Ocean, GCP, Metal3, vSphere and still counting.</p>\n<p>With extensibility addressed, the focus shifted to features, like automatic control plane and etcd management, health-based machine remediation, machine rollout strategies and more.</p>\n<p>Fast forwarding to 2021, with lots of companies using Cluster API to manage fleets of Kubernetes clusters running workloads in production, the community focused its effort on stabilization of both code, APIs, documentation, and on extensive test signals which inform Kubernetes releases.</p>\n<p>With solid foundations in place, and a vibrant and welcoming community that still continues to grow, it was time to plan another iteration on our UX for both new and advanced users.</p>\n<p>Enter ClusterClass and Managed Topologies, tada!</p>\n<h2 id=\"clusterclass\">ClusterClass</h2>\n<p>As the name suggests, ClusterClass and managed topologies are built in two parts.</p>\n<p>The idea behind ClusterClass is simple: define the shape of your cluster once, and reuse it many times, abstracting the complexities and the internals of a Kubernetes cluster away.</p>\n<p><img src=\"https://kubernetes.io/images/blog/2021-10-08-clusterclass-and-managed-topologies/clusterclass.svg\" alt=\"Defining a ClusterClass\"></p>\n<p>ClusterClass, at its heart, is a collection of Cluster and Machine templates. You can use it as a ‚Äústamp‚Äù that can be leveraged to create many clusters of a similar shape.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ClusterClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>my-amazing-cluster-class<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">controlPlane</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>controlplane.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeadmControlPlaneTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>high-availability-control-plane<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">machineInfrastructure</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>infrastructure.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>DockerMachineTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>control-plane-machine<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">workers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">machineDeployments</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">class</span>:<span style=\"color:#bbb\"> </span>type1-workers<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">bootstrap</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>bootstrap.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeadmConfigTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>type1-bootstrap<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">infrastructure</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>infrastructure.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>DockerMachineTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>type1-machine<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">class</span>:<span style=\"color:#bbb\"> </span>type2-workers<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">bootstrap</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>bootstrap.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeadmConfigTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>type2-bootstrap<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">infrastructure</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>DockerMachineTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>infrastructure.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>type2-machine<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">infrastructure</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">ref</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>infrastructure.cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>DockerClusterTemplate<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>cluster-infrastructure<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>The possibilities are endless; you can get a default ClusterClass from the community, ‚Äúoff-the-shelf‚Äù classes from your vendor of choice, ‚Äúcertified‚Äù classes from the platform admin in your company, or even create custom ones for advanced scenarios.</p>\n<h2 id=\"managed-topologies\">Managed Topologies</h2>\n<p>Managed Topologies let you put the power of ClusterClass into action.</p>\n<p>Given a ClusterClass, you can create many Clusters of a similar shape by providing a single resource, the Cluster.</p>\n<p><img src=\"https://kubernetes.io/images/blog/2021-10-08-clusterclass-and-managed-topologies/create-cluster.svg\" alt=\"Create a Cluster with ClusterClass\"></p>\n<p>Here is an example:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>cluster.x-k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Cluster<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>my-amazing-cluster<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>bar<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">topology</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># define a managed topology</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">class</span>:<span style=\"color:#bbb\"> </span>my-amazing-cluster-class<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># use the ClusterClass mentioned earlier</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">version</span>:<span style=\"color:#bbb\"> </span>v1.21.2<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">controlPlane</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">replicas</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">3</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">workers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">machineDeployments</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">class</span>:<span style=\"color:#bbb\"> </span>type1-workers<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>big-pool-of-machines<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">replicas</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">5</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">class</span>:<span style=\"color:#bbb\"> </span>type2-workers<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>small-pool-of-machines<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">replicas</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">1</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>But there is more than simplified cluster creation. Now the Cluster acts as a single control point for your entire topology.</p>\n<p>All the power of Cluster API, extensibility, lifecycle automation, stability, all the features required for managing an enterprise grade Kubernetes cluster on the infrastructure provider of your choice are now at your fingertips: you can create your Cluster, add new machines, upgrade to the next Kubernetes version, and all from a single place.</p>\n<p>It is just as simple as it looks!</p>\n<h2 id=\"what-s-next\">What‚Äôs next</h2>\n<p>While the amazing Cluster API community is working hard to deliver the first version of ClusterClass and managed topologies later this year, we are already looking forward to what comes next for the project and its ecosystem.</p>\n<p>There are a lot of great ideas and opportunities ahead!</p>\n<p>We want to make managed topologies even more powerful and flexible, allowing users to dynamically change bits of a ClusterClass according to the specific needs of a Cluster; this will ensure the same simple and intuitive UX for solving complex problems like e.g. selecting machine image for a specific Kubernetes version and for a specific region of your infrastructure provider, or injecting proxy configurations in the entire Cluster, and so on.</p>\n<p>Stay tuned for what comes next, and if you have any questions, comments or suggestions:</p>\n<ul>\n<li>Chat with us on the Kubernetes <a href=\"http://slack.k8s.io/\">Slack</a>:<a href=\"https://kubernetes.slack.com/archives/C8TSNPY4T\">#cluster-api</a></li>\n<li>Join the SIG Cluster Lifecycle <a href=\"https://groups.google.com/g/kubernetes-sig-cluster-lifecycle\">Google Group</a> to receive calendar invites and gain access to documents</li>\n<li>Join our <a href=\"https://zoom.us/j/861487554\">Zoom meeting</a>, every Wednesday at 10:00 Pacific Time</li>\n<li>Check out the <a href=\"https://cluster-api.sigs.k8s.io/user/quick-start.html\">ClusterClass quick-start</a> for the Docker provider (CAPD) in the Cluster API book.</li>\n<li><em>UPDATE</em>: Check out the <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/index.html\">ClusterClass experimental feature</a> documentation in the Cluster API book.</li>\n</ul>","PublishedAt":"2021-10-08 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/10/08/capi-clusterclass-and-managed-topologies/","SourceName":"Kubernetes"}},{"node":{"ID":1252,"Title":"Blog: A Closer Look at NSA/CISA Kubernetes Hardening Guidance","Description":"<p><strong>Authors:</strong> Jim Angel (Google), Pushkar Joglekar (VMware), and Savitha\nRaghunathan (Red Hat)</p>\n<div class=\"alert alert-primary\" role=\"alert\">\n<h4 class=\"alert-heading\">Disclaimer</h4>\nThe open source tools listed in this article are to serve as examples only\nand are in no way a direct recommendation from the Kubernetes community or authors.\n</div>\n<h2 id=\"background\">Background</h2>\n<p>USA's National Security Agency (NSA) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA)\nreleased, &quot;<a href=\"https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF\">Kubernetes Hardening Guidance</a>&quot;\non August 3rd, 2021. The guidance details threats to Kubernetes environments\nand provides secure configuration guidance to minimize risk.</p>\n<p>The following sections of this blog correlate to the sections in the NSA/CISA guidance.\nAny missing sections are skipped because of limited opportunities to add\nanything new to the existing content.</p>\n<p><em>Note</em>: This blog post is not a substitute for reading the guide. Reading the published\nguidance is recommended before proceeding as the following content is\ncomplementary.</p>\n<h2 id=\"introduction-and-threat-model\">Introduction and Threat Model</h2>\n<p>Note that the threats identified as important by the NSA/CISA, or the intended audience of this guidance, may be different from the threats that other enterprise users of Kubernetes consider important. This section\nis still useful for organizations that care about data, resource theft and\nservice unavailability.</p>\n<p>The guidance highlights the following three sources of compromises:</p>\n<ul>\n<li>Supply chain risks</li>\n<li>Malicious threat actors</li>\n<li>Insider threats (administrators, users, or cloud service providers)</li>\n</ul>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Threat_model\">threat model</a> tries to take a step back and review threats that not only\nexist within the boundary of a Kubernetes cluster but also include the underlying\ninfrastructure and surrounding workloads that Kubernetes does not manage.</p>\n<p>For example, when a workload outside the cluster shares the same physical\nnetwork, it has access to the kubelet and to control plane components: etcd, controller manager, scheduler and API\nserver. Therefore, the guidance recommends having network level isolation\nseparating Kubernetes clusters from other workloads that do not need connectivity\nto Kubernetes control plane nodes. Specifically, scheduler, controller-manager,\netcd only need to be accessible to the API server. Any interactions with Kubernetes\nfrom outside the cluster can happen by providing access to API server port.</p>\n<p>List of ports and protocols for each of these components are\ndefined in <a href=\"https://kubernetes.io/docs/reference/ports-and-protocols/\">Ports and Protocols</a>\nwithin the Kubernetes documentation.</p>\n<blockquote>\n<p>Special note: kube-scheduler and kube-controller-manager uses different ports than the ones mentioned in the guidance</p>\n</blockquote>\n<p>The <a href=\"https://cnsmap.netlify.app/threat-modelling\">Threat modelling</a> section\nfrom the CNCF <a href=\"https://github.com/cncf/tag-security/tree/main/security-whitepaper\">Cloud Native Security Whitepaper + Map</a>\nprovides another perspective on approaching threat modelling Kubernetes, from a\ncloud native lens.</p>\n<h2 id=\"kubernetes-pod-security\">Kubernetes Pod security</h2>\n<p>Kubernetes by default does not guarantee strict workload isolation between pods\nrunning in the same node in a cluster. However, the guidance provides several\ntechniques to enhance existing isolation and reduce the attack surface in case of a\ncompromise.</p>\n<h3 id=\"non-root-containers-and-rootless-container-engines\">&quot;Non-root&quot; containers and &quot;rootless&quot; container engines</h3>\n<p>Several best practices related to basic security principle of least privilege\ni.e. provide only the permissions are needed; no more, no less, are worth a\nsecond look.</p>\n<p>The guide recommends setting non-root user at build time instead of relying on\nsetting <code>runAsUser</code> at runtime in your Pod spec. This is a good practice and provides\nsome level of defense in depth. For example, if the container image is built with user <code>10001</code>\nand the Pod spec misses adding the <code>runAsuser</code> field in its <code>Deployment</code> object. In this\ncase there are certain edge cases that are worth exploring for awareness:</p>\n<ol>\n<li>Pods can fail to start, if the user defined at build time is different from\nthe one defined in pod spec and some files are as a result inaccessible.</li>\n<li>Pods can end up sharing User IDs unintentionally. This can be problematic\neven if the User IDs are non-zero in a situation where a container escape to\nhost file system is possible. Once the attacker has access to the host file\nsystem, they get access to all the file resources that are owned by other\nunrelated pods that share the same UID.</li>\n<li>Pods can end up sharing User IDs, with other node level processes not managed\nby Kubernetes e.g. node level daemons for auditing, vulnerability scanning,\ntelemetry. The threat is similar to the one above where host file system\naccess can give attacker full access to these node level daemons without\nneeding to be root on the node.</li>\n</ol>\n<p>However, none of these cases will have as severe an impact as a container\nrunning as root being able to escape as a root user on the host, which can provide\nan attacker with complete control of the worker node, further allowing lateral\nmovement to other worker or control plane nodes.</p>\n<p>Kubernetes 1.22 introduced\nan <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/\">alpha feature</a>\nthat specifically reduces the impact of such a control plane component running\nas root user to a non-root user through user namespaces.</p>\n<p>That (<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages\">alpha stage</a>) support for user namespaces / rootless mode is available with\nthe following container runtimes:</p>\n<ul>\n<li><a href=\"https://docs.docker.com/engine/security/rootless/\">Docker Engine</a></li>\n<li><a href=\"https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics\">Podman</a></li>\n</ul>\n<p>Some distributions support running in rootless mode, like the following:</p>\n<ul>\n<li><a href=\"https://kind.sigs.k8s.io/docs/user/rootless/\">kind</a></li>\n<li><a href=\"https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental\">k3s</a></li>\n<li><a href=\"https://github.com/rootless-containers/usernetes\">Usernetes</a></li>\n</ul>\n<h3 id=\"immutable-container-filesystems\">Immutable container filesystems</h3>\n<p>The NSA/CISA Kubernetes Hardening Guidance highlights an often overlooked feature <code>readOnlyRootFileSystem</code>, with a\nworking example in <a href=\"https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=42\">Appendix B</a>. This example limits execution and tampering of\ncontainers at runtime. Any read/write activity can then be limited to few\ndirectories by using <code>tmpfs</code> volume mounts.</p>\n<p>However, some applications that modify the container filesystem at runtime, like exploding a WAR or JAR file at container startup,\ncould face issues when enabling this feature. To avoid this issue, consider making minimal changes to the filesystem at runtime\nwhen possible.</p>\n<h3 id=\"building-secure-container-images\">Building secure container images</h3>\n<p>Kubernetes Hardening Guidance also recommends running a scanner at deploy time as an admission controller,\nto prevent vulnerable or misconfigured pods from running in the cluster.\nTheoretically, this sounds like a good approach but there are several caveats to\nconsider before this can be implemented in practice:</p>\n<ul>\n<li>Depending on network bandwidth, available resources and scanner of choice,\nscanning for vulnerabilities for an image can take an indeterminate amount of\ntime. This could lead to slower or unpredictable pod start up times, which\ncould result in spikes of unavailability when apps are serving peak load.</li>\n<li>If the policy that allows or denies pod startup is made using incorrect or\nincomplete data it could result in several false positive or false negative\noutcomes like the following:\n<ul>\n<li>inside a container image, the <code>openssl</code> package is detected as vulnerable. However,\nthe application is written in Golang and uses the Go <code>crypto</code> package for TLS. Therefore, this vulnerability\nis not in the code execution path and as such has minimal impact if it\nremains unfixed.</li>\n<li>A vulnerability is detected in the <code>openssl</code> package for a Debian base image.\nHowever, the upstream Debian community considers this as a Minor impact\nvulnerability and as a result does not release a patch fix for this\nvulnerability. The owner of this image is now stuck with a vulnerability that\ncannot be fixed and a cluster that does not allow the image to run because\nof predefined policy that does not take into account whether the fix for a\nvulnerability is available or not</li>\n<li>A Golang app is built on top of a <a href=\"https://github.com/GoogleContainerTools/distroless\">distroless</a>\nimage, but it is compiled with a Golang version that uses a vulnerable <a href=\"https://pkg.go.dev/std\">standard library</a>.\nThe scanner has\nno visibility into golang version but only on OS level packages. So it\nallows the pod to run in the cluster in spite of the image containing an\napp binary built on vulnerable golang.</li>\n</ul>\n</li>\n</ul>\n<p>To be clear, relying on vulnerability scanners is absolutely a good idea but\npolicy definitions should be flexible enough to allow:</p>\n<ul>\n<li>Creation of exception lists for images or vulnerabilities through labelling</li>\n<li>Overriding the severity with a risk score based on impact of a vulnerability</li>\n<li>Applying the same policies at build time to catch vulnerable images with\nfixable vulnerabilities before they can be deployed into Kubernetes clusters</li>\n</ul>\n<p>Special considerations like offline vulnerability database fetch, may also be\nneeded, if the clusters run in an air-gapped environment and the scanners\nrequire internet access to update the vulnerability database.</p>\n<h3 id=\"pod-security-policies\">Pod Security Policies</h3>\n<p>Since Kubernetes v1.21, the <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-policy/\">PodSecurityPolicy</a>\nAPI and related features are <a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\">deprecated</a>,\nbut some of the guidance in this section will still apply for the next few years, until cluster operators\nupgrade their clusters to newer Kubernetes versions.</p>\n<p>The Kubernetes project is working on a replacement for PodSecurityPolicy.\nKubernetes v1.22 includes an alpha feature called <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\">Pod Security Admission</a>\nthat is intended to allow enforcing a minimum level of isolation between pods.</p>\n<p>The built-in isolation levels for Pod Security Admission are derived\nfrom <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a>, which is a superset of all the components mentioned in Table I <a href=\"https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=17\">page 10</a> of\nthe guidance.</p>\n<p>Information about migrating from PodSecurityPolicy to the Pod Security\nAdmission feature is available\nin\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/\">Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.</p>\n<p>One important behavior mentioned in the guidance that remains the same between\nPod Security Policy and its replacement is that enforcing either of them does\nnot affect pods that are already running. With both PodSecurityPolicy and Pod Security Admission,\nthe enforcement happens during the pod creation\nstage.</p>\n<h3 id=\"hardening-container-engines\">Hardening container engines</h3>\n<p>Some container workloads are less trusted than others but may need to run in the\nsame cluster. In those cases, running them on dedicated nodes that include\nhardened container runtimes that provide stricter pod isolation boundaries can\nact as a useful security control.</p>\n<p>Kubernetes supports\nan API called <a href=\"https://kubernetes.io/docs/concepts/containers/runtime-class/\">RuntimeClass</a> that is\nstable / GA (and, therefore, enabled by default) stage as of Kubernetes v1.20.\nRuntimeClass allows you to ensure that Pods requiring strong isolation are scheduled onto\nnodes that can offer it.</p>\n<p>Some third-party projects that you can use in conjunction with RuntimeClass are:</p>\n<ul>\n<li><a href=\"https://github.com/kata-containers/kata-containers/blob/main/docs/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md#create-runtime-class-for-kata-containers\">kata containers</a></li>\n<li><a href=\"https://gvisor.dev/docs/user_guide/containerd/quick_start/\">gvisor</a></li>\n</ul>\n<p>As discussed here and in the guidance, many features and tooling exist in and around\nKubernetes that can enhance the isolation boundaries between\npods. Based on relevant threats and risk posture, you should pick and choose\nbetween them, instead of trying to apply all the recommendations. Having said that, cluster\nlevel isolation i.e. running workloads in dedicated clusters, remains the strictest workload\nisolation mechanism, in spite of improvements mentioned earlier here and in the guide.</p>\n<h2 id=\"network-separation-and-hardening\">Network Separation and Hardening</h2>\n<p>Kubernetes Networking can be tricky and this section focuses on how to secure\nand harden the relevant configurations. The guide identifies the following as key\ntakeaways:</p>\n<ul>\n<li>Using NetworkPolicies to create isolation between resources,</li>\n<li>Securing the control plane</li>\n<li>Encrypting traffic and sensitive data</li>\n</ul>\n<h3 id=\"network-policies\">Network Policies</h3>\n<p>Network policies can be created with the help of network plugins. In order to\nmake the creation and visualization easier for users, Cilium supports\na <a href=\"https://editor.cilium.io\">web GUI tool</a>. That web GUI lets you create Kubernetes\nNetworkPolicies (a generic API that nevertheless requires a compatible CNI plugin),\nand / or Cilium network policies (CiliumClusterwideNetworkPolicy and CiliumNetworkPolicy,\nwhich only work in clusters that use the Cilium CNI plugin).\nYou can use these APIs to restrict network traffic between pods, and therefore minimize the\nattack vector.</p>\n<p>Another scenario that is worth exploring is the usage of external IPs. Some\nservices, when misconfigured, can create random external IPs. An attacker can take\nadvantage of this misconfiguration and easily intercept traffic. This vulnerability\nhas been reported\nin <a href=\"https://www.cvedetails.com/cve/CVE-2020-8554/\">CVE-2020-8554</a>.\nUsing <a href=\"https://github.com/kubernetes-sigs/externalip-webhook\">externalip-webhook</a>\ncan mitigate this vulnerability by preventing the services from using random\nexternal IPs. <a href=\"https://github.com/kubernetes-sigs/externalip-webhook\">externalip-webhook</a>\nonly allows creation of services that don't require external IPs or whose\nexternal IPs are within the range specified by the administrator.</p>\n<blockquote>\n<p>CVE-2020-8554 - Kubernetes API server in all versions allow an attacker\nwho is able to create a ClusterIP service and set the <code>spec.externalIPs</code> field,\nto intercept traffic to that IP address. Additionally, an attacker who is able to\npatch the <code>status</code> (which is considered a privileged operation and should not\ntypically be granted to users) of a LoadBalancer service can set the\n<code>status.loadBalancer.ingress.ip</code> to similar effect.</p>\n</blockquote>\n<h3 id=\"resource-policies\">Resource Policies</h3>\n<p>In addition to configuring ResourceQuotas and limits, consider restricting how many process\nIDs (PIDs) a given Pod can use, and also to reserve some PIDs for node-level use to avoid\nresource exhaustion. More details to apply these limits can be\nfound in <a href=\"https://kubernetes.io/docs/concepts/policy/pid-limiting/\">Process ID Limits And Reservations</a>.</p>\n<h3 id=\"control-plane-hardening\">Control Plane Hardening</h3>\n<p>In the next section, the guide covers control plane hardening. It is worth\nnoting that\nfrom <a href=\"https://github.com/kubernetes/kubernetes/issues/91506\">Kubernetes 1.20</a>,\ninsecure port from API server, has been removed.</p>\n<h3 id=\"etcd\">Etcd</h3>\n<p>As a general rule, the etcd server should be configured to only trust\ncertificates assigned to the API server. It limits the attack surface and prevents a\nmalicious attacker from gaining access to the cluster. It might be beneficial to\nuse a separate CA for etcd, as it by default trusts all the certificates issued\nby the root CA.</p>\n<h3 id=\"kubeconfig-files\">Kubeconfig Files</h3>\n<p>In addition to specifying the token and certificates directly, <code>.kubeconfig</code>\nsupports dynamic retrieval of temporary tokens using auth provider plugins.\nBeware of the possibility of malicious\nshell <a href=\"https://banzaicloud.com/blog/kubeconfig-security/\">code execution</a> in a\n<code>kubeconfig</code> file. Once attackers gain access to the cluster, they can steal ssh\nkeys/secrets or more.</p>\n<h3 id=\"secrets\">Secrets</h3>\n<p>Kubernetes <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/\">Secrets</a> is the native way of managing secrets as a Kubernetes\nAPI object. However, in some scenarios such as a desire to have a single source of truth for all app secrets, irrespective of whether they run on Kubernetes or not, secrets can be managed loosely coupled with\nKubernetes and consumed by pods through side-cars or init-containers with minimal usage of Kubernetes Secrets API.</p>\n<p><a href=\"https://github.com/external-secrets/kubernetes-external-secrets\">External secrets providers</a>\nand <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver\">csi-secrets-store</a>\nare some of these alternatives to Kubernetes Secrets</p>\n<h2 id=\"log-auditing\">Log Auditing</h2>\n<p>The NSA/CISA guidance stresses monitoring and alerting based on logs. The key points\ninclude logging at the host level, application level, and on the cloud. When\nrunning Kubernetes in production, it's important to understand who's\nresponsible, and who's accountable, for each layer of logging.</p>\n<h3 id=\"kubernetes-api-auditing\">Kubernetes API auditing</h3>\n<p>One area that deserves more focus is what exactly should alert or be logged. The\ndocument outlines a sample policy in <a href=\"https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=55\">Appendix L: Audit Policy</a> that logs all\nRequestResponse's including metadata and request / response bodies. While helpful for a demo, it may not be practical for production.</p>\n<p>Each organization needs to evaluate their\nown threat model and build an audit policy that complements or helps troubleshooting incident response. Think\nabout how someone would attack your organization and what audit trail could identify it. Review more advanced options for tuning audit logs in the official <a href=\"https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy\">audit logging documentation</a>.\nIt's crucial to tune your audit logs to only include events that meet your threat model. A minimal audit policy that logs everything at <code>metadata</code> level can also be a good starting point.</p>\n<p>Audit logging configurations can also be tested with\nkind following these <a href=\"https://kind.sigs.k8s.io/docs/user/auditing\">instructions</a>.</p>\n<h3 id=\"streaming-logs-and-auditing\">Streaming logs and auditing</h3>\n<p>Logging is important for threat and anomaly detection. As the document outlines,\nit's a best practice to scan and alert on logs as close to real time as possible\nand to protect logs from tampering if a compromise occurs. It's important to\nreflect on the various levels of logging and identify the critical areas such as\nAPI endpoints.</p>\n<p>Kubernetes API audit logging can stream to a webhook and there's an example in <a href=\"https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=58\">Appendix N: Webhook configuration</a>. Using a webhook could be a method that\nstores logs off cluster and/or centralizes all audit logs. Once logs are\ncentrally managed, look to enable alerting based on critical events. Also ensure\nyou understand what the baseline is for normal activities.</p>\n<h3 id=\"alert-identification\">Alert identification</h3>\n<p>While the guide stressed the importance of notifications, there is not a blanket\nevent list to alert from. The alerting requirements vary based on your own\nrequirements and threat model. Examples include the following events:</p>\n<ul>\n<li>Changes to the <code>securityContext</code> of a Pod</li>\n<li>Updates to admission controller configs</li>\n<li>Accessing certain files / URLs</li>\n</ul>\n<h3 id=\"additional-logging-resources\">Additional logging resources</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=OPuu8wsu2Zc\">Seccomp Security Profiles and You: A Practical Guide - Duffie Cooley</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=ZJgaGJm9NJE\">TGI Kubernetes 119: Gatekeeper and OPA</a></li>\n<li><a href=\"https://www.lacework.com/blog/hiding-in-plaintext-sight-abusing-the-lack-of-kubernetes-auditing-policies/\">Abusing The Lack of Kubernetes Auditing Policies</a></li>\n<li><a href=\"https://kubernetes.io/blog/2021/08/25/seccomp-default/\">Enable seccomp for all workloads with a new v1.22 alpha feature</a></li>\n<li><a href=\"https://www.twitch.tv/videos/1147889860\">This Week in Cloud Native: Auditing / Pod Security</a></li>\n</ul>\n<h2 id=\"upgrading-and-application-security-practices\">Upgrading and Application Security practices</h2>\n<p>Kubernetes releases three times per year, so upgrade-related toil is a common problem for\npeople running production clusters. In addition to this, operators must\nregularly upgrade the underlying node's operating system and running\napplications. This is a best practice to ensure continued support and to reduce\nthe likelihood of bugs or vulnerabilities.</p>\n<p>Kubernetes supports the three most recent stable releases. While each Kubernetes\nrelease goes through a large number of tests before being published, some\nteams aren't comfortable running the latest stable release until some time has\npassed. No matter what version you're running, ensure that patch upgrades\nhappen frequently or automatically. More information can be found in\nthe <a href=\"https://kubernetes.io/releases/version-skew-policy/\">version skew</a> policy\npages.</p>\n<p>When thinking about how you'll manage node OS upgrades, consider ephemeral\nnodes. Having the ability to destroy and add nodes allows your team to respond\nquicker to node issues. In addition, having deployments that tolerate node\ninstability (and a culture that encourages frequent deployments) allows for\neasier cluster upgrades.</p>\n<p>Additionally, it's worth reiterating from the guidance that periodic\nvulnerability scans and penetration tests can be performed on the various system\ncomponents to proactively look for insecure configurations and vulnerabilities.</p>\n<h3 id=\"finding-release-security-information\">Finding release &amp; security information</h3>\n<p>To find the most recent Kubernetes supported versions, refer to\n<a href=\"https://k8s.io/releases\">https://k8s.io/releases</a>, which includes minor versions. It's good to stay up to date with\nyour minor version patches.</p>\n<p>If you're running a managed Kubernetes offering, look for their release\ndocumentation and find their various security channels.</p>\n<p>Subscribe to\nthe <a href=\"https://groups.google.com/g/kubernetes-announce\">Kubernetes Announce mailing list</a>.\nThe Kubernetes Announce mailing list is searchable for terms such\nas &quot;<a href=\"https://groups.google.com/g/kubernetes-announce/search?q=%5BSecurity%20Advisory%5D\">Security Advisories</a>&quot;.\nYou can set up alerts and email notifications as long as you know what key\nwords to alert on.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, it is fantastic to see security practitioners sharing this\nlevel of detailed guidance in public. This guidance further highlights\nKubernetes going mainstream and how securing Kubernetes clusters and the\napplication containers running on Kubernetes continues to need attention and focus of\npractitioners. Only a few weeks after the guidance was published, an open source\ntool <a href=\"https://github.com/armosec/kubescape\">kubescape</a> to validate cluster\nagainst this guidance became available.</p>\n<p>This tool can be a great starting point to check the current state of your\nclusters, after which you can use the information in this blog post and in the guidance to assess\nwhere improvements can be made.</p>\n<p>Finally, it is worth reiterating that not all controls in this guidance will\nmake sense for all practitioners. The best way to know which controls matter is\nto rely on the threat model of your own Kubernetes environment.</p>\n<p><em>A special shout out and thanks to Rory McCune (@raesene) for his inputs to this blog post</em></p>","PublishedAt":"2021-10-05 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/","SourceName":"Kubernetes"}},{"node":{"ID":1253,"Title":"Blog: How to Handle Data Duplication in Data-Heavy Kubernetes Environments","Description":"<p><strong>Authors:</strong>\nAugustinas Stirbis (CAST AI)</p>\n<h2 id=\"why-duplicate-data\">Why Duplicate Data?</h2>\n<p>It‚Äôs convenient to create a copy of your application with a copy of its state for each team.\nFor example, you might want a separate database copy to test some significant schema changes\nor develop other disruptive operations like bulk insert/delete/update...</p>\n<p><strong>Duplicating data takes a lot of time.</strong> That‚Äôs because you need first to download\nall the data from a source block storage provider to compute and then send\nit back to a storage provider again. There‚Äôs a lot of network traffic and CPU/RAM used in this process.\nHardware acceleration by offloading certain expensive operations to dedicated hardware is\n<strong>always a huge performance boost</strong>. It reduces the time required to complete an operation by orders\nof magnitude.</p>\n<h2 id=\"volume-snapshots-to-the-rescue\">Volume Snapshots to the rescue</h2>\n<p>Kubernetes introduced <a href=\"https://kubernetes.io/docs/concepts/storage/volume-snapshots/\">VolumeSnapshots</a> as alpha in 1.12,\nbeta in 1.17, and the Generally Available version in 1.20.\nVolumeSnapshots use specialized APIs from storage providers to duplicate volume of data.</p>\n<p>Since data is already in the same storage device (array of devices), duplicating data is usually\na metadata operation for storage providers with local snapshots (majority of on-premise storage providers).\nAll you need to do is point a new disk to an immutable snapshot and only\nsave deltas (or let it do a full-disk copy). As an operation that is inside the storage back-end,\nit‚Äôs much quicker and usually doesn‚Äôt involve sending traffic over the network.\nPublic Clouds storage providers under the hood work a bit differently. They save snapshots\nto Object Storage and then copy back from Object storage to Block storage when &quot;duplicating&quot; disk.\nTechnically there is a lot of Compute and network resources spent on Cloud providers side,\nbut from Kubernetes user perspective VolumeSnapshots work the same way whether is it local or\nremote snapshot storage provider and no Compute and Network resources are involved in this operation.</p>\n<h2 id=\"sounds-like-we-have-our-solution-right\">Sounds like we have our solution, right?</h2>\n<p>Actually, VolumeSnapshots are namespaced, and Kubernetes protects namespaced data from\nbeing shared between tenants (Namespaces). This Kubernetes limitation is a conscious design\ndecision so that a Pod running in a different namespace can‚Äôt mount another application‚Äôs\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims\">PersistentVolumeClaim</a> (PVC).</p>\n<p>One way around it would be to create multiple volumes with duplicate data in one namespace.\nHowever, you could easily reference the wrong copy.</p>\n<p>So the idea is to separate teams/initiatives by namespaces to avoid that and generally\nlimit access to the production namespace.</p>\n<h2 id=\"solution-creating-a-golden-snapshot-externally\">Solution? Creating a Golden Snapshot externally</h2>\n<p>Another way around this design limitation is to create Snapshot externally (not through Kubernetes).\nThis is also called pre-provisioning a snapshot manually. Next, I will import it\nas a multi-tenant golden snapshot that can be used for many namespaces. Below illustration will be\nfor AWS EBS (Elastic Block Storage) and GCE PD (Persistent Disk) services.</p>\n<h3 id=\"high-level-plan-for-preparing-the-golden-snapshot\">High-level plan for preparing the Golden Snapshot</h3>\n<ol>\n<li>Identify Disk (EBS/Persistent Disk) that you want to clone with data in the cloud provider</li>\n<li>Make a Disk Snapshot (in cloud provider console)</li>\n<li>Get Disk Snapshot ID</li>\n</ol>\n<h3 id=\"high-level-plan-for-cloning-data-for-each-team\">High-level plan for cloning data for each team</h3>\n<ol>\n<li>Create Namespace ‚Äúsandbox01‚Äù</li>\n<li>Import Disk Snapshot (ID) as VolumeSnapshotContent to Kubernetes</li>\n<li>Create VolumeSnapshot in the Namespace &quot;sandbox01&quot; mapped to VolumeSnapshotContent</li>\n<li>Create the PersistentVolumeClaim from VolumeSnapshot</li>\n<li>Install Deployment or StatefulSet with PVC</li>\n</ol>\n<h2 id=\"step-1-identify-disk\">Step 1: Identify Disk</h2>\n<p>First, you need to identify your golden source. In my case, it‚Äôs a PostgreSQL database\non PersistentVolumeClaim ‚Äúpostgres-pv-claim‚Äù in the ‚Äúproduction‚Äù namespace.</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">kubectl -n &lt;namespace&gt; get pvc &lt;pvc-name&gt; -o jsonpath=&#39;{.spec.volumeName}&#39;\n</code></pre><p>The output will look similar to:</p>\n<pre tabindex=\"0\"><code>pvc-3096b3ba-38b6-4fd1-a42f-ec99176ed0d90\n</code></pre><h2 id=\"step-2-prepare-your-golden-source\">Step 2: Prepare your golden source</h2>\n<p>You need to do this once or every time you want to refresh your golden data.</p>\n<h3 id=\"make-a-disk-snapshot\">Make a Disk Snapshot</h3>\n<p>Go to AWS EC2 or GCP Compute Engine console and search for an EBS volume\n(on AWS) or Persistent Disk (on GCP), that has a label matching the last output.\nIn this case I saw: <code>pvc-3096b3ba-38b6-4fd1-a42f-ec99176ed0d9</code>.</p>\n<p>Click on Create snapshot and give it a name. You can do it in Console manually,\nin AWS CloudShell / Google Cloud Shell, or in the terminal. To create a snapshot in the\nterminal you must have the AWS CLI tool (<code>aws</code>) or Google's CLI (<code>gcloud</code>)\ninstalled and configured.</p>\n<p>Here‚Äôs the command to create snapshot on GCP:</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">gcloud compute disks snapshot &lt;cloud-disk-id&gt; --project=&lt;gcp-project-id&gt; --snapshot-names=&lt;set-new-snapshot-name&gt; --zone=&lt;availability-zone&gt; --storage-location=&lt;region&gt;\n</code></pre>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-09-07-data-duplication-in-data-heavy-k8s-env/create-volume-snapshot-gcp.png\"\nalt=\"Screenshot of a terminal showing volume snapshot creation on GCP\"/> <figcaption>\n<h4>GCP snapshot creation</h4>\n</figcaption>\n</figure>\n<p>GCP identifies the disk by its PVC name, so it‚Äôs direct mapping. In AWS, you need to\nfind volume by the CSIVolumeName AWS tag with PVC name value first that will be used for snapshot creation.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-09-07-data-duplication-in-data-heavy-k8s-env/identify-volume-aws.png\"\nalt=\"Screenshot of AWS web console, showing EBS volume identification\"/> <figcaption>\n<h4>Identify disk ID on AWS</h4>\n</figcaption>\n</figure>\n<p>Mark done Volume (volume-id) <code>vol-00c7ecd873c6fb3ec</code> and ether create EBS snapshot in AWS Console, or use <code>aws cli</code>.</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">aws ec2 create-snapshot --volume-id &#39;&lt;volume-id&gt;&#39; --description &#39;&lt;set-new-snapshot-name&gt;&#39; --tag-specifications &#39;ResourceType=snapshot&#39;\n</code></pre><h2 id=\"step-3-get-your-disk-snapshot-id\">Step 3: Get your Disk Snapshot ID</h2>\n<p>In AWS, the command above will output something similar to:</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">&#34;SnapshotId&#34;: &#34;snap-09ed24a70bc19bbe4&#34;\n</code></pre><p>If you‚Äôre using the GCP cloud, you can get the snapshot ID from the gcloud command by querying for the snapshot‚Äôs given name:</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">gcloud compute snapshots --project=&lt;gcp-project-id&gt; describe &lt;new-snapshot-name&gt; | grep id:\n</code></pre><p>You should get similar output to:</p>\n<pre tabindex=\"0\"><code>id: 6645363163809389170\n</code></pre><h2 id=\"step-4-create-a-development-environment-for-each-team\">Step 4: Create a development environment for each team</h2>\n<p>Now I have my Golden Snapshot, which is immutable data. Each team will get a copy\nof this data, and team members can modify it as they see fit, given that a new EBS/persistent\ndisk will be created for each team.</p>\n<p>Below I will define a manifest for each namespace. To save time, you can replace\nthe namespace name (such as changing ‚Äúsandbox01‚Äù ‚Üí ‚Äúsandbox42‚Äù) using tools\nsuch as <code>sed</code> or <code>yq</code>, with Kubernetes-aware templating tools like\n<a href=\"https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/\">Kustomize</a>,\nor using variable substitution in a CI/CD pipeline.</p>\n<p>Here's an example manifest:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshotContent<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>postgresql-orders-db-sandbox01<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>sandbox01<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">deletionPolicy</span>:<span style=\"color:#bbb\"> </span>Retain<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>pd.csi.storage.gke.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">snapshotHandle</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#39;gcp/projects/staging-eu-castai-vt5hy2/global/snapshots/6645363163809389170&#39;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeSnapshotRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>postgresql-orders-db-snap<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>sandbox01<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>postgresql-orders-db-snap<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>sandbox01<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeSnapshotContentName</span>:<span style=\"color:#bbb\"> </span>postgresql-orders-db-sandbox01<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>In Kubernetes, VolumeSnapshotContent (VSC) objects are not namespaced.\nHowever, I need a separate VSC for each different namespace to use, so the\n<code>metadata.name</code> of each VSC must also be different. To make that straightfoward,\nI used the target namespace as part of the name.</p>\n<p>Now it‚Äôs time to replace the driver field with the CSI (Container Storage Interface) driver\ninstalled in your K8s cluster. Major cloud providers have CSI driver for block storage that\nsupport VolumeSnapshots but quite often CSI drivers are not installed by default, consult\nwith your Kubernetes provider.</p>\n<p>That manifest above defines a VSC that works on GCP.\nOn AWS, driver and SnashotHandle values might look like:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-YAML\" data-lang=\"YAML\"><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>ebs.csi.aws.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">snapshotHandle</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;snap-07ff83d328c981c98&#34;</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>At this point, I need to use the <em>Retain</em> policy, so that the CSI driver doesn‚Äôt try to\ndelete my manually created EBS disk snapshot.</p>\n<p>For GCP, you will have to build this string by hand - add a full project ID and snapshot ID.\nFor AWS, it‚Äôs just a plain snapshot ID.</p>\n<p>VSC also requires specifying which VolumeSnapshot (VS) will use it, so VSC and VS are\nreferencing each other.</p>\n<p>Now I can create PersistentVolumeClaim from VS above. It‚Äôs important to set this first:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#00f;font-weight:bold\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>postgres-pv-claim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>sandbox01<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSource</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>postgresql-orders-db-snap<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>21Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>If default StorageClass has <a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode\">WaitForFirstConsumer</a> policy,\nthen the actual Cloud Disk will be created from the Golden Snapshot only when some Pod bounds that PVC.</p>\n<p>Now I assign that PVC to my Pod (in my case, it‚Äôs Postgresql) as I would with any other PVC.</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">kubectl -n &lt;namespace&gt; get volumesnapshotContent,volumesnapshot,pvc,pod\n</code></pre><p>Both VS and VSC should be <em>READYTOUSE</em> true, PVC bound, and the Pod (from Deployment or StatefulSet) running.</p>\n<p><strong>To keep on using data from my Golden Snapshot, I just need to repeat this for the\nnext namespace and voil√†! No need to waste time and compute resources on the duplication process.</strong></p>","PublishedAt":"2021-09-29 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/09/29/how-to-handle-data-duplication-in-data-heavy-kubernetes-environments/","SourceName":"Kubernetes"}},{"node":{"ID":1254,"Title":"Blog: Spotlight on SIG Node","Description":"<p><strong>Author:</strong> Dewan Ahmed, Red Hat</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>In Kubernetes, a <em>Node</em> is a representation of a single machine in your cluster. <a href=\"https://github.com/kubernetes/community/tree/master/sig-node\">SIG Node</a> owns that very important Node component and supports various subprojects such as Kubelet, Container Runtime Interface (CRI) and more to support how the pods and host resources interact. In this blog, we have summarized our conversation with <a href=\"https://twitter.com/ehashdn\">Elana Hashman (EH)</a> &amp; <a href=\"https://twitter.com/SergeyKanzhelev\">Sergey Kanzhelev (SK)</a>, who walk us through the various aspects of being a part of the SIG and share some insights about how others can get involved.</p>\n<h2 id=\"a-summary-of-our-conversation\">A summary of our conversation</h2>\n<h3 id=\"could-you-tell-us-a-little-about-what-sig-node-does\">Could you tell us a little about what SIG Node does?</h3>\n<p>SK: SIG Node is a vertical SIG responsible for the components that support the controlled interactions between the pods and host resources. We manage the lifecycle of pods that are scheduled to a node. This SIG's focus is to enable a broad set of workload types, including workloads with hardware specific or performance sensitive requirements. All while maintaining isolation boundaries between pods on a node, as well as the pod and the host. This SIG maintains quite a few components and has many external dependencies (like container runtimes or operating system features), which makes the complexity we deal with huge. We tame the complexity and aim to continuously improve node reliability.</p>\n<h3 id=\"sig-node-is-a-vertical-sig-could-you-explain-a-bit-more\">&quot;SIG Node is a vertical SIG&quot; could you explain a bit more?</h3>\n<p>EH: There are two kinds of SIGs: horizontal and vertical. Horizontal SIGs are concerned with a particular function of every component in Kubernetes: for example, SIG Security considers security aspects of every component in Kubernetes, or SIG Instrumentation looks at the logs, metrics, traces and events of every component in Kubernetes. Such SIGs don't tend to own a lot of code.</p>\n<p>Vertical SIGs, on the other hand, own a single component, and are responsible for approving and merging patches to that code base. SIG Node owns the &quot;Node&quot; vertical, pertaining to the kubelet and its lifecycle. This includes the code for the kubelet itself, as well as the node controller, the container runtime interface, and related subprojects like the node problem detector.</p>\n<h3 id=\"how-did-the-ci-subproject-start-is-this-specific-to-sig-node-and-how-does-it-help-the-sig\">How did the CI subproject start? Is this specific to SIG Node and how does it help the SIG?</h3>\n<p>SK: The subproject started as a follow up after one of the releases was blocked by numerous test failures of critical tests. These tests haven‚Äôt started falling all at once, rather continuous lack of attention led to slow degradation of tests quality. SIG Node was always prioritizing quality and reliability, and forming of the subproject was a way to highlight this priority.</p>\n<h3 id=\"as-the-3rd-largest-sig-in-terms-of-number-of-issues-and-prs-how-does-your-sig-juggle-so-much-work\">As the 3rd largest SIG in terms of number of issues and PRs, how does your SIG juggle so much work?</h3>\n<p>EH: It helps to be organized. When I increased my contributions to the SIG in January of 2021, I found myself overwhelmed by the volume of pull requests and issues and wasn't sure where to start. We were already tracking test-related issues and pull requests on the CI subproject board, but that was missing a lot of our bugfixes and feature work. So I began putting together a triage board for the rest of our pull requests, which allowed me to sort each one by status and what actions to take, and documented its use for other contributors. We closed or merged over 500 issues and pull requests tracked by our two boards in each of the past two releases. The Kubernetes devstats showed that we have significantly increased our velocity as a result.</p>\n<p>In June, we ran our first bug scrub event to work through the backlog of issues filed against SIG Node, ensuring they were properly categorized. We closed over 130 issues over the course of this 48 hour global event, but as of writing we still have 333 open issues.</p>\n<h3 id=\"why-should-new-and-existing-contributors-consider-joining-sig-node\">Why should new and existing contributors consider joining SIG Node?</h3>\n<p>SK: Being a SIG Node contributor gives you skills and recognition that are rewarding and useful. Understanding under the hood of a kubelet helps architecting better apps, tune and optimize those apps, and gives leg up in issues troubleshooting. If you are a new contributor, SIG Node gives you the foundational knowledge that is key to understanding why other Kubernetes components are designed the way they are. Existing contributors may benefit as many features will require SIG Node changes one way or another. So being a SIG Node contributor helps building features in other SIGs faster.</p>\n<p>SIG Node maintains numerous components, many of which have dependency on external projects or OS features. This makes the onboarding process quite lengthy and demanding. But if you are up for a challenge, there is always a place for you, and a group of people to support.</p>\n<h3 id=\"what-do-you-do-to-help-new-contributors-get-started\">What do you do to help new contributors get started?</h3>\n<p>EH: Getting started in SIG Node can be intimidating, since there is so much work to be done, our SIG meetings are very large, and it can be hard to find a place to start.</p>\n<p>I always encourage new contributors to work on things that they have some investment in already. In SIG Node, that might mean volunteering to help fix a bug that you have personally been affected by, or helping to triage bugs you care about by priority.</p>\n<p>To come up to speed on any open source code base, there are two strategies you can take: start by exploring a particular issue deeply, and follow that to expand the edges of your knowledge as needed, or briefly review as many issues and change requests as you possibly can to get a higher level picture of how the component works. Ultimately, you will need to do both if you want to become a Node reviewer or approver.</p>\n<p><a href=\"https://twitter.com/dims\">Davanum Srinivas</a> and I each ran a cohort of group mentoring to help teach new contributors the skills to become Node reviewers, and if there's interest we can work to find a mentor to run another session. I also encourage new contributors to attend our Node CI Subproject meeting: it's a smaller audience and we don't record the triage sessions, so it can be a less intimidating way to get started with the SIG.</p>\n<h3 id=\"are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn\">Are there any particular skills you‚Äôd like to recruit for? What skills are contributors to SIG Usability likely to learn?</h3>\n<p>SK: SIG Node works on many workstreams in very different areas. All of these areas are on system level. For the typical code contributions you need to have a passion for building and utilizing low level APIs and writing performant and reliable components. Being a contributor you will learn how to debug and troubleshoot, profile, and monitor these components, as well as user workload that is run by these components. Often, with the limited to no access to Nodes, as they are running production workloads.</p>\n<p>The other way of contribution is to help document SIG node features. This type of contribution requires a deep understanding of features, and ability to explain them in simple terms.</p>\n<p>Finally, we are always looking for feedback on how best to run your workload. Come and explain specifics of it, and what features in SIG Node components may help to run it better.</p>\n<h3 id=\"what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-node\">What are you getting positive feedback on, and what‚Äôs coming up next for SIG Node?</h3>\n<p>EH: Over the past year SIG Node has adopted some new processes to help manage our feature development and Kubernetes enhancement proposals, and other SIGs have looked to us for inspiration in managing large workloads. I hope that this is an area we can continue to provide leadership in and further iterate on.</p>\n<p>We have a great balance of new features and deprecations in flight right now. Deprecations of unused or difficult to maintain features help us keep technical debt and maintenance load under control, and examples include the dockershim and DynamicKubeletConfiguration deprecations. New features will unlock additional functionality in end users' clusters, and include exciting features like support for cgroups v2, swap memory, graceful node shutdowns, and device management policies.</p>\n<h3 id=\"any-closing-thoughts-resources-you-d-like-to-share\">Any closing thoughts/resources you‚Äôd like to share?</h3>\n<p>SK/EH: It takes time and effort to get to any open source community. SIG Node may overwhelm you at first with the number of participants, volume of work, and project scope. But it is totally worth it. Join our welcoming community! <a href=\"https://github.com/kubernetes/community/tree/master/sig-node\">SIG Node GitHub Repo</a> contains many useful resources including Slack, mailing list and other contact info.</p>\n<h2 id=\"wrap-up\">Wrap Up</h2>\n<p>SIG Node hosted a <a href=\"https://www.youtube.com/watch?v=z5aY4e2RENA\">KubeCon + CloudNativeCon Europe 2021 talk</a> with an intro and deep dive to their awesome SIG. Join the SIG's meetings to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream Node team as a contributor!</p>","PublishedAt":"2021-09-27 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/09/27/sig-node-spotlight-2021/","SourceName":"Kubernetes"}},{"node":{"ID":1255,"Title":"Blog: Introducing Single Pod Access Mode for PersistentVolumes","Description":"<p><strong>Author:</strong> Chris Henzie (Google)</p>\n<p>Last month's release of Kubernetes v1.22 introduced a new ReadWriteOncePod access mode for <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes\">PersistentVolumes</a> and <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims\">PersistentVolumeClaims</a>.\nWith this alpha feature, Kubernetes allows you to restrict volume access to a single pod in the cluster.</p>\n<h2 id=\"what-are-access-modes-and-why-are-they-important\">What are access modes and why are they important?</h2>\n<p>When using storage, there are different ways to model how that storage is consumed.</p>\n<p>For example, a storage system like a network file share can have many users all reading and writing data simultaneously.\nIn other cases maybe everyone is allowed to read data but not write it.\nFor highly sensitive data, maybe only one user is allowed to read and write data but nobody else.</p>\n<p>In the world of Kubernetes, <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\">access modes</a> are the way you can define how durable storage is consumed.\nThese access modes are a part of the spec for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>shared-cache<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteMany<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># Allow many nodes to access shared-cache simultaneously.</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Before v1.22, Kubernetes offered three access modes for PVs and PVCs:</p>\n<ul>\n<li>ReadWriteOnce ‚Äì the volume can be mounted as read-write by a single node</li>\n<li>ReadOnlyMany ‚Äì the volume can be mounted read-only by many nodes</li>\n<li>ReadWriteMany ‚Äì the volume can be mounted as read-write by many nodes</li>\n</ul>\n<p>These access modes are enforced by Kubernetes components like the <code>kube-controller-manager</code> and <code>kubelet</code> to ensure only certain pods are allowed to access a given PersistentVolume.</p>\n<h2 id=\"what-is-this-new-access-mode-and-how-does-it-work\">What is this new access mode and how does it work?</h2>\n<p>Kubernetes v1.22 introduced a fourth access mode for PVs and PVCs, that you can use for CSI volumes:</p>\n<ul>\n<li>ReadWriteOncePod ‚Äì the volume can be mounted as read-write by a single pod</li>\n</ul>\n<p>If you create a pod with a PVC that uses the ReadWriteOncePod access mode, Kubernetes ensures that pod is the only pod across your whole cluster that can read that PVC or write to it.</p>\n<p>If you create another pod that references the same PVC with this access mode, the pod will fail to start because the PVC is already in use by another pod.\nFor example:</p>\n<pre tabindex=\"0\"><code>Events:\nType Reason Age From Message\n---- ------ ---- ---- -------\nWarning FailedScheduling 1s default-scheduler 0/1 nodes are available: 1 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode.\n</code></pre><h3 id=\"how-is-this-different-than-the-readwriteonce-access-mode\">How is this different than the ReadWriteOnce access mode?</h3>\n<p>The ReadWriteOnce access mode restricts volume access to a single <em>node</em>, which means it is possible for multiple pods on the same node to read from and write to the same volume.\nThis could potentially be a major problem for some applications, especially if they require at most one writer for data safety guarantees.</p>\n<p>With ReadWriteOncePod these issues go away.\nSet the access mode on your PVC, and Kubernetes guarantees that only a single pod has access.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>The ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes.\nAs a first step you need to enable the ReadWriteOncePod <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates\">feature gate</a> for <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kubelet</code>.\nYou can enable the feature by setting command line arguments:</p>\n<pre tabindex=\"0\"><code>--feature-gates=&#34;...,ReadWriteOncePod=true&#34;\n</code></pre><p>You also need to update the following CSI sidecars to these versions or greater:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0\">csi-provisioner:v3.0.0+</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0\">csi-attacher:v3.3.0+</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0\">csi-resizer:v1.3.0+</a></li>\n</ul>\n<h3 id=\"creating-a-persistentvolumeclaim\">Creating a PersistentVolumeClaim</h3>\n<p>In order to use the ReadWriteOncePod access mode for your PVs and PVCs, you will need to create a new PVC with the access mode:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>single-writer-only<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOncePod<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># Allow only a single pod to access single-writer-only.</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>1Gi<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>If your storage plugin supports <a href=\"https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/\">dynamic provisioning</a>, new PersistentVolumes will be created with the ReadWriteOncePod access mode applied.</p>\n<h4 id=\"migrating-existing-persistentvolumes\">Migrating existing PersistentVolumes</h4>\n<p>If you have existing PersistentVolumes, they can be migrated to use ReadWriteOncePod.</p>\n<p>In this example, we already have a &quot;cat-pictures-pvc&quot; PersistentVolumeClaim that is bound to a &quot;cat-pictures-pv&quot; PersistentVolume, and a &quot;cat-pictures-writer&quot; Deployment that uses this PersistentVolumeClaim.</p>\n<p>As a first step, you need to edit your PersistentVolume's <code>spec.persistentVolumeReclaimPolicy</code> and set it to <code>Retain</code>.\nThis ensures your PersistentVolume will not be deleted when we delete the corresponding PersistentVolumeClaim:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl patch pv cat-pictures-pv -p <span style=\"color:#b44\">&#39;{&#34;spec&#34;:{&#34;persistentVolumeReclaimPolicy&#34;:&#34;Retain&#34;}}&#39;</span>\n</span></span></code></pre></div><p>Next you need to stop any workloads that are using the PersistentVolumeClaim bound to the PersistentVolume you want to migrate, and then delete the PersistentVolumeClaim.</p>\n<p>Once that is done, you need to clear your PersistentVolume's <code>spec.claimRef.uid</code> to ensure PersistentVolumeClaims can bind to it upon recreation:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl scale --replicas<span style=\"color:#666\">=</span><span style=\"color:#666\">0</span> deployment cat-pictures-writer\n</span></span><span style=\"display:flex;\"><span>kubectl delete pvc cat-pictures-pvc\n</span></span><span style=\"display:flex;\"><span>kubectl patch pv cat-pictures-pv -p <span style=\"color:#b44\">&#39;{&#34;spec&#34;:{&#34;claimRef&#34;:{&#34;uid&#34;:&#34;&#34;}}}&#39;</span>\n</span></span></code></pre></div><p>After that you need to replace the PersistentVolume's access modes with ReadWriteOncePod:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl patch pv cat-pictures-pv -p <span style=\"color:#b44\">&#39;{&#34;spec&#34;:{&#34;accessModes&#34;:[&#34;ReadWriteOncePod&#34;]}}&#39;</span>\n</span></span></code></pre></div><div class=\"alert alert-info note callout\" role=\"alert\">\n<strong>Note:</strong> The ReadWriteOncePod access mode cannot be combined with other access modes.\nMake sure ReadWriteOncePod is the only access mode on the PersistentVolume when updating, otherwise the request will fail.\n</div>\n<p>Next you need to modify your PersistentVolumeClaim to set ReadWriteOncePod as the only access mode.\nYou should also set your PersistentVolumeClaim's <code>spec.volumeName</code> to the name of your PersistentVolume.</p>\n<p>Once this is done, you can recreate your PersistentVolumeClaim and start up your workloads:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to:</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># - Set ReadWriteOncePod as the only access mode</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#080;font-style:italic\"># - Set spec.volumeName to &#34;cat-pictures-pv&#34;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f cat-pictures-pvc.yaml\n</span></span><span style=\"display:flex;\"><span>kubectl apply -f cat-pictures-writer-deployment.yaml\n</span></span></code></pre></div><p>Lastly you may edit your PersistentVolume's <code>spec.persistentVolumeReclaimPolicy</code> and set to it back to <code>Delete</code> if you previously changed it.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex;\"><span>kubectl patch pv cat-pictures-pv -p <span style=\"color:#b44\">&#39;{&#34;spec&#34;:{&#34;persistentVolumeReclaimPolicy&#34;:&#34;Delete&#34;}}&#39;</span>\n</span></span></code></pre></div><p>You can read <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\">Configure a Pod to Use a PersistentVolume for Storage</a> for more details on working with PersistentVolumes and PersistentVolumeClaims.</p>\n<h2 id=\"what-volume-plugins-support-this\">What volume plugins support this?</h2>\n<p>The only volume plugins that support this are CSI drivers.\nSIG Storage does not plan to support this for in-tree plugins because they are being deprecated as part of <a href=\"https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#what-is-the-timeline-status\">CSI migration</a>.\nSupport may be considered for beta for users that prefer to use the legacy in-tree volume APIs with CSI migration enabled.</p>\n<h2 id=\"as-a-storage-vendor-how-do-i-add-support-for-this-access-mode-to-my-csi-driver\">As a storage vendor, how do I add support for this access mode to my CSI driver?</h2>\n<p>The ReadWriteOncePod access mode will work out of the box without any required updates to CSI drivers, but <a href=\"#update-your-csi-sidecars\">does require updates to CSI sidecars</a>.\nWith that being said, if you would like to stay up to date with the latest changes to the CSI specification (v1.5.0+), read on.</p>\n<p>Two new access modes were introduced to the CSI specification in order to disambiguate the legacy <a href=\"https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L418-L420\"><code>SINGLE_NODE_WRITER</code></a> access mode.\nThey are <a href=\"https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L437-L447\"><code>SINGLE_NODE_SINGLE_WRITER</code> and <code>SINGLE_NODE_MULTI_WRITER</code></a>.\nIn order to communicate to sidecars (like the <a href=\"https://github.com/kubernetes-csi/external-provisioner\">external-provisioner</a>) that your driver understands and accepts these two new CSI access modes, your driver will also need to advertise the <code>SINGLE_NODE_MULTI_WRITER</code> capability for the <a href=\"https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1073-L1081\">controller service</a> and <a href=\"https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1515-L1524\">node service</a>.</p>\n<p>If you'd like to read up on the motivation for these access modes and capability bits, you can also read the <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md#csi-specification-changes-volume-capabilities\">CSI Specification Changes, Volume Capabilities</a> section of KEP-2485 (ReadWriteOncePod PersistentVolume Access Mode).</p>\n<h3 id=\"update-your-csi-driver-to-use-the-new-interface\">Update your CSI driver to use the new interface</h3>\n<p>As a first step you will need to update your driver's <code>container-storage-interface</code> dependency to v1.5.0+, which contains support for these new access modes and capabilities.</p>\n<h3 id=\"accept-new-csi-access-modes\">Accept new CSI access modes</h3>\n<p>If your CSI driver contains logic for validating CSI access modes for requests , it may need updating.\nIf it currently accepts <code>SINGLE_NODE_WRITER</code>, it should be updated to also accept <code>SINGLE_NODE_SINGLE_WRITER</code> and <code>SINGLE_NODE_MULTI_WRITER</code>.</p>\n<p>Using the <a href=\"https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/utils.go#L116-L130\">GCP PD CSI driver validation logic</a> as an example, here is how it can be extended:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-diff\" data-lang=\"diff\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">diff --git a/pkg/gce-pd-csi-driver/utils.go b/pkg/gce-pd-csi-driver/utils.go\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">index 281242c..b6c5229 100644\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\"></span><span style=\"color:#a00000\">--- a/pkg/gce-pd-csi-driver/utils.go\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a00000\"></span><span style=\"color:#00a000\">+++ b/pkg/gce-pd-csi-driver/utils.go\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\"></span><span style=\"color:#800080;font-weight:bold\">@@ -123,6 +123,8 @@ func validateAccessMode(am *csi.VolumeCapability_AccessMode) error {\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#800080;font-weight:bold\"></span> case csi.VolumeCapability_AccessMode_SINGLE_NODE_READER_ONLY:\n</span></span><span style=\"display:flex;\"><span> case csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY:\n</span></span><span style=\"display:flex;\"><span> case csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER:\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\">+ case csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\">+ case csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER:\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\"></span> default:\n</span></span><span style=\"display:flex;\"><span> return fmt.Errorf(&#34;%v access mode is not supported for for PD&#34;, am.GetMode())\n</span></span><span style=\"display:flex;\"><span> }\n</span></span></code></pre></div><h3 id=\"advertise-new-csi-controller-and-node-service-capabilities\">Advertise new CSI controller and node service capabilities</h3>\n<p>Your CSI driver will also need to return the new <code>SINGLE_NODE_MULTI_WRITER</code> capability as part of the <code>ControllerGetCapabilities</code> and <code>NodeGetCapabilities</code> RPCs.</p>\n<p>Using the <a href=\"https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/gce-pd-driver.go#L54-L77\">GCP PD CSI driver capability advertisement logic</a> as an example, here is how it can be extended:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-diff\" data-lang=\"diff\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">diff --git a/pkg/gce-pd-csi-driver/gce-pd-driver.go b/pkg/gce-pd-csi-driver/gce-pd-driver.go\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">index 45903f3..0d7ea26 100644\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\"></span><span style=\"color:#a00000\">--- a/pkg/gce-pd-csi-driver/gce-pd-driver.go\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a00000\"></span><span style=\"color:#00a000\">+++ b/pkg/gce-pd-csi-driver/gce-pd-driver.go\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\"></span><span style=\"color:#800080;font-weight:bold\">@@ -56,6 +56,8 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#800080;font-weight:bold\"></span> csi.VolumeCapability_AccessMode_SINGLE_NODE_WRITER,\n</span></span><span style=\"display:flex;\"><span> csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY,\n</span></span><span style=\"display:flex;\"><span> csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\">+ csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\">+ csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\"></span> }\n</span></span><span style=\"display:flex;\"><span> gceDriver.AddVolumeCapabilityAccessModes(vcam)\n</span></span><span style=\"display:flex;\"><span> csc := []csi.ControllerServiceCapability_RPC_Type{\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#800080;font-weight:bold\">@@ -67,12 +69,14 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#800080;font-weight:bold\"></span> csi.ControllerServiceCapability_RPC_EXPAND_VOLUME,\n</span></span><span style=\"display:flex;\"><span> csi.ControllerServiceCapability_RPC_LIST_VOLUMES,\n</span></span><span style=\"display:flex;\"><span> csi.ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES,\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\">+ csi.ControllerServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\"></span> }\n</span></span><span style=\"display:flex;\"><span> gceDriver.AddControllerServiceCapabilities(csc)\n</span></span><span style=\"display:flex;\"><span> ns := []csi.NodeServiceCapability_RPC_Type{\n</span></span><span style=\"display:flex;\"><span> csi.NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME,\n</span></span><span style=\"display:flex;\"><span> csi.NodeServiceCapability_RPC_EXPAND_VOLUME,\n</span></span><span style=\"display:flex;\"><span> csi.NodeServiceCapability_RPC_GET_VOLUME_STATS,\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\">+ csi.NodeServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#00a000\"></span> }\n</span></span><span style=\"display:flex;\"><span> gceDriver.AddNodeServiceCapabilities(ns)\n</span></span></code></pre></div><h3 id=\"implement-nodepublishvolume-behavior\">Implement <code>NodePublishVolume</code> behavior</h3>\n<p>The CSI spec outlines expected behavior for the <code>NodePublishVolume</code> RPC when called more than once for the same volume but with different arguments (like the target path).\nPlease refer to <a href=\"https://github.com/container-storage-interface/spec/blob/v1.5.0/spec.md#nodepublishvolume\">the second table in the NodePublishVolume section of the CSI spec</a> for more details on expected behavior when implementing in your driver.</p>\n<h3 id=\"update-your-csi-sidecars\">Update your CSI sidecars</h3>\n<p>When deploying your CSI drivers, you must update the following CSI sidecars to versions that depend on CSI spec v1.5.0+ and the Kubernetes v1.22 API.\nThe minimum required versions are:</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0\">csi-provisioner:v3.0.0+</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0\">csi-attacher:v3.3.0+</a></li>\n<li><a href=\"https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0\">csi-resizer:v1.3.0+</a></li>\n</ul>\n<h2 id=\"what-s-next\">What‚Äôs next?</h2>\n<p>As part of the beta graduation for this feature, SIG Storage plans to update the Kubernetes scheduler to support pod preemption in relation to ReadWriteOncePod storage.\nThis means if two pods request a PersistentVolumeClaim with ReadWriteOncePod, the pod with highest priority will gain access to the PersistentVolumeClaim and any pod with lower priority will be preempted from the node and be unable to access the PersistentVolumeClaim.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>Please see <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md\">KEP-2485</a> for more details on the ReadWriteOncePod access mode and motivations for CSI spec changes.</p>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>The <a href=\"https://kubernetes.slack.com/messages/csi\">Kubernetes #csi Slack channel</a> and any of the <a href=\"https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact\">standard SIG Storage communication channels</a> are great mediums to reach out to the SIG Storage and the CSI teams.</p>\n<p>Special thanks to the following people for their insightful reviews and design considerations:</p>\n<ul>\n<li>Abdullah Gharaibeh (ahg-g)</li>\n<li>Aldo Culquicondor (alculquicondor)</li>\n<li>Ben Swartzlander (bswartz)</li>\n<li>Deep Debroy (ddebroy)</li>\n<li>Hemant Kumar (gnufied)</li>\n<li>Humble Devassy Chirammal (humblec)</li>\n<li>James DeFelice (jdef)</li>\n<li>Jan ≈†afr√°nek (jsafrane)</li>\n<li>Jing Xu (jingxu97)</li>\n<li>Jordan Liggitt (liggitt)</li>\n<li>Michelle Au (msau42)</li>\n<li>Saad Ali (saad-ali)</li>\n<li>Tim Hockin (thockin)</li>\n<li>Xing Yang (xing-yang)</li>\n</ul>\n<p>If you‚Äôre interested in getting involved with the design and development of CSI or any part of the Kubernetes storage system, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group</a> (SIG).\nWe‚Äôre rapidly growing and always welcome new contributors.</p>","PublishedAt":"2021-09-13 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/","SourceName":"Kubernetes"}},{"node":{"ID":1256,"Title":"Blog: Alpha in Kubernetes v1.22: API Server Tracing","Description":"<p><strong>Authors:</strong> David Ashpole (Google)</p>\n<p>In distributed systems, it can be hard to figure out where problems are. You grep through one component's logs just to discover that the source of your problem is in another component. You search there only to discover that you need to enable debug logs to figure out what really went wrong... And it goes on. The more complex the path your request takes, the harder it is to answer questions about where it went. I've personally spent many hours doing this dance with a variety of Kubernetes components. Distributed tracing is a tool which is designed to help in these situations, and the Kubernetes API Server is, perhaps, the most important Kubernetes component to be able to debug. At Kubernetes' Sig Instrumentation, our mission is to make it easier to understand what's going on in your cluster, and we are happy to announce that distributed tracing in the Kubernetes API Server reached alpha in 1.22.</p>\n<h2 id=\"what-is-tracing\">What is Tracing?</h2>\n<p>Distributed tracing links together a bunch of super-detailed information from multiple different sources, and structures that telemetry into a single tree for that request. Unlike logging, which limits the quantity of data ingested by using log levels, tracing collects all of the details and uses sampling to collect only a small percentage of requests. This means that once you have a trace which demonstrates an issue, you should have all the information you need to root-cause the problem--no grepping for object UID required! My favorite aspect, though, is how useful the visualizations of traces are. Even if you don't understand the inner workings of the API Server, or don't have a clue what an etcd &quot;Transaction&quot; is, I'd wager you (yes, you!) could tell me roughly what the order of events was, and which components were involved in the request. If some step takes a long time, it is easy to tell where the problem is.</p>\n<h2 id=\"why-opentelemetry\">Why OpenTelemetry?</h2>\n<p>It's important that Kubernetes works well for everyone, regardless of who manages your infrastructure, or which vendors you choose to integrate with. That is particularly true for Kubernetes' integrations with telemetry solutions. OpenTelemetry, being a CNCF project, shares these core values, and is creating exactly what we need in Kubernetes: A set of open standards for Tracing client library APIs and a standard trace format. By using OpenTelemetry, we can ensure users have the freedom to choose their backend, and ensure vendors have a level playing field. The timing couldn't be better: the OpenTelemetry golang API and SDK are very close to their 1.0 release, and will soon offer backwards-compatibility for these open standards.</p>\n<h2 id=\"why-instrument-the-api-server\">Why instrument the API Server?</h2>\n<p>The Kubernetes API Server is a great candidate for tracing for a few reasons:</p>\n<ul>\n<li>It follows the standard &quot;RPC&quot; model (serve a request by making requests to downstream components), which makes it easy to instrument.</li>\n<li>Users are latency-sensitive: If a request takes more than 10 seconds to complete, many clients will time-out.</li>\n<li>It has a complex service topology: A single request could require consulting a dozen webhooks, or involve multiple requests to etcd.</li>\n</ul>\n<h2 id=\"trying-out-apiserver-tracing-with-a-webhook\">Trying out APIServer Tracing with a webhook</h2>\n<h3 id=\"enabling-api-server-tracing\">Enabling API Server Tracing</h3>\n<ol>\n<li>\n<p>Enable the APIServerTracing <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\">feature-gate</a>.</p>\n</li>\n<li>\n<p>Set our configuration for tracing by pointing the <code>--tracing-config-file</code> flag on the kube-apiserver at our config file, which contains:</p>\n</li>\n</ol>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>apiserver.config.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>TracingConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#080;font-style:italic\"># 1% sampling rate</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">samplingRatePerMillion</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#666\">10000</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"enabling-etcd-tracing\">Enabling Etcd Tracing</h3>\n<p>Add <code>--experimental-enable-distributed-tracing</code>, <code>--experimental-distributed-tracing-address=0.0.0.0:4317</code>, <code>--experimental-distributed-tracing-service-name=etcd</code> flags to etcd to enable tracing. Note that this traces every request, so it will probably generate a lot of traces if you enable it. Required etcd version is <a href=\"https://etcd.io/docs/v3.5/op-guide/monitoring/#distributed-tracing\">v3.5+</a>.</p>\n<h3 id=\"example-trace-list-nodes\">Example Trace: List Nodes</h3>\n<p>I could've used any trace backend, but decided to use Jaeger, since it is one of the most popular open-source tracing projects. I deployed <a href=\"https://hub.docker.com/r/jaegertracing/all-in-one\">the Jaeger All-in-one container</a> in my cluster, deployed <a href=\"https://github.com/open-telemetry/opentelemetry-collector\">the OpenTelemetry collector</a> on my control-plane node (<a href=\"https://github.com/dashpole/dashpole_demos/tree/master/otel/controlplane\">example</a>), and captured traces like this one:</p>\n<p><img src=\"https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-1.png\" alt=\"Jaeger screenshot showing API server and etcd trace\" title=\"Jaeger screenshot showing API server and etcd trace\"></p>\n<p>The teal lines are from the API Server, and includes it serving a request to <code>/api/v1/nodes</code>, and issuing a grpc <code>Range</code> RPC to ETCD. The yellow-ish line is from ETCD handling the <code>Range</code> RPC.</p>\n<h3 id=\"example-trace-create-pod-with-mutating-webhook\">Example Trace: Create Pod with Mutating Webhook</h3>\n<p>I instrumented the <a href=\"https://github.com/kubernetes-sigs/controller-runtime/tree/master/examples/builtins\">example webhook</a> with OpenTelemetry (I had to <a href=\"https://github.com/dashpole/controller-runtime/commit/85fdda7ba03dd2c22ef62c1a3dbdf5aa651f90da\">patch</a> controller-runtime, but it makes a neat demo), and routed traces to Jaeger as well. I collected traces like this one:</p>\n<p><img src=\"https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-2.png\" alt=\"Jaeger screenshot showing API server, admission webhook, and etcd trace\" title=\"Jaeger screenshot showing API server, admission webhook, and etcd trace\"></p>\n<p>Compared with the previous trace, there are two new spans: A teal span from the API Server making a request to the admission webhook, and a brown span from the admission webhook serving the request. Even if you didn't instrument your webhook, you would still get the span from the API Server making the request to the webhook.</p>\n<h2 id=\"get-involved\">Get involved!</h2>\n<p>As this is our first attempt at adding distributed tracing to a Kubernetes component, there is probably a lot we can improve! If my struggles resonated with you, or if you just want to try out the latest Kubernetes has to offer, please give the feature a try and open issues with any problem you encountered and ways you think the feature could be improved.</p>\n<p>This is just the very beginning of what we can do with distributed tracing in Kubernetes. If there are other components you think would benefit from distributed tracing, or want to help bring API Server Tracing to GA, join sig-instrumentation at our <a href=\"https://github.com/kubernetes/community/tree/master/sig-instrumentation#instrumentation-special-interest-group\">regular meetings</a> and get involved!</p>","PublishedAt":"2021-09-03 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/09/03/api-server-tracing/","SourceName":"Kubernetes"}},{"node":{"ID":1257,"Title":"Blog: Kubernetes 1.22: A New Design for Volume Populators","Description":"<p><strong>Authors:</strong>\nBen Swartzlander (NetApp)</p>\n<p>Kubernetes v1.22, released earlier this month, introduced a redesigned approach for volume\npopulators. Originally implemented\nin v1.18, the API suffered from backwards compatibility issues. Kubernetes v1.22 includes a new API\nfield called <code>dataSourceRef</code> that fixes these problems.</p>\n<h2 id=\"data-sources\">Data sources</h2>\n<p>Earlier Kubernetes releases already added a <code>dataSource</code> field into the\n<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims\">PersistentVolumeClaim</a> API,\nused for cloning volumes and creating volumes from snapshots. You could use the <code>dataSource</code> field when\ncreating a new PVC, referencing either an existing PVC or a VolumeSnapshot in the same namespace.\nThat also modified the normal provisioning process so that instead of yielding an empty volume, the\nnew PVC contained the same data as either the cloned PVC or the cloned VolumeSnapshot.</p>\n<p>Volume populators embrace the same design idea, but extend it to any type of object, as long\nas there exists a <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">custom resource</a>\nto define the data source, and a populator controller to implement the logic. Initially,\nthe <code>dataSource</code> field was directly extended to allow arbitrary objects, if the <code>AnyVolumeDataSource</code>\nfeature gate was enabled on a cluster. That change unfortunately caused backwards compatibility\nproblems, and so the new <code>dataSourceRef</code> field was born.</p>\n<p>In v1.22 if the <code>AnyVolumeDataSource</code> feature gate is enabled, the <code>dataSourceRef</code> field is\nadded, which behaves similarly to the <code>dataSource</code> field except that it allows arbitrary\nobjects to be specified. The API server ensures that the two fields always have the same\ncontents, and neither of them are mutable. The differences is that at creation time\n<code>dataSource</code> allows only PVCs or VolumeSnapshots, and ignores all other values, while\n<code>dataSourceRef</code> allows most types of objects, and in the few cases it doesn't allow an\nobject (core objects other than PVCs) a validation error occurs.</p>\n<p>When this API change graduates to stable, we would deprecate using <code>dataSource</code> and recommend\nusing <code>dataSourceRef</code> field for all use cases.\nIn the v1.22 release, <code>dataSourceRef</code> is available (as an alpha feature) specifically for cases\nwhere you want to use for custom volume populators.</p>\n<h2 id=\"using-populators\">Using populators</h2>\n<p>Every volume populator must have one or more CRDs that it supports. Administrators may\ninstall the CRD and the populator controller and then PVCs with a <code>dataSourceRef</code> specifies\na CR of the type that the populator supports will be handled by the populator controller\ninstead of the CSI driver directly.</p>\n<p>Underneath the covers, the CSI driver is still invoked to create an empty volume, which\nthe populator controller fills with the appropriate data. The PVC doesn't bind to the PV\nuntil it's fully populated, so it's safe to define a whole application manifest including\npod and PVC specs and the pods won't begin running until everything is ready, just as if\nthe PVC was a clone of another PVC or VolumeSnapshot.</p>\n<h2 id=\"how-it-works\">How it works</h2>\n<p>PVCs with data sources are still noticed by the external-provisioner sidecar for the\nrelated storage class (assuming a CSI provisioner is used), but because the sidecar\ndoesn't understand the data source kind, it doesn't do anything. The populator controller\nis also watching for PVCs with data sources of a kind that it understands and when it\nsees one, it creates a temporary PVC of the same size, volume mode, storage class,\nand even on the same topology (if topology is used) as the original PVC. The populator\ncontroller creates a worker pod that attaches to the volume and writes the necessary\ndata to it, then detaches from the volume and the populator controller rebinds the PV\nfrom the temporary PVC to the orignal PVC.</p>\n<h2 id=\"trying-it-out\">Trying it out</h2>\n<p>The following things are required to use volume populators:</p>\n<ul>\n<li>Enable the <code>AnyVolumeDataSource</code> feature gate</li>\n<li>Install a CRD for the specific data source / populator</li>\n<li>Install the populator controller itself</li>\n</ul>\n<p>Populator controllers may use the <a href=\"https://github.com/kubernetes-csi/lib-volume-populator\">lib-volume-populator</a>\nlibrary to do most of the Kubernetes API level work. Individual populators only need to\nprovide logic for actually writing data into the volume based on a particular CR\ninstance. This library provides a sample populator implementation.</p>\n<p>These optional components improve user experience:</p>\n<ul>\n<li>Install the VolumePopulator CRD</li>\n<li>Create a VolumePopulator custom respource for each specific data source</li>\n<li>Install the <a href=\"https://github.com/kubernetes-csi/volume-data-source-validator\">volume data source validator</a>\ncontroller (alpha)</li>\n</ul>\n<p>The purpose of these components is to generate warning events on PVCs with data sources\nfor which there is no populator.</p>\n<h2 id=\"putting-it-all-together\">Putting it all together</h2>\n<p>To see how this works, you can install the sample &quot;hello&quot; populator and try it\nout.</p>\n<p>First install the volume-data-source-validator controller.</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/master/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/master/deploy/kubernetes/rbac-data-source-validator.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/master/deploy/kubernetes/setup-data-source-validator.yaml\n</code></pre><p>Next install the example populator.</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/master/example/hello-populator/crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/master/example/hello-populator/deploy.yaml\n</code></pre><p>Create an instance of the <code>Hello</code> CR, with some text.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>hello.k8s.io/v1alpha1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fileName</span>:<span style=\"color:#bbb\"> </span>example.txt<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">fileContents</span>:<span style=\"color:#bbb\"> </span>Hello, world!<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Create a PVC that refers to that CR as its data source.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-pvc<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOnce<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>10Mi<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSourceRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>hello.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-hello<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMode</span>:<span style=\"color:#bbb\"> </span>Filesystem<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Next, run a job that reads the file in the PVC.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>batch/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-job<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">template</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-container<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>busybox:latest<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">command</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- cat<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- /mnt/example.txt<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeMounts</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>vol<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">mountPath</span>:<span style=\"color:#bbb\"> </span>/mnt<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">restartPolicy</span>:<span style=\"color:#bbb\"> </span>Never<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>vol<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">persistentVolumeClaim</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">claimName</span>:<span style=\"color:#bbb\"> </span>example-pvc<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Wait for the job to complete (including all of its dependencies).</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">kubectl wait --for=condition=Complete job/example-job\n</code></pre><p>And last examine the log from the job.</p>\n<pre tabindex=\"0\"><code class=\"language-terminal\" data-lang=\"terminal\">kubectl logs job/example-job\nHello, world!\n</code></pre><p>Note that the volume already contained a text file with the string contents from\nthe CR. This is only the simplest example. Actual populators can set up the volume\nto contain arbitrary contents.</p>\n<h2 id=\"how-to-write-your-own-volume-populator\">How to write your own volume populator</h2>\n<p>Developers interested in writing new poplators are encouraged to use the\n<a href=\"https://github.com/kubernetes-csi/lib-volume-populator\">lib-volume-populator</a> library\nand to only supply a small controller wrapper around the library, and a pod image\ncapable of attaching to volumes and writing the appropriate data to the volume.</p>\n<p>Individual populators can be extremely generic such that they work with every type\nof PVC, or they can do vendor specific things to rapidly fill a volume with data\nif the volume was provisioned by a specific CSI driver from the same vendor, for\nexample, by communicating directly with the storage for that volume.</p>\n<h2 id=\"the-future\">The future</h2>\n<p>As this feature is still in alpha, we expect to update the out of tree controllers\nwith more tests and documentation. The community plans to eventually re-implement\nthe populator library as a sidecar, for ease of operations.</p>\n<p>We hope to see some official community-supported populators for some widely-shared\nuse cases. Also, we expect that volume populators will be used by backup vendors\nas a way to &quot;restore&quot; backups to volumes, and possibly a standardized API to do\nthis will evolve.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<p>The enhancement proposal,\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators\">Volume Populators</a>, includes lots of detail about the history and technical implementation\nof this feature.</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources\">Volume populators and data sources</a>, within the documentation topic about persistent volumes,\nexplains how to use this feature in your cluster.</p>\n<p>Please get involved by joining the Kubernetes storage SIG to help us enhance this\nfeature. There are a lot of good ideas already and we'd be thrilled to have more!</p>","PublishedAt":"2021-08-30 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/","SourceName":"Kubernetes"}},{"node":{"ID":1258,"Title":"Blog: Minimum Ready Seconds for StatefulSets","Description":"<p><strong>Authors:</strong> Ravi Gudimetla (Red Hat), Maciej Szulik (Red Hat)</p>\n<p>This blog describes the notion of Availability for <code>StatefulSet</code> workloads, and a new alpha feature in Kubernetes 1.22 which adds <code>minReadySeconds</code> configuration for <code>StatefulSets</code>.</p>\n<h2 id=\"what-problems-does-this-solve\">What problems does this solve?</h2>\n<p>Prior to Kubernetes 1.22 release, once a <code>StatefulSet</code> <code>Pod</code> is in the <code>Ready</code> state it is considered <code>Available</code> to receive traffic. For some of the <code>StatefulSet</code> workloads, it may not be the case. For example, a workload like Prometheus with multiple instances of Alertmanager, it should be considered <code>Available</code> only when Alertmanager's state transfer is complete, not when the <code>Pod</code> is in <code>Ready</code> state. Since <code>minReadySeconds</code> adds buffer, the state transfer may be complete before the <code>Pod</code> becomes <code>Available</code>. While this is not a fool proof way of identifying if the state transfer is complete or not, it gives a way to the end user to express their intention of waiting for sometime before the <code>Pod</code> is considered <code>Available</code> and it is ready to serve requests.</p>\n<p>Another case, where <code>minReadySeconds</code> helps is when using <code>LoadBalancer</code> <code>Services</code> with cloud providers. Since <code>minReadySeconds</code> adds latency after a <code>Pod</code> is <code>Ready</code>, it provides buffer time to prevent killing pods in rotation before new pods show up. Imagine a load balancer in unhappy path taking 10-15s to propagate. If you have 2 replicas then, you'd kill the second replica only after the first one is up but in reality, first replica cannot be seen because it is not yet ready to serve requests.</p>\n<p>So, in general, the notion of <code>Availability</code> in <code>StatefulSets</code> is pretty useful and this feature helps in solving the above problems. This is a feature that already exists for <code>Deployments</code> and <code>DaemonSets</code> and we now have them for <code>StatefulSets</code> too to give users consistent workload experience.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>The statefulSet controller watches for both <code>StatefulSets</code> and the <code>Pods</code> associated with them. When the feature gate associated with this feature is enabled, the statefulSet controller identifies how long a particular <code>Pod</code> associated with a <code>StatefulSet</code> has been in the <code>Running</code> state.</p>\n<p>If this value is greater than or equal to the time specified by the end user in <code>.spec.minReadySeconds</code> field, the statefulSet controller updates a field called <code>availableReplicas</code> in the <code>StatefulSet</code>'s status subresource to include this <code>Pod</code>. The <code>status.availableReplicas</code> in <code>StatefulSet</code>'s status is an integer field which tracks the number of pods that are <code>Available</code>.</p>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>You are required to prepare the following things in order to try out the feature:</p>\n<ul>\n<li>Download and install a kubectl greater than v1.22.0 version</li>\n<li>Switch on the feature gate with the command line flag <code>--feature-gates=StatefulSetMinReadySeconds=true</code> on <code>kube-apiserver</code> and <code>kube-controller-manager</code></li>\n</ul>\n<p>After successfully starting <code>kube-apiserver</code> and <code>kube-controller-manager</code>, you will see <code>AvailableReplicas</code> in the status and <code>minReadySeconds</code> of spec (with a default value of 0).</p>\n<p>Specify a value for <code>minReadySeconds</code> for any StatefulSet and you can check if <code>Pods</code> are available or not by checking <code>AvailableReplicas</code> field using:\n<code>kubectl get statefulset/&lt;name_of_the_statefulset&gt; -o yaml</code></p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>Read the KEP: <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2599-minreadyseconds-for-statefulsets#readme\">minReadySeconds for StatefulSets</a></li>\n<li>Read the documentation: <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds\">Minimum ready seconds</a> for StatefulSet</li>\n<li>Review the <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/\">API definition</a> for StatefulSet</li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>Please reach out to us in the <a href=\"https://kubernetes.slack.com/archives/C18NZM5K9\">#sig-apps</a> channel on Slack (visit <a href=\"https://slack.k8s.io/\">https://slack.k8s.io/</a> for an invitation if you need one), or on the SIG Apps mailing list: <a href=\"mailto:kubernetes-sig-apps@googlegroups.com\">kubernetes-sig-apps@googlegroups.com</a></p>","PublishedAt":"2021-08-27 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/","SourceName":"Kubernetes"}},{"node":{"ID":1259,"Title":"Blog: Enable seccomp for all workloads with a new v1.22 alpha feature","Description":"<p><strong>Author:</strong> Sascha Grunert, Red Hat</p>\n<p>This blog post is about a new Kubernetes feature introduced in v1.22, which adds\nan additional security layer on top of the existing seccomp support. Seccomp is\na security mechanism for Linux processes to filter system calls (syscalls) based\non a set of defined rules. Applying seccomp profiles to containerized workloads\nis one of the key tasks when it comes to enhancing the security of the\napplication deployment. Developers, site reliability engineers and\ninfrastructure administrators have to work hand in hand to create, distribute\nand maintain the profiles over the applications life-cycle.</p>\n<p>You can use the <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1\"><code>securityContext</code></a> field of Pods and their\ncontainers can be used to adjust security related configurations of the\nworkload. Kubernetes introduced dedicated <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1\">seccomp related API\nfields</a> in this <code>SecurityContext</code> with the <a href=\"https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#graduated-to-stable\">graduation of seccomp to\nGeneral Availability (GA)</a> in v1.19.0. This enhancement allowed an easier\nway to specify if the whole pod or a specific container should run as:</p>\n<ul>\n<li><code>Unconfined</code>: seccomp will not be enabled</li>\n<li><code>RuntimeDefault</code>: the container runtimes default profile will be used</li>\n<li><code>Localhost</code>: a node local profile will be applied, which is being referenced\nby a relative path to the seccomp profile root (<code>&lt;kubelet-root-dir&gt;/seccomp</code>)\nof the kubelet</li>\n</ul>\n<p>With the graduation of seccomp, nothing has changed from an overall security\nperspective, because <code>Unconfined</code> is still the default. This is totally fine if\nyou consider this from the upgrade path and backwards compatibility perspective of\nKubernetes releases. But it also means that it is more likely that a workload\nruns without seccomp at all, which should be fixed in the long term.</p>\n<h2 id=\"seccompdefault-to-the-rescue\"><code>SeccompDefault</code> to the rescue</h2>\n<p>Kubernetes v1.22.0 introduces a new kubelet <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates\">feature gate</a>\n<code>SeccompDefault</code>, which has been added in <code>alpha</code> state as every other new\nfeature. This means that it is disabled by default and can be enabled manually\nfor every single Kubernetes node.</p>\n<p>What does the feature do? Well, it just changes the default seccomp profile from\n<code>Unconfined</code> to <code>RuntimeDefault</code>. If not specified differently in the pod\nmanifest, then the feature will add a higher set of security constraints by\nusing the default profile of the container runtime. These profiles may differ\nbetween runtimes like <a href=\"https://github.com/cri-o/cri-o/blob/fe30d62/vendor/github.com/containers/common/pkg/seccomp/default_linux.go#L45\">CRI-O</a> or <a href=\"https://github.com/containerd/containerd/blob/e1445df/contrib/seccomp/seccomp_default.go#L51\">containerd</a>. They also differ for\nits used hardware architectures. But generally speaking, those default profiles\nallow a common amount of syscalls while blocking the more dangerous ones, which\nare unlikely or unsafe to be used in a containerized application.</p>\n<h3 id=\"enabling-the-feature\">Enabling the feature</h3>\n<p>Two kubelet configuration changes have to be made to enable the feature:</p>\n<ol>\n<li><strong>Enable the feature</strong> gate by setting the <code>SeccompDefault=true</code> via the command\nline (<code>--feature-gates</code>) or the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file\">kubelet configuration</a> file.</li>\n<li><strong>Turn on the feature</strong> by enabling the feature by adding the\n<code>--seccomp-default</code> command line flag or via the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file\">kubelet\nconfiguration</a> file (<code>seccompDefault: true</code>).</li>\n</ol>\n<p>The kubelet will error on startup if only one of the above steps have been done.</p>\n<h3 id=\"trying-it-out\">Trying it out</h3>\n<p>If the feature is enabled on a node, then you can create a new workload like\nthis:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-container<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>nginx:1.21<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>Now it is possible to inspect the used seccomp profile by using\n<a href=\"https://github.com/kubernetes-sigs/cri-tools\"><code>crictl</code></a> while investigating the containers <a href=\"https://github.com/opencontainers/runtime-spec/blob/0c021c1/config-linux.md#seccomp\">runtime\nspecification</a>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#b8860b\">CONTAINER_ID</span><span style=\"color:#666\">=</span><span style=\"color:#a2f;font-weight:bold\">$(</span>sudo crictl ps -q --name<span style=\"color:#666\">=</span>test-container<span style=\"color:#a2f;font-weight:bold\">)</span>\n</span></span><span style=\"display:flex;\"><span>sudo crictl inspect <span style=\"color:#b8860b\">$CONTAINER_ID</span> | jq .info.runtimeSpec.linux.seccomp\n</span></span></code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span>{<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;defaultAction&#34;: </span><span style=\"color:#b44\">&#34;SCMP_ACT_ERRNO&#34;</span>,<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;architectures&#34;: </span>[<span style=\"color:#b44\">&#34;SCMP_ARCH_X86_64&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;SCMP_ARCH_X86&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;SCMP_ARCH_X32&#34;</span>],<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;syscalls&#34;: </span>[<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>{<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;names&#34;: </span>[<span style=\"color:#b44\">&#34;_llseek&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;_newselect&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;accept&#34;</span>,<span style=\"color:#bbb\"> </span>‚Ä¶, &#34;write&#34;, &#34;writev&#34;],<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">&#34;action&#34;: </span><span style=\"color:#b44\">&#34;SCMP_ACT_ALLOW&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>},<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>‚Ä¶<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span>}<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>You can see that the lower level container runtime (<a href=\"https://github.com/cri-o/cri-o\">CRI-O</a> and\n<a href=\"https://github.com/opencontainers/runc\">runc</a> in our case), successfully applied the default seccomp profile.\nThis profile denies all syscalls per default, while allowing commonly used ones\nlike <a href=\"https://man7.org/linux/man-pages/man2/accept.2.html\"><code>accept</code></a> or <a href=\"https://man7.org/linux/man-pages/man2/write.2.html\"><code>write</code></a>.</p>\n<p>Please note that the feature will not influence any Kubernetes API for now.\nTherefore, it is not possible to retrieve the used seccomp profile via <code>kubectl</code>\n<code>get</code> or <code>describe</code> if the <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1\"><code>SeccompProfile</code></a> field is unset within the\n<code>SecurityContext</code>.</p>\n<p>The feature also works when using multiple containers within a pod, for example\nif you create a pod like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>Pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-pod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">containers</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-container-nginx<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>nginx:1.21<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">securityContext</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">seccompProfile</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">type</span>:<span style=\"color:#bbb\"> </span>Unconfined<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>test-container-redis<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">image</span>:<span style=\"color:#bbb\"> </span>redis:6.2<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>then you should see that the <code>test-container-nginx</code> runs without a seccomp profile:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>sudo crictl inspect <span style=\"color:#a2f;font-weight:bold\">$(</span>sudo crictl ps -q --name<span style=\"color:#666\">=</span>test-container-nginx<span style=\"color:#a2f;font-weight:bold\">)</span> |\n</span></span><span style=\"display:flex;\"><span> jq <span style=\"color:#b44\">&#39;.info.runtimeSpec.linux.seccomp == null&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f\">true</span>\n</span></span></code></pre></div><p>Whereas the container <code>test-container-redis</code> runs with <code>RuntimeDefault</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>sudo crictl inspect <span style=\"color:#a2f;font-weight:bold\">$(</span>sudo crictl ps -q --name<span style=\"color:#666\">=</span>test-container-redis<span style=\"color:#a2f;font-weight:bold\">)</span> |\n</span></span><span style=\"display:flex;\"><span> jq <span style=\"color:#b44\">&#39;.info.runtimeSpec.linux.seccomp != null&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f\">true</span>\n</span></span></code></pre></div><p>The same applies to the pod itself, which also runs with the default profile:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>sudo crictl inspectp <span style=\"color:#666\">(</span>sudo crictl pods -q --name test-pod<span style=\"color:#666\">)</span> |\n</span></span><span style=\"display:flex;\"><span> jq <span style=\"color:#b44\">&#39;.info.runtimeSpec.linux.seccomp != null&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a2f\">true</span>\n</span></span></code></pre></div><h3 id=\"upgrade-strategy\">Upgrade strategy</h3>\n<p>It is recommended to enable the feature in multiple steps, whereas different\nrisks and mitigations exist for each one.</p>\n<h4 id=\"feature-gate-enabling\">Feature gate enabling</h4>\n<p>Enabling the feature gate at the kubelet level will not turn on the feature, but\nwill make it possible by using the <code>SeccompDefault</code> kubelet configuration or the\n<code>--seccomp-default</code> CLI flag. This can be done by an administrator for the whole\ncluster or only a set of nodes.</p>\n<h4 id=\"testing-the-application\">Testing the Application</h4>\n<p>If you're trying this within a dedicated test environment, you have to ensure\nthat the application code does not trigger syscalls blocked by the\n<code>RuntimeDefault</code> profile before enabling the feature on a node. This can be done\nby:</p>\n<ul>\n<li>\n<p><em>Recommended</em>: Analyzing the code (manually or by running the application with\n<a href=\"https://man7.org/linux/man-pages/man1/strace.1.html\">strace</a>) for any executed syscalls which may be blocked by the\ndefault profiles. If that's the case, then you can override the default by\nexplicitly setting the pod or container to run as <code>Unconfined</code>. Alternatively,\nyou can create a custom seccomp profile (see optional step below).\nprofile based on the default by adding the additional syscalls to the\n<code>&quot;action&quot;: &quot;SCMP_ACT_ALLOW&quot;</code> section.</p>\n</li>\n<li>\n<p><em>Recommended</em>: Manually set the profile to the target workload and use a\nrolling upgrade to deploy into production. Rollback the deployment if the\napplication does not work as intended.</p>\n</li>\n<li>\n<p><em>Optional</em>: Run the application against an end-to-end test suite to trigger\nall relevant code paths with <code>RuntimeDefault</code> enabled. If a test fails, use\nthe same mitigation as mentioned above.</p>\n</li>\n<li>\n<p><em>Optional</em>: Create a custom seccomp profile based on the default and change\nits default action from <code>SCMP_ACT_ERRNO</code> to <code>SCMP_ACT_LOG</code>. This means that\nthe seccomp filter for unknown syscalls will have no effect on the application\nat all, but the system logs will now indicate which syscalls may be blocked.\nThis requires at least a Kernel version 4.14 as well as a recent <a href=\"https://github.com/opencontainers/runc\">runc</a>\nrelease. Monitor the application hosts audit logs (defaults to\n<code>/var/log/audit/audit.log</code>) or syslog entries (defaults to <code>/var/log/syslog</code>)\nfor syscalls via <code>type=SECCOMP</code> (for audit) or <code>type=1326</code> (for syslog).\nCompare the syscall ID with those <a href=\"https://github.com/torvalds/linux/blob/7bb7f2a/arch/x86/entry/syscalls/syscall_64.tbl\">listed in the Linux Kernel\nsources</a> and add them to the custom profile. Be aware that custom\naudit policies may lead into missing syscalls, depending on the configuration\nof auditd.</p>\n</li>\n<li>\n<p><em>Optional</em>: Use cluster additions like the <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator\">Security Profiles Operator</a>\nfor profiling the application via its <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#record-profiles-from-workloads-with-profilerecordings\">log enrichment</a> capabilities or\nrecording a profile by using its <a href=\"https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#using-the-log-enricher\">recording feature</a>. This makes the\nabove mentioned manual log investigation obsolete.</p>\n</li>\n</ul>\n<h4 id=\"deploying-the-modified-application\">Deploying the modified application</h4>\n<p>Based on the outcome of the application tests, it may be required to change the\napplication deployment by either specifying <code>Unconfined</code> or a custom seccomp\nprofile. This is not the case if the application works as intended with\n<code>RuntimeDefault</code>.</p>\n<h4 id=\"enable-the-kubelet-configuration\">Enable the kubelet configuration</h4>\n<p>If everything went well, then the feature is ready to be enabled by the kubelet\nconfiguration or its corresponding CLI flag. This should be done on a per-node\nbasis to reduce the overall risk of missing a syscall during the investigations\nwhen running the application tests. If it's possible to monitor audit logs\nwithin the cluster, then it's recommended to do this for eventually missed\nseccomp events. If the application works as intended then the feature can be\nenabled for further nodes within the cluster.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Thank you for reading this blog post! I hope you enjoyed to see how the usage of\nseccomp profiles has been evolved in Kubernetes over the past releases as much\nas I do. On your own cluster, change the default seccomp profile to\n<code>RuntimeDefault</code> (using this new feature) and see the security benefits, and, of\ncourse, feel free to reach out any time for feedback or questions.</p>\n<hr>\n<p><em>Editor's note: If you have any questions or feedback about this blog post, feel\nfree to reach out via the <a href=\"https://kubernetes.slack.com/messages/sig-node\">Kubernetes slack in #sig-node</a>.</em></p>","PublishedAt":"2021-08-25 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/08/25/seccomp-default/","SourceName":"Kubernetes"}},{"node":{"ID":1260,"Title":"Blog: Alpha in v1.22: Windows HostProcess Containers","Description":"<p><strong>Authors:</strong> Brandon Smith (Microsoft)</p>\n<p>Kubernetes v1.22 introduced a new alpha feature for clusters that\ninclude Windows nodes: HostProcess containers.</p>\n<p>HostProcess containers aim to extend the Windows container model to enable a wider\nrange of Kubernetes cluster management scenarios. HostProcess containers run\ndirectly on the host and maintain behavior and access similar to that of a regular\nprocess. With HostProcess containers, users can package and distribute management\noperations and functionalities that require host access while retaining versioning\nand deployment methods provided by containers. This allows Windows containers to\nbe used for a variety of device plugin, storage, and networking management scenarios\nin Kubernetes. With this comes the enablement of host network mode‚Äîallowing\nHostProcess containers to be created within the host's network namespace instead of\ntheir own. HostProcess containers can also be built on top of existing Windows server\n2019 (or later) base images, managed through the Windows container runtime, and run\nas any user that is available on or in the domain of the host machine.</p>\n<p>Linux privileged containers are currently used for a variety of key scenarios in\nKubernetes, including kube-proxy (via kubeadm), storage, and networking scenarios.\nSupport for these scenarios in Windows previously required workarounds via proxies\nor other implementations. Using HostProcess containers, cluster operators no longer\nneed to log onto and individually configure each Windows node for administrative\ntasks and management of Windows services. Operators can now utilize the container\nmodel to deploy management logic to as many clusters as needed with ease.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>Windows HostProcess containers are implemented with Windows <em>Job Objects</em>, a break from the\nprevious container model using server silos. Job objects are components of the Windows OS which offer the ability to\nmanage a group of processes as a group (a.k.a. <em>jobs</em>) and assign resource constraints to the\ngroup as a whole. Job objects are specific to the Windows OS and are not associated with the Kubernetes <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Job API</a>. They have no process or file system isolation,\nenabling the privileged payload to view and edit the host file system with the\ncorrect permissions, among other host resources. The init process, and any processes\nit launches or that are explicitly launched by the user, are all assigned to the\njob object of that container. When the init process exits or is signaled to exit,\nall the processes in the job will be signaled to exit, the job handle will be\nclosed and the storage will be unmounted.</p>\n<p>HostProcess and Linux privileged containers enable similar scenarios but differ\ngreatly in their implementation (hence the naming difference). HostProcess containers\nhave their own pod security policies. Those used to configure Linux privileged\ncontainers <strong>do not</strong> apply. Enabling privileged access to a Windows host is a\nfundamentally different process than with Linux so the configuration and\ncapabilities of each differ significantly. Below is a diagram detailing the\noverall architecture of Windows HostProcess containers:</p>\n<figure>\n<img src=\"https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/hostprocess-architecture.png\"\nalt=\"HostProcess Architecture\"/>\n</figure>\n<h2 id=\"how-do-i-use-it\">How do I use it?</h2>\n<p>HostProcess containers can be run from within a\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod\">HostProcess Pod</a>.\nWith the feature enabled on Kubernetes version 1.22, a containerd container runtime of\n1.5.4 or higher, and the latest version of hcsshim, deploying a pod spec with the\n<a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/#before-you-begin\">correct HostProcess configuration</a>\nwill enable you to run HostProcess containers. To get started with running\nWindows containers see the general guidance for <a href=\"https://kubernetes.io/docs/setup/production-environment/windows/\">Windows in Kubernetes</a></p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>\n<p>Work through <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/\">Create a Windows HostProcess Pod</a></p>\n</li>\n<li>\n<p>Read about Kubernetes <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\">Pod Security Standards</a></p>\n</li>\n<li>\n<p>Read the enhancement proposal <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-windows/1981-windows-privileged-container-support\">Windows Privileged Containers and Host Networking Mode</a> (KEP-1981)</p>\n</li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>HostProcess containers are in active development. SIG Windows welcomes suggestions from the community.\nGet involved with <a href=\"https://github.com/kubernetes/community/tree/master/sig-windows\">SIG Windows</a>\nto contribute!</p>","PublishedAt":"2021-08-16 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/","SourceName":"Kubernetes"}},{"node":{"ID":1261,"Title":"Blog: Kubernetes Memory Manager moves to beta","Description":"<p><strong>Authors:</strong> Artyom Lukianov (Red Hat), Cezary Zukowski (Samsung)</p>\n<p>The blog post explains some of the internals of the <em>Memory manager</em>, a beta feature\nof Kubernetes 1.22. In Kubernetes, the Memory Manager is a\n<a href=\"https://kubernetes.io/docs/concepts/overview/components/#kubelet\">kubelet</a> subcomponent.\nThe memory manage provides guaranteed memory (and hugepages)\nallocation for pods in the <code>Guaranteed</code> <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes\">QoS class</a>.</p>\n<p>This blog post covers:</p>\n<ol>\n<li><a href=\"#Why-do-you-need-it?\">Why do you need it?</a></li>\n<li><a href=\"#How-does-it-work?\">The internal details of how the <strong>MemoryManager</strong> works</a></li>\n<li><a href=\"#Current-limitations\">Current limitations of the <strong>MemoryManager</strong></a></li>\n<li><a href=\"#Future-work-for-the-Memory-Manager\">Future work for the <strong>MemoryManager</strong></a></li>\n</ol>\n<h2 id=\"why-do-you-need-it\">Why do you need it?</h2>\n<p>Some Kubernetes workloads run on nodes with\n<a href=\"https://en.wikipedia.org/wiki/Non-uniform_memory_access\">non-uniform memory access</a> (NUMA).\nSuppose you have NUMA nodes in your cluster. In that case, you'll know about the potential for extra latency when\ncompute resources need to access memory on the different NUMA locality.</p>\n<p>To get the best performance and latency for your workload, container CPUs,\nperipheral devices, and memory should all be aligned to the same NUMA\nlocality.\nBefore Kubernetes v1.22, the kubelet already provided a set of managers to\nalign CPUs and PCI devices, but you did not have a way to align memory.\nThe Linux kernel was able to make best-effort attempts to allocate\nmemory for tasks from the same NUMA node where the container is\nexecuting are placed, but without any guarantee about that placement.</p>\n<h2 id=\"how-does-it-work\">How does it work?</h2>\n<p>The memory manager is doing two main things:</p>\n<ul>\n<li>provides the topology hint to the Topology Manager</li>\n<li>allocates the memory for containers and updates the state</li>\n</ul>\n<p>The overall sequence of the Memory Manager under the Kubelet</p>\n<p><img src=\"https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/MemoryManagerDiagram.svg\" alt=\"MemoryManagerDiagram\" title=\"MemoryManagerDiagram\"></p>\n<p>During the Admission phase:</p>\n<ol>\n<li>When first handling a new pod, the kubelet calls the TopologyManager's <code>Admit()</code> method.</li>\n<li>The Topology Manager is calling <code>GetTopologyHints()</code> for every hint provider including the Memory Manager.</li>\n<li>The Memory Manager calculates all possible NUMA nodes combinations for every container inside the pod and returns hints to the Topology Manager.</li>\n<li>The Topology Manager calls to <code>Allocate()</code> for every hint provider including the Memory Manager.</li>\n<li>The Memory Manager allocates the memory under the state according to the hint that the Topology Manager chose.</li>\n</ol>\n<p>During Pod creation:</p>\n<ol>\n<li>The kubelet calls <code>PreCreateContainer()</code>.</li>\n<li>For each container, the Memory Manager looks the NUMA nodes where it allocated the\nmemory for the container and then returns that information to the kubelet.</li>\n<li>The kubelet creates the container, via CRI, using a container specification\nthat incorporates information from the Memory Manager information.</li>\n</ol>\n<h3 id=\"let-s-talk-about-the-configuration\">Let's talk about the configuration</h3>\n<p>By default, the Memory Manager runs with the <code>None</code> policy, meaning it will just\nrelax and not do anything. To make use of the Memory Manager, you should set\ntwo command line options for the kubelet:</p>\n<ul>\n<li><code>--memory-manager-policy=Static</code></li>\n<li><code>--reserved-memory=&quot;&lt;numaNodeID&gt;:&lt;resourceName&gt;=&lt;quantity&gt;&quot;</code></li>\n</ul>\n<p>The value for <code>--memory-manager-policy</code> is straightforward: <code>Static</code>. Deciding what to specify for <code>--reserved-memory</code> takes more thought. To configure it correctly, you should follow two main rules:</p>\n<ul>\n<li>The amount of reserved memory for the <code>memory</code> resource must be greater than zero.</li>\n<li>The amount of reserved memory for the resource type must be equal\nto <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable\">NodeAllocatable</a>\n(<code>kube-reserved + system-reserved + eviction-hard</code>) for the resource.\nYou can read more about memory reservations in <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/\">Reserve Compute Resources for System Daemons</a>.</li>\n</ul>\n<p><img src=\"https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/ReservedMemory.svg\" alt=\"Reserved memory\"></p>\n<h2 id=\"current-limitations\">Current limitations</h2>\n<p>The 1.22 release and promotion to beta brings along enhancements and fixes, but the Memory Manager still has several limitations.</p>\n<h3 id=\"single-vs-cross-numa-node-allocation\">Single vs Cross NUMA node allocation</h3>\n<p>The NUMA node can not have both single and cross NUMA node allocations. When the container memory is pinned to two or more NUMA nodes, we can not know from which NUMA node the container will consume the memory.</p>\n<p><img src=\"https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/SingleCrossNUMAAllocation.svg\" alt=\"Single vs Cross NUMA allocation\" title=\"SingleCrossNUMAAllocation\"></p>\n<ol>\n<li>The <code>container1</code> started on the NUMA node 0 and requests <em>5Gi</em> of the memory but currently is consuming only <em>3Gi</em> of the memory.</li>\n<li>For container2 the memory request is 10Gi, and no single NUMA node can satisfy it.</li>\n<li>The <code>container2</code> consumes <em>3.5Gi</em> of the memory from the NUMA node 0, but once the <code>container1</code> will require more memory, it will not have it, and the kernel will kill one of the containers with the <em>OOM</em> error.</li>\n</ol>\n<p>To prevent such issues, the Memory Manager will fail the admission of the <code>container2</code> until the machine has two NUMA nodes without a single NUMA node allocation.</p>\n<h3 id=\"works-only-for-guaranteed-pods\">Works only for Guaranteed pods</h3>\n<p>The Memory Manager can not guarantee memory allocation for Burstable pods,\nalso when the Burstable pod has specified equal memory limit and request.</p>\n<p>Let's assume you have two Burstable pods: <code>pod1</code> has containers with\nequal memory request and limits, and <code>pod2</code> has containers only with a\nmemory request set. You want to guarantee memory allocation for the <code>pod1</code>.\nTo the Linux kernel, processes in either pod have the same <em>OOM score</em>,\nonce the kernel finds that it does not have enough memory, it can kill\nprocesses that belong to pod <code>pod1</code>.</p>\n<h3 id=\"memory-fragmentation\">Memory fragmentation</h3>\n<p>The sequence of Pods and containers that start and stop can fragment the memory on NUMA nodes.\nThe alpha implementation of the Memory Manager does not have any mechanism to balance pods and defragment memory back.</p>\n<h2 id=\"future-work-for-the-memory-manager\">Future work for the Memory Manager</h2>\n<p>We do not want to stop with the current state of the Memory Manager and are looking to\nmake improvements, including in the following areas.</p>\n<h3 id=\"make-the-memory-manager-allocation-algorithm-smarter\">Make the Memory Manager allocation algorithm smarter</h3>\n<p>The current algorithm ignores distances between NUMA nodes during the\ncalculation of the allocation. If same-node placement isn't available, we can still\nprovide better performance compared to the current implementation, by changing the\nMemory Manager to prefer the closest NUMA nodes for cross-node allocation.</p>\n<h3 id=\"reduce-the-number-of-admission-errors\">Reduce the number of admission errors</h3>\n<p>The default Kubernetes scheduler is not aware of the node's NUMA topology, and it can be a reason for many admission errors during the pod start.\nWe're hoping to add a KEP (Kubernetes Enhancement Proposal) to cover improvements in this area.\nFollow <a href=\"https://github.com/kubernetes/enhancements/issues/2044\">Topology aware scheduler plugin in kube-scheduler</a> to see how this idea progresses.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>With the promotion of the Memory Manager to beta in 1.22, we encourage everyone to give it a try and look forward to any feedback you may have. While there are still several limitations, we have a set of enhancements planned to address them and look forward to providing you with many new features in upcoming releases.\nIf you have ideas for additional enhancements or a desire for certain features, please let us know. The team is always open to suggestions to enhance and improve the Memory Manager.\nWe hope you have found this blog informative and helpful! Let us know if you have any questions or comments.</p>\n<p>You can contact us via:</p>\n<ul>\n<li>The Kubernetes <a href=\"https://kubernetes.slack.com/messages/sig-node\">#sig-node </a>\nchannel in Slack (visit <a href=\"https://slack.k8s.io/\">https://slack.k8s.io/</a> for an invitation if you need one)</li>\n<li>The SIG Node mailing list, <a href=\"https://groups.google.com/g/kubernetes-sig-node\">kubernetes-sig-node@googlegroups.com</a></li>\n</ul>","PublishedAt":"2021-08-11 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/","SourceName":"Kubernetes"}}]}},"pageContext":{"source":"Kubernetes"}},"staticQueryHashes":["3649515864"]}