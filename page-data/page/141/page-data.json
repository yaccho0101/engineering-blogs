{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/141","result":{"data":{"allPost":{"edges":[{"node":{"ID":351,"Title":"Building a Monorepo with Yarn 2","Description":"<p>In true JavaScript fashion, there was no shortage of releases in the JavaScript ecosystem this year. This includes the Yarn project’s release of Yarn 2 with a compressed cache of JavaScript dependencies, including a Yarn binary to reference,  that can be used for a zero-install deployment. </p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1608668413-yarn.png\" alt=\"Ball of yarn and knitting needles illustration\"></p>\n\n<p>Yarn is a package manager that also provides developers a project management toolset. Now, Yarn 2 is now officially supported by Heroku, and Heroku developers are able to take advantage of leveraging zero-installs during their Node.js builds. We’ll go over a popular use case for Yarn that is enhanced by Yarn 2: using workspaces to manage dependencies for your monorepo.</p>\n\n<p>We will cover taking advantage of Yarn 2’s cache to manage monorepo dependencies. Prerequisites for this include a development environment with Node installed. To follow these guides, set up an existing Node project that makes use of a <code>package.json</code> too. If you don’t have one, use the <a href=\"https://github.com/heroku/node-js-getting-started\">Heroku Getting Started with Node.js Project</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"workspaces\" href=\"#workspaces\">Workspaces</a>\n</h2>\n\n<p>First off, what are workspaces? Workspaces is Yarn’s solution to a monorepo structure for a JavaScript app or Node.js project. A monorepo refers to a project, in this case, a JavaScript project, that has more than one section of the code base. For example, you may have the following set up:</p>\n\n<pre><code class=\"language-javascript\">/app\n - package.json\n - /server\n   - package.json\n - /ui\n   - package.json\n</code></pre>\n\n<p>Your JavaScript server has source code, but there’s an additional front end application that will be built and made available to users separately. This is a popular pattern for setting up a separation of concerns with a custom API client, a build or testing tool, or something else that may not have a place in the application logic. Each of the subdirectory’s <code>package.json</code> will have their own dependencies. How can we manage them? How do we optimize caching? This is where Yarn workspaces comes in.</p>\n\n<p>In the root <code>package.json</code>, set up the subdirectories under the <code>workspaces</code> key. You should add this to your <code>package.json</code>:</p>\n\n<pre><code class=\"language-javascript\">\"workspaces\": [\n    \"server\",\n    \"ui\"\n]\n</code></pre>\n\n<p>For more on workspaces, visit here: <a href=\"https://yarnpkg.com/features/workspaces\">https://yarnpkg.com/features/workspaces</a></p>\n\n<p>Additionally, add the <code>workspaces-tools</code> plugin. This will be useful when running workspace scripts that you’ll use later. You can do this by running:</p>\n\n<pre><code class=\"language-javascript\">yarn plugin import workspace-tools\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-yarn\" href=\"#setting-up-yarn\">Setting up Yarn</a>\n</h2>\n\n<p>If you’re already using Yarn, you have a <code>yarn.lock</code> file already checked into your code base’s git repository. There’s other files and directories that you’ll need up to set up the cache. If you aren’t already using Yarn, install it globally.</p>\n\n<pre><code class=\"language-javascript\">npm install -g yarn\n</code></pre>\n\n<p><em>Note: If you don’t have Yarn &gt;=1.22.10 installed on your computer, update it with the same install command.</em></p>\n\n<p>Next, set up your Yarn version for this code base. One of the benefits of using Yarn 2 is that you’ll have a checked in Yarn binary that will be used by anyone that works on this code base and eliminates version conflicts between environments.</p>\n\n<pre><code class=\"language-javascript\">yarn set version berry\n</code></pre>\n\n<p>A <code>.yarn</code> directory and <code>.yarnrc.yml</code> file will both be created that need to be checked into git. These are the files that will set up your project’s local Yarn instance.</p>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-the-dependency-cache\" href=\"#setting-up-the-dependency-cache\">Setting Up the Dependency Cache</a>\n</h2>\n\n<p>Once Yarn is set up, you can set up your cache. Run yarn install:</p>\n\n<pre><code class=\"language-javascript\">yarn\n</code></pre>\n\n<p>Before anything else, make sure to add the following to the <code>.gitignore</code>:</p>\n\n<pre><code class=\"language-javascript\"># Yarn\n.yarn/*\n!.yarn/cache\n!.yarn/releases\n!.yarn/plugins\n!.yarn/sdks\n!.yarn/versions\n</code></pre>\n\n<p>The files that are ignored will be machine specific, and the remaining files you’ll want to check in. If you run <code>git status</code>, you’ll see the following:</p>\n\n<pre><code class=\"language-javascript\">Untracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    .pnp.js\n    .yarn/cache/\n    yarn.lock\n</code></pre>\n\n<p>You’ve created new files that will speed up your install process:</p>\n\n<ul>\n<li>\n<code>.pnp.js</code> - This is the Plug’n’Play (PnP) file. The PnP file tells your Node app or build how to find the dependencies that are stored in <code>.yarn/cache</code>.</li>\n<li>\n<code>.yarn/cache</code> - This directory will have the dependencies that are needed to run and build your app.</li>\n<li>\n<code>yarn.lock</code> - The lock file still is used to lock the versions that are resolved from the <code>package.json</code>.</li>\n</ul>\n\n<p>Check all of this in to git, and you’re set. For more information about Yarn 2’s zero-install philosophy, read here: <a href=\"https://yarnpkg.com/features/zero-installs\">https://yarnpkg.com/features/zero-installs</a></p>\n<h2 class=\"anchored\">\n  <a name=\"adding-dependencies-to-subdirectories\" href=\"#adding-dependencies-to-subdirectories\">Adding Dependencies to Subdirectories</a>\n</h2>\n\n<p>Now that Yarn and the cache are set up, we can start adding dependencies. As initially shown, we have a <code>server</code> directory and a <code>ui</code> directory. We can assume that each of these will be built and hosted differently. For example, my server is written in TypeScript, using Express.js for routing, and running on a Heroku web dyno. For the front end app, it is using Next.js. The build will be run during the app’s build process.</p>\n\n<p>Add <code>express</code> to the server <code>dependencies</code>. </p>\n\n<pre><code class=\"language-javascript\">yarn workspace server add express\n</code></pre>\n\n<p>Additionally, add <code>@types/express</code> and <code>typescript</code> to the <code>devDependencies</code>. You can use the <code>-D</code> flag to indicate that you’re adding <code>devDependencies</code>. </p>\n\n<pre><code class=\"language-javascript\">yarn workspace server add @types/express typescript -D\n</code></pre>\n\n<p>We now have our dependencies in our <code>server</code> workspace. We just need to create our <code>ui</code> workspace. Next, build a Next.js app with the <code>yarn create</code> command.</p>\n\n<pre><code class=\"language-javascript\">yarn create next-app ui\n</code></pre>\n\n<p>Finally, run <code>yarn</code> again to update the cache and check these changes into git.</p>\n<h2 class=\"anchored\">\n  <a name=\"running-scripts-with-workspaces\" href=\"#running-scripts-with-workspaces\">Running Scripts with Workspaces</a>\n</h2>\n\n<p>The last piece is to run scripts within the workspaces. If you look through your source code, you’ll see that there’s one global cache for all dependencies under your app’s root directory. Run the following to see all the compressed dependencies:</p>\n\n<pre><code class=\"language-javascript\">ls .yarn/cache\n</code></pre>\n\n<p>Now, lets run build scripts with workspaces. First, set up the workspace. For server, use <code>tsc</code> to build the TypeScript app. You’ll need to set up a TypeScript config and a <code>.ts</code> file first:</p>\n\n<pre><code class=\"language-javascript\">cd server\nyarn dlx --package typescript tsc --init\ntouch index.ts\n</code></pre>\n\n<p><code>yarn dlx</code> will run a command from a package so that it doesn’t need to be installed globally. It’s useful for one-off initializing commands, like initializing a TypeScript app.</p>\n\n<p>Next, add the build step to the <code>server/package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node index.js\"\n},\n</code></pre>\n\n<p>Change directories back to the application level, and run the build.</p>\n\n<pre><code class=\"language-javascript\">cd ..\nyarn workspace server build\n</code></pre>\n\n<p>You’ll see that a <code>server/index.js</code> file is created. Add <code>server/*.js</code> to the <code>.gitignore</code>.</p>\n\n<p>Since we already have <code>build</code> and <code>start</code> scripts in our Next.js app (created by the <code>yarn create</code> command), add a build script at the root level <code>package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\"\n},\n</code></pre>\n\n<p>This is when the <code>workspaces-tool</code> plugin is used. Run <code>yarn build</code> from your app’s root, and both of your workspaces will build. Open a second terminal, and you’ll be able to run <code>yarn workspace server start</code> and <code>yarn workspace ui start</code> in each terminal and run the Express and Next servers in parallel.</p>\n<h2 class=\"anchored\">\n  <a name=\"deploy-to-heroku\" href=\"#deploy-to-heroku\">Deploy to Heroku</a>\n</h2>\n\n<p>Finally, we can deploy our code to Heroku. Since Heroku will run the script is in the <code>package.json</code> under <code>start</code>, add a script to the <code>package.json</code>.</p>\n\n<pre><code class=\"language-javascript\">\"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\",\n    \"start\": \"yarn workspaces server start\"\n},\n</code></pre>\n\n<p><a href=\"https://devcenter.heroku.com/articles/deploying-nodejs#specifying-a-start-script\">Heroku will use the <code>start</code> script</a> from the <code>package.json</code> to start the <code>web</code> process on your app.</p>\n<h2 class=\"anchored\">\n  <a name=\"conclusion\" href=\"#conclusion\">Conclusion</a>\n</h2>\n\n<p>There are <a href=\"https://yarnpkg.com/features\">plenty more features</a> that Yarn, and specifically Yarn 2, offers that are useful for Heroku developers. Check out the Yarn docs to see if there are additional workspace features that may work nicely with Heroku integration. As always, if you have any feedback or issues, please <a href=\"https://github.com/heroku/heroku-buildpack-nodejs/issues/new/choose\">open an Issue on GitHub</a>.</p>","PublishedAt":"2020-12-22 20:53:08+00:00","OriginURL":"https://blog.heroku.com/building-a-monorepo-with-yarn-2","SourceName":"Heroku"}},{"node":{"ID":489,"Title":"How to parse and verify JPQR, the standardized QR code of Japan","Description":"<p>Like PayPay, other cashless payment services each have their unique QR code, </p>","PublishedAt":"2020-12-22 03:43:04+00:00","OriginURL":"https://blog.paypay.ne.jp/en/paypay-adapting-to-jpqr/","SourceName":"Paypay"}},{"node":{"ID":258,"Title":"GIPHY Search Trends During the 2020 Presidential Election","Description":"Major social and cultural events mean “all hands on deck” here at GIPHY, as millions of people across the world use GIFs to express themselves as these events unfold. Award shows, championship games, and holidays (such as New Year’s Eve) require resources across the company — from our Editorial team “live-GIFing” video feeds in real-time [&#8230;]","PublishedAt":"2020-12-14 20:23:05+00:00","OriginURL":"https://engineering.giphy.com/giphy-search-trends-during-the-2020-presidential-election/","SourceName":"GIPHY"}},{"node":{"ID":352,"Title":"Extend Flows with Heroku Compute: An Event-Driven Pattern","Description":"<p><em>This post <a href=\"https://medium.com/salesforce-architects/extend-flows-with-heroku-compute-an-event-driven-pattern-a9840a91ce5b\">previously appeared</a> on the Salesforce Architects blog.</em></p>\n\n<p>Event-driven application architectures have proven to be effective for implementing enterprise solutions using loosely coupled services that interact by exchanging asynchronous events. Salesforce enables event-driven architectures (EDAs) with Platform Events and Change Data Capture (CDC) events as well as triggers and Apex callouts, which makes the Salesforce Platform a great way to build all of your <a href=\"https://www.salesforce.com/products/platform/best-practices/understanding-digital-customer-experience/\">digital customer experiences</a>. This post is the first in a series that covers various EDA patterns, considerations for using them, and examples deployed on the Salesforce Platform.</p>\n<h2 class=\"anchored\">\n  <a name=\"expanding-the-event-driven-architecture-of-the-salesforce-platform\" href=\"#expanding-the-event-driven-architecture-of-the-salesforce-platform\">Expanding the event-driven architecture of the Salesforce Platform</a>\n</h2>\n\n<p>Back in April, Frank Caron wrote a <a href=\"https://developer.salesforce.com/blogs/2020/04/event-driven-app-architecture-on-the-customer-360-platform.html\">blog post</a> describing the power of EDAs. In it, he covered the event-driven approach and the benefits of loosely coupled service interactions. He focused mainly on use cases where events triggered actions across platform services as well as how incorporating third-party external services can greatly expand the power of applications developed using declarative low-code tools like Salesforce Flow.</p>\n\n<p>As powerful as flows can be for accessing third-party services, even greater power comes when your own custom applications, running your own business logic on the Salesforce Platform, are part of flows. </p>\n\n<p>API-first, event-driven <a href=\"https://medium.com/adobetech/three-principles-of-api-first-design-fa6666d9f694\">design</a> is the kind of development that frequently requires collaboration across different members of you team. Low-code builders with domain expertise who are familiar with the business requirements can build the flows. Programmers are typically necessary to develop the back-end services that implement the business logic. An enterprise architect may get involved as well to design the service APIs.</p>\n\n<p>However you are organized, you will need to expose your services with APIs and enable them to produce and consume events. The Salesforce Platform enables this with the <a href=\"https://www.salesforce.com/video/1771211/\">Salesforce Event Bus</a>, <a href=\"https://developer.salesforce.com/blogs/2019/11/introducing-salesforce-evergreen.html\">Salesforce Functions</a>, and <a href=\"https://developer.salesforce.com/docs/atlas.en-us.228.0.api_streaming.meta/api_streaming/intro_stream.htm\">Streaming API</a> as well as support for OpenAPI specification for external services. </p>\n\n<p>Heroku capabilities on the Salesforce Platform include event streaming, relational data stores, and key-value caches seamlessly integrated with elastic compute. These capabilities, combined with deployment automation and hands-off operational excellence, lets your developers focus entirely on delivering your unique business requirements. Seamless integration with the rest of Salesforce makes your apps deployed on Heroku the foundation for complete, compelling, economical, secure, and successful solutions.</p>\n\n<p>This post focuses on expanding flows with Heroku compute. Specifically, how to expose Heroku apps as external services and securely access them via flows using Flow Builder as the low-code development environment. Subsequent posts will expand this idea to include event-driven interactions between Heroku apps and the rest of the Salesforce Platform as well as other examples of how Salesforce Platform based EDAs address common challenges we see across many of our customers including:</p>\n\n<ul>\n<li>Multi-organization visibility and reporting</li>\n<li>Shared event bus designs</li>\n<li>B2C apps with Lightning Web Components</li>\n</ul>\n<h2 class=\"anchored\">\n  <a name=\"building-salesforce-flows-with-your-own-business-logic\" href=\"#building-salesforce-flows-with-your-own-business-logic\">Building Salesforce flows with your own business logic</a>\n</h2>\n\n<p>Salesforce external services are a great way to access third-party services from a flow. All you need are the services’ OpenAPI spec schema (OAS schema), and you’re set to go. There are some great examples of how to register your external services <a href=\"https://andyinthecloud.com/2017/07/23/simplified-api-integrations-with-external-services/\">here</a>, with a more detailed example of how to generate an Apex client and explore your schema <a href=\"https://andyinthecloud.com/2017/09/30/swagger-open-api-salesforce-like/\">here</a>. </p>\n\n<p>But what if you want to incorporate custom business logic into your flow app? What if you wanted to extend and complement the declarative programming model of flows with an imperative model with full programming semantics? What if you wanted to make your app available to flow developers in other organizations, or possibly accessed as a stand-alone service behind a Lightning Web Components based app?</p>\n\n<p>This kind of service deployment typically requires off-platform development, bringing with it all the complexity and operational overhead that goes with meeting the scalability, availability, and reliability requirements of your business critical apps. </p>\n\n<p>The following steps show you how you can deploy your own apps using Heroku on the Salesforce Platform without any of this operational overhead. We’re going to walk through an example of how to build and deploy custom business logic into your own service and access it in a flow. Deployment will be via a Heroku app, which brings the power and flexibility to write your own code, without having to worry about the operational burden of production app deployment or DevOps toolchains. </p>\n\n<p>This approach works well in scenarios where you have programmers and low-code builders working together to deploy a new app. The team first collaborates on what the app needs to do and defines the API that a flow can access. Once the API is designed, this specification then becomes the <a href=\"https://swagger.io/blog/api-strategy/benefits-of-openapi-api-development/\">contract</a> between the two teams. As progress is made on each side, they iterate, perfect their design, and ultimately deliver the app. All the code used for this example is available on <a href=\"https://github.com/chrismarino/flowapp\">GitHub</a>, so that you can try it out for yourself.</p>\n\n<p><em>Note: Apex is a great way to customize Salesforce, but there are times when a standalone app might be the better way to go. If your team prefers Python, Node, or some other programming language, or perhaps you already have an app running on premises or in the cloud, and you want to run it all within the secure perimeter of the Salesforce Platform, a standalone Heroku app is the way to go.</em></p>\n<h2 class=\"anchored\">\n  <a name=\"api-spec-defines-the-interface\" href=\"#api-spec-defines-the-interface\">API spec defines the interface</a>\n</h2>\n\n<p>The example application an on-line shopping site that lets users login, browse products, and make a purchase. We’ll describe the process of building out this app in a number of posts, but for this first part we’ll simply build a flow and an external service that lists products and updates inventory in Salesforce. For the API, we’re using a sample API available on <a href=\"https://swagger.io/tools/swaggerhub/\">Swagger Hub</a>. There are a variety of tools and systems that can do this, including the MuleSoft <a href=\"https://www.mulesoft.com/platform/api-design\">Anypoint Platform API Designer</a>. For this example, however, we’re using this simple shopping cart spec to bootstrap the API design and provide the initial application stub for development. </p>\n\n<p>From the API spec, API Portals can produce server side application stubs to jumpstart application development. In this example, we’ve downloaded the node.js API stub as the starting point for API and app development. We’ve also modified the code so that it can run on Heroku by adding a <a href=\"https://devcenter.heroku.com/articles/procfile\">Procfile</a> and changing the port configuration. If you want to try it yourself, you can <a href=\"https://heroku.com/deploy?template=https://github.com/chrismarino/flowapp\">Deploy to Heroku</a>, or click the Deploy to Heroku button on the <a href=\"https://github.com/chrismarino/flowapp/blob/master/README.md\">GitHub</a> page. </p>\n\n<p>Let’s begin by looking at the initial <a href=\"http://swagger-shop.herokuapp.com/docs/#/Internal_calls\">API spec</a> for the application. These API docs are being served from a deployment of the app stub on Heroku. The actual YAML spec (which we will modify later in this post) is included in the <a href=\"https://github.com/chrismarino/flowapp/blob/master/api/swagger.yaml\">repo as well</a>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639439-1%2AYhqGS0YRXgD3lAYVqyuhQQ.png\" alt=\"\"></p>\n\n<p>As you can see in the spec, there are definitions for each of the methods that specify which parameters are required and what the response payload will look like. Since this is a valid OpenAPI spec, we can register this API as an external service as described in <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/external-services/get-started-with-external-services\">Get Started with External Services</a>.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639550-1%2A5NKtci6bYvqVp1DHex7VfQ.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"external-service-authorization\" href=\"#external-service-authorization\">External service authorization</a>\n</h2>\n\n<p>The flow needs a Named Credential in Salesforce to access the external service. Salesforce offers many alternatives for how the app can use the Named Credential including per-user credentials that can help you track and control access. For this example, though, we’re going to use a single login for all flow access using basic HTTP authentication.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639589-1%2AU2DxWullRClXrHinOJ7_6g.png\" alt=\"\"></p>\n\n<p>You can find the code that implements this method is in the app included in the <a href=\"https://github.com/chrismarino/flowapp/blob/master/controllers/InternalCalls.js#L6\">repo</a>. </p>\n\n<p>App access to the organization is authorized via a Salesforce JWT account token and implemented in the app in <a href=\"https://github.com/chrismarino/flowapp/blob/master/service/SFAuthService.js\">SFAuthService.js</a>:</p>\n\n<pre><code class=\"lang-javascript\">'use strict';\n\nconst jwt = require('salesforce-jwt-bearer-token-flow');\nconst jsforce = require('jsforce');\n\nrequire('dotenv').config();\n\nconst { SF_CONSUMER_KEY, SF_USERNAME, SF_LOGIN_URL } = process.env;\n\nlet SF_PRIVATE_KEY = process.env.SF_PRIVATE_KEY;\nif (!SF_PRIVATE_KEY) {\n    SF_PRIVATE_KEY = require('fs').readFileSync('private.pem').toString('utf8');\n}\n\nexports.getSalesforceConnection = function () {\n    return new Promise(function (resolve, reject) {\n        jwt.getToken(\n            {\n                iss: SF_CONSUMER_KEY,\n                sub: SF_USERNAME,\n                aud: SF_LOGIN_URL,\n                privateKey: SF_PRIVATE_KEY\n            },\n            (err, tokenResponse) =&gt; {\n                if (tokenResponse) {\n                    let conn = new jsforce.Connection({\n                        instanceUrl: tokenResponse.instance_url,\n                        accessToken: tokenResponse.access_token\n                    });\n                    resolve(conn);\n                } else {\n                    reject('Authentication to Salesforce failed');\n                }\n            }\n        );\n    });\n};\n</code></pre>\n\n<p>The private key is configured in Heroku as a configuration variable and is installed when the app is deployed.</p>\n<h2 class=\"anchored\">\n  <a name=\"register-the-external-service-methods\" href=\"#register-the-external-service-methods\">Register the external service methods</a>\n</h2>\n\n<p>Individual methods for the ShoppingService external service are easily added to a flow just as they would be for any external service. Here we’ve added the Get Products and Get Order methods, as shown in Flow Builder below. But since Flow Builder can register an external service method using only the API spec, they are just references to the stub methods that we still need to build out. We’ll program something for them later in this post.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639692-1%2A5uLV0XtFHdqwubLxGn74UA.png\" alt=\"\"></p>\n\n<p>These are familiar steps to anyone that has registered an external service for a low, but if you want more detail on how to do this, check out the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/external-services/get-started-with-external-services\">Get Started with External Services Trailhead</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"define-the-api-and-build-out-the-methods\" href=\"#define-the-api-and-build-out-the-methods\">Define the API and build out the methods</a>\n</h2>\n\n<p>With the authorizations in place and the methods defined, we are now ready to build out the external service in a way that meets our company’s specific needs. For this, we need to implement each of the API methods. </p>\n\n<p>To illustrate this, here is the Node function that has been stubbed out by the API for the Get Order method. It is here that your business logic is implemented.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639765-1%2AM8e6Ge18JmmIMe0hHTUBKw.png\" alt=\"\"></p>\n\n<p>For each of the API methods, we’ve implemented some simple logic that we will use to test interactions with the flow. For example, here’s the code for getting a list of all orders:</p>\n\n<pre><code class=\"lang-javascript\">/**\n * Get list of all orders for the user\n *\n * type String json or xml\n * pOSTDATA List Creates a new employee in DB (optional)\n * returns List\n **/\nexports.typePost_orderPOST = function(type,pOSTDATA) {\n  return new Promise(function(resolve, reject) {\n    var examples = {};\n    examples['application/json'] = [ {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n}, {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n} ];\n    if (Object.keys(examples).length &gt; 0) {\n      resolve(examples[Object.keys(examples)[0]]);\n    } else {\n      resolve();\n    }\n  });\n}\n</code></pre>\n\n<p>You can examine the code that implements each of the methods in the repo:</p>\n\n<ul>\n<li>Get Products Method</li>\n<li>Post Order Method</li>\n<li>Get Order Method</li>\n<li>Get Orders Method</li>\n</ul>\n\n<p>Now that we have some simple logic executing in each of these methods, we can build a simple flow that logs in using the Named Credential, accesses the external service, and returns product data.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639841-1%2AnmANPe2Z75KWm1M76YLjCg.png\" alt=\"\"></p>\n\n<p>Running this flow shows the product data from the stub app. The successful display of product data here indicates that the flow has been able to successfully log in to the app, call the Get Product method, and get the proper response.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639862-1%2AMPICEeMp8XjIMsyNRkTL7Q.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"update-the-api\" href=\"#update-the-api\">Update the API</a>\n</h2>\n\n<p>So now that we have our basic flow defined and accessing the app, we can complete the API with new methods necessary for the app to do what it needs to do. </p>\n\n<p>Let’s imagine that the app has up-to-date product inventory data and we want to use that data to update the Product object in Salesforce with the current quantity in stock. For this, the app would need to be able to access Salesforce and update the Product object. </p>\n\n<p>To do this, the flow needs to make a request to a Get Inventory method. But that method does not yet exist. However, we can modify the API to include any new methods we need. Here our teams work together to determine what the flow needs and what methods are necessary in the app. </p>\n\n<p>After discussion, we determine that a single Get Inventory method will satisfy the requirements. So, now we update the API spec to include a new <a href=\"https://github.com/chrismarino/flowapp/blob/ExternalService/api/swagger.yaml#L174-L205\">method</a>:</p>\n\n<pre><code class=\"lang-yaml\">/{type}/get_inventory/:\n    get:\n      tags:\n      - \"Internal calls\"\n      description: \"Get Inventory\"\n      operationId: \"typeGet_inventoryGET\"\n      consumes:\n      - \"application/json\"\n      produces:\n      - \"application/json\"\n      - \"application/xml\"\n      parameters:\n      - name: \"type\"\n        in: \"path\"\n        description: \"json or xml\"\n        required: true\n        type: \"string\"\n      - name: \"product_id\"\n        in: \"query\"\n        description: \"Product Id\"\n        required: true\n        type: \"integer\"\n      responses:\n        \"200\":\n          description: \"OK\"\n          schema:\n            type: \"array\"\n            items:\n              $ref: \"#/definitions/Inventory\"\n      security:\n      - basic: []\n</code></pre>\n\n<p>With this updated API, we can update the external service so that we can use it in a flow.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639924-1%2A8I3LMY2IrvvZXItY219arg.png\" alt=\"\"></p>\n\n<p>And with the updated API spec, we can automatically generate a stub method as well. From the empty stub method we can <a href=\"https://github.com/chrismarino/flowapp/blob/ExternalService/service/InternalCallsService.js#L4-L25\">complete the function</a> with the necessary logic to access Salesforce directly and update the Product object. Note that it uses the SFAuthService.js code from above and an API token to access the organization data.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639965-1%2AVALT9oPUv4gQEocwMuxhgA.png\" alt=\"\"></p>\n<h2 class=\"anchored\">\n  <a name=\"platform-events-and-eda\" href=\"#platform-events-and-eda\">Platform events and EDA</a>\n</h2>\n\n<p>Now that this inventory method is available, we can check the operation with a simple flow that triggers on a Platform Event and updates the Product object. When we run this test flow, it updates the iPhone Product object in the organization.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1607639993-1%2A9w82sZ_SIfNW1Ve_S-A0TA.png\" alt=\"\"></p>\n\n<p>How and when the flow might need to update the product inventory would be up to the actual business needs. However, triggering the flow to update the object can be done using Platform Events. The flow can respond to any Platform Event with a call to the Get Inventory method.</p>\n<h2 class=\"anchored\">\n  <a name=\"deploy-the-business-logic\" href=\"#deploy-the-business-logic\">Deploy the business logic</a>\n</h2>\n\n<p>The process described in this post can go on until the flow and app API converge on a stable design. Once stable, our programmer and low-code builder can complete their work independently to complete the app. The flow designer can build in all the decision logic that surrounds the flow and build out screens for the user to interact with.</p>\n\n<p>Separately, and independently from the flow designer, the programmer can code each of the methods in the stub app to implement the business logic.</p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>We’ve just started building out this app and running it as an external service. In this post, however, you’ve already seen the the basic steps that would be part of every development cycle: defining an API, registering methods that a flow can call, building out the stub app, and authorizing access for the flow, app, and Platform Event triggers. </p>\n\n<p>Future posts in this series will take these basic elements and methodology to expand the flow to execute the business logic contained in the app via user interface elements for a complete process automation solution running entirely on the Salesforce Platform.</p>\n\n<p>To learn more, see the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/heroku_enterprise_baiscs?trail_id=heroku_enterprise\">Heroku Enterprise Basics Trailhead module</a> and the <a href=\"https://trailhead.salesforce.com/en/content/learn/modules/flow-basics\">Flow Basics Trailhead module</a>. Please share your feedback with <a href=\"https://twitter.com/SalesforceArchs\">@SalesforceArchs</a> on Twitter.</p>","PublishedAt":"2020-12-11 16:30:00+00:00","OriginURL":"https://blog.heroku.com/extend-flows-heroku-event-driven","SourceName":"Heroku"}},{"node":{"ID":622,"Title":"How To Get Fooled By Metrics","Description":"Metrics are one of the main building blocks in the topic of observability and we use them heavily. This story is about an incident where we tried to find and resolve a problem that we saw in these metrics. We went down a rabbit hole of potential fixes, only to discover that the metrics were correct all along.","PublishedAt":"2020-12-04 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-12-04-howtogetfooledbymetrics/","SourceName":"Trivago"}},{"node":{"ID":259,"Title":"Part 1: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning","Description":"Motivation GIPHY search is powered by ElasticSearch, where GIFs are represented as documents with various features, such as tags and captions. Tags in particular have a great impact on the search performance. Therefore, at GIPHY Engineering, we continuously track and improve their quality with both manual and automated verification. However, for the last couple of [&#8230;]","PublishedAt":"2020-12-03 19:42:02+00:00","OriginURL":"https://engineering.giphy.com/computer-vision-giphy-how-we-created-an-autotagging-model-using-deep-learning/","SourceName":"GIPHY"}},{"node":{"ID":426,"Title":"Indeed + Hacktoberfest 2020: By The Numbers","Description":"<p>Indeed + Hacktoberfest 2020 is in the books! We’re thrilled to share our results. External focus Internal focus As a Hacktoberfest Community Partner, we engaged directly with the external community. 1 external landing page 1 case study 6 supported open source projects tagged with the ‘hacktoberfest’ label 11 virtual office hours 437 commits into our [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-12-03 18:20:44+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/12/hacktoberfest-2020-by-the-numbers/","SourceName":"Indeed"}},{"node":{"ID":798,"Title":"Leveraging a Manager Weekly Newsletter for Team Communication","Description":"I started my journey as an Engineering Manager at SoundCloud close to a year ago. This came after working as a Software Engineer for more…","PublishedAt":"2020-11-25 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/manager-weekly-newsletter-for-team-communication","SourceName":"Soundcloud"}},{"node":{"ID":623,"Title":"Exploring the Page Visibility API for Detecting Page Background State","Description":"","PublishedAt":"2020-11-17 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-11-17-exploringthepagevisibilityapifordetectin/","SourceName":"Trivago"}},{"node":{"ID":427,"Title":"k8dash: Indeed’s Open Source Kubernetes Dashboard","Description":"<p>So you’ve got your first Kubernetes (also known as k8s) cluster up and running. Congratulations! Now, how do you operate the thing? Deployments, replica sets, stateful sets, pods, ingress, oh my! Getting Kubernetes running can feel like a big enough challenge in and of itself, but does day two of operations need to be just [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-11-05 17:13:25+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/11/k8dash-indeeds-open-source-kubernetes-dashboard/","SourceName":"Indeed"}},{"node":{"ID":260,"Title":"GIPHY SDK Debuts its Latest Service, GIPHY Text, with Select Launch Partners","Description":"GIPHY users have relied on us to help them better express themselves since day one. First, with GIFs, and later followed by Stickers, animated emoji &#38; text, Video, etc. We know, based on the frequent use of captions on GIFs, that sometimes text is important to contextualize visual media. This led the way to the [&#8230;]","PublishedAt":"2020-10-28 07:00:00+00:00","OriginURL":"https://engineering.giphy.com/giphy-sdk-debuts-its-latest-service-giphy-text-with-select-launch-partners/","SourceName":"GIPHY"}},{"node":{"ID":490,"Title":"The PayPay Mini App Platform: Build and deploy your service easily into PayPay","Description":"<p>One of the biggest challenges developers face is getting their app discovered </p>","PublishedAt":"2020-10-26 01:14:40+00:00","OriginURL":"https://blog.paypay.ne.jp/en/paypay-miniapp-platform/","SourceName":"Paypay"}},{"node":{"ID":624,"Title":"Deep Dive Into Data Science at trivago","Description":"","PublishedAt":"2020-10-22 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-10-22-deepdiveintodatascienceattrivago/","SourceName":"Trivago"}},{"node":{"ID":491,"Title":"What are the different levels of automation testing? : Web Testing","Description":"<p>This is Part 5 of our automation series. Read Part 1 to </p>","PublishedAt":"2020-10-21 01:51:36+00:00","OriginURL":"https://blog.paypay.ne.jp/en/what-are-the-different-levels-of-web-automation-testing/","SourceName":"Paypay"}},{"node":{"ID":428,"Title":"Jackson: A Growing User Base Presents New Challenges","Description":"<p>Jackson is a mature and feature-rich open source project that we use, support, and contribute to here at Indeed. In my previous post, I introduced Jackson’s core competency as a JSON library for Java. I went on to describe the additional data formats, data types, and JVM languages Jackson supports. In this post, I will [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-10-19 13:00:02+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/10/jackson-a-growing-user-base-presents-new-challenges/","SourceName":"Indeed"}},{"node":{"ID":799,"Title":"Testing SQL for BigQuery","Description":"“To me, legacy code is simply code without tests.” — Michael Feathers If untested code is legacy code, why aren’t we testing data pipelines or ETLs (extract, transform, load)? In particular, data pipelines built in SQL are rarely tested. However, as software engineers, we know all our code should be tested. So in this post, I’ll describe how we started testing SQL data pipelines at SoundCloud.","PublishedAt":"2020-10-16 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/testing-sql-for-bigquery","SourceName":"Soundcloud"}},{"node":{"ID":719,"Title":"New “Fully Customizable Video” Zoom SDK Announced","Description":"","PublishedAt":"2020-10-14 17:03:04+00:00","OriginURL":"https://medium.com/zoom-developer-blog/new-fully-customizable-video-zoom-sdk-announced-fb9c05ce0d6a?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":353,"Title":"Incident Response at Heroku","Description":"<p><em>This post is an update on a <a href=\"https://blog.heroku.com/incident-response-at-heroku\">previous post</a> about how Heroku handles incident response.</em></p>\n\n<p>As a service provider, when things go wrong, you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s.</p>\n\n<p>At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful.</p>\n<h2 class=\"anchored\">\n  <a name=\"incident-response-and-the-incident-commander-role\" href=\"#incident-response-and-the-incident-commander-role\">Incident Response and the Incident Commander Role</a>\n</h2>\n\n<p>We describe Heroku’s incident response framework below. It’s based on the <a href=\"https://en.wikipedia.org/wiki/Incident_Command_System\">Incident Command System</a> used in natural disaster response and other emergency response fields. Our response framework and the Incident Commander role in particular help us to successfully respond to a variety of incidents.</p>\n\n<p>When an incident occurs, we follow these steps:</p>\n<h3 class=\"anchored\">\n  <a name=\"page-an-incident-commander\" href=\"#page-an-incident-commander\">Page an Incident Commander</a>\n</h3>\n\n<p>They will assess the issue, and decide if it’s worth investigating further</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548057-Slack%20Page.png\" alt=\"Slack Page\"></p>\n<h3 class=\"anchored\">\n  <a name=\"move-to-a-dedicated-chat-room\" href=\"#move-to-a-dedicated-chat-room\">Move to a dedicated chat room</a>\n</h3>\n\n<p>The Incident Commander creates a new room in Slack, to centralize all the information for this specific incident</p>\n<h3 class=\"anchored\">\n  <a name=\"update-public-status-site\" href=\"#update-public-status-site\">Update public status site</a>\n</h3>\n\n<p>Our customers want information about incidents as quickly as possible, even if it is preliminary. As soon as possible, the IC designates someone to take on the communications role (“comms”) with a first responsibility of updating the <a href=\"https://status.heroku.com/\">status site</a> with our current understanding of the incident and how it’s affecting customers. The admin section of Heroku’s status site helps the comms operator to get this update out quickly:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548128-public-post.png\" alt=\"public-post\"></p>\n\n<p>The status update then appears on <a href=\"https://status.heroku.com/\">status.heroku.com</a> and is sent to customers and internal communication channels via SMS, email, and Slack bot. It also shows on twitter:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548203-twitter-post.png\" alt=\"twitter-post\"></p>\n<h3 class=\"anchored\">\n  <a name=\"send-out-internal-situation-report\" href=\"#send-out-internal-situation-report\">Send out internal Situation Report</a>\n</h3>\n\n<p>Next the IC compiles and sends out the first situation report (“sitrep”) to the internal team describing the incident. It includes what we know about the problem, who is working on it and in what roles, and open issues. As the incident evolves, the sitrep acts as a concise description of the current state of the incident and our response to it. A good sitrep provides information to active incident responders, helps new responders get quickly up to date about the situation, and gives context to other observers like customer support staff.</p>\n\n<p>The Heroku status site has a form for the sitrep, so that the IC can update it and the public-facing status details at the same time. When a sitrep is created or updated, it’s automatically distributed internally via email and Slack bot. A versioned log of sitreps is also maintained for later review:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548277-sitrep2.png\" alt=\"sitrep2\">\n<img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601548298-Sitrep-emails.png\" alt=\"Sitrep-emails\"></p>\n<h3 class=\"anchored\">\n  <a name=\"assess-problem\" href=\"#assess-problem\">Assess problem</a>\n</h3>\n\n<p>The next step is to assess the problem in more detail. The goals here are to gain better information for the public status communication (e.g. what users are affected and how, what they can do to work around the problem) and more detail that will help engineers fix the problem (e.g. what internal components are affected, the underlying technical cause). The IC collects this information and reflects it in the sitrep so that everyone involved can see it. It includes the severity, going from SEV0 (critical disruption), to SEV4 (minor feature impacted)</p>\n<h3 class=\"anchored\">\n  <a name=\"mitigate-problem\" href=\"#mitigate-problem\">Mitigate problem</a>\n</h3>\n\n<p>Once the response team has some sense of the problem, it will try to mitigate customer-facing effects if possible. For example, we may put the Platform API in maintenance mode to reduce load on infrastructure systems, or boot additional instances in our fleet to temporarily compensate for capacity issues. A successful mitigation will reduce the impact of the incident on customer apps and actions, or at least prevent the customer-facing issues from getting worse.</p>\n<h3 class=\"anchored\">\n  <a name=\"coordinate-response\" href=\"#coordinate-response\">Coordinate response</a>\n</h3>\n\n<p>In coordinating the response, the IC focuses on bringing in the right people to solve the problem and making sure that they have the information they need. The IC can use a Slack bot to page in additional teams as needed (the page will route to the on-call person for that team), or page teams directly.</p>\n<h3 class=\"anchored\">\n  <a name=\"manage-ongoing-response\" href=\"#manage-ongoing-response\">Manage ongoing response</a>\n</h3>\n\n<p>As the response evolves, the IC acts as an information radiator to keep the team informed about what’s going on. The IC will keep track of who’s active on the response, what problems have been solved and are still open, the current resolution methods being attempted, when we last communicated with customers, and reflect this back to the team regularly with the sitrep mechanism. Finally, the IC is making sure that nothing falls through the cracks: that no problems go unaddressed and that decisions are made in a timely manner.</p>\n<h3 class=\"anchored\">\n  <a name=\"post-incident-cleanup\" href=\"#post-incident-cleanup\">Post-incident cleanup</a>\n</h3>\n\n<p>Once the immediate incident has been resolved, the IC calls for the team to unwind any temporary changes made during the response. For example, alerts may have been silenced and need to be turned back on. The team double-checks that all monitors are green and that all incidents in PagerDuty have been resolved.</p>\n<h3 class=\"anchored\">\n  <a name=\"post-incident-follow-up\" href=\"#post-incident-follow-up\">Post-incident follow-up</a>\n</h3>\n\n<p>Finally, the Production Engineering Department will tee up a post-incident follow up. Depending on the severity of the incident, this could be a quick discussion in the normal weekly operational review or a dedicated internal post-mortem with associated public post-mortem post. The post-mortem process often informs changes that we should make to our infrastructure, testing, and process; these are tracked over time within engineering as incident remediation items.</p>\n<h2 class=\"anchored\">\n  <a name=\"when-everything-goes-south\" href=\"#when-everything-goes-south\">When everything goes south</a>\n</h2>\n\n<p>As Heroku is part of the Salesforce Platform, we leverage Salesforce Incident Response, and Crisis communication center when things gets really bad.</p>\n\n<p>If the severity decided by the IC is SEV1 or worse, Salesforce’s Critical Incident Center (CIC) gets involved. Their role is to assist the Heroku Incident Commander with support around customer communication, and keep the executives informed of the situation. They also can engage the legal teams if needed, mostly for customer communication.</p>\n\n<p>In the case where the incident is believed to be a SEV0 ( major disruption for example ), the Heroku Incident Commander can also request assistance from the Universal Command (UC) Leadership. They will help to assess the issue, and determine if the incident really rises to the level of Sev 0. </p>\n\n<p>Once it is determined to be the case, the UC will spin up a conference call ( called  bridge ) involving executives, in order for them to have a single source of truth to follow-up on the incident’s evolution. One of the goals is that executives don’t first learn failures from outsides sources. This may seem obvious, but amidst the stress of a significant incident when we're solely focused on fixing a problem impacting customers, it's easy to overlook communicating status to those not directly involved with solving the problem. They are also much better suited to answer to customers requests, and keep them informed of the incident response.</p>\n<h2 class=\"anchored\">\n  <a name=\"incident-response-in-other-fields\" href=\"#incident-response-in-other-fields\">Incident Response in Other Fields</a>\n</h2>\n\n<p>The incident response framework described above draws from decades of related work in emergency response: natural disaster response, firefighting, aviation, and other fields that need to manage response to critical incidents. We try to learn from this body of work where possible to avoid inventing our incident response policy from first principles.\nTwo areas of previous work particularly influenced how we approach incident response:</p>\n<h3 class=\"anchored\">\n  <a name=\"incident-command-system\" href=\"#incident-command-system\">Incident Command System</a>\n</h3>\n\n<p>Our framework draws most directly from the <a href=\"http://en.wikipedia.org/wiki/Incident_Command_System\">Incident Command System</a> used to manage natural disaster and other large-scale incident responses. This prior art informs our Incident Commander role and our explicit focus on facilitating incident response in addition to directly addressing the technical issues.</p>\n<h3 class=\"anchored\">\n  <a name=\"crew-resource-management\" href=\"#crew-resource-management\">Crew Resource Management</a>\n</h3>\n\n<p>The ideas of <a href=\"http://en.wikipedia.org/wiki/Crew_resource_management\">Crew Resource Management</a> (a different “CRM”) originated in aviation but have since been successfully applied to other fields such as medicine and firefighting. We draw lessons on communication, leadership, and decision-making from CRM into our incident response thinking.\nWe believe that learning from fields outside of software engineering is a valuable practice, both for operations and other aspects of our business.</p>\n<h2 class=\"anchored\">\n  <a name=\"summary\" href=\"#summary\">Summary</a>\n</h2>\n\n<p>Heroku’s incident response framework helps us quickly resolve issues while keeping customers informed about what’s happening. We hope you’ve found these details about our incident response framework interesting and that they may even inspire changes in how you think about incident response at your own company.\nAt Heroku we’re continuing to learn from our own experiences and the work of others in related fields. Over time this will mean even better incident response for our platform and better experiences for our customers.</p>","PublishedAt":"2020-10-08 12:53:43+00:00","OriginURL":"https://blog.heroku.com/incident-response-at-heroku-2020","SourceName":"Heroku"}},{"node":{"ID":354,"Title":"How I Broke `git push heroku main`","Description":"<p>Incidents are inevitable. Any platform, large or small will have them. While resiliency work will definitely be an important factor in reducing the number of incidents, hoping to remove all of them (and therefore reach 100% uptime) is not an achievable goal.</p>\n\n<p>We should, however, learn as much as we can from incidents, so we can avoid repeating them.</p>\n\n<p>In this post, we will look at one of those incidents, <a href=\"https://status.heroku.com/incidents/2105\">#2105</a>, see how it happened (spoiler: I messed up), and what we’re doing to avoid it from happening again (spoiler: I’m not fired).</p>\n\n<!-- more -->\n<h2 class=\"anchored\">\n  <a name=\"git-push-inception\" href=\"#git-push-inception\">Git push inception</a>\n</h2>\n\n<p>Our Git server is a component written in Go which can listen for HTTP and SSH connections to process a Git command.\nWhile we try to run all our components as Heroku apps on our platform just like Heroku customers, this component is different, as it has several constraints which make it unsuitable for running on the Heroku platform. Indeed, Heroku currently only provides HTTP routing, so it can’t handle incoming SSH connections.</p>\n\n<p>This component is therefore hosted as a “kernel app” using an internal framework which mimics the behavior of Heroku, but runs directly on virtual servers.</p>\n\n<p>Whenever we deploy new code for this component, we will mark instances running the previous version of the code as poisoned. They won’t be able to receive new requests but will have the time they need to finish processing any ongoing requests (every Git push is one request, and those can take up to one hour).\nOnce they don’t have any active requests open, the process will stop and restart using the new code.</p>\n\n<p>When all selected instances have been deployed to, we can move to another batch, and repeat until all instances are running the new code.</p>\n<h2 class=\"anchored\">\n  <a name=\"it-was-such-a-nice-morning\" href=\"#it-was-such-a-nice-morning\">It was such a nice morning</a>\n</h2>\n\n<p>On September 3, I had to deploy a change to switch from calling one internal API endpoint to another. It included a new authentication method between components.</p>\n\n<p>This deploy was unusual because it required setting a new configuration variable, which includes the following manual actions:</p>\n\n<ol>\n<li>Set the new config variable with the framework handling our instances</li>\n<li>Run a command to have the new config variable transmitted to every instance</li>\n<li>Trigger the deploy so the config variables starts being used</li>\n</ol>\n\n<p>So, on that morning, I started deploying our staging instance. I set the new configuration variable on both staging and production.\nThen, I had the config variables transmitted to every instance, but only in staging as I figured I’d avoid touching production right now.\nFinally, I kicked off the staging deployment, and started monitoring that everything went smoothly, which it did.</p>\n\n<p>A few hours later, I went on to production.</p>\n<h2 class=\"anchored\">\n  <a name=\"houston-we-have-a-problem\" href=\"#houston-we-have-a-problem\">Houston, we have a problem</a>\n</h2>\n\n<p>I started my production deployment. Since I had set the configuration variable earlier, I went straight to deploying the new code.</p>\n\n<p>You may see what I did wrong now.</p>\n\n<p>So my code change went to a batch of instances. I didn’t move to another batch though, as I was about to go to lunch. There was no rush to move forward right away, especially since deploying every instance can take several hours.</p>\n\n<p>So I went to lunch, but came back a few minutes later as an alert had gone off.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1601509354-Screenshot%202020-09-09%20at%2010.24.13.png\" alt=\"Screenshot 2020-09-09 at 10\"></p>\n\n<p>The spike you can see on this graph is HTTP 401 responses.</p>\n\n<p>If you read carefully the previous section, you may have noticed that I set the new configuration variable in production, but didn’t apply it to the instances.\nSo my deploy to a batch of servers didn’t have the new configuration variable, meaning we were making unauthenticated calls to a private API, which gave us 401 responses. Hence the 401s being sent back publicly.</p>\n\n<p>Once I realized that, I ran the script to transmit the configuration variables to the instances, killed the impacted processes, which restarted using the updated configuration variables, and the problem was resolved.</p>\n<h2 class=\"anchored\">\n  <a name=\"did-i-mess-up\" href=\"#did-i-mess-up\">Did I mess up?</a>\n</h2>\n\n<p>An untrained eye could say “wow, you messed up bad. Why didn’t you run that command?”, and they would be right. Except they actually wouldn’t.</p>\n\n<p>The problem isn’t that I forgot to run one command. It’s that the system has allowed me to go forward with the deployment when it could have helped me avoid the issue.</p>\n\n<p>Before figuring out any solution, the real fix is to do a truly blameless retrospective. If we had been blaming me for forgetting to run a command instead of focusing on why the system still permitted the deployment, I would probably have felt unsafe reporting this issue, and we would not have been able to improve our systems so that this doesn’t happen again.</p>\n\n<p>Then we can focus on solutions. In this specific case, we are going to merge the two steps of updating configuration variables and deploying code into a single step.\nThat way there isn’t an additional step to remember to run from time to time.</p>\n\n<p>If we didn’t want or were unable to merge the two steps, we could also have added a safeguard in the form of a confirmation warning if we’re trying to deploy the application’s code while configuration variables aren’t synchronized.</p>\n<h2 class=\"anchored\">\n  <a name=\"computers-are-dumb-but-they-don-t-make-mistakes\" href=\"#computers-are-dumb-but-they-don-t-make-mistakes\">Computers are dumb, but they don’t make mistakes</a>\n</h2>\n\n<p>Relying on humans to perform multiple manual actions,  especially when some of them are only required rarely (we don’t change configuration variables often) is a recipe for incidents.</p>\n\n<p>Our job as engineers is to  build systems that avoid those human flaws, so we can do our human job of thinking about new things, and computers can do theirs: performing laborious and repetitive tasks.</p>\n\n<p>This incident shows how a blameless culture benefits everyone in a company (and customers!). Yes, I messed up. But the fix is to improve the process, not to assign blame. We can’t expect folks to be robots who never make mistakes. Instead, we need to build a system that’s safe enough so those mistakes can’t happen.</p>","PublishedAt":"2020-10-01 15:30:00+00:00","OriginURL":"https://blog.heroku.com/how-i-broke-git-push-heroku-main","SourceName":"Heroku"}},{"node":{"ID":429,"Title":"Jackson: More than JSON for Java","Description":"<p>Jackson is a mature and feature-rich open source project that we use, support, and contribute to here at Indeed. As Jackson’s creator and primary maintainer, I want to highlight Jackson’s core competencies, extensions, and challenges in this two-part series. Jackson’s core competency If you’re creating a web service in Java that reads or returns JSON, [&#8230;]</p>\n<p> </p>\n","PublishedAt":"2020-09-23 19:55:48+00:00","OriginURL":"https://engineering.indeedblog.com/blog/2020/09/jackson-more-than-json-for-java/","SourceName":"Indeed"}},{"node":{"ID":625,"Title":"Beyond trivago Tech Pt. 2: Four More Side Projects from Our Developers","Description":"","PublishedAt":"2020-09-22 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-09-22-developersideprojectspt-2/","SourceName":"Trivago"}},{"node":{"ID":800,"Title":"Pagination Updates on Our API","Description":"As part of our efforts to improve our APIs, we’re introducing updates on how we paginate over tracks. This only affects developers and apps…","PublishedAt":"2020-09-21 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/pagination-updates-on-our-api","SourceName":"Soundcloud"}},{"node":{"ID":355,"Title":"The Life-Changing Magic of Tidying Ruby Object Allocations","Description":"<p>Your app is slow. It does not spark joy. This post will use memory allocation profiling tools to discover performance hotspots, even when they're coming from inside a library. We will use this technique with a real-world application to identify a piece of optimizable code in Active Record that ultimately leads to a patch with a substantial impact on page speed.</p>\n\n<p>In addition to the talk, I've gone back and written a full technical recap of each section to revisit it any time you want without going through the video.</p>\n\n<p>I make heavy use of theatrics here, including a Japanese voiceover artist, animoji, and some edited clips of Marie Kondo's Netflix TV show. This recording was done at EuRuKo on a boat. If you've got the time, here's the talk:</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe src=\"https://www.youtube-nocookie.com/embed/Aczy01drwkg?start=287\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n<ol>\n<li><a href=\"#intro-to-tidying-object-allocations\">Intro to Tidying Object Allocations</a></li>\n<li><a href=\"#tidying-example-1-active-record-respond_to-logic\">Tidying Example 1: Active Record respond_to? logic</a></li>\n<li><a href=\"#performance-and-statistical-significance\">Performance and Statistical Significance</a></li>\n<li><a href=\"#tidying-example-2-converting-strings-to-time-takes-time\">Tidying example 2: Converting strings to time takes time</a></li>\n<li>\n<a href=\"tidying-example-3-lightning-fast-cache-keys\">Tidying Example 3: Lightning fast cache keys</a> </li>\n</ol>\n<h2 class=\"anchored\">\n  <a name=\"intro-to-tidying-object-allocations\" href=\"#intro-to-tidying-object-allocations\">Intro to Tidying Object Allocations</a>\n</h2>\n\n<p>The core premise of this talk is that we all want faster applications. Here I'm making the pitch that you can get significant speedups by focusing on your object allocations. To do that, I'll eventually show you a few real-world cases of PRs I made to Rails along with a \"how-to\" that shows how I used profiling and benchmarking to find and fix the hotspots.</p>\n\n<p>At a high level, the \"tidying\" technique looks like this:</p>\n\n<ol>\n<li>Take all your object allocations and put them in a pile where you can see them</li>\n<li>Consider each one: Does it spark joy?</li>\n<li>Keep only the objects that spark joy</li>\n</ol>\n\n<p>An object sparks joy if it is useful, keeps your code clean, and does not cause performance problems. If an object is absolutely necessary, and removing it causes your code to crash, it sparks joy.</p>\n\n<p>To put object allocations in front of us we'll use:</p>\n\n<ul>\n<li><a href=\"https://github.com/SamSaffron/memory_profiler\">memory_profiler</a></li>\n<li><a href=\"https://github.com/schneems/derailed_benchmarks\">derailed_benchmarks</a></li>\n</ul>\n\n<p>To get a sense of the cost of object allocation, we can benchmark two different ways to perform the same logic. One of these allocates an array while the other does not.</p>\n\n<pre><code class=\"lang-ruby\">require 'benchmark/ips'\n\ndef compare_max(a, b)\n  return a if a &gt; b\n  b\nend\n\ndef allocate_max(a, b)\n  array = [a, b] # &lt;===== Array allocation here\n  array.max\nend\n\nBenchmark.ips do |x|\n  x.report(\"allocate_max\") {\n    allocate_max(1, 2)\n  }\n  x.report(\"compare_max \") {\n    compare_max(1, 2)\n  }\n  x.compare!\nend\n</code></pre>\n\n<p>This gives us the results:</p>\n\n<pre><code class=\"lang-term\">Warming up --------------------------------------\n        allocate_max   258.788k i/100ms\n        compare_max    307.196k i/100ms\nCalculating -------------------------------------\n        allocate_max      6.665M (±14.6%) i/s -     32.090M in   5.033786s\n        compare_max      13.597M (± 6.0%) i/s -     67.890M in   5.011819s\n\nComparison:\n        compare_max : 13596747.2 i/s\n        allocate_max:  6664605.5 i/s - 2.04x  slower\n</code></pre>\n\n<p>In this example, allocating an array is 2x slower than making a direct comparison. It's a truism in most languages that allocating memory or creating objects is slow. In the <code>C</code> programming language, it's a truism that \"malloc is slow.\"</p>\n\n<p>Since we know that allocating in Ruby is slow, we can make our programs faster by removing allocations. As a simplifying assumption, I've found that a decrease in bytes allocated roughly corresponds to performance improvement. For example, if I can reduce the number of bytes allocated by 1% in a request, then on average, the request will have been sped up by about 1%. This assumption helps us benchmark faster as it's much easier to measure memory allocated than it is to repeatedly run hundreds or thousands of timing benchmarks.</p>\n<h2 class=\"anchored\">\n  <a name=\"tidying-example-1-active-record-code-respond_to-code-logic\" href=\"#tidying-example-1-active-record-code-respond_to-code-logic\">Tidying Example 1: Active Record <code>respond_to?</code> logic</a>\n</h2>\n\n<p>Using the target application <a href=\"https://www.codetriage.com\">CodeTriage.com</a> and derailed benchmarks, we get a \"pile\" of memory allocations:</p>\n\n<pre><code class=\"lang-term\">$ bundle exec derailed exec perf:objects\n\nallocated memory by gem\n-----------------------------------\n    227058  activesupport/lib\n    134366  codetriage/app\n    # ...\n\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11109  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n      8160  …/code/rails/actionview/lib/action_view/renderer/partial_renderer.rb\n      8000  …/code/rails/activerecord/lib/active_record/integration.rb\n      7880  …/code/rails/actionview/lib/action_view/log_subscriber.rb\n      7478  …/code/rails/actionview/lib/action_view/helpers/tag_helper.rb\n      7096  …/code/rails/actionview/lib/action_view/renderer/partial_renderer/collection_caching.rb\n      # ...\n</code></pre>\n\n<p>The <a href=\"https://gist.github.com/schneems/5ed597c85a0a49659413456652a1befc\">full output is massive</a>, so I've truncated it here.</p>\n\n<p>Once you've got your memory in a pile. I like to look at the \"allocated memory\" by file. I start at the top and look at each in turn. In this case, we'll look at this file:</p>\n\n<pre><code class=\"lang-term\">      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n</code></pre>\n\n<p>Once you have a file you want to look at, you can focus on it in derailed like this:</p>\n\n<pre><code class=\"lang-term\">$ ALLOW_FILES=active_record/attribute_methods.rb \\\n  bundle exec derailed exec perf:objects\n\nallocated memory by file\n-----------------------------------\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n\nallocated memory by location\n-----------------------------------\n      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n       320  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:221\n        80  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:189\n        40  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:187\n</code></pre>\n\n<p>Now we can see exactly where the memory is being allocated in this file. Starting at the top of the locations, I'll work my way down to understand how memory is allocated and used. Looking first at this line:</p>\n\n<pre><code class=\"lang-term\">      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n</code></pre>\n\n<p>We can open this in an editor and navigate to that location:</p>\n\n<pre><code class=\"lang-term\">$ bundle open activerecord\n</code></pre>\n\n<p>In that file, here's the line allocating the most memory:</p>\n\n<pre><code class=\"lang-ruby\">def respond_to?(name, include_private = false)\n  return false unless super\n\n  case name\n  when :to_partial_path\n    name = \"to_partial_path\"\n  when :to_model\n    name = \"to_model\"\n  else\n    name = name.to_s # &lt;=== Line 270 here\n  end\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes) &amp;&amp; self.class.column_names.include?(name)\n    return has_attribute?(name)\n  end\n\n  true\nend\n</code></pre>\n\n<p>Here we can see on line 270 that it's allocating a string. But why? To answer that question, we need more context. We need to understand how this code is used. When we call <code>respond_to</code> on an object, we want to know if a method by that name exists. Because Active Record is backed by a database, it needs to see if a column exists with that name.</p>\n\n<p>Typically when you call <code>respond_to</code> you pass in a symbol, for example, <code>user.respond_to?(:email)</code>. But in Active Record, columns are stored as strings. On line 270, we're ensuring that the <code>name</code> value is always a string.</p>\n\n<p>This is the code where name is used:</p>\n\n<pre><code class=\"lang-ruby\">  if defined?(@attributes) &amp;&amp; self.class.column_names.include?(name)\n</code></pre>\n\n<p>Here <code>column_names</code> returns an array of column names, and the <code>include?</code> method will iterate over each until it finds the column with that name, or its nothing (<code>nil</code>).</p>\n\n<p>To determine if we can get rid of this allocation, we have to figure out if there's a way to replace it without allocating memory. We need to refactor this code while maintaining correctness. I decided to add a method that converted the array of column names into a hash with symbol keys and string values:</p>\n\n<pre><code class=\"lang-ruby\"># lib/activerecord/model_schema.rb\ndef symbol_column_to_string(name_symbol) # :nodoc:\n  @symbol_column_to_string_name_hash ||= column_names.index_by(&amp;:to_sym)\n  @symbol_column_to_string_name_hash[name_symbol]\nend\n</code></pre>\n\n<p>This is how you would use it:</p>\n\n<pre><code class=\"lang-ruby\">User.symbol_column_to_string(:email) #=&gt; \"email\"\nUser.symbol_column_to_string(:foo)   #=&gt; nil\n</code></pre>\n\n<p>Since the value that is being returned every time by this method is from the same hash, we can re-use the same string and not have to allocate. The refactored <code>respond_to</code> code ends up looking like this:</p>\n\n<pre><code class=\"lang-ruby\">def respond_to?(name, include_private = false)\n  return false unless super\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes)\n    if name = self.class.symbol_column_to_string(name.to_sym)\n      return has_attribute?(name)\n    end\n  end\n\n  true\nend\n</code></pre>\n\n<p>Running our benchmarks, this patch yielded a reduction in memory of 1%. Using code that eventually became <code>derailed exec perf:library</code>, I verified that the patch made end-to-end request/response page speed on CodeTriage 1% faster.</p>\n<h2 class=\"anchored\">\n  <a name=\"performance-and-statistical-significance\" href=\"#performance-and-statistical-significance\">Performance and Statistical Significance</a>\n</h2>\n\n<p>When talking about benchmarks, it's important to talk about statistics and their impact. I talk a bit about this in <a href=\"https://schneems.com/2020/03/17/lies-damned-lies-and-averages-perc50-perc95-ex%0Aplained-for-programmers/\">Lies, Damned Lies, and Averages: Perc50, Perc95 explained for Programmers</a>. Essentially any time you measure a value, there's a chance that it could result from randomness. If you run a benchmark 3 times, it will give you 3 different results. If it shows that it was faster twice and slower once, how can you be certain that the results are because of the change and not random chance?</p>\n\n<p>That's precisely the question that \"statistical significance\" tries to answer. While we can never know, we can make an informed decision. How? Well, if you took a measurement of the same code many times, you would know any variation was the result of randomness. This would give you a distribution of randomness. Then you could use this distribution to understand how likely it is that your change was caused by randomness.</p>\n\n<p>In the talk, I go into detail on the origins of \"Student's T-Test.\" In derailed, I've switched to using Kolmogorov-Smirnov instead. When I ran benchmarks on CodeTriage, I wanted to be sure that my results were valid, so I ran them multiple times and ran Kolmogorov Smirnov on them. This gives me a confidence interval. If my results are in that interval, then I can say with 95% certainty that my results are not the result of random chance i.e., that they're valid and are statistically significant.</p>\n\n<p>If it's not significant, it could mean that the change is too small to detect, that you need more samples, or that there is no difference.</p>\n\n<p>In addition to running a significance check on your change, it's useful to see the distribution. Derailed benchmarks does this for you by default now. Here is a result from <code>derailed exec perf:library</code> used to compare the performance difference of two different commits in a library dependency:</p>\n\n<pre><code class=\"lang-term\">                  Histogram - [winner] \"I am the new commit.\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 12\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 22\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 30\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇ 6\n            [11.65, 11.73) ┤ 0\n            [11.73, 11.81) ┤ 0\n            [11.8 , 11.88) ┤ 0\n                           └                                        ┘\n                                      # of runs in range\n\n\n\n                  Histogram - [loser] \"Old commit\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇ 3\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 25\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 15\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.65, 11.73) ┤▇▇▇▇ 3\n            [11.73, 11.81) ┤▇▇▇▇ 3\n            [11.8 , 11.88) ┤▇▇▇ 2\n                           └                                        ┘\n                                      # of runs in range\n</code></pre>\n\n<p>The TLDR of this whole section is that in addition to showing my change as being faster, I was also able to show that the improvement was statistically significant.</p>\n<h2 class=\"anchored\">\n  <a name=\"tidying-example-2-converting-strings-to-time-takes-time\" href=\"#tidying-example-2-converting-strings-to-time-takes-time\">Tidying example 2: Converting strings to time takes time</a>\n</h2>\n\n<p>One percent faster is good, but it could be better. Let's do it again. First, get a pile of objects:</p>\n\n<pre><code class=\"lang-term\">$ bundle exec derailed exec perf:objects\n\n# ...\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11148  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n</code></pre>\n\n<p>Zoom in on a file:</p>\n\n<pre><code class=\"lang-term\">     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n</code></pre>\n\n<p>Isolate the file:</p>\n\n<pre><code class=\"lang-term\">$ ALLOW_FILE=active_model/type/helpers/time_value.rb \\\n  bundle exec derailed exec perf:objects\n\nTotal allocated: 39617 bytes (600 objects)\nTotal retained:  0 bytes (0 objects)\n\nallocated memory by gem\n-----------------------------------\n     39617  activemodel/lib\n\nallocated memory by file\n-----------------------------------\n     39617  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n\nallocated memory by location\n-----------------------------------\n     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n     12000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:74\n      6000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:73\n      4300  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:64\n</code></pre>\n\n<p>We're going to do the same thing by starting to look at the top location:</p>\n\n<pre><code class=\"lang-term\">     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n</code></pre>\n\n<p>Here's the code:</p>\n\n<pre><code class=\"lang-ruby\">def fast_string_to_time(string)\n if string =~ ISO_DATETIME # &lt;=== line 72 Here\n   microsec = ($7.to_r * 1_000_000).to_i\n   new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n end\nend\n</code></pre>\n\n<p>On line 72, we are matching the input string with a regular expression constant. This allocates a lot of memory because each grouped match of the regular expression allocates a new string. To understand if we can make this faster, we have to understand how it's used.</p>\n\n<p>This method takes in a string, then uses a regex to split it into parts, and then sends those parts to the <code>new_time</code> method.</p>\n\n<p>There's not much going on that can be sped up there, but what's happening on this line:</p>\n\n<pre><code class=\"lang-ruby\">   microsec = ($7.to_r * 1_000_000).to_i\n</code></pre>\n\n<p>Here's the regex:</p>\n\n<pre><code class=\"lang-ruby\">ISO_DATETIME = /\\A(\\d{4})-(\\d\\d)-(\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d)(\\.\\d+)?\\z/\n</code></pre>\n\n<p>When I ran the code and output $7 from the regex match, I found that it would contain a string that starts with a dot and then has numbers, for example:</p>\n\n<pre><code class=\"lang-ruby\">puts $7 # =&gt; \".1234567\"\n</code></pre>\n\n<p>This code wants microseconds as an integer, so it turns it into a \"rational\" and then multiplies it by a million and turns it into an integer.</p>\n\n<pre><code class=\"lang-ruby\">($7.to_r * 1_000_000).to_i # =&gt; 1234567\n</code></pre>\n\n<p>You might notice that it looks like we're basically dropping the period and then turning it into an integer. So why not do that directly?</p>\n\n<p>Here's what it looks like:</p>\n\n<pre><code class=\"lang-ruby\">def fast_string_to_time(string)\n  if string =~ ISO_DATETIME\n    microsec_part = $7\n    if microsec_part &amp;&amp; microsec_part.start_with?(\".\") &amp;&amp; microsec_part.length == 7\n      microsec_part[0] = \"\"         # &lt;=== HERE\n      microsec = microsec_part.to_i # &lt;=== HERE\n    else\n      microsec = (microsec_part.to_r * 1_000_000).to_i\n    end\n    new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n  end\n</code></pre>\n\n<p>We've got to guard this case by checking for the conditions of our optimization. Now the question is: is this faster?</p>\n\n<p>Here's a microbenchmark:</p>\n\n<pre><code class=\"lang-ruby\">original_string = \".443959\"\n\nrequire 'benchmark/ips'\n\nBenchmark.ips do |x|\n  x.report(\"multiply\") {\n    string = original_string.dup\n    (string.to_r * 1_000_000).to_i\n  }\n  x.report(\"new     \") {\n    string = original_string.dup\n    if string &amp;&amp; string.start_with?(\".\".freeze) &amp;&amp; string.length == 7\n      string[0] = ''.freeze\n      string.to_i\n    end\n  }\n  x.compare!\nend\n\n# Warming up --------------------------------------\n#             multiply   125.783k i/100ms\n#             new        146.543k i/100ms\n# Calculating -------------------------------------\n#             multiply      1.751M (± 3.3%) i/s -      8.805M in   5.033779s\n#             new           2.225M (± 2.1%) i/s -     11.137M in   5.007110s\n\n# Comparison:\n#             new     :  2225289.7 i/s\n#             multiply:  1751254.2 i/s - 1.27x  slower\n</code></pre>\n\n<p>The original code is 1.27x slower. YAY!</p>\n<h3 class=\"anchored\">\n  <a name=\"tidying-example-3-lightning-fast-cache-keys\" href=\"#tidying-example-3-lightning-fast-cache-keys\">Tidying Example 3: Lightning fast cache keys</a>\n</h3>\n\n<p>The last speedup is kind of underwhelming, so you might wonder why I added it. If you remember our first example of optimizing <code>respond_to</code>, it helped to understand the broader context of how it's used. Since this is such an expensive object allocation location, is there an opportunity to call it less or not call it at all?</p>\n\n<p>To find out, I added a <code>puts caller</code> in the code and re-ran it. Here's part of a backtrace:</p>\n\n<pre><code class=\"lang-term\">====================================================================================================\n…/code/rails/activemodel/lib/active_model/type/date_time.rb:25:in `cast_value'\n…/code/rails/activerecord/lib/active_record/connection_adapters/postgresql/oid/date_time.rb:16:in `cast_value'\n…/code/rails/activemodel/lib/active_model/type/value.rb:38:in `cast'\n…/code/rails/activemodel/lib/active_model/type/helpers/accepts_multiparameter_time.rb:12:in `block in initialize'\n…/code/rails/activemodel/lib/active_model/type/value.rb:24:in `deserialize'\n…/.rubies/ruby-2.5.3/lib/ruby/2.5.0/delegate.rb:349:in `block in delegating_block'\n…/code/rails/activerecord/lib/active_record/attribute_methods/time_zone_conversion.rb:8:in `deserialize'\n…/code/rails/activemodel/lib/active_model/attribute.rb:164:in `type_cast'\n…/code/rails/activemodel/lib/active_model/attribute.rb:42:in `value'\n…/code/rails/activemodel/lib/active_model/attribute_set.rb:48:in `fetch_value'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:77:in `_read_attribute'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:40:in `__temp__57074616475646f51647'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `public_send'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `try'\n…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:639:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `block in expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `collect'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:608:in `normalize_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:565:in `block in read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `each'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:387:in `block in read_multi'\n</code></pre>\n\n<p>I followed it backwards until I hit these two places:</p>\n\n<pre><code class=\"lang-term\">…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n</code></pre>\n\n<p>It looks like this expensive code is being called while generating a cache key.</p>\n\n<pre><code class=\"lang-ruby\">def cache_key(*timestamp_names)\n  if new_record?\n    \"#{model_name.cache_key}/new\"\n  else\n    if cache_version &amp;&amp; timestamp_names.none? # &lt;== line 68 here\n      \"#{model_name.cache_key}/#{id}\"\n    else\n      timestamp = if timestamp_names.any?\n        ActiveSupport::Deprecation.warn(&lt;&lt;-MSG.squish)\n          Specifying a timestamp name for #cache_key has been deprecated in favor of\n          the explicit #cache_version method that can be overwritten.\n        MSG\n\n        max_updated_column_timestamp(timestamp_names)\n      else\n        max_updated_column_timestamp\n      end\n\n      if timestamp\n        timestamp = timestamp.utc.to_s(cache_timestamp_format)\n        \"#{model_name.cache_key}/#{id}-#{timestamp}\"\n      else\n        \"#{model_name.cache_key}/#{id}\"\n      end\n    end\n  end\nend\n</code></pre>\n\n<p>On line 68 in the <code>cache_key</code> code it calls <code>cache_version</code>. Here's the code for <code>cache_version</code>:</p>\n\n<pre><code class=\"lang-ruby\">def cache_version # &lt;== line 99 here\n  if cache_versioning &amp;&amp; timestamp = try(:updated_at)\n    timestamp.utc.to_s(:usec)\n  end\nend\n</code></pre>\n\n<p>Here is our culprit:</p>\n\n<pre><code class=\"lang-ruby\">timestamp = try(:updated_at)\n</code></pre>\n\n<p>What is happening is that some database adapters, such as the one for Postgres, returned their values from the database driver as strings. Then active record will lazily cast them into Ruby objects when they are needed. In this case, our time value method is being called to convert the updated timestamp into a time object so we can use it to generate a cache version string.</p>\n\n<p>Here's the value before it's converted:</p>\n\n<pre><code class=\"lang-ruby\">User.first.updated_at_before_type_cast # =&gt; \"2019-04-24 21:21:09.232249\"\n</code></pre>\n\n<p>And here's the value after it's converted:</p>\n\n<pre><code class=\"lang-ruby\">User.first.updated_at.to_s(:usec)      # =&gt; \"20190424212109232249\"\n</code></pre>\n\n<p>Basically, all the code is doing is trimming out the non-integer characters. Like before, we need a guard that our optimization can be applied:</p>\n\n<pre><code class=\"lang-ruby\"># Detects if the value before type cast\n# can be used to generate a cache_version.\n#\n# The fast cache version only works with a\n# string value directly from the database.\n#\n# We also must check if the timestamp format has been changed\n# or if the timezone is not set to UTC then\n# we cannot apply our transformations correctly.\ndef can_use_fast_cache_version?(timestamp)\n  timestamp.is_a?(String) &amp;&amp;\n    cache_timestamp_format == :usec &amp;&amp;\n    default_timezone == :utc &amp;&amp;\n    !updated_at_came_from_user?\nend\n</code></pre>\n\n<p>Then once we're in that state, we can modify the string directly:</p>\n\n<pre><code class=\"lang-ruby\"># Converts a raw database string to `:usec`\n# format.\n#\n# Example:\n#\n#   timestamp = \"2018-10-15 20:02:15.266505\"\n#   raw_timestamp_to_cache_version(timestamp)\n#   # =&gt; \"20181015200215266505\"\n#\n# PostgreSQL truncates trailing zeros,\n# https://github.com/postgres/postgres/commit/3e1beda2cde3495f41290e1ece5d544525810214\n# to account for this we pad the output with zeros\ndef raw_timestamp_to_cache_version(timestamp)\n  key = timestamp.delete(\"- :.\")\n  if key.length &lt; 20\n    key.ljust(20, \"0\")\n  else\n    key\n  end\nend\n</code></pre>\n\n<p>There's some extra logic due to the Postgres truncation behavior linked above. The resulting code to <code>cache_version</code> becomes:</p>\n\n<pre><code class=\"lang-ruby\">def cache_version\n  return unless cache_versioning\n\n  if has_attribute?(\"updated_at\")\n    timestamp = updated_at_before_type_cast\n    if can_use_fast_cache_version?(timestamp)\n      raw_timestamp_to_cache_version(timestamp)\n    elsif timestamp = updated_at\n      timestamp.utc.to_s(cache_timestamp_format)\n    end\n  end\nend\n</code></pre>\n\n<p>That's the opportunity. What's the impact?</p>\n\n<pre><code class=\"lang-term\">Before: Total allocated: 743842 bytes (6626 objects)\nAfter:  Total allocated: 702955 bytes (6063 objects)\n</code></pre>\n\n<p>The bytes reduced is 5% fewer allocations. Which is pretty good. How does it translate to speed?</p>\n\n<p>It turns out that time conversion is very CPU intensive and changing this code makes the target application up to 1.12x faster. This means that if your app previously required 100 servers to run, it can now run with about 88 servers.</p>\n<h2 class=\"anchored\">\n  <a name=\"wrap-up\" href=\"#wrap-up\">Wrap up</a>\n</h2>\n\n<p>Adding together these optimizations and others brings the overall performance improvement to 1.23x or a net reduction of 19 servers. Basically, it's like buying 4 servers and getting 1 for free.</p>\n\n<p>These examples were picked from my changes to the Rails codebase, but you can use them to optimize your applications. The general framework looks like this:</p>\n\n<ul>\n<li>Get a list of all your memory</li>\n<li>Zoom in on a hotspot</li>\n<li>Figure out what is causing that memory to be allocated inside of your code</li>\n<li>Ask if you can refactor your code to not depend on those allocations</li>\n</ul>\n\n<p>If you want to learn more about memory, here are my recommendations:</p>\n\n<ul>\n<li>\n<a href=\"https://www.schneems.com/2019/11/07/why-does-my-apps-memory-usage-grow-asymptotically-over-time/\">Why does my App's Memory Use Grow Over Time?</a>  - Start here, an excellent high-level overview of what causes a system's memory to grow that will help you develop an understanding of how Ruby allocates and uses memory at the application level.</li>\n<li>\n<a href=\"https://www.railsspeed.com\">Complete Guide to Rails Performance (Book)</a> - This book is by Nate Berkopec and is excellent. I recommend it to someone at least once a week.</li>\n<li>\n<a href=\"https://www.sitepoint.com/ruby-uses-memory/\">How Ruby uses memory</a> - This is a lower level look at precisely what \"retained\" and \"allocated\" memory means. It uses small scripts to demonstrate Ruby memory behavior. It also explains why the \"total max\" memory of our system rarely goes down.</li>\n<li>\n<a href=\"https://www.schneems.com/2015/05/11/how-ruby-uses-memory.html\">How Ruby uses memory (Video)</a> - If you're new to the concepts of object allocation, this might be an excellent place to start (you can skip the first story in the video, the rest are about memory). Memory stuff starts at 13 minutes</li>\n<li>\n<a href=\"https://www.schneems.com/2017/04/12/jumping-off-the-memory-cliff/\">Jumping off the Ruby Memory Cliff</a> - Sometimes you might see a 'cliff' in your memory metrics or a saw-tooth pattern. This article explores why that behavior exists and what it means.</li>\n<li>\n<a href=\"https://devcenter.heroku.com/articles/ruby-memory-use\">Ruby Memory Use (Heroku Devcenter article I maintain)</a> - This article focuses on alleviating the symptoms of high memory use.</li>\n<li>\n<a href=\"https://blog.codeship.com/debugging-a-memory-leak-on-heroku/\">Debugging a memory leak on Heroku</a> - TLDR; It's probably not a leak. Still worth reading to see how you can come to the same conclusions yourself. Content is valid for other environments than Heroku. Lots of examples of using the tool <code>derailed_benchmarks</code> (that I wrote).</li>\n<li>\n<a href=\"https://www.youtube.com/watch?v=CS11WIalmPM&amp;feature=emb_title\">The Life-Changing Magic of Tidying Active Record Allocations (Video)</a> - This talk shows how I used tools to track down and eliminate memory allocations in real life. All of the examples are from patches I submitted to Rails, but the process works the same for finding allocations caused by your application logic.</li>\n</ul>\n\n<p><em>Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a <a href=\"https://www.schneems.com/mailinglist\">subscription to his mailing list</a>.</em></p>","PublishedAt":"2020-09-16 14:58:00+00:00","OriginURL":"https://blog.heroku.com/tidying-ruby-object-allocations","SourceName":"Heroku"}},{"node":{"ID":492,"Title":"What are the different levels of automation testing? : API Testing","Description":"<p>This is Part 4 of our automation series. Read Part 1 to </p>","PublishedAt":"2020-09-14 05:37:41+00:00","OriginURL":"https://blog.paypay.ne.jp/en/different-levels-of-automation/","SourceName":"Paypay"}},{"node":{"ID":626,"Title":"Beyond trivago Tech Pt.1: Side-Projects from Our Developers","Description":"","PublishedAt":"2020-09-08 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-09-08-developersideprojects/","SourceName":"Trivago"}},{"node":{"ID":261,"Title":"Spinnaker @ GIPHY","Description":"Like many companies, GIPHY Engineering has been using Kubernetes for the past several years to help our teams quickly build, compile into containers, and distribute applications to our AWS servers. One of the problems with any Kubernetes distribution is: well, the distribution. There is an amalgamation of tools out there vying for your attention (and, [&#8230;]","PublishedAt":"2020-08-28 16:22:24+00:00","OriginURL":"https://engineering.giphy.com/spinnaker-giphy/","SourceName":"GIPHY"}},{"node":{"ID":720,"Title":"New Webinar features for WebSDK","Description":"","PublishedAt":"2020-08-25 17:16:44+00:00","OriginURL":"https://medium.com/zoom-developer-blog/new-webinar-features-for-websdk-84be8dd3b685?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":627,"Title":"How Working as a Product Owner Helped Me Be a Better Engineer (and vice versa)","Description":"","PublishedAt":"2020-08-19 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-08-19-howworkingasaproductownerhelpedmebeabett/","SourceName":"Trivago"}},{"node":{"ID":262,"Title":"How GIPHY’s Public API Integrates with gRPC Services","Description":"We always work hard at GIPHY to help people find the right GIF at the right time. Adoption of gRPC helps us continue to keep our services fast, stable, and fault-tolerant as we scale to over 10 billion pieces of content a day. When the GIPHY API, which serves content to our third party integrations, [&#8230;]","PublishedAt":"2020-08-13 21:37:16+00:00","OriginURL":"https://engineering.giphy.com/how-giphys-public-api-integrates-with-grpc-services/","SourceName":"GIPHY"}},{"node":{"ID":493,"Title":"CI for Automation Testing Framework","Description":"<p>This is Part 3 of our automation series. Read Part 1 to </p>","PublishedAt":"2020-08-13 01:32:15+00:00","OriginURL":"https://blog.paypay.ne.jp/en/ci-for-automation-testing-framework/","SourceName":"Paypay"}}]}},"pageContext":{"limit":30,"skip":4200,"numPages":158,"currentPage":141}},"staticQueryHashes":["3649515864"]}