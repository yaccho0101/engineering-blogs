{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/142","result":{"data":{"allPost":{"edges":[{"node":{"ID":801,"Title":"Breaking Loose from Third-Party Lock-In with Custom Refactoring Tools","Description":"Code refactoring is an essential part of the job of software developers. As time goes on, technology evolves, product requirements changeâ€¦","PublishedAt":"2020-08-11 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/breaking-loose-from-third-party-lock-in-with-custom-refactoring-tools","SourceName":"Soundcloud"}},{"node":{"ID":494,"Title":"Join us for the First PayPay for Developers Webinar","Description":"<p>Join us for a live webinar, as we present our first integration </p>","PublishedAt":"2020-08-05 03:38:31+00:00","OriginURL":"https://blog.paypay.ne.jp/en/webinar-how-to-integrate-paypay-web-payments/","SourceName":"Paypay"}},{"node":{"ID":356,"Title":"Let's Debug a Node.js Application","Description":"<p>There are always challenges when it comes to debugging applications. Node.js' asynchronous workflows add an extra layer of complexity to this arduous process. Although there have been some updates made to the V8 engine in order to easily access asynchronous stack traces, most of the time, we just get errors on the main thread of our applications, which makes debugging a little bit difficult. As well, when our Node.js applications crash, we usually need to <a href=\"https://www.ibm.com/developerworks/library/wa-ibm-node-enterprise-dump-debug-sdk-nodejs-trs/index.html\">rely on some complicated CLI tooling to analyze the core dumps</a>.</p>\n\n<!-- more -->\n\n<p>In this article, we'll take a look at some easier ways to debug your Node.js applications.</p>\n<h2 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h2>\n\n<p>Of course, no developer toolkit is complete without logging. We tend to place <code>console.log</code> statements all over our code in local development, but this is not a really scalable strategy in production. You would likely need to do some filtering and cleanup, or implement a consistent logging strategy, in order to identify important information from genuine errors.</p>\n\n<p>Instead, to implement a proper log-oriented debugging strategy, use a logging tool like <a href=\"https://github.com/pinojs/pino\">Pino</a> or <a href=\"https://github.com/winstonjs/winston\">Winston</a>. These will allow you to set log levels (<code>INFO</code>, <code>WARN</code>, <code>ERROR</code>), allowing you to print verbose log messages locally and only severe ones for production. You can also stream these logs to aggregators, or other endpoints, like LogStash, Papertrail, or even Slack.</p>\n<h2 class=\"anchored\">\n  <a name=\"working-with-node-inspect-and-chrome-devtools\" href=\"#working-with-node-inspect-and-chrome-devtools\">Working with Node Inspect and Chrome DevTools</a>\n</h2>\n\n<p>Logging can only take us so far in understanding why an application is not working the way we would expect. For sophisticated debugging sessions, we will want to use breakpoints to inspect how our code behaves at the moment it is being executed.</p>\n\n<p>To do this, we can use Node Inspect. Node Inspect is a debugging tool which comes with Node.js. It's actually just an implementation of <a href=\"https://developers.google.com/web/tools/chrome-devtools/\">Chrome DevTools</a> for your program, letting you add breakpoints, control step-by-step execution, view variables, and follow the call stack.</p>\n\n<p>There are a couple of ways to launch Node Inspect, but the easiest is perhaps to just call your Node.js application with the <code>--inspect-brk</code> flag:</p>\n\n<pre><code class=\"bash\">$ node --inspect-brk $your_script_name\n</code></pre>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873126-image1.png\" alt=\"Node inspector\"></p>\n\n<p>After launching your program, head to the <code>chrome://inspect</code> URL in your Chrome browser to get to the Chrome DevTools. With Chrome DevTools, you have all of the capabilities you'd normally expect when debugging JavaScript in the browser. One of the nicer tools is <a href=\"https://developers.google.com/web/tools/chrome-devtools/memory-problems\">the ability to inspect memory</a>. You can <a href=\"https://developers.google.com/web/tools/chrome-devtools/memory-problems/heap-snapshots\">take heap snapshots</a> and profile memory usage to understand how memory is being allocated, and potentially, plug any memory leaks.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-a-supported-ide\" href=\"#using-a-supported-ide\">Using a supported IDE</a>\n</h3>\n\n<p>Rather than launching your program in a certain way, many modern IDEs also support debugging Node applications. In addition to having many of the features found in Chrome DevTools, they bring their own features, such as <a href=\"https://code.visualstudio.com/blogs/2018/07/12/introducing-logpoints-and-auto-attach\">creating logpoints</a> and allowing you to create multiple debugging profiles. Check out <a href=\"https://nodejs.org/en/docs/guides/debugging-getting-started/#inspector-clients\">the Node.js' guide on inspector clients</a> for more information on these IDEs.</p>\n<h3 class=\"anchored\">\n  <a name=\"using-ndb\" href=\"#using-ndb\">Using NDB</a>\n</h3>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873374-image4.png\" alt=\"NDB\"></p>\n\n<p>Another option is to install <a href=\"https://github.com/GoogleChromeLabs/ndb\">ndb</a>, a standalone debugger for Node.js. It makes use of the same DevTools that are available in the browser, just as an isolated, local debugger. It also has some extra features that aren't available in DevTools. It supports edit-in-place, which means you can make changes to your code and have the updated logic supported directly by the debugger platform. This is very useful for doing quick iterations.</p>\n<h2 class=\"anchored\">\n  <a name=\"post-mortem-debugging\" href=\"#post-mortem-debugging\">Post-Mortem Debugging</a>\n</h2>\n\n<p>Suppose your application crashes due to a catastrophic error, like a memory access error. These may be rare, but they do happen, particularly if your app relies on native code.</p>\n\n<p>To investigate these sorts of issues, you can use <a href=\"https://github.com/nodejs/llnode\">llnode</a>. When your program crashes, <code>llnode</code> can be used to inspect JavaScript stack frames and objects by mapping them to objects on the C/C++ side. In order to use it, you first need a core dump of your program. To do this, you will need to use <code>process.abort</code> instead of <code>process.exit</code> to shut down processes in your code. When you use <code>process.abort</code>, the Node process generates a core dump file on exit.</p>\n\n<p>To better understand what <code>llnode</code> can provide, <a href=\"https://asciinema.org/a/29589\">here is a video</a> which demonstrates some of its capabilities.</p>\n<h2 class=\"anchored\">\n  <a name=\"useful-node-modules\" href=\"#useful-node-modules\">Useful Node Modules</a>\n</h2>\n\n<p>Aside from all of the above, there are also a few third-party packages that we can recommend for further debugging.</p>\n<h3 class=\"anchored\">\n  <a name=\"debug\" href=\"#debug\">debug</a>\n</h3>\n\n<p>The first of these is called, simply enough, <a href=\"https://www.npmjs.com/package/debug\">debug</a>. With debug, you can assign a specific namespace to your log messages, based on a function name or an entire module. You can then selectively choose which messages are printed to the console via a specific environment variable.</p>\n\n<p>For example, here's a Node.js server which is logging several messages from the entire application and middleware stack, like <code>sequelize</code>, <code>express:application</code>, and <code>express:router</code>:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873233-image2.png\" alt=\"Debug module full output\"></p>\n\n<p>If we set the <code>DEBUG</code> environment variable to <code>express:router</code> and start the same program, only the messages tagged as <code>express:router</code> are shown:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595873290-image3.png\" alt=\"Debug module filtered output\"></p>\n\n<p>By filtering messages in this way, we can hone in on how a single segment of the application is behaving, without needing to drastically change the logging of the code.</p>\n<h3 class=\"anchored\">\n  <a name=\"trace-and-clarify\" href=\"#trace-and-clarify\">trace and clarify</a>\n</h3>\n\n<p>Two more modules that go together are <a href=\"https://github.com/AndreasMadsen/trace\">trace</a> and <a href=\"https://github.com/AndreasMadsen/clarify\">clarify</a>.</p>\n\n<p><code>trace</code> augments your asynchronous stack traces by providing much more detailed information on the async methods that are being called, a roadmap which Node.js does not provide by default. <code>clarify</code> helps by removing all of the information from stack traces which are specific to Node.js internals. This allows you to concentrate on the function calls that are just specific to your application.</p>\n\n<p>Neither of these modules are recommended for running in production! You should only enable them when debugging issues in your local development environment.</p>\n<h2 class=\"anchored\">\n  <a name=\"find-out-more\" href=\"#find-out-more\">Find out more</a>\n</h2>\n\n<p>If you'd like to follow along with how to use these debugging tools in practice, <a href=\"https://vimeo.com/428003519/f132859d08\">here is a video recording</a> which provides more detail. It includes some live demos of how to narrow in on problems in your code. Or, if you have any other questions, you can find me on Twitter <a href=\"https://twitter.com/julian_duque\">@julian_duque</a>!</p>","PublishedAt":"2020-08-03 16:08:55+00:00","OriginURL":"https://blog.heroku.com/debug-node-applications","SourceName":"Heroku"}},{"node":{"ID":628,"Title":"trivago Tech Check-in: Meet Fabian","Description":"","PublishedAt":"2020-08-03 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-08-03-trivagotechcheckinmeetfabian/","SourceName":"Trivago"}},{"node":{"ID":263,"Title":"Fixing Bugs in FFMPEG GIF Encoding","Description":"Here at GIPHY Engineering, we frequently use FFmpeg to resize and reformat GIFs. We generate around 40 different renditions for each GIF uploaded to our platform, so itâ€™s important we do so as efficiently as we can. While FFmpeg is powerful, it was designed for processing MP4 files, and its support for the GIF format [&#8230;]","PublishedAt":"2020-07-23 18:06:00+00:00","OriginURL":"https://engineering.giphy.com/fixing-bugs-in-ffmpeg-gif-encoding/","SourceName":"GIPHY"}},{"node":{"ID":357,"Title":"Ground Control to Major TOML: Why Buildpacks Use a Most Peculiar Format","Description":"<p>YAML files dominate configuration in the cloud native ecosystem. Theyâ€™re used by Kuberentes, Helm, Tekton, and many other projects to define custom configuration and workflows. But YAML has its oddities, which is why the Cloud Native Buildpacks project chose TOML as its primary configuration format.</p>\n\n<p>TOML is a minimal configuration file format that's easy to read because of its simple semantics. You can learn more about TOML from the <a href=\"https://toml.io/en/\">official documentation</a>, but a simple buildpack TOML file looks like this:</p>\n\n<!-- more -->\n\n<pre><code class=\"lang-toml\">api = \"0.2\"\n\n[buildpack]\nid = \"heroku/maven\"\nversion = \"1.0\"\nname = \"Maven\"\n</code></pre>\n\n<p>Unlike YAML, TOML doesnâ€™t rely on significant whitespace with difficult to read indentation. TOML is designed to be human readable, which is why it favors simple structures. Itâ€™s also easy for machines to read and write; you can even append to a TOML file without reading it first, which makes it a great data interchange format. But data interchange and machine readability arenâ€™t the main driver for using TOML in the Buildpacks project; itâ€™s humans.</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1595374389-major-toml.png\" alt=\"Blog post illustration\"></p>\n<h2 class=\"anchored\">\n  <a name=\"put-your-helmet-on\" href=\"#put-your-helmet-on\">Put Your Helmet On</a>\n</h2>\n\n<p>The first time you use Buildpacks, you probably wonâ€™t need to write a TOML file. Buildpacks are designed to get out of your way, and disappear into the details. Thatâ€™s why thereâ€™s no need for large configuration files like a <a href=\"https://helm.sh/docs/chart_template_guide/values_files/\">Helm values.yaml</a> or a <a href=\"https://kubernetes.io/docs/concepts/configuration/\">Kubernetes pod configuration</a>.</p>\n\n<p>Buildpacks favor convention over configuration, and therefore donâ€™t require complex customizations to tweak the inner workings of its tooling. Instead, Buildpacks detect what to do based on the contents of an application, which means configuration is usually limited to simple properties that are defined by a human.</p>\n\n<p>Buildpacks also favor infrastructure as <em>imperative</em> code (rather than <em>declarative</em>). Buildpacks themselves are functions that run against an application, and are best implemented in higher level languages, which can use libraries and testing.</p>\n\n<p>All of these properties lend to a simple configuration format and schema that doesnâ€™t define complex structures. But that doesnâ€™t mean the decision to use TOML was simple.</p>\n<h2 class=\"anchored\">\n  <a name=\"can-you-hear-me-major-toml\" href=\"#can-you-hear-me-major-toml\">Can You Hear Me, Major TOML?</a>\n</h2>\n\n<p>There are many other formats the Buildpacks project could have used besides YAML or TOML, and the Buildpacks core team considered all of these in the early days of the project.</p>\n\n<p>JSON has simple syntax and semantics that are great for data interchange, but it doesnâ€™t make a great human-readable format; in part because it doesnâ€™t allow for comments. Buildpacks use JSON for machine readable config, like the OCI image metadata. But it shouldnâ€™t be used for anything a human writes. </p>\n\n<p>XML has incredibly powerful properties including schema validation, transformation tools, and rich semantics. Itâ€™s great for markup (like HTML) but it's much too heavy of a format for what Buildpacks require.</p>\n\n<p>In the end, the Buildpacks project was comfortable choosing TOML because there was solid prior art (even though the format is somewhat obscure). In the cloud native ecosystem, the <a href=\"https://containerd.io/\">containerd</a> project uses TOML. Additionally, many language ecosystem tools like <a href=\"http://doc.crates.io/\">Cargo</a> (for Rust) and <a href=\"https://python-poetry.org/\">Poetry</a> (for Python) use TOML to configure application dependencies. </p>\n<h2 class=\"anchored\">\n  <a name=\"commencing-countdown-engines-on\" href=\"#commencing-countdown-engines-on\">Commencing Countdown, Engines On</a>\n</h2>\n\n<p>The main disadvantage of TOML is its ubiquity. Tools that parse and query TOML files (something comparable to <code>jq</code>) arenâ€™t readily available, and the format can still be jarring to new users even though itâ€™s fairly simple.</p>\n\n<p>Every trend has to start somewhere, and the Cloud Native Buildpacks project is happy to be one of the projects stepping through the door.</p>\n\n<p>If you want to learn more or have any questions around Cloud Native Buildpacks, we will be hosting a <a href=\"https://community.hackernoon.com/t/cloud-native-buildpacks-ama-with-terence-lee-and-joe-kutner-of-heroku/52494\">Live AMA at Hackernoon</a> on July 28th at 2pm PDT. See you there!</p>","PublishedAt":"2020-07-22 15:08:00+00:00","OriginURL":"https://blog.heroku.com/why-buildpacks-use-toml","SourceName":"Heroku"}},{"node":{"ID":629,"Title":"Google Cloud Workload-Placement-Guide","Description":"","PublishedAt":"2020-07-17 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-07-17-workloadplacementguidelines/","SourceName":"Trivago"}},{"node":{"ID":802,"Title":"DeveloperBridge: SoundCloudâ€™s Program for Training People from Diverse Backgrounds to Become Engineers","Description":"DeveloperBridge is a year-long, full-time, paid traineeship program where participants learn from and work with engineering teams atâ€¦","PublishedAt":"2020-07-17 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/developerbridge-training-program","SourceName":"Soundcloud"}},{"node":{"ID":358,"Title":"Making Time to Save You Time: How We Sped Up Time-Related Syscalls on Dynos","Description":"<p>I work on Herokuâ€™s Runtime Infrastructure team, which focuses on most of the underlying compute and containerization here at Heroku. Over the years, weâ€™ve tuned our infrastructure in a number of ways to improve performance of customer dynos and harden security.</p>\n\n<p>We recently received a support ticket from a customer inquiring about poor performance in two <a href=\"https://en.wikipedia.org/wiki/System_call\">system calls</a> (more commonly referred to as syscalls) their application was making frequently: <a href=\"https://manpages.ubuntu.com/manpages/bionic/man2/clock_getres.2.html\"><code>clock_gettime(3)</code></a> and <a href=\"https://manpages.ubuntu.com/manpages/bionic/man2/gettimeofday.2.html\"><code>gettimeofday(2)</code></a>.</p>\n\n<p>In this customerâ€™s case, they were using a tool to do transaction tracing to monitor the performance of their application. This tool made many such system calls to measure how long different parts of their application took to execute. Unfortunately, these two system calls were very slow for them. Every request was impacted waiting for the time to return, slowing down the app for their users.</p>\n\n<p>To help diagnose the problem we first examined our existing clocksource configuration. The clocksource determines how the Linux kernel gets the current time. The kernel attempts to choose the \"best\" clocksource from the sources available. In our case, the kernel was defaulting to the <code>xen</code> clocksource, which seems reasonable at a glance since the EC2 infrastructure that powers Herokuâ€™s Common Runtime and Private Spaces products uses the Xen hypervisor under the hood.</p>\n\n<p>Unfortunately, the version of Xen in use does not support a particular optimizationâ€”virtual dynamic shared object (or \"<a href=\"https://lwn.net/Articles/615809/\">vDSO</a>\")â€”for the two system calls in question. In short, vDSO allows certain operations to be performed entirely in userspace rather than having to context switch into kernelspace by mapping some kernel functionality into the current process. Context switching between userspace and kernelspace is a somewhat expensive operationâ€”it takes a lot of CPU time. Most applications wonâ€™t see a large impact from occasional context switches, but when context switches are happening hundreds or thousands of times per web request, they can add up very quickly!</p>\n\n<p>Thankfully, there are often several available clocksources to choose from. The available clocksources depends on a combination of the CPU, the Linux kernel version, and the hardware virtualization software being used. Our research revealed <code>tsc</code> seemed to be the most promising clocksource and would support vDSO. <code>tsc</code> utilizes the <a href=\"https://en.wikipedia.org/wiki/Time_Stamp_Counter\">Time Stamp Counter</a> to determine the System Time.</p>\n\n<p>During our research, we also encountered a few other <a href=\"https://blog.packagecloud.io/eng/2017/03/08/system-calls-are-much-slower-on-ec2/\">blog</a> <a href=\"https://heap.io/blog/engineering/clocksource-aws-ec2-vdso\">posts</a> about TSC. Every source we referenced agreed that non-vDSO accelerated system calls were significantly slower, but there was some disagreement on how safe use of TSC would be. The Wikipedia article linked in the previous paragraph also lists some of these safety concerns. The two primary concerns centered around backwards clock drift that could occur due to: (1) TSC inconsistency that plagued older processors in hyper-threaded or multi-CPU configurations, and (2) when freezing/unfreezing Xen virtual machines. To the first concern, Heroku uses newer Intel CPUs for all dynos that have significantly safer TSC implementations. To the second concern, EC2 instances, which Heroku dynos use, do not utilize freezing/unfreezing today. We decided that <code>tsc</code> would be the best clocksource choice to support vDSO for these system calls without introducing negative side effects.</p>\n\n<p>We were able to confirm using the <code>tsc</code> clocksource enabled vDSO acceleration with the excellent <a href=\"https://github.com/nlynch-mentor/vdsotest\">vdsotest</a> tool (although you can verify your own results using <code>strace</code>). After our internal testing, we deployed the <code>tsc</code> clocksource configuration change to the Heroku Common Runtime and Private Spaces dyno fleet.</p>\n\n<p>While the customer who filed the initial support ticket that led to this change noticed the improvement, the biggest surprise for us was when other customers started inquiring about unexpected performance improvements (which we knew to be a result of this change). Itâ€™s always nice for us when our work to solve a problem for a specific customer has a significant positive impact for all customers.</p>\n\n<p>We're glad to be able to make changes like this that benefit all Heroku users. Detailed diagnostic and tuning work like this may not be worth the time investment for an individual engineering team managing their own infrastructure outside of Heroku. Herokuâ€™s scale allows us to identify unique optimization opportunities and invest time into validating and implementing tweaks like this that make apps on Heroku run faster and more reliably.</p>","PublishedAt":"2020-07-16 16:50:00+00:00","OriginURL":"https://blog.heroku.com/clocksource-tuning","SourceName":"Heroku"}},{"node":{"ID":264,"Title":"Modifying FFMPEG to Support Transparent GIFs","Description":"A sticker (left) is just a GIF (right) with transparent pixels. Here at GIPHY, we differentiate between GIFs and Stickers in our business language, as the two products are served to different searches and customers. However, we still use the GIF format to store stickers &#8211; all they really are is GIFs with transparent pixels. [&#8230;]","PublishedAt":"2020-07-09 21:26:26+00:00","OriginURL":"https://engineering.giphy.com/modifying-ffmpeg-to-support-transparent-gifs/","SourceName":"GIPHY"}},{"node":{"ID":359,"Title":"A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem","Description":"<p>When API requests are made one-after-the-other they'll quickly hit rate limits and when that happens:</p>\n\n<p></p><blockquote class=\"twitter-tweet tw-align-center\">\n<p lang=\"en\" dir=\"ltr\">If you provide an API client that doesn't include rate limiting, you don't really have an API client. You've got an exception generator with a remote timer.</p>â€” Richard Schneeman ðŸ¤  Stay Inside (@schneems) <a href=\"https://twitter.com/schneems/status/1138899094137651200?ref_src=twsrc%5Etfw\">June 12, 2019</a>\n</blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<p>That tweet spawned a discussion that generated a quest to add rate throttling logic to the <a href=\"https://rubygems.org/gems/platform-api\"><code>platform-api</code></a> gem that Heroku maintains for talking to its API in Ruby.</p>\n\n<blockquote>\n<p>If the term \"rate throttling\" is new to you, read <a href=\"https://schneems.com/2020/06/25/rate-limiting-rate-throttling-and-how-they-work-together/\">Rate limiting, rate throttling, and how they work together</a></p>\n</blockquote>\n\n<p>The Heroku API uses <a href=\"https://brandur.org/rate-limiting\">Genetic Cell Rate Algorithm (GCRA) as described by Brandur in this post</a> on the server-side. Heroku's <a href=\"https://devcenter.heroku.com/articles/platform-api-reference#rate-limits\">API docs</a> state:</p>\n\n<blockquote>\n<p>The API limits the number of requests each user can make per hour to protect against abuse and buggy code. Each account has a pool of request tokens that can hold at most 4500 tokens. Each API call removes one token from the pool. Tokens are added to the account pool at a rate of roughly 75 per minute (or 4500 per hour), up to a maximum of 4500. If no tokens remain, further calls will return 429 Too Many Requests until more tokens become available.</p>\n</blockquote>\n\n<p>I needed to write an algorithm that never errored as a result of a 429 response. A \"simple\" solution would be to add a retry to all requests when they see a 429, but that would effectively DDoS the API. I made it a goal for the rate throttling client also to minimize its retry rate. That is, if the client makes 100 requests, and 10 of them are a 429 response that its retry rate is 10%. Since the code needed to be contained entirely in the client library, it needed to be able to function without distributed coordination between multiple clients on multiple machines except for whatever information the Heroku API returned.</p>\n<h2 class=\"anchored\">\n  <a name=\"making-client-throttling-maintainable\" href=\"#making-client-throttling-maintainable\">Making client throttling maintainable</a>\n</h2>\n\n<p>Before we can get into what logic goes into a quality rate throttling algorithm, I want to talk about the process that I used as I think the journey is just as fascinating as the destination.</p>\n\n<p>I initially started wanting to write tests for my rate throttling strategy. I quickly realized that while testing the behavior \"retries a request after a 429 response,\" it is easy to check. I also found that checking for quality \"this rate throttle strategy is better than others\" could not be checked quite as easily. The solution that I came up with was to write a simulator in addition to tests. I would simulate the server's behavior, and then boot up several processes and threads and hit the simulated server with requests to observe the system's behavior.</p>\n\n<p>I initially just output values to the CLI as the simulation ran, but found it challenging to make sense of them all, so I added charting. I found my simulation took too long to run and so I added a mechanism to speed up the simulated time. I used those two outputs to write what I thought was a pretty good rate throttling algorithm. The next task was wiring it up to the <code>platform-api</code> gem.</p>\n\n<p>To help out I paired with <a href=\"https://twitter.com/lolaodelola\">a Heroku Engineer, Lola</a>, we ended up making several PRs to a bunch of related projects, and that's its own story to tell. Finally, the day came where we were ready to get rate throttling into the <code>platform-api</code> gem; all we needed was a review.</p>\n\n<p>Unfortunately, the algorithm I developed from \"watching some charts for a few hours\" didn't make a whole lot of sense, and it was painfully apparent that it wasn't maintainable. While I had developed a good gut feel for what a \"good\" algorithm did and how it behaved, I had no way of solidifying that knowledge into something that others could run with. Imagine someone in the future wants to make a change to the algorithm, and I'm no longer here. The tests I had could prevent them from breaking some expectations, but there was nothing to help them make a better algorithm.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-making-of-an-algorithm\" href=\"#the-making-of-an-algorithm\">The making of an algorithm</a>\n</h2>\n\n<p>At this point, I could explain the approach I had taken to build an algorithm, but I had no way to quantify the \"goodness\" of my algorithm. That's when I decided to throw it all away and start from first principles. Instead of asking \"what would make my algorithm better,\" I asked, \"how would I know a change to my algorithm is better\" and then worked to develop some ways to quantify what \"better\" meant. Here are the goals I ended up coming up with:</p>\n\n<ul>\n<li>Minimize average retry rate: The fewer failed API requests, the better</li>\n<li>Minimize maximum sleep time: Rate throttling involves waiting, and no one wants to wait for too long</li>\n<li>Minimize variance of request count between clients: No one likes working with a greedy co-worker, API clients are no different. No client in the distributed system should be an extended outlier</li>\n<li>Minimize time to clear a large request capacity: As the system changes, clients should respond quickly to changes.</li>\n</ul>\n\n<p>I figured that if I could generate metrics on my rate-throttle algorithm and compare it to simpler algorithms, then I could show why individual decisions were made.</p>\n\n<p>I moved my hacky scripts for my simulation into a separate repo and, rather than relying on watching charts and logs, moved to have my simulation <a href=\"https://github.com/zombocom/rate_throttle_client/blob/master/lib/rate_throttle_client/demo.rb\">produce numbers that could be used to quantify and compare algorithms</a>.</p>\n\n<p>With that work under my belt, I threw away everything I knew about rate-throttling and decided to use science and measurement to guide my way.</p>\n<h2 class=\"anchored\">\n  <a name=\"writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff\" href=\"#writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff\">Writing a better rate-throttling algorithm with science: exponential backoff</a>\n</h2>\n\n<p>Earlier I mentioned that a \"simple\" algorithm would be to retry requests. A step up in complexity and functionality would be to retry requests after an exponential backoff. I coded it up and got some numbers for a simulated 30-minute run (which takes 3 minutes of real-time):</p>\n\n<pre><code>Avg retry rate:      60.08 %\nMax sleep time:      854.89 seconds\nStdev Request Count: 387.82\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.23 seconds\n</code></pre>\n\n<p>Now that we've got baseline numbers, how could we work to minimize any of these values? In my initial exponential backoff model, I multiplied sleep by a factor of 2.0, what would happen if I increased it to 3.0 or decreased it to 1.2?</p>\n\n<p>To find out, I plugged in those values and re-ran my simulations. I found that there was a correlation between retry rate and max sleep value with the backoff factor, but they were inverse. I could lower the retry rate by increasing the factor (to 3.0), but this increased my maximum sleep time. I could reduce the maximum sleep time by decreasing the factor (to 1.2), but it increased my retry rate.</p>\n\n<p>That experiment told me that if I wanted to optimize both retry rate and sleep time, I could not do it via only changing the exponential factor since an improvement in one meant a degradation in the other value.</p>\n\n<p>At this point, we could theoretically do anything, but our metrics judge our success. We could put a cap on the maximum sleep time, for example, we could write code that says \"don't sleep longer than 300 seconds\", but it too would hurt the retry rate. The biggest concern for me in this example is the maximum sleep time, 854 seconds is over 14 minutes which is WAAAYY too long for a single client to be sleeping.</p>\n\n<p>I ended up picking the 1.2 factor to decrease that value at the cost of a worse retry-rate:</p>\n\n<pre><code>Avg retry rate:      80.41 %\nMax sleep time:      46.72 seconds\nStdev Request Count: 147.84\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.33 seconds\n</code></pre>\n\n<p>Forty-six seconds is better than 14 minutes of sleep by a long shot. How could we get the retry rate down?</p>\n<h2 class=\"anchored\">\n  <a name=\"incremental-improvement-exponential-sleep-with-a-gradual-decrease\" href=\"#incremental-improvement-exponential-sleep-with-a-gradual-decrease\">Incremental improvement: exponential sleep with a gradual decrease</a>\n</h2>\n\n<p>In the exponential backoff model, it backs-off once it sees a 429, but as soon as it hits a success response, it doesn't sleep at all. One way to reduce the retry-rate would be to assume that once a request had been rate-throttled, that future requests would need to wait as well. Essentially we would make the sleep value \"sticky\" and sleep before all requests. If we only remembered the sleep value, our rate throttle strategy wouldn't be responsive to any changes in the system, and it would have a poor \"time to clear workload.\" Instead of only remembering the sleep value, we can gradually reduce it after every successful request. This logic is very similar to <a href=\"https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start\">TCP slow start</a>.</p>\n\n<p>How does it play out in the numbers?</p>\n\n<pre><code>Avg retry rate:      40.56 %\nMax sleep time:      139.91 seconds\nStdev Request Count: 867.73\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n115.54 seconds\n</code></pre>\n\n<p>Retry rate did go down by about half. Sleep time went up, but it's still well under the 14-minute mark we saw earlier. But there's a problem with a metric I've not talked about before, the \"stdev request count.\" It's easier to understand if you look at a chart to see what's going on:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232091-ExponentialBackoff.png\" alt=\"Exponential sleep with gradual decrease chart\"></p>\n\n<p>Here you can see one client is sleeping a lot (the red client) while other clients are not sleeping at all and chewing through all the available requests at the bottom. Not all the clients are behaving equitably. This behavior makes it harder to tune the system.</p>\n\n<p>One reason for this inequity is that all clients are decreasing by the same constant value for every successful request. For example, let's say we have a client A that is sleeping for 44 seconds, and client B that is sleeping for 11 seconds and both decrease their sleep value by 1 second after every request. If both clients ran for 45 seconds, it would look like this:</p>\n\n<pre><code>Client A) Sleep 44 (Decrease value: 1)\nClient B) Sleep 11 (Decrease value: 1)\nClient B) Sleep 10 (Decrease value: 1)\nClient B) Sleep  9 (Decrease value: 1)\nClient B) Sleep  8 (Decrease value: 1)\nClient B) Sleep  7 (Decrease value: 1)\nClient A) Sleep 43 (Decrease value: 1)\n</code></pre>\n\n<p>So while client A has decreased by 1 second total, client B has reduced by 4 seconds total, since it is firing 4x as fast (i.e., it's sleep time is 4x lower). So while the decrease rate is equal, it is not equitable. Ideally, we would want all clients to decrease at the same rate.</p>\n<h2 class=\"anchored\">\n  <a name=\"all-clients-created-equal-exponential-increase-proportional-decrease\" href=\"#all-clients-created-equal-exponential-increase-proportional-decrease\">All clients created equal: exponential increase proportional decrease</a>\n</h2>\n\n<p>Since clients cannot communicate with each other in our distributed system, one way to guaranteed proportional decreases is to use the sleep value in the decrease amount:</p>\n\n<pre><code>decrease_value = (sleep_time) / some_value\n</code></pre>\n\n<p>Where <code>some_value</code> is a magic number. In this scenario the same clients A and B running for 45 seconds would look like this with a value of 100:</p>\n\n<pre><code>Client A) Sleep 44\nClient B) Sleep 11\nClient B) Sleep 10.89 (Decrease value: 11.00/100 = 0.1100)\nClient B) Sleep 10.78 (Decrease value: 10.89/100 = 0.1089)\nClient B) Sleep 10.67 (Decrease value: 10.78/100 = 0.1078)\nClient B) Sleep 10.56 (Decrease value: 10.67/100 = 0.1067)\nClient A) Sleep 43.56 (Decrease value: 44.00/100 = 0.4400)\n</code></pre>\n\n<p>Now client A has had a decrease of 0.44, and client B has had a reduction of 0.4334 (11 seconds - 10.56 seconds), which is a lot more equitable than before. Since <code>some_value</code> is tunable, I wanted to use a larger number so that the retry rate would be lower than 40%. I chose 4500 since that's the maximum number of requests in the GCRA bucket for Heroku's API.</p>\n\n<p>Here's what the results looked like:</p>\n\n<pre><code>Avg retry rate:      3.66 %\nMax sleep time:      17.31 seconds\nStdev Request Count: 101.94\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n551.10 seconds\n</code></pre>\n\n<p>The retry rate went WAAAY down, which makes sense since we're decreasing slower than before (the constant decrease value previously was 0.8). Stdev went way down as well. It's about 8x lower. Surprisingly the max sleep time went down as well. I believe this to be a factor of a decrease in the number of required exponential backoff events. Here's what this algorithm looks like:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232161-ExponentialIncreaseProportionalDecrease.png\" alt=\"Exponential increase proportional decrease chart\"></p>\n\n<p>The only problem here is that the \"time to clear workload\" is 5x higher than before. What exactly is being measured here? In this scenario, we're simulating a cyclical workflow where clients are running under high load, then go through a light load, and then back to a high load. The simulation starts all clients with a sleep value, but the server's rate-limit is reset to 4500. The time is how long it takes the client to clear all 4500 requests.</p>\n\n<p>What this metric of 551 seconds is telling me is that this strategy is not very responsive to a change in the system. To illustrate this problem, I ran the same algorithm starting each client at 8 seconds of sleep instead of 1 second to see how long it would take to trigger a rate limit:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594143623-CleanShot%202020-07-07%20at%2010.39.35%402x.png\" alt=\"Exponential increase proportional decrease chart 7-hour torture test\"></p>\n\n<p>The graph shows that it takes about 7 hours to clear all these requests, which is not good. What we need is a way to clear requests faster when there are more requests.</p>\n<h2 class=\"anchored\">\n  <a name=\"the-only-remaining-option-exponential-increase-proportional-remaining-decrease\" href=\"#the-only-remaining-option-exponential-increase-proportional-remaining-decrease\">The only remaining option: exponential increase proportional remaining decrease</a>\n</h2>\n\n<p>When you make a request to the Heroku API, it tells you how many requests you have left remaining in your bucket in a header. Our problem with the \"proportional decrease\" is mostly that when there are lots of requests remaining in the bucket, it takes a long time to clear them (if the prior sleep rate was high, such as in a varying workload). To account for this, we can decrease the sleep value quicker when the remaining bucket is full and slower when the remaining bucket is almost empty. To express that in an expression, it might look like this:</p>\n\n<pre><code>decrease_value = (sleep_time * request_count_remaining) / some_value\n</code></pre>\n\n<p>In my case, I chose <code>some_value</code> to be the maximum number of requests possible in a bucket, which is 4500. You can imagine a scenario where workers were very busy for a period and being rate limited. Then no jobs came in for over an hour - perhaps the workday was over, and the number of requests remaining in the bucket re-filled to 4500. On the next request, this algorithm would reduce the sleep value by itself since 4500/4500 is one:</p>\n\n<pre><code>decrease_value = sleep_time * 4500 / 4500\n</code></pre>\n\n<p>That means it doesn't matter how immense the sleep value is, it will adjust fairly quickly to a change in workload. Good in theory, how does it perform in the simulation?</p>\n\n<pre><code>Avg retry rate:      3.07 %\nMax sleep time:      17.32 seconds\nStdev Request Count: 78.44\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n84.23 seconds\n</code></pre>\n\n<p>This rate throttle strategy performs very well on all metrics. It is the best (or very close) to several metrics. Here's a chart:</p>\n\n<p><img src=\"https://heroku-blog-files.s3.amazonaws.com/posts/1594232264-ExponentialIncreaseProportionalRemainingDecrease.png\" alt=\"Exponential increase proportional remaining decrease chart\"></p>\n\n<p>This strategy is the \"winner\" of my experiments and the algorithm that I  chose to go into the <code>platform-api</code> gem.</p>\n<h2 class=\"anchored\">\n  <a name=\"my-original-solution\" href=\"#my-original-solution\">My original solution</a>\n</h2>\n\n<p>While I originally built this whole elaborate scheme to prove how my solution was optimal, I did something by accident. By following a scientific and measurement-based approach, I accidentally found a simpler solution that performed better than my original answer. Which I'm happier about, it shows that the extra effort was worth it. To \"prove\" what I found by observation and tinkering could be not only quantified by numbers but improved upon is fantastic.</p>\n\n<p>While my original solution had some scripts and charts, this new solution has tests covering the behavior of the simulation and charting code. My initial solution was very brittle. I didn't feel very comfortable coming back and making changes to it; this new solution and the accompanying support code is a joy to work with. My favorite part though is that now if anyone asks me, \"what about trying <x>\" or \"have you considered <y>\" is that I can point them at <a href=\"https://github.com/zombocom/rate_throttle_client\">my rate client throttling library</a>, they have all the tools to implement their idea, test it, and report back with a swift feedback loop.</y></x></p>\n<h2 class=\"anchored\">\n  <a name=\"code-gem-39-platform-api-39-39-gt-3-0-39-code\" href=\"#code-gem-39-platform-api-39-39-gt-3-0-39-code\"><code>gem 'platform-api', '~&gt; 3.0'</code></a>\n</h2>\n\n<p>While I mostly wanted to talk about the process of writing rate-throttling code, this whole thing started from a desire to get client rate-throttling into the <code>platform-api</code> gem. Once I did the work to prove my solution was reasonable, we worked on a rollout strategy. We released a version of the gem in a minor bump with rate-throttling available, but with a \"null\" strategy that would preserve existing behavior. This release strategy allowed us to issue a warning to anyone depending on the original behavior. Then we released a major version with the rate-throttling strategy enabled by default. We did this first with \"pre\" release versions and then actual versions to be extra safe.</p>\n\n<p>So far, the feedback has been overwhelming that no one has noticed. We didn't cause any significant breaks or introduce any severe disfunction to any applications. If you've not already, I invite you to upgrade to 3.0.0+ of the <code>platform-api</code> gem and give it a spin. I would love to hear your feedback.</p>\n\n<p><em>Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a <a href=\"https://www.schneems.com/mailinglist\">subscription to his mailing list</a>.</em></p>","PublishedAt":"2020-07-07 20:30:00+00:00","OriginURL":"https://blog.heroku.com/rate-throttle-api-client","SourceName":"Heroku"}},{"node":{"ID":630,"Title":"Interview for BrowserStack's Breakpoint 2020 Conference","Description":"","PublishedAt":"2020-07-03 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-07-03-browsertack/","SourceName":"Trivago"}},{"node":{"ID":803,"Title":"Changing the Interview Process during Remote Working","Description":"Please also see Part 1: Rethinking the Backend Engineering Interview Take-Home Challenge and Part 2: The Recruiting Perspective and Resultsâ€¦","PublishedAt":"2020-06-30 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/changing-interview-process-during-remote-working","SourceName":"Soundcloud"}},{"node":{"ID":804,"Title":"Technical Interview Reform, Part 2: The Recruiting Perspective and Results","Description":"Please also see Part 1: Rethinking the Backend Engineering Interview Take-Home Challenge Among the engineering groups at SoundCloud, backendâ€¦","PublishedAt":"2020-06-26 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/backend-code-challenge-recruiting-perspective-and-results","SourceName":"Soundcloud"}},{"node":{"ID":805,"Title":"Technical Interview Reform, Part 1: Rethinking the Backend Engineering Interview Take-Home Challenge","Description":"Most SoundCloud backend engineers have good feelings about the old backend engineering take-home challenge. Itâ€™s commonly been characterizedâ€¦","PublishedAt":"2020-06-25 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/rethinking-the-backend-code-challenge","SourceName":"Soundcloud"}},{"node":{"ID":360,"Title":"Building a GraphQL API in JavaScript","Description":"<p>Over the last few years, <a href=\"https://graphql.org/\">GraphQL</a> has emerged as a very popular API specification that focuses on making data fetching easier for clients, whether the clients are a front-end or a third-party.</p>\n\n<p>In a traditional REST-based API approach, the client makes a request, and the server dictates the response:</p>\n\n<pre><code class=\"lang-term\">$ curl https://api.heroku.space/users/1\n\n{\n  \"id\": 1,\n  \"name\": \"Luke\",\n  \"email\": \"luke@heroku.space\",\n  \"addresses\": [\n    {\n    \"street\": \"1234 Rodeo Drive\",\n    \"city\": \"Los Angeles\",\n    \"country\": \"USA\"\n    }\n  ]\n}\n</code></pre>\n\n<p>But, in GraphQL, the client determines precisely the data it wants from the server. For example, the client may want only the user's name and email, and none of the address information:</p>\n\n<pre><code class=\"lang-term\">$ curl -X POST https://api.heroku.space/graphql -d '\nquery {\n  user(id: 1) {\n    name\n    email\n  }\n}\n'\n\n{\n  \"data\":\n    {\n    \"name\": \"Luke\",\n    \"email\": \"luke@heroku.space\"\n    }\n}\n</code></pre>\n\n<p>With this new paradigm, clients can make more efficient queries to a server by trimming down the response to meet their needs. For single-page apps (SPAs) or other front-end heavy client-side applications, this speeds up rendering time by reducing the payload size. However, as with any framework or language, GraphQL has its trade-offs. In this post, we'll take a look at some of the pros and cons of using GraphQL as a query language for APIs, as well as how to get started building an implementation.</p>\n<h2 class=\"anchored\">\n  <a name=\"why-would-you-choose-graphql\" href=\"#why-would-you-choose-graphql\">Why would you choose GraphQL?</a>\n</h2>\n\n<p>As with any technical decision, it's important to understand what advantages GraphQL offers to your project, rather than simply choosing it because it's a buzzword.</p>\n\n<p>Consider a SaaS application that uses an API to connect to a remote database; you'd like to render a user's profile page. You might need to make one API <code>GET</code> call to fetch information about the user, like their name or email. You might then need to make another API call to fetch information about the address, which is stored in a different table. As the application evolves, because of the way it's architected, you might need to continue to make more API calls to different locations. While each of these API calls can be done asynchronously, you must also handle their responses, whether there's an error, a network timeout, or even pausing the page render until all the data is received. As noted above, the payloads from these responses might be more than necessary to render your current pages. And each API call has network latency and the total latencies added up can be substantial. </p>\n\n<p>With GraphQL, instead of making several API calls, like <code>GET /user/:id</code> and <code>GET /user/:id/addresses</code>, you make one API call and submit your query to a single endpoint:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n    addresses {\n    street\n    city\n    country\n    }\n  }\n}\n</code></pre>\n\n<p>GraphQL, then, gives you just one endpoint to query for all the domain logic that you need. If your application grows, and you find yourself adding more data stores to your architectureâ€”PostgreSQL might be a good place to store user information, while Redis might be good for other kindsâ€”a single call to a GraphQL endpoint will resolve all of these disparate locations and respond to a client with the data they requested.</p>\n\n<p>If you're unsure of the needs of your application and how data will be stored in the future, GraphQL can prove useful here, too. To modify a query, you'd only need to add the name of the field you want:</p>\n\n<pre><code class=\"lang-diff\">    addresses {\n      street\n+     apartmentNumber   # new information\n      city\n      country\n    }\n</code></pre>\n\n<p>This vastly simplifies the process of evolving your application over time.</p>\n<h2 class=\"anchored\">\n  <a name=\"defining-a-graphql-schema\" href=\"#defining-a-graphql-schema\">Defining a GraphQL schema</a>\n</h2>\n\n<p>There are GraphQL server implementations in a variety of programming languages, but before you get started, you'll need to identify the objects in your business domain, as with any API. Just as a REST API might use something like <a href=\"https://json-schema.org/\">JSON schema</a>, GraphQL defines its schema using SDL, or <a href=\"https://graphql.org/learn/schema/\">Schema Definition Language</a>, an idempotent way to describe all the objects and fields available by your GraphQL API. The general format for an SDL entry looks like this:</p>\n\n<pre><code class=\"lang-graphql\">type $OBJECT_TYPE {\n  $FIELD_NAME($ARGUMENTS): $FIELD_TYPE\n}\n</code></pre>\n\n<p>Let's build on our earlier example by defining what entries for the user and address might look like:</p>\n\n<pre><code class=\"lang-graphql\">type User {\n  name:     String\n  email:    String\n  addresses:   [Address]\n}\n\ntype Address {\n  street:   String\n  city:     String\n  country:  String\n}\n</code></pre>\n\n<p><code>User</code> defines two <code>String</code> fields called <code>name</code> and <code>email</code>. It also includes a field called <code>addresses</code>, which is an array of <code>Address</code> objects. <code>Address</code> also defines a few fields of its own. (By the way, there's more to a GraphQL <a href=\"https://graphql.org/learn/schema/\">schema</a> than just objects, fields, and scalar types. You can also incorporate interfaces, unions, and arguments, to build more complex models, but we wonâ€™t cover those for this post.)</p>\n\n<p>There's one more type we need to define, which is the entry point to our GraphQL API. You'll remember that earlier, we said a GraphQL query looked like this:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>That <code>query</code> field belongs to a special reserved type called <code>Query</code>. This specifies the main entry point to fetching objects. (Thereâ€™s also a <code>Mutation</code> type for modifying objects.) Here, we define a <code>user</code> field, which returns a <code>User</code> object, so our schema needs to define this too:</p>\n\n<pre><code class=\"lang-graphql\">type Query {\n  user(id: Int!): User\n}\n\ntype User { ... }\ntype Address { ... }\n</code></pre>\n\n<p>Arguments on a field are a comma-separated list, which takes the form of <code>$NAME: $TYPE</code>. The <code>!</code> is GraphQL's way of denoting that the argument is requiredâ€”omitting means it's optional.</p>\n\n<p>Depending on your language of choice, the process of incorporating this schema into your server varies, but in general, consuming this information as a string is enough. Node.js has <a href=\"https://www.npmjs.com/package/graphql\">the <code>graphql</code> package</a> to prepare a GraphQL schema, but we're going to use <a href=\"https://www.npmjs.com/package/graphql-tools\">the <code>graphql-tools</code> package</a> instead, because it provides a few more niceties. Let's import the package and read our type definitions in preparation for future development:</p>\n\n<pre><code class=\"lang-javascript\">const fs = require('fs')\nconst { makeExecutableSchema } = require(\"graphql-tools\");\n\nlet typeDefs = fs.readFileSync(\"schema.graphql\", {\n  encoding: \"utf8\",\n  flag: \"r\",\n});\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-resolvers\" href=\"#setting-up-resolvers\">Setting up resolvers</a>\n</h2>\n\n<p>A schema sets up the ways in which queries can be constructed but establishing a schema to define your data model is just one part of the GraphQL specification. The other portion deals with actually fetching the data. This is done through the use of <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\"><em>resolvers</em></a>. A resolver is a function that returns a field's underlying value.</p>\n\n<p>Let's take a look at how you might implement resolvers in Node.js. The intent is to solidify concepts around how resolvers operate in conjunction with schemas, so we won't go into too much detail around how the data stores are set up. In the \"real world\", we might establish a database connection with something like <a href=\"https://knexjs.org/\">knex</a>. For now, let's just set up some dummy data:</p>\n\n<pre><code class=\"lang-javascript\">const users = {\n  1: {\n    name: \"Luke\",\n    email: \"luke@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Rodeo Drive\",\n          city: \"Los Angeles\",\n          country: \"USA\",\n    },\n    ],\n  },\n  2: {\n    name: \"Jane\",\n    email: \"jane@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Lincoln Place\",\n          city: \"Brooklyn\",\n          country: \"USA\",\n    },\n    ],\n  },\n};\n</code></pre>\n\n<p>GraphQL resolvers in Node.js amount to an Object with the key as the name of the field to be retrieved, and the value being a function that returns the data. Let's start with a barebones example of the initial <code>user</code> lookup by id:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (parent, { id }) {\n      // user lookup logic\n    },\n  },\n}\n</code></pre>\n\n<p>This resolver takes two arguments: an object representing the parent (which in the initial root query is often unused), and a JSON object containing the arguments passed to your field. Not every field will have arguments, but in this case, we will, because we need to retrieve our user by their ID. The rest of the function is straightforward:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  }\n}\n</code></pre>\n\n<p>You'll notice that we didn't explicitly define a resolver for <code>User</code> or <code>Addresses</code>. The <code>graphql-tools</code> package is intelligent enough to automatically map these for us. We can override these if we choose, but with our type definitions and resolvers now defined, we can build our complete schema:</p>\n\n<pre><code class=\"lang-javascript\">const schema = makeExecutableSchema({ typeDefs, resolvers });\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"running-the-server\" href=\"#running-the-server\">Running the server</a>\n</h2>\n\n<p>Finally, let's get this demo running! Since we're using Express, we can use <a href=\"https://www.npmjs.com/package/express-graphql\">the <code>express-graphql</code> package</a> to expose our schema as an endpoint. The package requires two arguments: your schema, and your root value. It takes one optional argument, <code>graphiql</code>, which we'll talk about in a bit.</p>\n\n<p>Set up your Express server on your favorite port with the GraphQL middleware like this:</p>\n\n<pre><code class=\"lang-javascript\">const express = require(\"express\");\nconst express_graphql = require(\"express-graphql\");\n\nconst app = express();\napp.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n  })\n);\napp.listen(5000, () =&gt; console.log(\"Express is now live at localhost:5000\"));\n</code></pre>\n\n<p>Navigate your browser to <code>http://localhost:5000/graphql</code>, and you should see a sort of IDE interface. On the left pane, you can enter any valid GraphQL query you like, and on your right you'll get the results. This is what <code>graphiql: true</code> provides: a convenient way of testing out your queries. You probably wouldn't want to expose this in a production environment, but it makes testing much easier.</p>\n\n<p>Try entering the query we demonstrated above:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>To explore GraphQL's typing capabilities, try passing in a string instead of an integer for the ID argument:</p>\n\n<pre><code class=\"lang-graphql\"># this doesn't work\nquery {\n  user(id: \"1\") {\n    name\n    email\n  }\n}\n</code></pre>\n\n<p>You can even try requesting fields that don't exist:</p>\n\n<pre><code class=\"lang-graphql\"># this doesn't work\nquery {\n  user(id: 1) {\n    name\n    zodiac\n  }\n}\n</code></pre>\n\n<p>With just a few clear lines of code expressed by the schema, a strongly-typed contract between the client and server is established. This protects your services from receiving bogus data and expresses errors clearly to the requester.</p>\n<h2 class=\"anchored\">\n  <a name=\"performance-considerations\" href=\"#performance-considerations\">Performance considerations</a>\n</h2>\n\n<p>For as much as GraphQL takes care of for you, it doesn't solve every problem inherent in building APIs. In particular, caching and authorization are just two areas that require some forethought to prevent performance issues. The GraphQL spec does not provide any guidance for implementing either of these, which means that the responsibility for building them falls onto you.</p>\n<h3 class=\"anchored\">\n  <a name=\"caching\" href=\"#caching\">Caching</a>\n</h3>\n\n<p>REST-based APIs don't need to be overly concerned when it comes to caching, because they can build on <a href=\"https://restfulapi.net/caching/\">existing HTTP header strategies</a> that the rest of the web uses. GraphQL doesn't come with these caching mechanisms, which can place undue processing burden on your servers for repeated requests. Consider the following two queries:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  user(id: 1) {\n    name\n  }\n}\n\nquery {\n  user(id: 1) {\n    email\n  }\n}\n</code></pre>\n\n<p>Without some sort of caching in place, this would result in two database queries to fetch the <code>User</code> with an ID of <code>1</code>, just to retrieve two different columns. In fact, since GraphQL also allows for <a href=\"https://graphql.org/learn/queries/#aliases\">aliases</a>, the following query is valid and also performs two lookups:</p>\n\n<pre><code class=\"lang-graphql\">query {\n  one: user(id: 1) {\n    name\n  }\n  two: user(id: 2) {\n    name\n  }\n}\n</code></pre>\n\n<p>This second example exposes the problem of how to batch queries. In order to be fast and efficient, we want GraphQL to access the same database rows with as few roundtrips as possible.</p>\n\n<p><a href=\"https://github.com/graphql/dataloader\">The <code>dataloader</code> package</a> was designed to handle both of these issues. Given an array of IDs, we will fetch all of those at once from the database; as well, subsequent calls to the same ID will fetch the item from the cache. To build this out using <code>dataloader</code>, we need two things. First, we need a function to load all of the requested objects. In our sample, that looks something like this:</p>\n\n<pre><code class=\"lang-javascript\">const DataLoader = require('dataloader');\nconst batchGetUserById = async (ids) =&gt; {\n   // in real life, this would be a DB call\n  return ids.map(id =&gt; users[id]);\n};\n// userLoader is now our \"batch loading function\"\nconst userLoader = new DataLoader(batchGetUserById);\n</code></pre>\n\n<p>This takes care of the issue with batching. To load the data, and work with the cache, we'll replace our previous data lookup with a call to the <code>load</code> method and pass in our user ID:</p>\n\n<pre><code class=\"lang-javascript\">const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return userLoader.load(id);\n    },\n  },\n}\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"authorization\" href=\"#authorization\">Authorization</a>\n</h3>\n\n<p>Authorization is an entirely different problem with GraphQL. In a nutshell, it's the process of identifying whether a given user has permission to see some data. We can imagine scenarios where an authenticated user can execute queries to get their own address information, but they should not be able to get the addresses of other users.</p>\n\n<p>To handle this, we need to modify our resolver functions. In addition to a field's arguments, a resolver also has access to its parent, as well as a special  <em>context</em> value passed in, which can provide information about the currently authenticated user. Since we know that <code>addresses</code> is a sensitive field, we need to change our code such that a call to users doesn't just return a list of addresses, but actually, calls out to some business logic to validate the request:</p>\n\n<pre><code class=\"lang-javascript\">const getAddresses = function(currUser, user) {\n  if (currUser.id == user.id) {\n    return user.addresses\n  }\n\n  return [];\n}\n\nconst resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  },\n  User: {\n    addresses: function (parentObj, {}, context) {\n          return getAddresses(context.currUser, parentObj);\n    },\n  },\n};\n</code></pre>\n\n<p>Again, we don't need to explicitly define a resolver for each <code>User</code> fieldâ€”only the one which we want to modify.</p>\n\n<p>By default, <code>express-graphql</code> passes the current HTTP <code>request</code> as a value for <code>context</code>, but this can be changed when setting up your server:</p>\n\n<pre><code class=\"lang-javascript\">app.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n    context: {\n      currUser: user // currently authenticated user\n    }\n  })\n);\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"schema-best-practices\" href=\"#schema-best-practices\">Schema best practices</a>\n</h2>\n\n<p>One aspect missing from the GraphQL spec is the lack of guidance on versioning schemas. As applications grow and change over time, so too will their APIs, and it's likely that GraphQL fields and objects will need to be removed or modified. But this downside can also be positive: by designing your GraphQL schema carefully, you can avoid pitfalls apparent in easier to implement (and easier to break) REST endpoints, such as inconsistencies in naming and confusing relationships. Marc-Andre has <a href=\"https://www.apollographql.com/blog/graphql-schema-design-building-evolvable-schemas-1501f3c59ed5\">listed several strategies</a> for building evolvable schemas which we highly recommend reading through.</p>\n\n<p>In addition, you should try to keep as much of <a href=\"https://graphql.org/learn/thinking-in-graphs/#business-logic-layer\">your business logic separate from your resolver logic</a>. Your business logic should be a single source of truth for your entire application. It can be tempting to perform validation checks within a resolver, but as your schema grows, it will become an untenable strategy.</p>\n<h2 class=\"anchored\">\n  <a name=\"when-is-graphql-not-a-good-fit\" href=\"#when-is-graphql-not-a-good-fit\">When is GraphQL not a good fit?</a>\n</h2>\n\n<p>GraphQL doesn't mold precisely to the needs of HTTP communication the same way that REST does. For example, GraphQL specifies only a single status codeâ€”<code>200 OK</code>â€”regardless of the queryâ€™s success. A special <code>errors</code> key is returned in this response for clients to parse and identify what went wrong. Because of this, error handling can be a bit trickier.</p>\n\n<p>As well, GraphQL is just a specification, and it won't automatically solve every problem your application faces. Performance issues won't disappear, database queries won't become faster, and in general, you'll need to rethink everything about your API: authorization, logging, monitoring, caching. Versioning your GraphQL API can also be a challenge, as the official spec currently has no support for handling breaking changes, an inevitable part of building any software. If you're interested in exploring GraphQL, you will need to dedicate some time to learning how to best integrate it with your needs.</p>\n<h2 class=\"anchored\">\n  <a name=\"learning-more\" href=\"#learning-more\">Learning more</a>\n</h2>\n\n<p>The community has rallied around this new paradigm and come up with <a href=\"https://github.com/chentsulin/awesome-graphql\">a list of awesome GraphQL resources</a>, for both frontend and backend engineers. You can also see what queries and types look like by <a href=\"https://graphql.org/swapi-graphql/\">making real requests on the official playground</a>.</p>\n\n<p>We also have a <a href=\"https://www.heroku.com/podcasts/codeish/44-graphqls-benefits-and-costs\">Code[ish] podcast episode</a> dedicated entirely to the benefits and costs of GraphQL.</p>","PublishedAt":"2020-06-24 15:30:00+00:00","OriginURL":"https://blog.heroku.com/building-graphql-api-javascript","SourceName":"Heroku"}},{"node":{"ID":361,"Title":"From Project to Productionized with Python","Description":"<p>We hope that you and your loved ones are staying safe from the COVID-19 pandemic. As a result of its effect on large gatherings, <a href=\"https://pycon.blogspot.com/2020/03/pycon-us-2020-in-pittsburgh.html\">PyCon 2020 was <del>cancelled</del> changed to an online event</a>. Although not being able to gather in person was disheartening for organizers, speakers, and attendees, the Python community shared virtual high-fives and hugs with <a href=\"https://us.pycon.org/2020/online/\">PyCon US 2020 Online.</a> We <a href=\"https://www.youtube.com/watch?v=1923eduj0Gg\">recorded our planned Heroku workshop for the event</a>, on which this blog post is based.</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe title=\"From Project to Productionized on Heroku\" src=\"https://www.youtube-nocookie.com/embed/1923eduj0Gg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n\n<p>Imagine that you've just spent the last two weeks pouring all your energy into an application. It's magnificent, and you're finally ready to share it on the Internet. How do you do it? In this post, we're going to walk through the hands-on process aimed at Python developers deploying their local application to Heroku.</p>\n\n<p>An application running on Heroku works best as <a href=\"https://12factor.net/\">a 12-factor application</a>. This is actually a concept that Heroku championed over 10 years ago. It's the idea that you build an application with robust redeployments in mind. Most of this workshop is actually not specific to Heroku, but rather, about taking a regular Django application and making it meet the 12 factor app methodology, which has become a standard that most cloud deployment providers not only support but recommend.</p>\n<h2 class=\"anchored\">\n  <a name=\"prerequisites\" href=\"#prerequisites\">Prerequisites</a>\n</h2>\n\n<p>Before completing this workshop, we're going to make a few assumptions about you, dear reader. First, this is not going to be a Django tutorial. If you're looking for an introduction to Django, <a href=\"https://www.djangoproject.com/start/\">their documentation has some excellent tutorials to follow</a>. You will also need a little bit of  <a href=\"https://git-scm.com\">Git</a> familiarity, and have it installed on your machine.</p>\n\n<p>In order to complete this workshop, you'll need a few things:</p>\n\n<ol>\n<li>\n<a href=\"https://signup.heroku.com/\">An account on Heroku</a>. This is completely free and doesn't require any payment information.</li>\n<li>\n<a href=\"https://devcenter.heroku.com/articles/heroku-cli#download-and-install\">The Heroku CLI</a>. Once your application is on Heroku, this will make managing it much easier.</li>\n<li>You'll need to <a href=\"https://github.com/heroku-python/PyCon2020\">clone the repository for this workship</a>, and be able to open it in a text editor.</li>\n</ol>\n\n<p>With all that sorted, it's time to begin!</p>\n<h2 class=\"anchored\">\n  <a name=\"look-around-you\" href=\"#look-around-you\">Look around you</a>\n</h2>\n\n<p>With the project cloned and available on your computer, take a moment to explore its structure. We'll be modifying the <code>manage.py</code> and <code>requirements.txt</code> files, as well as <code>settings.py</code> and <code>wsgi.py</code> in the <code>gettingstarted</code> folder.</p>\n<h3 class=\"anchored\">\n  <a name=\"updating-code-gitignore-code\" href=\"#updating-code-gitignore-code\">Updating <code>.gitignore</code></a>\n</h3>\n\n<p>To begin with, we'll be updating the gitignore file. <a href=\"https://git-scm.com/docs/gitignore\">A gitignore file excludes files</a> which you don't want to check into your repository. In order to deploy to Heroku, you don't technically need a gitignore file. You can deploy successfully without one, but it's highly recommended to always have one (and not just for Heroku). A gitignore can be essential for keeping out passwords and credentials keys, large binary files, local configurations, or anything else that you don't want to expose to the public.</p>\n\n<p>Copy the following block of code and paste it into the gitignore file in the root of your project:</p>\n\n<pre><code>/venv\n__pycache__\ndb.sqlite3          # not needed if you're using Postgres locally\ngettingstarted/static/\n</code></pre>\n\n<p>The <code>venv</code> directory contains <a href=\"https://docs.python.org/3/tutorial/venv.html\">a virtual environment</a> with the packages necessary for your local Python version. Similarly, the <code>__pycache__</code> directory contains <a href=\"https://docs.python.org/3/tutorial/modules.html#compiled-python-files\">precompiled modules unique to your system</a>. We don't want to check in our database (<code>db.sqlite3</code>), as we don't want to expose any local data. Last, the static files will be automatically generated for us during the build and deploy process to Heroku, so we'll exclude the <code>gettingstarted/static/</code> directory.</p>\n\n<p>Go ahead and run <code>git status</code> on your terminal to make sure that gitignore is the only file that's been modified. After that, call <code>git add</code>, then <code>git commit -m \"step 1 add git ignore\"</code>.</p>\n<h3 class=\"anchored\">\n  <a name=\"modularize-your-settings\" href=\"#modularize-your-settings\">Modularize your settings</a>\n</h3>\n\n<p>Next up, we want to modularize our Django settings. To do that, add a new folder within <code>gettingstarted</code> called <code>settings</code>. Then, move the <code>settings.py</code> file into that directory. Since this naming scheme is a bit confusing, let's go ahead and rename that file to <code>base.py</code>. We'll call it that because it will serve as the base (or default) configuration that all the other configurations are going to pull from. If something like <code>dev.py</code> or <code>local.py</code> makes more sense to you, feel free to use that instead!</p>\n\n<p>Local projects only have one environment to keep track of: your local machine. But once you want to deploy to different places, it's important to keep track of what settings go where. Nesting our settings files this way makes it easy for us to keep track of where those settings are, as well as take advantage of Heroku's continuous delivery tool pipelines.</p>\n\n<p>By moving and renaming the settings file, our Django application now has two broken references. Let's fix them before we move on.</p>\n\n<p>The first is in the <code>wsgi.py</code> in your <code>gettingstarted</code> folder. Open it up, and on <a href=\"https://github.com/heroku-python/PyCon2020/blob/1a4cf7eabfcc994f60f4b8efeed1f0d9a245e768/gettingstarted/wsgi.py#L12\">line 12</a> you'll see that a default Django settings module is being set to <code>gettingstarted.settings</code>, a file which no longer exists:</p>\n\n<pre><code class=\"lang-python\">os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings\")\n</code></pre>\n\n<p>To fix this, append the name of the file you just created in the settings subfolder. For example, since we called ours <code>base.py</code>, the line should now look like this:</p>\n\n<pre><code class=\"lang-python\">os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings.base\")\n</code></pre>\n\n<p>After saving that, navigate up one directory to <code>manage.py</code>. On <a href=\"https://github.com/heroku-python/PyCon2020/blob/1a4cf7eabfcc994f60f4b8efeed1f0d9a245e768/manage.py#L6\">line 6</a>, you'll see the same default being set for the Django settings module. Once again, append <code>.base</code> to the end of this line, then commit both of them to Git.</p>\n<h2 class=\"anchored\">\n  <a name=\"continuous-delivery-pipelines\" href=\"#continuous-delivery-pipelines\">Continuous delivery pipelines</a>\n</h2>\n\n<p>In an application's deployment lifecycle, there are typically four stages:</p>\n\n<ol>\n<li>You build your app in the development stage on your local machine to make sure it works.</li>\n<li>Next comes the review stage, where you check to see if your changes pass with the full test suite of your code base.</li>\n<li>If that goes well, you merge your changes to staging. This is where you have conditions as close to public as possible, perhaps with some dummy data available, in order to more accurately predict how the change will impact your users.</li>\n<li>Lastly, if all that goes well, you push to production, where the change is now live for your customers.</li>\n</ol>\n\n<p>Continuous delivery (CD) workflows are designed to test your change in conditions progressively closer and closer to production and with more and more detail. Continuous delivery is a powerful workflow that can make all of the difference in your experience as a developer once you've productionized your application. Heroku can save you a lot of time here, as we've already built the tools for you to have a continuous delivery workflow. From your dashboard on Heroku, you canâ€”with the mere click of a button!â€“<a href=\"https://devcenter.heroku.com/articles/pipelines\">set up a pipeline</a>, add applications to staging and production, and deploy them.</p>\n\n<p>If you <a href=\"https://devcenter.heroku.com/articles/github-integration\">connect your GitHub repository</a>, pipelines can also automatically deploy and test new PRs opened on your repo. By providing the tooling and automating these processes, Heroku's continuous delivery workflow is powerful enough to help you keep up with your development cycle.</p>\n<h2 class=\"anchored\">\n  <a name=\"adding-new-middleware-to-code-base-py-code\" href=\"#adding-new-middleware-to-code-base-py-code\">Adding new middleware to <code>base.py</code></a>\n</h2>\n\n<p>Modularizing your Django settings is a great way to take advantage of this continuous delivery workflow by splitting up your settings, whether you're deploying to Heroku or elsewhere, but there's one more change we have to make to <code>base.py</code>.</p>\n\n<p>Django static assets work best when you also use the <a href=\"http://whitenoise.evans.io/en/stable/django.html\">whitenoise</a> package to manage your static assets. It's really easy to add to your project.</p>\n\n<p>In your <code>base.py</code> file, scroll down to about line 43, and you should see an array of package names like this:</p>\n\n<pre><code class=\"lang-python\">MIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    # Whitenoise goes here\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n</code></pre>\n\n<p>This is your list of <a href=\"https://docs.djangoproject.com/en/3.0/topics/http/middleware/\">Django middleware</a>, which are sort of like plugins for your server. Django loads your middleware in the order that it's listed, so you always want your security middleware first, but it's important to add whitenoise as the second step in this base file.</p>\n\n<p>Copy the following line of code and replace the line that says <code>Whitenoise goes here</code> with this:</p>\n\n<pre><code class=\"lang-python\">\"whitenoise.middleware.WhiteNoiseMiddleware\",\n</code></pre>\n\n<p>We've loaded whitenoise as middleware, but to actually <em>use</em> the whitenoise compression, we need to set one more variable. Copy the following code and paste it right at the end of your <code>base.py</code> file:</p>\n\n<pre><code class=\"lang-python\">STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n</code></pre>\n\n<p>With that, we're done with <code>base.py</code>. Congratulations! Save your work and commit it to Git.</p>\n<h2 class=\"anchored\">\n  <a name=\"setting-up-code-heroku-py-code\" href=\"#setting-up-code-heroku-py-code\">Setting up <code>heroku.py</code></a>\n</h2>\n\n<p>Our base settings are complete, but now we need our Heroku-specific settings. Create a new file under <code>gettingstarted/settings</code> called <code>heroku.py</code> and paste the following block of code:</p>\n\n<pre><code class=\"lang-python\">\"\"\"\nProduction Settings for Heroku\n\"\"\"\n\nimport environ\n\n# If using in your own project, update the project namespace below\nfrom gettingstarted.settings.base import *\n\nenv = environ.Env(\n    # set casting, default value\n    DEBUG=(bool, False)\n)\n\n# False if not in os.environ\nDEBUG = env('DEBUG')\n\n# Raises django's ImproperlyConfigured exception if SECRET_KEY not in os.environ\nSECRET_KEY = env('SECRET_KEY')\n\nALLOWED_HOSTS = env.list('ALLOWED_HOSTS')\n\n# Parse database connection url strings like psql://user:pass@127.0.0.1:8458/db\nDATABASES = {\n    # read os.environ['DATABASE_URL'] and raises ImproperlyConfigured exception if not found\n    'default': env.db(),\n}\n</code></pre>\n\n<p>You can see in this file the values that we're listing here are the ones that we're overriding from our base settings, so these are the settings that will be different and unique for Heroku.</p>\n\n<p>To do this, we're using one of my favorite packages, <a href=\"https://github.com/joke2k/django-environ\">Django-environ</a>. This allows us to quickly and easily interface with the operating system environment without knowing much about it. It has built-in type conversions, and in particular it has automatic database parsing. This is all we need in order to parse our Heroku Postgres database URL that we will be given. It's just really convenient.</p>\n<h2 class=\"anchored\">\n  <a name=\"heroku-specific-files\" href=\"#heroku-specific-files\">Heroku-specific files</a>\n</h2>\n\n<p>That's all the work we need to do to get our application into 12 factored shape, but there are three more files we need in order to deploy to Heroku.</p>\n<h3 class=\"anchored\">\n  <a name=\"code-requirements-txt-code\" href=\"#code-requirements-txt-code\"><code>requirements.txt</code></a>\n</h3>\n\n<p>In addition to the packages your project already uses, there are a few more you need to deploy to Heroku. If we take a look at the provided <code>requirements.txt</code> file, you can see these required packages here. We've already talked about Django, Django-environ, and whitenoise, and we've already configured those for use. But the other two are also important and needed for deployment.</p>\n\n<p>The first one is called <a href=\"https://gunicorn.org/\">Gunicorn</a>. This is the recommended WSGI server for Heroku. We'll take a look at configuring this in just a bit. The next one is <a href=\"https://www.psycopg.org/\">psychopg2</a>. This is a Postgres database adapter. You need it in your <code>requirements.txt</code> file to deploy, but you don't need any code changes in order to activate it.</p>\n\n<p>A quick side note: we're keeping our discussion on packages simple for the purpose of this demo, but when you're ready to deploy a real project to Heroku, consider freezing your dependencies. You can do this with the <a href=\"https://pip.pypa.io/en/stable/reference/pip_freeze/\"><code>pip freeze</code></a> command. This will make your build a little bit more predictable by locking your exact dependency versions into your Git repo. If your dependencies aren't locked, you might find yourself deploying one version of Django one day and a new one the next.</p>\n<h3 class=\"anchored\">\n  <a name=\"code-runtime-txt-code\" href=\"#code-runtime-txt-code\"><code>runtime.txt</code></a>\n</h3>\n\n<p>Heroku will install a default Python version if you don't specify one, but if you want to pick your Python version, you'll need a <code>runtime.txt</code> file. Create one in the root directory, next to your <code>requirements.txt</code>, <code>manage.py</code>, <code>.gitignore</code> and the rest. Specify your Python version with the prefix <code>python-</code>, followed by the major, minor, and patch version that you want your application to run on:</p>\n\n<pre><code class=\"lang-txt\">python-3.8.2\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"code-procfile-code\" href=\"#code-procfile-code\"><code>Procfile</code></a>\n</h3>\n\n<p>The last file we need to add is a file specific to Heroku: <a href=\"https://devcenter.heroku.com/articles/procfile\">the <code>Procfile</code></a>. This is what we use to specify the processes our application should run. The processes specified in this file will automatically boot on deploy to Heroku. Create a file named <code>Procfile</code> in the root level directory, right next to your <code>requirements.txt</code> and <code>runtime.txt</code> files. (Make sure to capitalize the P of Procfile otherwise Heroku might not recognize it!) Copy-paste the following lines into it:</p>\n\n<pre><code class=\"lang-txt\">release: python3 manage.py migrate\nweb: gunicorn gettingstarted.wsgi --preload --log-file -\n</code></pre>\n\n<p><a href=\"https://devcenter.heroku.com/articles/release-phase\">The <code>release</code> phase</a> of a Heroku deployment is the best place to run tasks, like migrations or updates. The command we will run during this phase is to simply run the <code>migrate</code> task defined in <code>manage.py</code>.</p>\n\n<p>The other process is the <code>web</code> process, which is very important, if not outright essential, for any web application. This is where we pass our Gunicorn config, the same things we need when running the server locally. We pass it our WSGI file, which is located in the <code>gettingstarted</code> directory, and then we pass a few more flags to add it a bit more configuration. The <code>--preload</code> flag ensures that the app can receive requests just a little bit faster; the <code>--logfile</code> just specifies that the log file should get routed to Heroku.</p>\n<h2 class=\"anchored\">\n  <a name=\"readying-for-deployment\" href=\"#readying-for-deployment\">Readying for deployment</a>\n</h2>\n\n<p>Take a second before moving on and just double check that you've saved and committed all of your changes to Git. Remember, we need those changes in the Git repo in order for them to successfully deploy. After that, let's get ready to make an app!</p>\n<h3 class=\"anchored\">\n  <a name=\"creating-an-app-with-code-heroku-create-code\" href=\"#creating-an-app-with-code-heroku-create-code\">Creating an app with <code>heroku create</code></a>\n</h3>\n\n<p>Since we have the Heroku CLI installed, we can <a href=\"https://devcenter.heroku.com/articles/creating-apps\">call <code>heroku create</code> on the command line to have an app generated</a> for us:</p>\n\n<pre><code class=\"lang-term\">$ heroku create\nCreating app... done, â¬¢ mystic-wind-83\nCreated http://mystic-wind-83.herokuapp.com/ | git@heroku.com:mystic-wind-83.git\n</code></pre>\n\n<p>Your app will be assigned a random nameâ€”in this example, it's <code>mystic-wind-83</code>â€”as well as a publicly accessible URL.</p>\n<h3 class=\"anchored\">\n  <a name=\"setting-environment-variables-on-heroku\" href=\"#setting-environment-variables-on-heroku\">Setting environment variables on Heroku</a>\n</h3>\n\n<p>When we created our <code>heroku.py</code> settings file, we used Django-environ to load environment variables into our settings config. <a href=\"https://devcenter.heroku.com/articles/config-vars\">Those environment variables also need to be present in our Heroku environment</a>, so let's set those now.</p>\n\n<p>The Heroku CLI command we'll be using for this is <code>heroku config:set</code>. This will take in key-value pairs as arguments and set them in your Heroku runtime environment. First, let's configure our allowed hosts. Type the following line, and replace <code>YOUR_UNIQUE_URL</code> with the URL generated by <code>heroku create</code>:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set ALLOWED_HOSTS=&lt;YOUR_UNIQUE_URL&gt;\n</code></pre>\n\n<p>Next, let's set our Django settings module. This is what determines what settings configuration we use on this platform. Instead of using the default of <code>base</code>, we want the Heroku-specific settings:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set DJANGO_SETTINGS_MODULE=gettingstarted.settings.heroku\n</code></pre>\n\n<p>Lastly, we'll need to create a <code>SECRET_KEY</code>. For this demo, it doesn't matter what its value is. You can use a secure hash generator like <code>md5</code>, or a password manager's generator. Just be sure to keep this value secure, don't reuse it, and NEVER check it into source code! You can set it using the same CLI command:</p>\n\n<pre><code class=\"lang-term\">$ heroku config:set SECRET_KEY=&lt;gobbledygook&gt;\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"provisioning-our-database\" href=\"#provisioning-our-database\">Provisioning our database</a>\n</h2>\n\n<p>Locally, Django is configured to use a SQLite database but we're productionizing. We need something a little bit more robust. Let's provision a Postgres database for production.</p>\n\n<p>First, let's check if we have a database already. The <a href=\"https://devcenter.heroku.com/articles/heroku-cli-commands#heroku-addons-all-app-app\"><code>heroku addons</code></a> command will tell us if one exists:</p>\n\n<pre><code class=\"lang-term\">$ heroku addons\nNo add-ons for app mystic-wind-83.\n</code></pre>\n\n<p>No add-ons exist for our app, which makes senseâ€”we just created it! To add a Postgres database, we can use the <code>addons:create</code> command like this:</p>\n\n<pre><code class=\"lang-term\">$ heroku addons:create heroku-postgresql:hobby-dev\n</code></pre>\n\n<p>Heroku offers several tiers of Postgres databases. <code>hobby-dev</code> is the free tier, so you can play around with this without paying a dime.</p>\n<h2 class=\"anchored\">\n  <a name=\"going-live\" href=\"#going-live\">Going live</a>\n</h2>\n\n<p>It is time. Your code is ready, your Heroku app is configured, you are ready to deploy. This is the easy part!</p>\n\n<p>Just type out</p>\n\n<pre><code class=\"lang-term\">$ git push heroku master\n</code></pre>\n\n<p>And we'll take care of the rest! You'll see your build logs scrolling through your terminal. This will show you what we're installing on your behalf and where you are in the build process. You'll also see the <code>release</code> phase as well that we specified earlier.</p>\n<h2 class=\"anchored\">\n  <a name=\"scaling-up\" href=\"#scaling-up\">Scaling up</a>\n</h2>\n\n<p>The last step is to scale up our web process. This creates new dynos, or, in other words, copies of your code on Heroku servers to handle more web traffic. You can do this using the following command:</p>\n\n<pre><code class=\"lang-term\">$ heroku ps:scale web=1\n</code></pre>\n\n<p>To see your app online, enter <code>heroku open</code> on the terminal. This should pop open a web browser with the site you just built.</p>\n<h2 class=\"anchored\">\n  <a name=\"debugging\" href=\"#debugging\">Debugging</a>\n</h2>\n\n<p>If you hit some snags, don't worry, we have some tips that might help:</p>\n\n<ul>\n<li>Are all of your changes saved and checked into Git?</li>\n<li>Are your changes on the <code>master</code> branch or are they on a different branch? Make sure that whatever you're deploying, all of your changes are in that Git branch.</li>\n<li>Did you deploy from the root directory of your project? Did you also call <code>heroku create</code> from the root directory of your project? If not, this could absolutely cause a trip up.</li>\n<li>Did you remove anything from the code in the provided demo that we didn't discuss?</li>\n</ul>\n<h3 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h3>\n\n<p>If you've run through this list and still have issues, take a look at your log files. In addition to your build logsâ€”which will tell you whether your application successfully deployed or notâ€”you have access to all logs produced by Heroku and by your application. You can get to these through a couple of different ways, but the quickest way is just to run the following command:</p>\n\n<pre><code class=\"lang-term\">$ heroku logs --tail\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"remote-console\" href=\"#remote-console\">Remote console</a>\n</h3>\n\n<p>Another tool you have is the <code>heroku run bash</code> command. This provides you with direct access from your terminal to a Heroku dyno with your code deployed to it. If you type <code>ls</code>, you can see that this is your deployed application. It can be useful to check that what is up here matches what is locally on your machine. If not, you might see some issues.</p>\n<h2 class=\"anchored\">\n  <a name=\"wrapping-up\" href=\"#wrapping-up\">Wrapping up</a>\n</h2>\n\n<p>Congratulations on successfully deploying your productionized app onto Heroku!</p>\n\n<p>To help you learn about Heroku, we also have a wealth of technical documentation. Our <a href=\"https://devcenter.heroku.com\">Dev Center</a> is where you'll find most of our technical how-to and supported technologies information. If you're having a technical issue, chances are someone else has asked the same question and it's been answered on our help docs. Use these resources to solve your problems as well as to learn about best practices when deploying to Heroku.</p>","PublishedAt":"2020-06-22 16:00:00+00:00","OriginURL":"https://blog.heroku.com/from-project-to-productionized-python","SourceName":"Heroku"}},{"node":{"ID":806,"Title":"Announcing Twinagle: Twirp and Protobuf for Finagle","Description":"A previous post on this blog ended with the following paragraph: â€œWe might also replace JSON with a more efficient serialization protocolâ€¦","PublishedAt":"2020-06-12 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/announcing-twinagle","SourceName":"Soundcloud"}},{"node":{"ID":631,"Title":"Cross-Cluster Traffic Mirroring with Istio","Description":"","PublishedAt":"2020-06-10 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-06-10-crossclustertrafficmirroringwithistio/","SourceName":"Trivago"}},{"node":{"ID":632,"Title":"ElasticWars Episode IV: A new field","Description":"","PublishedAt":"2020-06-03 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2020-06-03-elasticwarsepisodeivanewfield/","SourceName":"Trivago"}},{"node":{"ID":807,"Title":"Stretch Opportunities for Engineers","Description":"Stretch opportunities are tasks or projects that are a bit beyond your current skill or knowledge level and that allow you to improve andâ€¦","PublishedAt":"2020-05-20 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/stretch-opportunities-for-engineers","SourceName":"Soundcloud"}},{"node":{"ID":721,"Title":"2020 Zoom Developer Survey Results","Description":"","PublishedAt":"2020-05-08 03:01:01+00:00","OriginURL":"https://medium.com/zoom-developer-blog/2020-zoom-developer-survey-results-33bc25c1fff?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":722,"Title":"Introducing: @zoomus/chatbot-cli","Description":"","PublishedAt":"2020-05-07 22:53:57+00:00","OriginURL":"https://medium.com/zoom-developer-blog/introducing-zoomus-chatbot-cli-bf8d1e5bc45b?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":808,"Title":"Open Sourcing Intervene","Description":"A little while back, the web team at SoundCloud got an urgent report that our upload page looked weird in the US. Web engineering is basedâ€¦","PublishedAt":"2020-05-05 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/open-sourcing-intervene","SourceName":"Soundcloud"}},{"node":{"ID":723,"Title":"Required Passwords and your Zoom Integration","Description":"","PublishedAt":"2020-05-04 21:52:20+00:00","OriginURL":"https://medium.com/zoom-developer-blog/required-passwords-and-your-zoom-integration-1c02828c5e68?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":362,"Title":"Evolving Alongside your Tech Stack","Description":"<p><em>This blog post is adapted from a discussion during <a href=\"https://www.heroku.com/podcasts/codeish/39-evolving-alongside-your-tech-stack\">an episode of our podcast, Code[ish]</a>.</em></p>\n\n<p>Over the last twenty years, software development has advanced so rapidly that it's possible to create amazing user experiences, powerful machine learning algorithms, and memory efficient applications with incredible ease. But as the capabilities tech provides has changed, so too have the requirements of individual developers morphed to encompass a variety of skills. Not only should you be writing efficient code; you need to understand how that code communicates with all the other systems involved and make it all work together.</p>\n\n<p>In this post, we'll explore how you can stay on top of the changing software development landscape, without sacrificing your desires to learn or the success of your product.</p>\n<h2 class=\"anchored\">\n  <a name=\"user-experience-depends-on-technical-expertise\" href=\"#user-experience-depends-on-technical-expertise\">User experience depends on technical expertise</a>\n</h2>\n\n<p>When the iPhone first came out in 2007, it was rather limited in technical capabilities. There was no support for multitasking and gestures, no ability to copy and paste text, and there wasn't any support for third-party software. It's not that these ideas were not useful, itâ€™s just that the first generation of the phone's hardware and operating system could not support such features. This serves as a good example to underscore how UX has sometimes been constrained by technology.</p>\n\n<p>Now, the situation has changed somewhat. Tools have advanced to the point where it's really easy to create a desktop or mobile app which accepts a variety of gestures and inputs. The consequences of this are twofold. First, users have come to expect a certain level of quality in software. Gone are the days of simply \"throwing something together\"; software, websites, and mobile apps all need to look polished. This requires developers to have a high level of design sensibility (or work with someone else who does). Second, it means that the role of the engineer has expanded beyond just writing code. They need to understand why they're building whatever it is they're building, why it's important to their users, and how it functionally integrates with the rest of the app. If you design an API, for example, youâ€™ll need to secure it against abuse; if you design a custom search index, you need to make sure users can actually find what theyâ€™re looking for.</p>\n\n<p>On the one hand, because you're running on the same devices and platforms as your users (whether that a smartphone or an operating system), you're intricately familiar with the best UI patternsâ€”how a button should operate, which transitions to make between screensâ€”because every other app has made similar considerations. But on the other hand, you also need to deal with details such as memory management and CPU load to ensure the app is running optimally. </p>\n\n<p>Itâ€™s not enough for an app to work well, as it must also look good. It's important to find a balance of both design sensibilities and technical limitationsâ€”or at least, a baseline knowledge of how everything worksâ€”in order to ship quality software. </p>\n<h2 class=\"anchored\">\n  <a name=\"follow-everything-but-only-learn-em-some-em-things\" href=\"#follow-everything-but-only-learn-em-some-em-things\">Follow everything but only learn <em>some</em> things</a>\n</h2>\n\n<p>When it comes to personal growth, learning to prioritize solutions to the problems you encounter can be critical in your development. For example, suppose you notice one day that your Postgres queries are executing slower than you would like. You should have a general awareness of how higher rates of traffic affects your database querying strategies, or how frequent writes affect the physical tables on disk. But that doesn't necessarily mean that you should sink a massive amount of time and effort to fine-tune these issues towards the most optimal strategy. When developing software, you will always have one of several choices to make, and rarely does one become the only true path forward. Sometimes, having the insight to know the trade-offs and accepting one sub-optimal approach above another makes it easier to cut losses and focus on the parts of your software which matter.</p>\n\n<h2 class=\"pull-quote\" style=\"font-style: italic;\">Sometimes, having the insight to know the trade-offs and accepting one sub-optimal approach above another makes it easier to cut losses and focus on the parts of your software which matter.</h2>\n\n<p>It seems like every year, a new web framework or programming language is released. This makes it difficult, if not impossible, to follow every single new item when they are announced. The inverse is also true. We might feel that adopting new technologies is one way to stay \"relevant,\" but this attitude can be quite dangerous. If you are an early adopter, you run the risk of being on the hook for finding bugs, distracting you from your actual goal of shipping features for your own application. You should take a calculated approach to the pros and cons of any new tech. For example, switching your database entirely to MemSQL because you heard it's \"faster\" is less reasonable than making a switch after reading someone's careful evaluation of the technology, and realizing that it matched your own needs as well.</p>\n<h2 class=\"anchored\">\n  <a name=\"keeping-calm-and-steady\" href=\"#keeping-calm-and-steady\">Keeping calm and steady</a>\n</h2>\n\n<p>At the end of the day, you should be very invested in your own stack and the ecosystem you work in. That work can be something as simple as reading Medium posts or following Twitter accounts. Broaden your knowledge of other services outside your own realm of expertise only if you come across someone confronting problems similar to yours. You should own tools which you know how to operate, rather than keep a shed full of all sorts of shiny objects.</p>","PublishedAt":"2020-04-29 19:59:00+00:00","OriginURL":"https://blog.heroku.com/evolving-alongside-tech-stack","SourceName":"Heroku"}},{"node":{"ID":265,"Title":"How we made FastText faster","Description":"FastText is a library for efficient text classification and representation learning. Like its sibling, Word2Vec, it produces meaningful word embeddings from a given corpus of text. Unlike its sibling, FastText uses n-grams for word representations, making it great for text-classification projects like language detection, sentiment analysis, and topic modeling.&#160; Here at GIPHY, we use FastText [&#8230;]","PublishedAt":"2020-04-24 16:25:28+00:00","OriginURL":"https://engineering.giphy.com/how-we-made-fasttext-faster/","SourceName":"GIPHY"}},{"node":{"ID":363,"Title":"Building and Scaling a Global Chatbot using Heroku + Terraform","Description":"<p>Text-based communication has a long history weaved into the evolution of the Internet, from IRC and XMPP to Slack and Discord. And where there have been humans, there have also been chatbots: scriptable programs that respond to a userâ€™s commands, like messages in a chat room.</p>\n\n<p>Chatbots don't require much in terms of computational power or disk storage, as they rely heavily on APIs to send actions and receive responses. But as with any kind of software, scaling them to support millions of userâ€™s requests across the world requires a fail-safe operational strategy. Salesforce offers <a href=\"https://www.salesforce.com/products/service-cloud/features/live-agent/\">a Live Agent support product</a> with a chatbot integration that reacts to customer inquiries.</p>\n\n<p>In this post, we'll take a look at how the team uses Heroku for their chatbot's multi-regional requirements.</p>\n<h2 class=\"anchored\">\n  <a name=\"how-users-interact-with-the-chatbot\" href=\"#how-users-interact-with-the-chatbot\">How users interact with the chatbot</a>\n</h2>\n\n<p>Live Agent is an embeddable chatbot that can be added to any website or mobile app. Users can engage in a conversation with the chatbot, asking questions and performing actions along the way. For example, if a bank customer wants to learn how to set up two-factor authentication, they could ask the chatbot for guidance, rather than call the bank directly.</p>\n\n<p>The aim of Live Agent is to augment a human support agent's capabilities for responding to events that happen at a high scale. Because everybody learns and interacts a little bit differently, it's advantageous to provide help through various mediums, like videos and documentation. Chatbots offer another channel, with <em>guided</em> feedback that offers more interactive information. Rather than providing a series of webpages with static images, a chatbot can make processes friendlier by confirming to users their progress as they go through a sequence of steps.</p>\n\n<p>Live Agent hooks into <a href=\"https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/apex_intro_what_is_apex.htm\">Apex</a>, a Java-like programming language that is tied directly into Salesforce's object models, allowing it to modify and call up CRM records directly. You can also have a Live Agent chatbot call out to any API and pretty much do anything on the web.</p>\n\n<p>With their open-ended nature, chatbots can perform endless operations across a variety of communication platforms. Facebook Messenger, for example, is <a href=\"https://www.businessinsider.com/most-used-smartphone-apps-2017-8?r=US&amp;IR=T#6-instagram-5\">the third most popular app in the world</a>, and you could have a Live Agent backend running on <a href=\"https://developers.facebook.com/docs/messenger-platform\">the Messenger platform</a> to respond to user queries.</p>\n<h2 class=\"anchored\">\n  <a name=\"running-live-agent-on-heroku\" href=\"#running-live-agent-on-heroku\">Running Live Agent on Heroku</a>\n</h2>\n\n<p>With such a large scope across disparate mediums, there's a significant number of requests coming into Live Agent chatbots and vast amounts of data they can access. It may surprise you to learn that there are only eight engineers responsible for running Live Agent! In addition to coding the features, they own the entire product. This means that they are also responsible for being on-call for pager rotations and ensuring that the chatbots can keep up with incoming traffic.</p>\n\n<p>The small team didn't want to waste time configuring their platform to run on bare metal or on a cloud VM, and they didn't want the administrative overhead of managing databases or other third-party services. Since Salesforce customers reside all over the world, the Live Agent chatbots must also be highly available across multiple regions.</p>\n\n<p>The Live Agent team put its trust into Heroku to take care of all of those operational burdens. Heroku already manages millions of Postgres databases for our customers, and we have a dedicated staff to manage backups, perform updates, and respond to potential outages. The Live Agent chatbot runs on Java, and Heroku's platform supports the entire Java ecosystem, with dedicated Java experts to handle language and framework updates, providing new features and responding to security issues.</p>\n\n<p>In order to serve their customers worldwide, the core Live Agent infrastructure matches <a href=\"https://devcenter.heroku.com/articles/regions\">Heroku's availability in every region around the world</a>. All of their services are managed by Heroku, ensuring that their Heroku Postgres, Redis, and Apache Kafka dependencies are blazing fast no matter where a request comes from.</p>\n\n<p>The beauty of it all is how simple it is to scale, without any of Live Agent's team needing to be responsible for any of the maintenance and upkeep.</p>\n<h2 class=\"anchored\">\n  <a name=\"leveraging-terraform-for-replication-and-private-spaces-for-security\" href=\"#leveraging-terraform-for-replication-and-private-spaces-for-security\">Leveraging Terraform for replication and Private Spaces for security</a>\n</h2>\n\n<p>The Live Agent platform is comprised of ten separate apps, each with their own managed add-ons and services. To fully isolate the boundaries of communication, the collection of apps are deployed into a <a href=\"https://www.heroku.com/private-spaces\">Heroku Private Space</a>. Private Spaces establish an isolated runtime for the apps to ensure that the data contained within the network is inaccessible from any outside service.</p>\n\n<p>Private Spaces are available in a variety of regions; if a new region becomes available, the Live Agent team wanted to be able to automatically redeploy the same apps and add-ons there. And if they ever need to create a new app, they also wanted to add it to all of the Private Spaces in those geographic locations.</p>\n\n<p>To easily replicate their architecture, the Live Agent team uses <a href=\"https://www.terraform.io/intro/index.html\">Terraform</a> to automate deployment and configuration of the Live Agent platform. Terraform is the driver behind everything they do on Heroku. With it, they can explicitly and programmatically define their infrastructure--the apps and add-ons, custom domains, and logging and profiling setup--and have it securely available in any region, instantly. Whenever a new configuration is necessary, they can implement that update with just a few lines of code and make it live everywhere with the merge of a pull request.</p>\n\n<p>For example, to automatically set up a Node.js Heroku app that requires a Postgres database and logging through <a href=\"https://elements.heroku.com/addons/papertrail\">Papertrail</a>, a Terraform config file might just look something like this:</p>\n\n<pre><code class=\"language-hcl\">resource \"heroku_app\" \"server\" {\n  name = \"my-app\"\n  region = \"us\"\n\n\n  provisioner \"local-exec\" {\n    command = \"heroku buildpacks:set heroku/nodejs --app ${heroku_app.server.name}\"\n  }\n}\n\nresource \"heroku_addon\" \"database\" {\n  app  = \"${heroku_app.server.name}\"\n  plan = \"heroku-postgresql:hobby-dev\"\n}\n\n# Papertrail addon (for logging)\n\nresource \"heroku_addon\" \"logging\" {\n  app = \"${heroku_app.server.name}\"\n  plan = \"papertrail:choklad\"\n}\n</code></pre>\n\n<p>Here are some details on how to <a href=\"https://devcenter.heroku.com/articles/using-terraform-with-heroku\">use Terraform with Heroku</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"learning-more\" href=\"#learning-more\">Learning more</a>\n</h2>\n\n<p>If you'd like to learn more about how Live Agent uses Heroku to scale their platform, our podcast Code[ish], has <a href=\"https://www.heroku.com/podcasts/codeish/30-the-infrastructure-behind-salesforces-chatbots\">an interview with their team</a>, where they dive into more of the technical specifics.</p>\n\n<p>We also have not <a href=\"https://dev.to/heroku/eight-devops-things-heroku-does-so-you-don-t-have-to-4o0b\">one</a> but <a href=\"https://dev.to/heroku/seven-more-devops-things-heroku-does-so-you-don-t-have-to-1g1n\"><em>two</em></a> posts on <a href=\"https://dev.to/\">dev.to</a> listing all the DevOps chores which Heroku automatically takes care of for you.</p>","PublishedAt":"2020-04-22 15:33:17+00:00","OriginURL":"https://blog.heroku.com/chatbots-with-heroku-terraform","SourceName":"Heroku"}},{"node":{"ID":724,"Title":"Zoom Web SDK with Angular","Description":"","PublishedAt":"2020-04-21 18:03:48+00:00","OriginURL":"https://medium.com/zoom-developer-blog/zoom-web-sdk-with-angular-6b080a6be38c?source=rss----4a85731adaff---4","SourceName":"Zoom"}},{"node":{"ID":725,"Title":"Developer Platform updates Spring 20'","Description":"","PublishedAt":"2020-04-21 18:02:21+00:00","OriginURL":"https://medium.com/zoom-developer-blog/developer-platform-updates-spring-20-668fa683762d?source=rss----4a85731adaff---4","SourceName":"Zoom"}}]}},"pageContext":{"limit":30,"skip":4230,"numPages":158,"currentPage":142}},"staticQueryHashes":["3649515864"]}