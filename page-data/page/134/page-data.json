{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/134","result":{"data":{"allPost":{"edges":[{"node":{"ID":1233,"Title":"Blog: Securing Admission Controllers","Description":"<p><strong>Author:</strong> Rory McCune (Aqua Security)</p>\n<p><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">Admission control</a> is a key part of Kubernetes security, alongside authentication and authorization. Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organization’s security requirements.</p>\n<p>However, as with any additional component added to a cluster, security risks can present themselves. A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately, the <a href=\"https://github.com/kubernetes/community/tree/master/sig-security#security-docs\">security documentation</a> subgroup of SIG Security has spent some time developing a <a href=\"https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control\">threat model for admission controllers</a>. This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.</p>\n<p>From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.</p>\n<h2 id=\"admission-controllers-and-good-practices-for-security\">Admission controllers and good practices for security</h2>\n<p>From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.</p>\n<h3 id=\"secure-webhook-configuration\">Secure webhook configuration</h3>\n<p>It’s important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers</p>\n<ul>\n<li><strong>Correctly configured TLS for all webhook traffic</strong>. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities</li>\n<li><strong>Only authenticated access allowed</strong>. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.</li>\n<li><strong>Admission controller fails closed</strong>. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the cluster’s threat model. If an admission controller fails closed, when the API server can’t get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the cluster’s operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.</li>\n<li><strong>Regular reviews of webhook configuration</strong>. Configuration mistakes can lead to security issues, so it’s important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.</li>\n</ul>\n<h3 id=\"secure-cluster-configuration-for-admission-control\">Secure cluster configuration for admission control</h3>\n<p>In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, it’s important to ensure that Kubernetes' security features that could impact its operation are well configured.</p>\n<ul>\n<li><strong>Restrict <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\">RBAC</a> rights</strong>. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So it’s important to make sure that only cluster administrators have those rights.</li>\n<li><strong>Prevent privileged workloads</strong>. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster they’re protecting, it’s important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.</li>\n<li><strong>Strictly control external system access</strong>. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, <a href=\"https://kubernetes.io/docs/concepts/services-networking/network-policies/\">network policies</a> should be used to restrict the admission controller services access to external networks.</li>\n<li><strong>Each cluster has a dedicated webhook</strong>. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where it’s shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.</li>\n</ul>\n<h3 id=\"admission-controller-rules\">Admission controller rules</h3>\n<p>A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.</p>\n<ul>\n<li><strong>Regularly test and review rules</strong>. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.</li>\n</ul>","PublishedAt":"2022-01-19 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/","SourceName":"Kubernetes"}},{"node":{"ID":1098,"Title":"Introduction of Web Platform","Description":"<p>Author: @urahiroshi, Engineering manager of Web Platform team The mission of Web Platform team Web Platform team was originally created to maintain network infrastructure and session management service for Mercari JP web app. But we’re changing our mission to &quot;Providing web microservice platform as a service for web products in mercari group&quot;. Let me clarify [&hellip;]</p>\n","PublishedAt":"2022-01-18 15:44:24+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20220118-introduction-of-web-platform/","SourceName":"Mercari"}},{"node":{"ID":238,"Title":"How Data is Helping Organizations to Improve the Employee Lifecycle","Description":"<p>Key uses cases showing how data can help drive societal change</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/how-data-is-helping-organizations-to-improve-the-employee-lifecycle/\">How Data is Helping Organizations to Improve the Employee Lifecycle</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2022-01-18 13:54:39+00:00","OriginURL":"https://blog.cloudera.com/how-data-is-helping-organizations-to-improve-the-employee-lifecycle/","SourceName":"Cloudera"}},{"node":{"ID":1099,"Title":"Blog Series of Introduction of Developer Productivity Engineering at Mercari","Description":"<p>Author: @deeeeeet, Engineering head of Developer Productivity Engineering Developer Productivity Engineering Camp (“Camp” is a unit and a term we use internally at Mercari to logically group the related teams) is a division which is mainly responsible for the entire Mercari group’s base infrastructure and DevOps toolings and services. It consists of multiple teams and [&hellip;]</p>\n","PublishedAt":"2022-01-17 08:58:05+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20220116-blog-series-of-introduction-of-developer-productivity-engineering-at-mercari/","SourceName":"Mercari"}},{"node":{"ID":1210,"Title":"Two New Tutorials for 1.18","Description":"Two new tutorials have been published in preparation for the release of Go 1.18.","PublishedAt":"2022-01-14 00:00:00+00:00","OriginURL":"https://go.dev/blog/tutorials-go1.18","SourceName":"The Go Blog"}},{"node":{"ID":1031,"Title":"A closer look at how LinkedIn integrates fairness into its AI products","Description":"Co-authors: Heloise Logan, Preetam Nandy, Kinjal Basu, and Sakshi Jain At LinkedIn, we work constantly to improve our platform with evolving AI models and systems. Delivering fair and equitable experiences for each of our nearly 800M members is paramount to this work, and we have designed our AI systems in ways that help us provide the right protections, mitigate unintended consequences, and ultimately better serve our members, customers, and society. As part of our ongoing journey to build on our Responsible AI program, we wanted to share more insight into how we think about [&#8230;]","PublishedAt":"2022-01-13 22:11:00+00:00","OriginURL":"https://engineering.linkedin.com/blog/2022/a-closer-look-at-how-linkedin-integrates-fairness-into-its-ai-pr","SourceName":"Linkedin"}},{"node":{"ID":461,"Title":"Debugging with product analytics","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2022/01/20220105_BlogHero_Insights_Debugging-1024x577.png\" class=\"type:primaryImage\" /></figure>\n<p>As a product manager, deep-diving into problems like high churn, bugs, and crashes is part of the job. Over the years, I’ve learned that finding solutions to these kinds of problems is a lot like debugging software code: You always want to start with trying to isolate the issue as much as possible by breaking</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/insights-debugging-with-product-analytics/\">Debugging with product analytics</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2022-01-13 21:41:00+00:00","OriginURL":"https://mixpanel.com/blog/insights-debugging-with-product-analytics/","SourceName":"Mixpanel"}},{"node":{"ID":396,"Title":"Building for Balance","Description":"","PublishedAt":"2022-01-12 19:12:45+00:00","OriginURL":"https://tech.instacart.com/building-for-balance-e61fb9511893?source=rss----587883b5d2ee---4","SourceName":"Instacart"}},{"node":{"ID":529,"Title":"Automated code generation from GraphQL operations","Description":"","PublishedAt":"2022-01-12 15:03:13+00:00","OriginURL":"https://engblog.nextdoor.com/automated-code-generation-from-graphql-operations-181293dec4df?source=rss----5e54f11cdfdf---4","SourceName":"Nextdoor"}},{"node":{"ID":608,"Title":"Being on-call as a software engineer - a challenging and fast learning experience","Description":"","PublishedAt":"2022-01-12 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2022-01-12-engineeroncall/","SourceName":"Trivago"}},{"node":{"ID":761,"Title":"The New Version of Orbit (v1.1) is Released: The Improvements, Design Changes, and Exciting Collaborations","Description":"<h1>Introduction</h1>\n<p><span style=\"font-weight: 400;\">The previous </span><a href=\"https://eng.uber.com/orbit/\"><span style=\"font-weight: 400;\">post</span></a><span style=\"font-weight: 400;\"> gave an overview of </span><a href=\"https://github.com/uber/orbit\"><span style=\"font-weight: 400;\">Orbit</span></a><span style=\"font-weight: 400;\">, a Python package developed by Uber in order to perform Bayesian time-series analysis and forecasting. This post provides the details of the version 1.1 updates—in particular, changes in syntax of </span>&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/the-new-version-of-orbit-v1-1-is-released/\">The New Version of Orbit (v1.1) is Released: The Improvements, Design Changes, and Exciting Collaborations</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n","PublishedAt":"2022-01-11 17:30:50+00:00","OriginURL":"https://eng.uber.com/the-new-version-of-orbit-v1-1-is-released/","SourceName":"Uber"}},{"node":{"ID":462,"Title":"3 updates we’ve made to Dashboards to improve workflows across teams","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2022/01/Blog-Hero-Image@2x-1024x576.png\" class=\"type:primaryImage\" /></figure>\n<p>After 10 years of working alongside product teams, we&#8217;ve found the most successful teams are constantly—and collectively—questioning their assumptions, iterating, and sharing their findings cross-functionally. Because of all this, two things product teams prioritize today are workflow and accessibility: &#8220;How easily can the various members of my team get all the data and analysis they</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/3-updates-weve-made-to-dashboards-to-improve-workflows-across-teams/\">3 updates we’ve made to Dashboards to improve workflows across teams</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2022-01-10 16:01:00+00:00","OriginURL":"https://mixpanel.com/blog/3-updates-weve-made-to-dashboards-to-improve-workflows-across-teams/","SourceName":"Mixpanel"}},{"node":{"ID":1234,"Title":"Blog: Meet Our Contributors - APAC (India region)","Description":"<p><strong>Authors &amp; Interviewers:</strong> <a href=\"https://github.com/anubha-v-ardhan\">Anubhav Vardhan</a>, <a href=\"https://github.com/Atharva-Shinde\">Atharva Shinde</a>, <a href=\"https://github.com/AvineshTripathi\">Avinesh Tripathi</a>, <a href=\"https://github.com/Debanitrkl\">Debabrata Panigrahi</a>, <a href=\"https://github.com/verma-kunal\">Kunal Verma</a>, <a href=\"https://github.com/PranshuSrivastava\">Pranshu Srivastava</a>, <a href=\"https://github.com/CIPHERTron\">Pritish Samal</a>, <a href=\"https://github.com/PurneswarPrasad\">Purneswar Prasad</a>, <a href=\"https://github.com/vedant-kakde\">Vedant Kakde</a></p>\n<p><strong>Editor:</strong> <a href=\"https://psaggu.com\">Priyanka Saggu</a></p>\n<hr>\n<p>Good day, everyone 👋</p>\n<p>Welcome to the first episode of the APAC edition of the &quot;Meet Our Contributors&quot; blog post series.</p>\n<p>In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.</p>\n<p>💫 <em>Let's get started, so without further ado…</em></p>\n<h2 id=\"arsh-sharma-https-github-com-rinkiyakedad\"><a href=\"https://github.com/RinkiyaKeDad\">Arsh Sharma</a></h2>\n<p>Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.</p>\n<p>He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the <a href=\"https://github.com/cert-manager/infrastructure\">cert-manager</a> tools development work that is being done under the aegis of SIG Architecture.</p>\n<p>To the newcomers, Arsh helps plan their early contributions sustainably.</p>\n<blockquote>\n<p><em>I would encourage folks to contribute in a way that's sustainable. What I mean by that\nis that it's easy to be very enthusiastic early on and take up more stuff than one can\nactually handle. This can often lead to burnout in later stages. It's much more sustainable\nto work on things iteratively.</em></p>\n</blockquote>\n<h2 id=\"kunal-kushwaha-https-github-com-kunal-kushwaha\"><a href=\"https://github.com/kunal-kushwaha\">Kunal Kushwaha</a></h2>\n<p>Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the <a href=\"https://community.cncf.io/cloud-native-students/\">CNCF Students Program</a>.. He also served as a Communications role shadow during the 1.22 release cycle.</p>\n<p>At the end of his first year, Kunal began contributing to the <a href=\"https://github.com/fabric8io/kubernetes-client\">fabric8io kubernetes-client</a> project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.</p>\n<p>As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.</p>\n<blockquote>\n<p><em>I believe if you find yourself in a place where you do not know much about the\nproject, that's a good thing because now you can learn while contributing and the\ncommunity is there to help you. It has helped me a lot in gaining skills, meeting\npeople from around the world and also helping them. You can learn on the go,\nyou don't have to be an expert. Make sure to also check out no code contributions\nbecause being a beginner is a skill and you can bring new perspectives to the\norganisation.</em></p>\n</blockquote>\n<h2 id=\"madhav-jivarajani-https-github-com-madhavjivrajani\"><a href=\"https://github.com/MadhavJivrajani\">Madhav Jivarajani</a></h2>\n<p>Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).</p>\n<p>Among several significant contributions are his recent efforts toward the Archival of <a href=\"https://github.com/kubernetes/community/issues/6055\">design proposals</a>, refactoring the <a href=\"https://github.com/kubernetes/k8s.io/pull/2713\">&quot;groups&quot; codebase</a> under k8s-infra repository to make it mockable and testable, and improving the functionality of the <a href=\"https://github.com/kubernetes/test-infra/issues/23129\">GitHub k8s bot</a>.</p>\n<p>In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly &quot;KEP reading club&quot; sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing <a href=\"https://github.com/kubernetes-sigs/contributor-katacoda\">Katacoda scenarios</a> to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several <a href=\"https://www.youtube.com/watch?v=FgsXbHBRYIc\">new contributors workshops (NCW)</a>.</p>\n<blockquote>\n<p><em>I initially did not know much about Kubernetes. I joined because the community was\nsuper friendly. But what made me stay was not just the people, but the project itself.\nMy solution to not feeling overwhelmed in the community was to gain as much context\nand knowledge into the topics that I was interested in and were being discussed. And\nas a result I continued to dig deeper into Kubernetes and the design of it.\nI am a systems nut &amp; thus Kubernetes was an absolute goldmine for me.</em></p>\n</blockquote>\n<h2 id=\"rajas-kakodkar-https-github-com-rajaskakodkar\"><a href=\"https://github.com/rajaskakodkar\">Rajas Kakodkar</a></h2>\n<p>Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.</p>\n<p>He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the <a href=\"https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/\">NetworkPolicy++</a> and <a href=\"https://github.com/kubernetes-sigs/kpng\"><code>kpng</code></a> sub-projects.</p>\n<p>One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.</p>\n<blockquote>\n<p><em>I enjoy contributing to Kubernetes not just because I get to work on\ncutting edge tech but more importantly because I get to work with\nawesome people and help in solving real world problems.</em></p>\n</blockquote>\n<h2 id=\"rajula-vineet-reddy-https-github-com-rajula96reddy\"><a href=\"https://github.com/rajula96reddy\">Rajula Vineet Reddy</a></h2>\n<p>Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.</p>\n<p>He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.</p>\n<p>According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.</p>\n<blockquote>\n<p><em>I find the community very helpful and it's always</em>\n“you get back as much as you contribute”.\n<em>The more involved you are, the more you will understand, get to learn and\ncontribute new things.</em></p>\n<p><em>The first step to</em> “come forward and start” <em>is hard. But it's all gonna be\nsmooth after that. Just take that jump.</em></p>\n</blockquote>\n<hr>\n<p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.</p>\n<p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋</p>","PublishedAt":"2022-01-10 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/01/10/meet-our-contributors-india-ep-01/","SourceName":"Kubernetes"}},{"node":{"ID":1235,"Title":"Blog: Kubernetes is Moving on From Dockershim: Commitments and Next Steps","Description":"<p><strong>Authors:</strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)</p>\n<p>Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited\nto reaffirm our community values by supporting open source container runtimes,\nenabling a smaller kubelet, and increasing engineering velocity for teams using\nKubernetes. If you <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/\">use Docker Engine as a container runtime</a>\nfor your Kubernetes cluster, get ready to migrate in 1.24! To check if you're\naffected, refer to <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/\">Check whether dockershim removal affects you</a>.</p>\n<h2 id=\"why-we-re-moving-away-from-dockershim\">Why we’re moving away from dockershim</h2>\n<p>Docker was the first container runtime used by Kubernetes. This is one of the\nreasons why Docker is so familiar to many Kubernetes users and enthusiasts.\nDocker support was hardcoded into Kubernetes – a component the project refers to\nas dockershim.\nAs containerization became an industry standard, the Kubernetes project added support\nfor additional runtimes. This culminated in the implementation of the\ncontainer runtime interface (CRI), letting system components (like the kubelet)\ntalk to container runtimes in a standardized way. As a result, dockershim became\nan anomaly in the Kubernetes project.\nDependencies on Docker and dockershim have crept into various tools\nand projects in the CNCF ecosystem ecosystem, resulting in fragile code.</p>\n<p>By removing the\ndockershim CRI, we're embracing the first value of CNCF: &quot;<a href=\"https://github.com/cncf/foundation/blob/master/charter.md#3-values\">Fast is better than\nslow</a>&quot;.\nStay tuned for future communications on the topic!</p>\n<h2 id=\"deprecation-timeline\">Deprecation timeline</h2>\n<p>We <a href=\"https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/\">formally announced</a> the dockershim deprecation in December 2020. Full removal is targeted\nin Kubernetes 1.24, in April 2022. This timeline\naligns with our <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior\">deprecation policy</a>,\nwhich states that deprecated behaviors must function for at least 1 year\nafter their announced deprecation.</p>\n<p>We'll support Kubernetes version 1.23, which includes\ndockershim, for another year in the Kubernetes project. For managed\nKubernetes providers, vendor support is likely to last even longer, but this is\ndependent on the companies themselves. Regardless, we're confident all cluster operations will have\ntime to migrate. If you have more questions about the dockershim removal, refer\nto the <a href=\"https://kubernetes.io/dockershim\">Dockershim Deprecation FAQ</a>.</p>\n<p>We asked you whether you feel prepared for the migration from dockershim in this\nsurvey: <a href=\"https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/\">Are you ready for Dockershim removal</a>.\nWe had over 600 responses. To everybody who took time filling out the survey,\nthank you.</p>\n<p>The results show that we still have a lot of ground to cover to help you to\nmigrate smoothly. Other container runtimes exist, and have been promoted\nextensively. However, many users told us they still rely on dockershim,\nand sometimes have dependencies that need to be re-worked. Some of these\ndependencies are outside of your control. Based on your feedback, here are some\nof the steps we are taking to help.</p>\n<h2 id=\"our-next-steps\">Our next steps</h2>\n<p>Based on the feedback you provided:</p>\n<ul>\n<li>CNCF and the 1.24 release team are committed to delivering documentation in\ntime for the 1.24 release. This includes more informative blog posts like this\none, updating existing code samples, tutorials, and tasks, and producing a\nmigration guide for cluster operators.</li>\n<li>We are reaching out to the rest of the CNCF community to help prepare them for\nthis change.</li>\n</ul>\n<p>If you're part of a project with dependencies on dockershim, or if you're\ninterested in helping with the migration effort, please join us! There's always\nroom for more contributors, whether to our transition tools or to our\ndocumentation. To get started, say hello in the\n<a href=\"https://kubernetes.slack.com/archives/C0BP8PW9G\">#sig-node</a>\nchannel on <a href=\"https://slack.kubernetes.io/\">Kubernetes Slack</a>!</p>\n<h2 id=\"final-thoughts\">Final thoughts</h2>\n<p>As a project, we've already seen cluster operators increasingly adopt other\ncontainer runtimes through 2021.\nWe believe there are no major blockers to migration. The steps we're taking to\nimprove the migration experience will light the path more clearly for you.</p>\n<p>We understand that migration from dockershim is yet another action you may need to\ndo to keep your Kubernetes infrastructure up to date. For most of you, this step\nwill be straightforward and transparent. In some cases, you will encounter\nhiccups or issues. The community has discussed at length whether postponing the\ndockershim removal would be helpful. For example, we recently talked about it in\nthe <a href=\"https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid\">SIG Node discussion on November 11th</a>\nand in the <a href=\"https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx\">Kubernetes Steering committee meeting held on December 6th</a>.\nWe already <a href=\"https://github.com/kubernetes/enhancements/pull/2481/\">postponed</a> it\nonce in 2021 because the adoption rate of other\nruntimes was lower than we wanted, which also gave us more time to identify\npotential blocking issues.</p>\n<p>At this point, we believe that the value that you (and Kubernetes) gain from\ndockershim removal makes up for the migration effort you'll have. Start planning\nnow to avoid surprises. We'll have more updates and guides before Kubernetes\n1.24 is released.</p>","PublishedAt":"2022-01-07 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/","SourceName":"Kubernetes"}},{"node":{"ID":783,"Title":"Obvious Ownership: A Sensible Humane Registry","Description":"Imagine yourself as an engineer who just joined SoundCloud. Besides meeting your colleagues and getting your new laptop, badge, and that…","PublishedAt":"2022-01-06 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/obvious-ownership-humane-registry","SourceName":"Soundcloud"}},{"node":{"ID":463,"Title":"How to be the go-to engineer for product analytics","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2022/01/MXP-Blog-GoToEngineerProductAnalytics-1920x1080-1-1024x576.png\" class=\"type:primaryImage\" /></figure>\n<p>Engineers are always looking for ways to differentiate themselves. Not only can niche expertise lock down employability, but it can also catapult one&#8217;s career in entirely unforeseen ways. One of those niches that&#8217;s become more valuable in recent years: product analytics. As data-driven product development continues to balloon in popularity, so does the need for</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/how-to-be-the-go-to-engineer-for-product-analytics/\">How to be the go-to engineer for product analytics</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2022-01-03 22:54:00+00:00","OriginURL":"https://mixpanel.com/blog/how-to-be-the-go-to-engineer-for-product-analytics/","SourceName":"Mixpanel"}},{"node":{"ID":1100,"Title":"(Career) Change is the only constant","Description":"<p>This post is for Day 24 of Mercari Advent Calendar 2021, brought to you by @kaustubh from the Mercari Metadata Ecosystem team. I recently changed teams at Mercari, and I wanted to share some realizations I discovered along the way. We will briefly discuss the Great Resignation and also how to retain talent, for companies [&hellip;]</p>\n","PublishedAt":"2021-12-24 10:00:57+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211224-career-change-is-the-only-constant/","SourceName":"Mercari"}},{"node":{"ID":1101,"Title":"Migrating a monolithic service under the bed (part 3 of 3)","Description":"<p>In the previous article, we covered the most challenging milestone of the Kauru migration project. We also introduced the PoC approach that the Product Catalog team adopted, and finally solved the schedule and management issue. Following that, this part 3 of 3 covers the final milestones of the migration project and concludes it by sharing [&hellip;]</p>\n","PublishedAt":"2021-12-23 16:36:17+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211223-migrating-a-monolithic-service-under-the-bed-part-3-of-3/","SourceName":"Mercari"}},{"node":{"ID":1102,"Title":"Migrating a monolithic service under the bed (part 2 of 3)","Description":"<p>In the previous article, we covered what the Kauru monolithic service was, why Mercari decided to migrate it to microservices, and how the Product Catalog team achieved the first milestone of the migration project. Following that, this part 2 of 3 covers the most difficult milestone of the migration project. Milestone 2: Kauru as a [&hellip;]</p>\n","PublishedAt":"2021-12-23 16:36:14+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211223-migrating-a-monolithic-service-under-the-bed-part-2-of-3/","SourceName":"Mercari"}},{"node":{"ID":1103,"Title":"Migrating a monolithic service under the bed (part 1 of 3)","Description":"<p>This is the 23rd entry in Mercari Advent Calendar 2021, by @greg.weng from Metadata Ecosystem team. Brief Overview In 2019, Mercari decided to close a four-year-old monolithic service (“Kauru”) and migrate its data and features to microservices. Before it was closed, the Kauru monolithic service cost ¥1,500,000 per month on the Google App Engine 1st [&hellip;]</p>\n","PublishedAt":"2021-12-23 16:36:11+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211223-migrating-a-monolithic-service-under-the-bed-part-1-of-3/","SourceName":"Mercari"}},{"node":{"ID":1104,"Title":"Exporting our Customers’ Personal Information with care","Description":"<p>This post is for Day 23 of Merpay Advent Calendar 2021, brought to you by @chris from the Merpay KYC team. What’s this export thing about? The KYC team is in charge of saving and retaining the personal information of our customers. In order to comply with laws and regulations, we sometimes have to get [&hellip;]</p>\n","PublishedAt":"2021-12-23 12:00:47+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211222-exporting-our-customer-personal-information-with-care/","SourceName":"Mercari"}},{"node":{"ID":762,"Title":"How We Saved 70K Cores Across 30 Mission-Critical Services (Large-Scale, Semi-Automated Go GC Tuning @Uber)","Description":"<h1><span style=\"font-weight: 400;\">Introduction</span></h1>\n<p><span style=\"font-weight: 400;\">As part of Uber engineering’s wide efforts to reach profitability, recently our team was focused on reducing cost of compute capacity by improving efficiency. Some of the most impactful work was around GOGC optimization. In this blog we want </span>&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/how-we-saved-70k-cores-across-30-mission-critical-services/\">How We Saved 70K Cores Across 30 Mission-Critical Services (Large-Scale, Semi-Automated Go GC Tuning @Uber)</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n","PublishedAt":"2021-12-22 17:30:17+00:00","OriginURL":"https://eng.uber.com/how-we-saved-70k-cores-across-30-mission-critical-services/","SourceName":"Uber"}},{"node":{"ID":81,"Title":"CX90: Rethinking, Redesigning and Reimplementing the Groupon User Experience","Description":"","PublishedAt":"2021-12-22 16:57:17+00:00","OriginURL":"https://medium.com/groupon-eng/cx90-rethinking-redesigning-and-reimplementing-the-groupon-user-experience-59a03b6c306c?source=rss----5c13a88f9872---4","SourceName":"Groupon"}},{"node":{"ID":1105,"Title":"The Mercari Client CI/CD Team: Improving the Productivity of a Development Organization That Continues to Expand  #TeamInterview","Description":"<p>CI/CD (continuous integration, continuous delivery) is crucial for developing software and delivering value to users quickly. CI/CD is required at each technology layer at both the backend and frontend. In this article, we spotlight the Client CI/CD team, which provides CI/CD for the mobile app and web versions of Mercari. We interviewed @y-kazama, @kaito, @thi, [&hellip;]</p>\n","PublishedAt":"2021-12-22 12:38:33+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211125-mercari-client-cicd-interview/","SourceName":"Mercari"}},{"node":{"ID":1106,"Title":"The Merpay QA Team: Maintaining Quality for Safe and Secure Products #TeamInterview","Description":"<p>Chiefly because Merpay provides its users with financial services, maintaining the quality of our services and products is paramount. It’s thanks to the Merpay QA (Quality Assurance) team and the support that they provide, that this is possible. In this entry, we interviewed @gaku and @miisan from the Merpay QA team to hear about their [&hellip;]</p>\n","PublishedAt":"2021-12-22 11:48:58+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211111-504f3da724/","SourceName":"Mercari"}},{"node":{"ID":1107,"Title":"Interpretability/explainability in machine learning","Description":"<p>* This article is a translation of the Japanese article written on Dec. 24th, 2019. Hello! On day 24 of the Merpay Advent Calendar 2019, I (@yuhi) from the Merpay Machine Learning team will share interpretability in machine learning. Table of Contents What is interpretability in machine learning? Why do we need interpretability? 1. Accountability [&hellip;]</p>\n","PublishedAt":"2021-12-22 10:15:16+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211222-2019-12-24-070000/","SourceName":"Mercari"}},{"node":{"ID":1108,"Title":"A Deep Dive into Table-Driven Testing in Golang","Description":"<p>This article is the 22nd entry in Merpay Advent Calendar 2021 by @adlerhsieh from the Payment Platform Team. We write a significant amount of Go code in the Merpay backend. While it&#8217;s fun, maintaining the production code quality is a challenge. One essential component to make that happen is to write effective unit tests. Great [&hellip;]</p>\n","PublishedAt":"2021-12-22 10:00:19+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20211221-a-deep-dive-into-table-driven-testing-in-golang/","SourceName":"Mercari"}},{"node":{"ID":1236,"Title":"Blog: Kubernetes-in-Kubernetes and the WEDOS PXE bootable server farm","Description":"<p><strong>Author</strong>: Andrei Kvapil (WEDOS)</p>\n<p>When you own two data centers, thousands of physical servers, virtual machines and hosting for hundreds of thousands sites, Kubernetes can actually simplify the management of all these things. As practice has shown, by using Kubernetes, you can declaratively describe and manage not only applications, but also the infrastructure itself. I work for the largest Czech hosting provider <strong>WEDOS Internet a.s</strong> and today I'll show you two of my projects — <a href=\"https://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> and <a href=\"https://github.com/kvaps/kubefarm\">Kubefarm</a>.</p>\n<p>With their help you can deploy a fully working Kubernetes cluster inside another Kubernetes using Helm in just a couple of commands. How and why?</p>\n<p>Let me introduce you to how our infrastructure works. All our physical servers can be divided into two groups: <strong>control-plane</strong> and <strong>compute</strong> nodes. Control plane nodes are usually set up manually, have a stable OS installed, and designed to run all cluster services including Kubernetes control-plane. The main task of these nodes is to ensure the smooth operation of the cluster itself. Compute nodes do not have any operating system installed by default, instead they are booting the OS image over the network directly from the control plane nodes. Their work is to carry out the workload.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme01.svg\"\nalt=\"Kubernetes cluster layout\"/>\n</figure>\n<p>Once nodes have downloaded their image, they can continue to work without keeping connection to the PXE server. That is, a PXE server is just keeping rootfs image and does not hold any other complex logic. After our nodes have booted, we can safely restart the PXE server, nothing critical will happen to them.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme02.svg\"\nalt=\"Kubernetes cluster after bootstrapping\"/>\n</figure>\n<p>After booting, the first thing our nodes do is join to the existing Kubernetes cluster, namely, execute the <strong>kubeadm join</strong> command so that kube-scheduler could schedule some pods on them and launch various workloads afterwards. From the beginning we used the scheme when nodes were joined into the same cluster used for the control-plane nodes.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme03.svg\"\nalt=\"Kubernetes scheduling containers to the compute nodes\"/>\n</figure>\n<p>This scheme worked stably for over two years. However later we decided to add containerized Kubernetes to it. And now we can spawn new Kubernetes-clusters very easily right on our control-plane nodes which are now member special admin-clusters. Now, compute nodes can be joined directly to their own clusters - depending on the configuration.</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme04.svg\"\nalt=\"Multiple clusters are running in single Kubernetes, compute nodes joined to them\"/>\n</figure>\n<h2 id=\"kubefarm\">Kubefarm</h2>\n<p>This project came with the goal of enabling anyone to deploy such an infrastructure in just a couple of commands using Helm and get about the same in the end.</p>\n<p>At this time, we moved away from the idea of a monocluster. Because it turned out to be not very convenient for managing work of several development teams in the same cluster. The fact is that Kubernetes was never designed as a multi-tenant solution and at the moment it does not provide sufficient means of isolation between projects. Therefore, running separate clusters for each team turned out to be a good idea. However, there should not be too many clusters, to let them be convenient to manage. Nor is it too small to have sufficient independence between development teams.</p>\n<p>The scalability of our clusters became noticeably better after that change. The more clusters you have per number of nodes, the smaller the failure domain and the more stable they work. And as a bonus, we got a fully declaratively described infrastructure. Thus, now you can deploy a new Kubernetes cluster in the same way as deploying any other application in Kubernetes.</p>\n<p>It uses <a href=\"http://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> as a basis, <a href=\"https://github.com/ltsp/ltsp/\">LTSP</a> as PXE-server from which the nodes are booted, and automates the DHCP server configuration using <a href=\"https://github.com/kvaps/dnsmasq-controller\">dnsmasq-controller</a>:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/kubefarm.png\"\nalt=\"Kubefarm\"/>\n</figure>\n<h2 id=\"how-it-works\">How it works</h2>\n<p>Now let's see how it works. In general, if you look at Kubernetes as from an application perspective, you can note that it follows all the principles of <a href=\"https://12factor.net/\">The Twelve-Factor App</a>, and is actually written very well. Thus, it means running Kubernetes as an app in a different Kubernetes shouldn't be a big deal.</p>\n<h3 id=\"running-kubernetes-in-kubernetes\">Running Kubernetes in Kubernetes</h3>\n<p>Now let's take a look at the <a href=\"https://github.com/kvaps/kubernetes-in-kubernetes\">Kubernetes-in-Kubernetes</a> project, which provides a ready-made Helm chart for running Kubernetes in Kubernetes.</p>\n<p>Here is the parameters that you can pass to Helm in the values file:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubernetes-in-kubernetes/tree/v0.13.1/deploy/helm/kubernetes\"><strong>kubernetes/values.yaml</strong></a></li>\n</ul>\n<img alt=\"Kubernetes is just five binaries\" style=\"float: right; max-height: 280px;\" src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/5binaries.png\">\n<p>Beside <strong>persistence</strong> (storage parameters for the cluster), the Kubernetes control-plane components are described here: namely: <strong>etcd cluster</strong>, <strong>apiserver</strong>, <strong>controller-manager</strong> and <strong>scheduler</strong>. These are pretty much standard Kubernetes components. There is a light-hearted saying that “Kubernetes is just five binaries”. So here is where the configuration for these binaries is located.</p>\n<p>If you ever tried to bootstrap a cluster using kubeadm, then this config will remind you it's configuration. But in addition to Kubernetes entities, you also have an admin container. In fact, it is a container which holds two binaries inside: <strong>kubectl</strong> and <strong>kubeadm</strong>. They are used to generate kubeconfig for the above components and to perform the initial configuration for the cluster. Also, in an emergency, you can always exec into it to check and manage your cluster.</p>\n<p>After the release <a href=\"https://asciinema.org/a/407280\">has been deployed</a>, you can see a list of pods: <strong>admin-container</strong>, <strong>apiserver</strong> in two replicas, <strong>controller-manager</strong>, <strong>etcd-cluster</strong>, <strong>scheduller</strong> and the initial job that initializes the cluster. In the end you have a command, which allows you to get shell into the admin container, you can use it to see what is happening inside:</p>\n<p><a href=\"https://asciinema.org/a/407280?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot01.svg\" alt=\"\"></a></p>\n<p>Also, let's take look at the certificates. If you've ever installed Kubernetes, then you know that it has a <em>scary</em> directory <code>/etc/kubernetes/pki</code> with a bunch of some certificates. In case of Kubernetes-in-Kubernetes, you have fully automated management of them with cert-manager. Thus, it is enough to pass all certificates parameters to Helm during installation, and all the certificates will automatically be generated for your cluster.</p>\n<p><a href=\"https://asciinema.org/a/407280?t=15&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot02.svg\" alt=\"\"></a></p>\n<p>Looking at one of the certificates, eg. apiserver, you can see that it has a list of DNS names and IP addresses. If you want to make this cluster accessible outside, then just describe the additional DNS names in the values file and update the release. This will update the certificate resource, and cert-manager will regenerate the certificate. You'll no longer need to think about this. If kubeadm certificates need to be renewed at least once a year, here the cert-manager will take care and automatically renew them.</p>\n<p><a href=\"https://asciinema.org/a/407280?t=25&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot03.svg\" alt=\"\"></a></p>\n<p>Now let's log into the admin container and look at the cluster and nodes. Of course, there are no nodes, yet, because at the moment you have deployed just the blank control-plane for Kubernetes. But in kube-system namespace you can see some coredns pods waiting for scheduling and configmaps already appeared. That is, you can conclude that the cluster is working:</p>\n<p><a href=\"https://asciinema.org/a/407280?t=30&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot04.svg\" alt=\"\"></a></p>\n<p>Here is the <a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html\">diagram of the deployed cluster</a>. You can see services for all Kubernetes components: <strong>apiserver</strong>, <strong>controller-manager</strong>, <strong>etcd-cluster</strong> and <strong>scheduler</strong>. And the pods on right side to which they forward traffic.</p>\n<p><a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd01.png\" alt=\"\"></a></p>\n<p><em>By the way, the diagram, is drawn in <a href=\"https://argoproj.github.io/argo-cd/\">ArgoCD</a> — the GitOps tool we use to manage our clusters, and cool diagrams are one of its features.</em></p>\n<h3 id=\"orchestrating-physical-servers\">Orchestrating physical servers</h3>\n<p>OK, now you can see the way how is our Kubernetes control-plane deployed, but what about worker nodes, how are we adding them? As I already said, all our servers are bare metal. We do not use virtualization to run Kubernetes, but we orchestrate all physical servers by ourselves.</p>\n<p>Also, we do use Linux network boot feature very actively. Moreover, this is exactly the booting, not some kind of automation of the installation. When the nodes are booting, they just run a ready-made system image for them. That is, to update any node, we just need to reboot it - and it will download a new image. It is very easy, simple and convenient.</p>\n<p>For this, the <a href=\"https://github.com/kvaps/kubefarm\">Kubefarm</a> project was created, which allows you to automate this. The most commonly used examples can be found in the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples\">examples</a> directory. The most standard of them named <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic\">generic</a>. Let's take a look at values.yaml:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/generic/values.yaml\"><strong>generic/values.yaml</strong></a></li>\n</ul>\n<p>Here you can specify the parameters which are passed into the upstream Kubernetes-in-Kubernetes chart. In order for you control-plane to be accessible from the outside, it is enough to specify the IP address here, but if you wish, you can specify some DNS name here.</p>\n<p>In the PXE server configuration you can specify a timezone. You can also add an SSH key for logging in without a password (but you can also specify a password), as well as kernel modules and parameters that should be applied during booting the system.</p>\n<p>Next comes the <strong>nodePools</strong> configuration, i.e. the nodes themselves. If you've ever used a terraform module for gke, then this logic will remind you of it. Here you statically describe all nodes with a set of parameters:</p>\n<ul>\n<li>\n<p><strong>Name</strong> (hostname);</p>\n</li>\n<li>\n<p><strong>MAC-addresses</strong> — we have nodes with two network cards, and each one can boot from any of the MAC addresses specified here.</p>\n</li>\n<li>\n<p><strong>IP-address</strong>, which the DHCP server should issue to this node.</p>\n</li>\n</ul>\n<p>In this example, you have two pools: the first has five nodes, the second has only one, the second pool has also two tags assigned. Tags are the way to describe configuration for specific nodes. For example, you can add specific DHCP options for some pools, options for the PXE server for booting (e.g. here is debug option enabled) and set of <strong>kubernetesLabels</strong> and <strong>kubernetesTaints</strong> options. What does that mean?</p>\n<p>For example, in this configuration you have a second nodePool with one node. The pool has <strong>debug</strong> and <strong>foo</strong> tags assigned. Now see the options for <strong>foo</strong> tag in <strong>kubernetesLabels</strong>. This means that the m1c43 node will boot with these two labels and taint assigned. Everything seems to be simple. Now <a href=\"https://asciinema.org/a/407282\">let's try</a> this in practice.</p>\n<h3 id=\"demo\">Demo</h3>\n<p>Go to <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples\">examples</a> and update previously deployed chart to Kubefarm. Just use the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic\">generic</a> parameters and look at the pods. You can see that a PXE server and one more job were added. This job essentially goes to the deployed Kubernetes cluster and creates a new token. Now it will run repeatedly every 12 hours to generate a new token, so that the nodes can connect to your cluster.</p>\n<p><a href=\"https://asciinema.org/a/407282?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot05.svg\" alt=\"\"></a></p>\n<p>In a <a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html\">graphical representation</a>, it looks about the same, but now apiserver started to be exposed outside.</p>\n<p><a href=\"https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd02.png\" alt=\"\"></a></p>\n<p>In the diagram, the IP is highlighted in green, the PXE server can be reached through it. At the moment, Kubernetes does not allow creating a single LoadBalancer service for TCP and UDP protocols by default, so you have to create two different services with the same IP address. One is for TFTP, and the second for HTTP, through which the system image is downloaded.</p>\n<p>But this simple example is not always enough, sometimes you might need to modify the logic at boot. For example, here is a directory <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network\">advanced_network</a>, inside which there is a <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network\">values file</a> with a simple shell script. Let's call it <code>network.sh</code>:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/advanced_network/values.yaml#L14-L78\"><strong>network.sh</strong></a></li>\n</ul>\n<p>All this script does is take environment variables at boot time, and generates a network configuration based on them. It creates a directory and puts the netplan config inside. For example, a bonding interface is created here. Basically, this script can contain everything you need. It can hold the network configuration or generate the system services, add some hooks or describe any other logic. Anything that can be described in bash or shell languages will work here, and it will be executed at boot time.</p>\n<p>Let's see how it can be <a href=\"https://asciinema.org/a/407284\">deployed</a>. Let's pass the generic values file as the first parameter, and an additional values file as the second parameter. This is a standard Helm feature. This way you can also pass the secrets, but in this case, the configuration is just expanded by the second file:</p>\n<p><a href=\"https://asciinema.org/a/407284?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot06.svg\" alt=\"\"></a></p>\n<p>Let's look at the configmap <strong>foo-kubernetes-ltsp</strong> for the netboot server and make sure that <code>network.sh</code> script is really there. These commands used to configure the network at boot time:</p>\n<p><a href=\"https://asciinema.org/a/407284?t=15&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot07.svg\" alt=\"\"></a></p>\n<p><a href=\"https://asciinema.org/a/407286\">Here</a> you can see how it works in principle. The chassis interface (we use HPE Moonshots 1500) have the nodes, you can enter <code>show node list</code> command to get a list of all the nodes. Now you can see the booting process.</p>\n<p><a href=\"https://asciinema.org/a/407286?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot08.svg\" alt=\"\"></a></p>\n<p>You can also get their MAC addresses by <code>show node macaddr all</code> command. We have a clever operator that collects MAC-addresses from chassis automatically and passes them to the DHCP server. Actually, it's just creating custom configuration resources for dnsmasq-controller which is running in same admin Kubernetes cluster. Also, trough this interface you can control the nodes themselves, e.g. turn them on and off.</p>\n<p>If you have no such opportunity to enter the chassis through iLO and collect a list of MAC addresses for your nodes, you can consider using <a href=\"https://asciinema.org/a/407287\">catchall cluster</a> pattern. Purely speaking, it is just a cluster with a dynamic DHCP pool. Thus, all nodes that are not described in the configuration to other clusters will automatically join to this cluster.</p>\n<p><a href=\"https://asciinema.org/a/407287?autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot09.svg\" alt=\"\"></a></p>\n<p>For example, you can see a special cluster with some nodes. They are joined to the cluster with an auto-generated name based on their MAC address. Starting from this point you can connect to them and see what happens there. Here you can somehow prepare them, for example, set up the file system and then rejoin them to another cluster.</p>\n<p>Now let's try connecting to the node terminal and see how it is booting. After the BIOS, the network card is configured, here it sends a request to the DHCP server from a specific MAC address, which redirects it to a specific PXE server. Later the kernel and initrd image are downloaded from the server using the standard HTTP protocol:</p>\n<p><a href=\"https://asciinema.org/a/407286?t=28&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot10.svg\" alt=\"\"></a></p>\n<p>After loading the kernel, the node downloads the rootfs image and transfers control to systemd. Then the booting proceeds as usual, and after that the node joins Kubernetes:</p>\n<p><a href=\"https://asciinema.org/a/407286?t=80&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot11.svg\" alt=\"\"></a></p>\n<p>If you take a look at <strong>fstab</strong>, you can see only two entries there: <strong>/var/lib/docker</strong> and <strong>/var/lib/kubelet</strong>, they are mounted as <strong>tmpfs</strong> (in fact, from RAM). At the same time, the root partition is mounted as <strong>overlayfs</strong>, so all changes that you make here on the system will be lost on the next reboot.</p>\n<p>Looking into the block devices on the node, you can see some nvme disk, but it has not yet been mounted anywhere. There is also a loop device - this is the exact rootfs image downloaded from the server. At the moment it is located in RAM, occupies 653 MB and mounted with the <strong>loop</strong> option.</p>\n<p>If you look in <strong>/etc/ltsp</strong>, you find the <code>network.sh</code> file that was executed at boot. From containers, you can see running <code>kube-proxy</code> and <code>pause</code> container for it.</p>\n<p><a href=\"https://asciinema.org/a/407286?t=100&amp;autoplay=1\"><img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot12.svg\" alt=\"\"></a></p>\n<h2 id=\"details\">Details</h2>\n<h3 id=\"network-boot-image\">Network Boot Image</h3>\n<p>But where does the main image come from? There is a little trick here. The image for the nodes is built through the <a href=\"https://github.com/kvaps/kubefarm/tree/v0.13.1/build/ltsp\">Dockerfile</a> along with the server. The <a href=\"https://docs.docker.com/develop/develop-images/multistage-build/\">Docker multi-stage build</a> feature allows you to easily add any packages and kernel modules exactly at the stage of the image build. It looks like this:</p>\n<ul>\n<li><a href=\"https://github.com/kvaps/kubefarm/blob/v0.13.1/build/ltsp/Dockerfile\"><strong>Dockerfile</strong></a></li>\n</ul>\n<p>What's going on here? First, we take a regular Ubuntu 20.04 and install all the packages we need. First of all we install the <strong>kernel</strong>, <strong>lvm</strong>, <strong>systemd</strong>, <strong>ssh</strong>. In general, everything that you want to see on the final node should be described here. Here we also install <code>docker</code> with <code>kubelet</code> and <code>kubeadm</code>, which are used to join the node to the cluster.</p>\n<p>And then we perform an additional configuration. In the last stage, we simply install <code>tftp</code> and <code>nginx</code> (which serves our image to clients), <strong>grub</strong> (bootloader). Then root of the previous stages copied into the final image and generate squashed image from it. That is, in fact, we get a docker image, which has both the server and the boot image for our nodes. At the same time, it can be easily updated by changing the Dockerfile.</p>\n<h3 id=\"webhooks-and-api-aggregation-layer\">Webhooks and API aggregation layer</h3>\n<p>I want to pay special attention to the problem of webhooks and aggregation layer. In general, webhooks is a Kubernetes feature that allows you to respond to the creation or modification of any resources. Thus, you can add a handler so that when resources are applied, Kubernetes must send request to some pod and check if configuration of this resource is correct, or make additional changes to it.</p>\n<p>But the point is, in order for the webhooks to work, the apiserver must have direct access to the cluster for which it is running. And if it is started in a separate cluster, like our case, or even separately from any cluster, then Konnectivity service can help us here. Konnectivity is one of the optional but officially supported Kubernetes components.</p>\n<p>Let's take cluster of four nodes for example, each of them is running a <code>kubelet</code> and we have other Kubernetes components running outside: <code>kube-apiserver</code>, <code>kube-scheduler</code> and <code>kube-controller-manager</code>. By default, all these components interact with the apiserver directly - this is the most known part of the Kubernetes logic. But in fact, there is also a reverse connection. For example, when you want to view the logs or run a <code>kubectl exec command</code>, the API server establishes a connection to the specific kubelet independently:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity01.svg\"\nalt=\"Kubernetes apiserver reaching kubelet\"/>\n</figure>\n<p>But the problem is that if we have a webhook, then it usually runs as a standard pod with a service in our cluster. And when apiserver tries to reach it, it will fail because it will try to access an in-cluster service named <strong>webhook.namespace.svc</strong> being outside of the cluster where it is actually running:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity02.svg\"\nalt=\"Kubernetes apiserver can&#39;t reach webhook\"/>\n</figure>\n<p>And here Konnectivity comes to our rescue. Konnectivity is a tricky proxy server developed especially for Kubernetes. It can be deployed as a server next to the apiserver. And Konnectivity-agent is deployed in several replicas directly in the cluster you want to access. The agent establishes a connection to the server and sets up a stable channel to make apiserver able to access all webhooks and all kubelets in the cluster. Thus, now all communication with the cluster will take place through the Konnectivity-server:</p>\n<figure>\n<img src=\"https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity03.svg\"\nalt=\"Kubernetes apiserver reaching webhook via konnectivity\"/>\n</figure>\n<h2 id=\"our-plans\">Our plans</h2>\n<p>Of course, we are not going to stop at this stage. People interested in the project often write to me. And if there will be a sufficient number of interested people, I hope to move Kubernetes-in-Kubernetes project under <a href=\"https://github.com/kubernetes-sigs\">Kubernetes SIGs</a>, by representing it in form of the official Kubernetes Helm chart. Perhaps, by making this project independent we'll gather an even larger community.</p>\n<p>I am also thinking of integrating it with the Machine Controller Manager, which would allow creating worker nodes, not only of physical servers, but also, for example, for creating virtual machines using kubevirt and running them in the same Kubernetes cluster. By the way, it also allows to spawn virtual machines in the clouds, and have a control-plane deployed locally.</p>\n<p>I am also considering the option of integrating with the Cluster-API so that you can create physical Kubefarm clusters directly through the Kubernetes environment. But at the moment I'm not completely sure about this idea. If you have any thoughts on this matter, I'll be happy to listen to them.</p>","PublishedAt":"2021-12-22 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/22/kubernetes-in-kubernetes-and-pxe-bootable-server-farm/","SourceName":"Kubernetes"}},{"node":{"ID":239,"Title":"Cloudera Data Engineering 2021 Year End Review","Description":"<p>Since the release of Cloudera Data Engineering (CDE)  more than a year ago, our number one goal was operationalizing Spark pipelines at scale with first class tooling designed to streamline automation and observability.   In working with thousands of customers deploying Spark applications, we saw significant challenges with managing Spark as well as automating, delivering, [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blog.cloudera.com/cloudera-data-engineering-2021-year-end-review/\">Cloudera Data Engineering 2021 Year End Review</a> appeared first on <a rel=\"nofollow\" href=\"https://blog.cloudera.com\">Cloudera Blog</a>.</p>\n","PublishedAt":"2021-12-21 15:13:32+00:00","OriginURL":"https://blog.cloudera.com/cloudera-data-engineering-2021-year-end-review/","SourceName":"Cloudera"}},{"node":{"ID":1237,"Title":"Blog: Using Admission Controllers to Detect Container Drift at Runtime","Description":"<p><strong>Author:</strong> Saifuding Diliyaer (Box)\n<figure>\n<img src=\"https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/intro-illustration.png\"\nalt=\"Introductory illustration\"/> <figcaption>\n<p>Illustration by Munire Aireti</p>\n</figcaption>\n</figure>\n</p>\n<p>At Box, we use Kubernetes (K8s) to manage hundreds of micro-services that enable Box to stream data at a petabyte scale. When it comes to the deployment process, we run <a href=\"https://github.com/box/kube-applier\">kube-applier</a> as part of the GitOps workflows with declarative configuration and automated deployment. Developers declare their K8s apps manifest into a Git repository that requires code reviews and automatic checks to pass, before any changes can get merged and applied inside our K8s clusters. With <code>kubectl exec</code> and other similar commands, however, developers are able to directly interact with running containers and alter them from their deployed state. This interaction could then subvert the change control and code review processes that are enforced in our CI/CD pipelines. Further, it allows such impacted containers to continue receiving traffic long-term in production.</p>\n<p>To solve this problem, we developed our own K8s component called <a href=\"https://github.com/box/kube-exec-controller\">kube-exec-controller</a> along with its corresponding <a href=\"https://github.com/box/kube-exec-controller#kubectl-pi\">kubectl plugin</a>. They function together in detecting and terminating potentially mutated containers (caused by interactive kubectl commands), as well as revealing the interaction events directly to the target Pods for better visibility.</p>\n<h2 id=\"admission-control-for-interactive-kubectl-commands\">Admission control for interactive kubectl commands</h2>\n<p>Once a request is sent to K8s, it needs to be authenticated and authorized by the API server to proceed. Additionally, K8s has a separate layer of protection called <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\">admission controllers</a>, which can intercept the request before an object is persisted in <em>etcd</em>. There are various predefined admission controls compiled into the API server binary (e.g. ResourceQuota to enforce hard resource usage limits per namespace). Besides, there are two dynamic admission controls named <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook\">MutatingAdmissionWebhook</a> and <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook\">ValidatingAdmissionWebhook</a>, used for mutating or validating K8s requests respectively. The latter is what we adopted to detect container drift at runtime caused by interactive kubectl commands. This whole process can be divided into three steps as explained in detail below.</p>\n<h3 id=\"1-admit-interactive-kubectl-command-requests\">1. Admit interactive kubectl command requests</h3>\n<p>First of all, we needed to enable a validating webhook that sends qualified requests to <em>kube-exec-controller</em>. To add the new validation mechanism applying to interactive kubectl commands specifically, we configured the webhook’s rules with resources as <code>[pods/exec, pods/attach]</code>, and operations as <code>CONNECT</code>. These rules tell the cluster's API server that all <code>exec</code> and <code>attach</code> requests should be subject to our admission control webhook. In the ValidatingAdmissionWebhook that we configured, we specified a <code>service</code> reference (could also be replaced with <code>url</code> that gives the location of the webhook) and <code>caBundle</code> to allow validating its X.509 certificate, both under the <code>clientConfig</code> stanza.</p>\n<p>Here is a short example of what our ValidatingWebhookConfiguration object looks like:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>admissionregistration.k8s.io/v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>ValidatingWebhookConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-validating-webhook-config<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">webhooks</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>validate-pod-interaction.example.com<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">sideEffects</span>:<span style=\"color:#bbb\"> </span>None<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">rules</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- <span style=\"color:#008000;font-weight:bold\">apiGroups</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;*&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;*&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">operations</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;CONNECT&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;pods/exec&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;pods/attach&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">failurePolicy</span>:<span style=\"color:#bbb\"> </span>Fail<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">clientConfig</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">service</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># reference to kube-exec-controller service deployed inside the K8s cluster</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>example-service<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>kube-exec-controller<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">path</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;/admit-pod-interaction&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">caBundle</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;{{VALUE}}&#34;</span><span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># PEM encoded CA bundle to validate kube-exec-controller&#39;s certificate</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">admissionReviewVersions</span>:<span style=\"color:#bbb\"> </span>[<span style=\"color:#b44\">&#34;v1&#34;</span>,<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;v1beta1&#34;</span>]<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"2-label-the-target-pod-with-potentially-mutated-containers\">2. Label the target Pod with potentially mutated containers</h3>\n<p>Once a request of <code>kubectl exec</code> comes in, <em>kube-exec-controller</em> makes an internal note to label the associated Pod. The added labels mean that we can not only query all the affected Pods, but also enable the security mechanism to retrieve previously identified Pods, in case the controller service itself gets restarted.</p>\n<p>The admission control process cannot directly modify the targeted in its admission response. This is because the <code>pods/exec</code> request is against a subresource of the Pod API, and the API kind for that subresource is <code>PodExecOptions</code>. As a result, there is a separate process in <em>kube-exec-controller</em> that patches the labels asynchronously. The admission control always permits the <code>exec</code> request, then acts as a client of the K8s API to label the target Pod and to log related events. Developers can check whether their Pods are affected or not using <code>kubectl</code> or similar tools. For example:</p>\n<pre tabindex=\"0\"><code>$ kubectl get pod --show-labels\nNAME READY STATUS RESTARTS AGE LABELS\ntest-pod 1/1 Running 0 2s box.com/podInitialInteractionTimestamp=1632524400,box.com/podInteractorUsername=username-1,box.com/podTTLDuration=1h0m0s\n$ kubectl describe pod test-pod\n...\nEvents:\nType Reason Age    From                            Message\n----       ------       ----   ----                            -------\nWarning PodInteraction 5s admission-controller-service Pod was interacted with &#39;kubectl exec&#39; command by user &#39;username-1&#39; initially at time 2021-09-24 16:00:00 -0800 PST\nWarning PodInteraction 5s admission-controller-service Pod will be evicted at time 2021-09-24 17:00:00 -0800 PST (in about 1h0m0s).\n</code></pre><h3 id=\"3-evict-the-target-pod-after-a-predefined-period\">3. Evict the target Pod after a predefined period</h3>\n<p>As you can see in the above event messages, the affected Pod is not evicted immediately. At times, developers might have to get into their running containers necessarily for debugging some live issues. Therefore, we define a time to live (TTL) of affected Pods based on the environment of clusters they are running. In particular, we allow a longer time in our dev clusters as it is more common to run <code>kubectl exec</code> or other interactive commands for active development.</p>\n<p>For our production clusters, we specify a lower time limit so as to avoid the impacted Pods serving traffic abidingly. The <em>kube-exec-controller</em> internally sets and tracks a timer for each Pod that matches the associated TTL. Once the timer is up, the controller evicts that Pod using K8s API. The eviction (rather than deletion) is to ensure service availability, since the cluster respects any configured <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\">PodDisruptionBudget</a> (PDB). Let's say if a user has defined <em>x</em> number of Pods as critical in their PDB, the eviction (as requested by <em>kube-exec-controller</em>) does not continue when the target workload has fewer than <em>x</em> Pods running.</p>\n<p>Here comes a sequence diagram of the entire workflow mentioned above:</p>\n<!-- Mermaid Live Editor link - https://mermaid-js.github.io/mermaid-live-editor/edit/#pako:eNp9kjFPAzEMhf-KlalIbWd0QpUQdGJB3JrFTUyJmjhHzncFof53nGtpqYTYEuu958-Wv4zLnkxjenofiB09BtwWTJbRSS6QCLCHu01ZPdJIMXdUYNZTGYOjRd4zlRvLHRYJLnTIArvbtozV83TbAnZhUcVUrkXo04OU2I6uKu99Cn0fMsNDZik5Rm3SHntYTrRYrabUBl4GBmt2w4acRKAPcrBcLq0Bl1NC9pYnoRouHZopX9RX9aotddJeADaf4DDGwFuQN4IRY_Ao9bunzVvOO13COeYCcR9j3k-OCQDP9KfgC8TJsFbZIHSxnGljzp1lgKs2v9HXugMBwe2WPHTZ94CvottB6Ap5eg2s9cBaUnrLVEP_Yp5ynrOf3fxPV2V1lBOhmZtEJWHweiFfldQa1SWyptGnAuAQxRrLB5UOna6P1j7o4ZhGykBzg4Pk9pPdz_-oOR3ZsXj4BjrP5rU-->\n<p><img src=\"https://kubernetes.io/images/sequence_diagram.svg\" alt=\"Sequence Diagram\"></p>\n<h2 id=\"a-new-kubectl-plugin-for-better-user-experience\">A new kubectl plugin for better user experience</h2>\n<p>Our admission controller component works great for solving the container drift issue we had on the platform. It is also able to submit all related Events to the target Pod that has been affected. However, K8s clusters don't retain Events very long (the default retention period is one hour). We need to provide other ways for developers to get their Pod interaction activity. A <a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\">kubectl plugin</a> is a perfect choice for us to expose this information. We named our plugin <code>kubectl pi</code> (short for <code>pod-interaction</code>) and provide two subcommands: <code>get</code> and <code>extend</code>.</p>\n<p>When the <code>get</code> subcommand is called, the plugin checks the metadata attached by our admission controller and transfers it to human-readable information. Here is an example output from running <code>kubectl pi get</code>:</p>\n<pre tabindex=\"0\"><code>$ kubectl pi get test-pod\nPOD-NAME INTERACTOR POD-TTL EXTENSION EXTENSION-REQUESTER EVICTION-TIME\ntest-pod  username-1  1h0m0s   /          /                    2021-09-24 17:00:00 -0800 PST\n</code></pre><p>The plugin can also be used to extend the TTL for a Pod that is marked for future eviction. This is useful in case developers need extra time to debug ongoing issues. To achieve this, a developer uses the <code>kubectl pi extend</code> subcommand, where the plugin patches the relevant <em>annotations</em> for the given Pod. These <em>annotations</em> include the duration and username who made the extension request for transparency (displayed in the table returned from the <code>kubectl pi get</code> command).</p>\n<p>Correspondingly, there is another webhook defined in <em>kube-exec-controller</em> which admits valid annotation updates. Once admitted, those updates reset the eviction timer of the target Pod as requested. An example of requesting the extension from the developer side would be:</p>\n<pre tabindex=\"0\"><code>$ kubectl pi extend test-pod --duration=30m\nSuccessfully extended the termination time of pod/test-pod with a duration=30m\n \n$ kubectl pi get test-pod\nPOD-NAME  INTERACTOR  POD-TTL  EXTENSION  EXTENSION-REQUESTER  EVICTION-TIME\ntest-pod  username-1  1h0m0s   30m        username-2           2021-09-24 17:30:00 -0800 PST\n</code></pre><h2 id=\"future-improvement\">Future improvement</h2>\n<p>Although our admission controller service works great in handling interactive requests to a Pod, it could as well evict the Pod while the actual commands are no-op in these requests. For instance, developers sometimes run <code>kubectl exec</code> merely to check their service logs stored on hosts. Nevertheless, the target Pods would still get bounced despite the state of their containers not changing at all. One of the improvements here could be adding the ability to distinguish the commands that are passed to the interactive requests, so that no-op commands should not always force a Pod eviction. However, this becomes challenging when developers get a shell to a running container and execute commands inside the shell, since they will no longer be visible to our admission controller service.</p>\n<p>Another item worth pointing out here is the choice of using K8s <em>labels</em> and <em>annotations</em>. In our design, we decided to have all immutable metadata attached as <em>labels</em> for better enforcing the immutability in our admission control. Yet some of these metadata could fit better as <em>annotations</em>. For instance, we had a label with the key <code>box.com/podInitialInteractionTimestamp</code> used to list all affected Pods in <em>kube-exec-controller</em> code, although its value would be unlikely to query for. As a more ideal design in the K8s world, a single <em>label</em> could be preferable in our case for identification with other metadata applied as <em>annotations</em> instead.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>With the power of admission controllers, we are able to secure our K8s clusters by detecting potentially mutated containers at runtime, and evicting their Pods without affecting service availability. We also utilize kubectl plugins to provide flexibility of the eviction time and hence, bringing a better and more self-independent experience to service owners. We are proud to announce that we have open-sourced the whole project for the community to leverage in their own K8s clusters. Any contribution is more than welcomed and appreciated. You can find this project hosted on GitHub at <a href=\"https://github.com/box/kube-exec-controller\">https://github.com/box/kube-exec-controller</a></p>\n<p><em>Special thanks to Ayush Sobti and Ethan Goldblum for their technical guidance on this project.</em></p>","PublishedAt":"2021-12-21 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/","SourceName":"Kubernetes"}}]}},"pageContext":{"limit":30,"skip":3990,"numPages":158,"currentPage":134}},"staticQueryHashes":["3649515864"]}