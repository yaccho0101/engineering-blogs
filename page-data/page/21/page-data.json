{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/21","result":{"data":{"allPost":{"edges":[{"node":{"ID":4100,"Title":"Being one of the only woman in her computer science program didn’t stop Hannah Mudge from pursuing her dream of becoming a software engineer","Description":"","PublishedAt":"2023-07-07 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/culture-hannah-mudge-dream-of-becoming-software-engineer","SourceName":"Elastic"}},{"node":{"ID":4102,"Title":"Accelerating R&D in pharma with Elasticsearch, ESRE, LLMs, and LangChain — Part 1","Description":"","PublishedAt":"2023-07-07 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/research-development-pharma-elasticsearch-esre-llms-langchain-1","SourceName":"Elastic"}},{"node":{"ID":4097,"Title":"Accessibility considerations behind code search and code view","Description":"A look at how we improved the readability of code on GitHub.","PublishedAt":"2023-07-06 19:00:51+00:00","OriginURL":"https://github.blog/2023-07-06-accessibility-considerations-behind-code-search-and-code-view/","SourceName":"GitHub"}},{"node":{"ID":4098,"Title":"Powering home automation with WebSocket APIs","Description":"","PublishedAt":"2023-07-06 17:04:32+00:00","OriginURL":"https://medium.com/better-practices/powering-home-automation-with-websocket-apis-8885a7601523?source=rss----410f2fbc015d---4","SourceName":"Postman"}},{"node":{"ID":4093,"Title":"Why knowledge management is foundational to AI success","Description":"<p>Providing the right context to AI can improve accuracy and reduce hallucinations.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/07/06/why-knowledge-management-is-foundational-to-ai-success/\">Why knowledge management is foundational to AI success</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-07-06 13:34:49+00:00","OriginURL":"https://stackoverflow.blog/2023/07/06/why-knowledge-management-is-foundational-to-ai-success/","SourceName":"Stack Overflow"}},{"node":{"ID":4092,"Title":"Spark: Bringing generative AI to Mixpanel","Description":"<figure><img src=\"https://mixpanel.com/wp-content/uploads/2023/07/Hero-Prompt-Bar-1024x575.png\" class=\"type:primaryImage\" /></figure>\n<p>The rules of SaaS are changing. For so long, using the apps and services we need to be productive has required technical formulas or exhausting interfaces. Generative AI is unframing all of that. Have some scratch notes you’d like expanded into a new product requirements document (PRD)? You can now click a button to get</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mixpanel.com/blog/spark-bringing-generative-ai-to-mixpanel/\">Spark: Bringing generative AI to Mixpanel</a> appeared first on <a rel=\"nofollow\" href=\"https://mixpanel.com\">Mixpanel</a>.</p>\n","PublishedAt":"2023-07-06 12:30:00+00:00","OriginURL":"https://mixpanel.com/blog/spark-bringing-generative-ai-to-mixpanel/","SourceName":"Mixpanel"}},{"node":{"ID":4415,"Title":"Spark: Bringing generative AI to Mixpanel","Description":"<figure><img src=\"https://mxpnlcms.wpengine.com/wp-content/uploads/2023/07/Hero-Prompt-Bar-1024x575.png\" class=\"type:primaryImage\" /></figure>\n<p>The rules of SaaS are changing. For so long, using the apps and services we need to be productive has required technical formulas or exhausting interfaces. Generative AI is unframing all of that. Have some scratch notes you’d like expanded into a new product requirements document (PRD)? You can now click a button to get</p>\n<p>The post <a rel=\"nofollow\" href=\"https://mxpnlcms.wpengine.com/blog/spark-bringing-generative-ai-to-mixpanel/\">Spark: Bringing generative AI to Mixpanel</a> appeared first on <a rel=\"nofollow\" href=\"https://mxpnlcms.wpengine.com\">Mixpanel</a>.</p>\n","PublishedAt":"2023-07-06 12:30:00+00:00","OriginURL":"https://mxpnlcms.wpengine.com/blog/spark-bringing-generative-ai-to-mixpanel/","SourceName":"Mixpanel"}},{"node":{"ID":4428,"Title":"Spark: Bringing generative AI to Mixpanel","Description":"<figure><img src=\"/wp-content/uploads/2023/07/Hero-Prompt-Bar-1024x575.png\" class=\"type:primaryImage\" /></figure>\n<p>The rules of SaaS are changing. For so long, using the apps and services we need to be productive has required technical formulas or exhausting interfaces. Generative AI is unframing all of that. Have some scratch notes you’d like expanded into a new product requirements document (PRD)? You can now click a button to get</p>\n<p>The post <a rel=\"nofollow\" href=\"/blog/spark-bringing-generative-ai-to-mixpanel/\">Spark: Bringing generative AI to Mixpanel</a> appeared first on <a rel=\"nofollow\" href=\"https://mxpnlcms.wpengine.com\">Mixpanel</a>.</p>\n","PublishedAt":"2023-07-06 12:30:00+00:00","OriginURL":"/blog/spark-bringing-generative-ai-to-mixpanel/","SourceName":"Mixpanel"}},{"node":{"ID":4104,"Title":"Bucket full of secrets &#8211; Terraform exfiltration","Description":"<p>Background At Mercari, we utilize many microservices developed across multiple different teams. Each team has ownership over not only their code, but also the infrastructure necessary to run their services. To allow developers to take ownership of their infrastructure we use HashiCorp Terraform to define the infrastructure as code. Developers can use Terraform native resources [&hellip;]</p>\n","PublishedAt":"2023-07-06 11:21:39+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20230706-bucket-full-of-secrets-terraform-exfiltration/","SourceName":"Mercari"}},{"node":{"ID":4090,"Title":"Meet Miro Engineering: Elena Ignatik and the evolution of the Developer Platform","Description":"","PublishedAt":"2023-07-06 07:41:41+00:00","OriginURL":"https://medium.com/miro-engineering/meet-miro-engineering-elena-ignatik-and-the-evolution-of-the-developer-platform-d878c170f1b0?source=rss----555f7fd62a50---4","SourceName":"Miro Engineering"}},{"node":{"ID":4089,"Title":"Blog: Confidential Kubernetes: Use Confidential Virtual Machines and Enclaves to improve your cluster security","Description":"<p><strong>Authors:</strong> Fabian Kammel (Edgeless Systems), Mikko Ylinen (Intel), Tobin Feldman-Fitzthum (IBM)</p>\n<p>In this blog post, we will introduce the concept of Confidential Computing (CC) to improve any computing environment's security and privacy properties. Further, we will show how\nthe Cloud-Native ecosystem, particularly Kubernetes, can benefit from the new compute paradigm.</p>\n<p>Confidential Computing is a concept that has been introduced previously in the cloud-native world. The\n<a href=\"https://confidentialcomputing.io/\">Confidential Computing Consortium</a> (CCC) is a project community in the Linux Foundation\nthat already worked on\n<a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/85/2019/12/CCC_Overview.pdf\">Defining and Enabling Confidential Computing</a>.\nIn the <a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/85/2023/01/CCC-A-Technical-Analysis-of-Confidential-Computing-v1.3_Updated_November_2022.pdf\">Whitepaper</a>,\nthey provide a great motivation for the use of Confidential Computing:</p>\n<blockquote>\n<p>Data exists in three states: in transit, at rest, and in use. …Protecting sensitive data\nin all of its states is more critical than ever. Cryptography is now commonly deployed\nto provide both data confidentiality (stopping unauthorized viewing) and data integrity\n(preventing or detecting unauthorized changes). While techniques to protect data in transit\nand at rest are now commonly deployed, the third state - protecting data in use - is the new frontier.</p>\n</blockquote>\n<p>Confidential Computing aims to primarily solve the problem of <strong>protecting data in use</strong>\nby introducing a hardware-enforced Trusted Execution Environment (TEE).</p>\n<h2 id=\"trusted-execution-environments\">Trusted Execution Environments</h2>\n<p>For more than a decade, Trusted Execution Environments (TEEs) have been available in commercial\ncomputing hardware in the form of <a href=\"https://en.wikipedia.org/wiki/Hardware_security_module\">Hardware Security Modules</a>\n(HSMs) and <a href=\"https://www.iso.org/standard/50970.html\">Trusted Platform Modules</a> (TPMs). These\ntechnologies provide trusted environments for shielded computations. They can\nstore highly sensitive cryptographic keys and carry out critical cryptographic operations\nsuch as signing or encrypting data.</p>\n<p>TPMs are optimized for low cost, allowing them to be integrated into mainboards and act as a\nsystem's physical root of trust. To keep the cost low, TPMs are limited in scope, i.e., they\nprovide storage for only a few keys and are capable of just a small subset of cryptographic operations.</p>\n<p>In contrast, HSMs are optimized for high performance, providing secure storage for far\nmore keys and offering advanced physical attack detection mechanisms. Additionally, high-end HSMs\ncan be programmed so that arbitrary code can be compiled and executed. The downside\nis that they are very costly. A managed CloudHSM from AWS costs\n<a href=\"https://aws.amazon.com/cloudhsm/pricing/\">around $1.50 / hour</a> or ~$13,500 / year.</p>\n<p>In recent years, a new kind of TEE has gained popularity. Technologies like\n<a href=\"https://developer.amd.com/sev/\">AMD SEV</a>,\n<a href=\"https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html\">Intel SGX</a>,\nand <a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-trust-domain-extensions.html\">Intel TDX</a>\nprovide TEEs that are closely integrated with userspace. Rather than low-power or high-performance\ndevices that support specific use cases, these TEEs shield normal processes or virtual machines\nand can do so with relatively low overhead. These technologies each have different design goals,\nadvantages, and limitations, and they are available in different environments, including consumer\nlaptops, servers, and mobile devices.</p>\n<p>Additionally, we should mention\n<a href=\"https://www.arm.com/technologies/trustzone-for-cortex-a\">ARM TrustZone</a>, which is optimized\nfor embedded devices such as smartphones, tablets, and smart TVs, as well as\n<a href=\"https://aws.amazon.com/ec2/nitro/nitro-enclaves/\">AWS Nitro Enclaves</a>, which are only available\non <a href=\"https://aws.amazon.com/\">Amazon Web Services</a> and have a different threat model compared\nto the CPU-based solutions by Intel and AMD.</p>\n<p><a href=\"https://www.ibm.com/docs/en/linux-on-systems?topic=virtualization-secure-execution\">IBM Secure Execution for Linux</a>\nlets you run your Kubernetes cluster's nodes as KVM guests within a trusted execution environment on\nIBM Z series hardware. You can use this hardware-enhanced virtual machine isolation to\nprovide strong isolation between tenants in a cluster, with hardware attestation about the (virtual) node's integrity.</p>\n<h3 id=\"security-properties-and-feature-set\">Security properties and feature set</h3>\n<p>In the following sections, we will review the security properties and additional features\nthese new technologies bring to the table. Only some solutions will provide all properties;\nwe will discuss each technology in further detail in their respective section.</p>\n<p>The <strong>Confidentiality</strong> property ensures that information cannot be viewed while it is\nin use in the TEE. This provides us with the highly desired feature to secure\n<strong>data in use</strong>. Depending on the specific TEE used, both code and data may be protected\nfrom outside viewers. The differences in TEE architectures and how their use\nin a cloud native context are important considerations when designing end-to-end security\nfor sensitive workloads with a minimal <strong>Trusted Computing Base</strong> (TCB) in mind. CCC has recently\nworked on a <a href=\"https://confidentialcomputing.io/wp-content/uploads/sites/85/2023/01/Common-Terminology-for-Confidential-Computing.pdf\">common vocabulary and supporting material</a>\nthat helps to explain where confidentiality boundaries are drawn with the different TEE\narchitectures and how that impacts the TCB size.</p>\n<p>Confidentiality is a great feature, but an attacker can still manipulate\nor inject arbitrary code and data for the TEE to execute and, therefore, easily leak critical\ninformation. <strong>Integrity</strong> guarantees a TEE owner that neither code nor data can be\ntampered with while running critical computations.</p>\n<p><strong>Availability</strong> is a basic property often discussed in the context of information\nsecurity. However, this property is outside the scope of most TEEs. Usually, they can be controlled\n(shut down, restarted, …) by some higher level abstraction. This could be the CPU itself, the\nhypervisor, or the kernel. This is to preserve the overall system's availability,\nnot the TEE itself. When running in the cloud, availability is usually guaranteed by\nthe cloud provider in terms of Service Level Agreements (SLAs) and is not cryptographically enforceable.</p>\n<p>Confidentiality and Integrity by themselves are only helpful in some cases. For example,\nconsider a TEE running in a remote cloud. How would you know the TEE is genuine and running\nyour intended software? It could be an imposter stealing your data as soon as you send it over.\nThis fundamental problem is addressed by <strong>Attestability</strong>. Attestation allows us to verify\nthe identity, confidentiality, and integrity of TEEs based on cryptographic certificates issued\nfrom the hardware itself. This feature can also be made available to clients outside of the\nconfidential computing hardware in the form of remote attestation.</p>\n<p>TEEs can hold and process information that predates or outlives the trusted environment. That\ncould mean across restarts, different versions, or platform migrations. Therefore <strong>Recoverability</strong>\nis an important feature. Data and the state of a TEE need to be sealed before they are written\nto persistent storage to maintain confidentiality and integrity guarantees. The access to such\nsealed data needs to be well-defined. In most cases, the unsealing is bound to a TEE's identity.\nHence, making sure the recovery can only happen in the same confidential context.</p>\n<p>This does not have to limit the flexibility of the overall system.\n<a href=\"https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf\">AMD SEV-SNP's migration agent (MA)</a>\nallows users to migrate a confidential virtual machine to a different host system\nwhile keeping the security properties of the TEE intact.</p>\n<h2 id=\"feature-comparison\">Feature comparison</h2>\n<p>These sections of the article will dive a little bit deeper into the specific implementations,\ncompare supported features and analyze their security properties.</p>\n<h3 id=\"amd-sev\">AMD SEV</h3>\n<p>AMD's <a href=\"https://developer.amd.com/sev/\">Secure Encrypted Virtualization (SEV)</a> technologies\nare a set of features to enhance the security of virtual machines on AMD's server CPUs. SEV\ntransparently encrypts the memory of each VM with a unique key. SEV can also calculate a\nsignature of the memory contents, which can be sent to the VM's owner as an attestation that\nthe initial guest memory was not manipulated.</p>\n<p>The second generation of SEV, known as\n<a href=\"https://www.amd.com/system/files/TechDocs/Protecting%20VM%20Register%20State%20with%20SEV-ES.pdf\">Encrypted State</a>\nor SEV-ES, provides additional protection from the hypervisor by encrypting all\nCPU register contents when a context switch occurs.</p>\n<p>The third generation of SEV,\n<a href=\"https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf\">Secure Nested Paging</a>\nor SEV-SNP, is designed to prevent software-based integrity attacks and reduce the risk associated with\ncompromised memory integrity. The basic principle of SEV-SNP integrity is that if a VM can read\na private (encrypted) memory page, it must always read the value it last wrote.</p>\n<p>Additionally, by allowing the guest to obtain remote attestation statements dynamically,\nSNP enhances the remote attestation capabilities of SEV.</p>\n<p>AMD SEV has been implemented incrementally. New features and improvements have been added with\neach new CPU generation. The Linux community makes these features available as part of the KVM hypervisor\nand for host and guest kernels. The first SEV features were discussed and implemented in 2016 - see\n<a href=\"https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/kaplan\">AMD x86 Memory Encryption Technologies</a>\nfrom the 2016 Usenix Security Symposium. The latest big addition was\n<a href=\"https://www.phoronix.com/news/AMD-SEV-SNP-Arrives-Linux-5.19\">SEV-SNP guest support in Linux 5.19</a>.</p>\n<p><a href=\"https://azure.microsoft.com/en-us/updates/azureconfidentialvm/\">Confidential VMs based on AMD SEV-SNP</a>\nare available in Microsoft Azure since July 2022. Similarly, Google Cloud Platform (GCP) offers\n<a href=\"https://cloud.google.com/compute/confidential-vm/docs/about-cvm\">confidential VMs based on AMD SEV-ES</a>.</p>\n<h3 id=\"intel-sgx\">Intel SGX</h3>\n<p>Intel's\n<a href=\"https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html\">Software Guard Extensions</a>\nhas been available since 2015 and were introduced with the Skylake architecture.</p>\n<p>SGX is an instruction set that enables users to create a protected and isolated process called\nan <em>enclave</em>. It provides a reverse sandbox that protects enclaves from the operating system,\nfirmware, and any other privileged execution context.</p>\n<p>The enclave memory cannot be read or written from outside the enclave, regardless of\nthe current privilege level and CPU mode. The only way to call an enclave function is\nthrough a new instruction that performs several protection checks. Its memory is encrypted.\nTapping the memory or connecting the DRAM modules to another system will yield only encrypted\ndata. The memory encryption key randomly changes every power cycle. The key is stored\nwithin the CPU and is not accessible.</p>\n<p>Since the enclaves are process isolated, the operating system's libraries are not usable as is;\ntherefore, SGX enclave SDKs are required to compile programs for SGX. This also implies applications\nneed to be designed and implemented to consider the trusted/untrusted isolation boundaries.\nOn the other hand, applications get built with very minimal TCB.</p>\n<p>An emerging approach to easily transition to process-based confidential computing\nand avoid the need to build custom applications is to utilize library OSes. These OSes\nfacilitate running native, unmodified Linux applications inside SGX enclaves.\nA library OS intercepts all application requests to the host OS and processes them securely\nwithout the application knowing it's running a TEE.</p>\n<p>The 3rd generation Xeon CPUs (aka Ice Lake Server - &quot;ICX&quot;) and later generations did switch to using a technology called\n<a href=\"https://www.intel.com/content/www/us/en/developer/articles/news/runtime-encryption-of-memory-with-intel-tme-mk.html\">Total Memory Encryption - Multi-Key</a>\n(TME-MK) that uses AES-XTS, moving away from the\n<a href=\"https://eprint.iacr.org/2016/204.pdf\">Memory Encryption Engine</a>\nthat the consumer and Xeon E CPUs used. This increased the possible\n<a href=\"https://sgx101.gitbook.io/sgx101/sgx-bootstrap/enclave#enclave-page-cache-epc\">enclave page cache</a>\n(EPC) size (up to 512GB/CPU) and improved performance. More info\nabout SGX on multi-socket platforms can be found in the\n<a href=\"https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/supporting-intel-sgx-on-mulit-socket-platforms.pdf\">Whitepaper</a>.</p>\n<p>A <a href=\"https://ark.intel.com/content/www/us/en/ark/search/featurefilter.html?productType=873\">list of supported platforms</a>\nis available from Intel.</p>\n<p>SGX is available on\n<a href=\"https://azure.microsoft.com/de-de/updates/intel-sgx-based-confidential-computing-vms-now-available-on-azure-dedicated-hosts/\">Azure</a>,\n<a href=\"https://www.alibabacloud.com/help/en/elastic-compute-service/latest/build-an-sgx-encrypted-computing-environment\">Alibaba Cloud</a>,\n<a href=\"https://cloud.ibm.com/docs/bare-metal?topic=bare-metal-bm-server-provision-sgx\">IBM</a>, and many more.</p>\n<h3 id=\"intel-tdx\">Intel TDX</h3>\n<p>Where Intel SGX aims to protect the context of a single process,\n<a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-trust-domain-extensions.html\">Intel's Trusted Domain Extensions</a>\nprotect a full virtual machine and are, therefore, most closely comparable to AMD SEV.</p>\n<p>As with SEV-SNP, guest support for TDX was <a href=\"https://www.phoronix.com/news/Intel-TDX-For-Linux-5.19\">merged in Linux Kernel 5.19</a>.\nHowever, hardware support will land with <a href=\"https://en.wikipedia.org/wiki/Sapphire_Rapids\">Sapphire Rapids</a> during 2023:\n<a href=\"https://www.alibabacloud.com/help/en/elastic-compute-service/latest/build-a-tdx-confidential-computing-environment\">Alibaba Cloud provides</a>\ninvitational preview instances, and\n<a href=\"https://techcommunity.microsoft.com/t5/azure-confidential-computing/preview-introducing-dcesv5-and-ecesv5-series-confidential-vms/ba-p/3800718\">Azure has announced</a>\nits TDX preview opportunity.</p>\n<h2 id=\"overhead-analysis\">Overhead analysis</h2>\n<p>The benefits that Confidential Computing technologies provide via strong isolation and enhanced\nsecurity to customer data and workloads are not for free. Quantifying this impact is challenging and\ndepends on many factors: The TEE technology, the benchmark, the metrics, and the type of workload\nall have a huge impact on the expected performance overhead.</p>\n<p>Intel SGX-based TEEs are hard to benchmark, as <a href=\"https://arxiv.org/pdf/2205.06415.pdf\">shown</a>\n<a href=\"https://www.ibr.cs.tu-bs.de/users/mahhouk/papers/eurosec2021.pdf\">by</a>\n<a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3533737.3535098\">different papers</a>. The chosen SDK/library\nOS, the application itself, as well as the resource requirements (especially large memory requirements)\nhave a huge impact on performance. A single-digit percentage overhead can be expected if an application\nis well suited to run inside an enclave.</p>\n<p>Confidential virtual machines based on AMD SEV-SNP require no changes to the executed program\nand operating system and are a lot easier to benchmark. A\n<a href=\"https://community.amd.com/t5/business/microsoft-azure-confidential-computing-powered-by-3rd-gen-epyc/ba-p/497796\">benchmark from Azure and AMD</a>\nshows that SEV-SNP VM overhead is &lt;10%, sometimes as low as 2%.</p>\n<p>Although there is a performance overhead, it should be low enough to enable real-world workloads\nto run in these protected environments and improve the security and privacy of our data.</p>\n<h2 id=\"confidential-computing-compared-to-fhe-zkp-and-mpc\">Confidential Computing compared to FHE, ZKP, and MPC</h2>\n<p>Fully Homomorphic Encryption (FHE), Zero Knowledge Proof/Protocol (ZKP), and Multi-Party\nComputations (MPC) are all a form of encryption or cryptographic protocols that offer\nsimilar security guarantees to Confidential Computing but do not require hardware support.</p>\n<p>Fully (also partially and somewhat) homomorphic encryption allows one to perform\ncomputations, such as addition or multiplication, on encrypted data. This provides\nthe property of encryption in use but does not provide integrity protection or attestation\nlike confidential computing does. Therefore, these two technologies can <a href=\"https://confidentialcomputing.io/2023/03/29/confidential-computing-and-homomorphic-encryption/\">complement to each other</a>.</p>\n<p>Zero Knowledge Proofs or Protocols are a privacy-preserving technique (PPT) that\nallows one party to prove facts about their data without revealing anything else about\nthe data. ZKP can be used instead of or in addition to Confidential Computing to protect\nthe privacy of the involved parties and their data. Similarly, Multi-Party Computation\nenables multiple parties to work together on a computation, i.e., each party provides\ntheir data to the result without leaking it to any other parties.</p>\n<h2 id=\"use-cases-of-confidential-computing\">Use cases of Confidential Computing</h2>\n<p>The presented Confidential Computing platforms show that both the isolation of a single container\nprocess and, therefore, minimization of the trusted computing base and the isolation of a\n``\nfull virtual machine are possible. This has already enabled a lot of interesting and secure\nprojects to emerge:</p>\n<h3 id=\"confidential-containers\">Confidential Containers</h3>\n<p><a href=\"https://github.com/confidential-containers\">Confidential Containers</a> (CoCo) is a\nCNCF sandbox project that isolates Kubernetes pods inside of confidential virtual machines.</p>\n<p>CoCo can be installed on a Kubernetes cluster with an operator.\nThe operator will create a set of runtime classes that can be used to deploy\npods inside an enclave on several different platforms, including\nAMD SEV, Intel TDX, Secure Execution for IBM Z, and Intel SGX.</p>\n<p>CoCo is typically used with signed and/or encrypted container images\nwhich are pulled, verified, and decrypted inside the enclave.\nSecrets, such as image decryption keys, are conditionally provisioned\nto the enclave by a trusted Key Broker Service that validates the\nhardware evidence of the TEE prior to releasing any sensitive information.</p>\n<p>CoCo has several deployment models. Since the Kubernetes control plane\nis outside the TCB, CoCo is suitable for managed environments. CoCo can\nbe run in virtual environments that don't support nesting with the help of an\nAPI adaptor that starts pod VMs in the cloud. CoCo can also be run on\nbare metal, providing strong isolation even in multi-tenant environments.</p>\n<h3 id=\"managed-confidential-kubernetes\">Managed confidential Kubernetes</h3>\n<p><a href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-node-pool-aks\">Azure</a> and\n<a href=\"https://cloud.google.com/blog/products/identity-security/announcing-general-availability-of-confidential-gke-nodes\">GCP</a>\nboth support the use of confidential virtual machines as worker nodes for their managed Kubernetes offerings.</p>\n<p>Both services aim for better workload protection and security guarantees by enabling memory encryption\nfor container workloads. However, they don't seek to fully isolate the cluster or workloads against\nthe service provider or infrastructure. Specifically, they don't offer a dedicated confidential control\nplane or expose attestation capabilities for the confidential cluster/nodes.</p>\n<p>Azure also enables\n<a href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-nodes-aks-overview\">Confidential Containers</a>\nin their managed Kubernetes offering. They support the creation based on\n<a href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-containers-enclaves\">Intel SGX enclaves</a>\nand <a href=\"https://techcommunity.microsoft.com/t5/azure-confidential-computing/microsoft-introduces-preview-of-confidential-containers-on-azure/ba-p/3410394\">AMD SEV-based VMs</a>.</p>\n<h3 id=\"constellation\">Constellation</h3>\n<p><a href=\"https://github.com/edgelesssys/constellation\">Constellation</a> is a Kubernetes engine that aims to\nprovide the best possible data security. Constellation wraps your entire Kubernetes cluster into\na single confidential context that is shielded from the underlying cloud infrastructure. Everything\ninside is always encrypted, including at runtime in memory. It shields both the worker and control\nplane nodes. In addition, it already integrates with popular CNCF software such as Cilium for\nsecure networking and provides extended CSI drivers to write data securely.</p>\n<h3 id=\"occlum-and-gramine\">Occlum and Gramine</h3>\n<p><a href=\"https://occlum.io/\">Occlum</a> and <a href=\"https://gramineproject.io/\">Gramine</a> are examples of open source\nlibrary OS projects that can be used to run unmodified applications in SGX enclaves. They\nare member projects under the CCC, but similar projects and products maintained by companies\nalso exist. With these libOS projects, existing containerized applications can be\neasily converted into confidential computing enabled containers. Many curated prebuilt\ncontainers are also available.</p>\n<h2 id=\"where-are-we-today-vendors-limitations-and-foss-landscape\">Where are we today? Vendors, limitations, and FOSS landscape</h2>\n<p>As we hope you have seen from the previous sections, Confidential Computing is a powerful new concept\nto improve security, but we are still in the (early) adoption phase. New products are\nstarting to emerge to take advantage of the unique properties.</p>\n<p>Google and Microsoft are the first major cloud providers to have confidential offerings that\ncan run unmodified applications inside a protected boundary.\nStill, these offerings are limited to compute, while end-to-end solutions for confidential\ndatabases, cluster networking, and load balancers have to be self-managed.</p>\n<p>These technologies provide opportunities to bring even the most\nsensitive workloads into the cloud and enables them to leverage all the\ntools in the CNCF landscape.</p>\n<h2 id=\"call-to-action\">Call to action</h2>\n<p>If you are currently working on a high-security product that struggles to run in the\npublic cloud due to legal requirements or are looking to bring the privacy and security\nof your cloud-native project to the next level: Reach out to all the great projects\nwe have highlighted! Everyone is keen to improve the security of our ecosystem, and you can\nplay a vital role in that journey.</p>\n<ul>\n<li><a href=\"https://github.com/confidential-containers\">Confidential Containers</a></li>\n<li><a href=\"https://github.com/edgelesssys/constellation\">Constellation: Always Encrypted Kubernetes</a></li>\n<li><a href=\"https://occlum.io/\">Occlum</a></li>\n<li><a href=\"https://gramineproject.io/\">Gramine</a></li>\n<li>CCC also maintains a <a href=\"https://confidentialcomputing.io/projects/\">list of projects</a></li>\n</ul>","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://kubernetes.io/blog/2023/07/06/confidential-kubernetes/","SourceName":"Kubernetes"}},{"node":{"ID":4091,"Title":"Making the boat faster: Advantages of embedding services and training in software sales","Description":"","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/advantages-embedding-services-training-software-sales","SourceName":"Elastic"}},{"node":{"ID":4094,"Title":"How tribal nations can use data to build exceptional customer experiences","Description":"","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/tribal-nations-data-customer-experiences","SourceName":"Elastic"}},{"node":{"ID":4095,"Title":"Elastic’s Courtney Wilburn on the 3 ways leaders can inspire their teams","Description":"","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/culture-courtney-wilburn-3-ways-leaders-can-inspire-teams","SourceName":"Elastic"}},{"node":{"ID":4096,"Title":"Monitor Windows event logs with Datadog","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/monitor-windows-event-logs-with-datadog/event_logs_hero.png\" width=\"100%\"/>Whenever an event occurs on your Windows machine, the operating system records an event log that includes details about the nature of the event (e.g., critical runtime error) or security identifiers (for audit events). Windows event logs not only record system and application activity but also user actions and background processes, making them an invaluable tool for monitoring the security and health of your systems.In this guide, we’ll cover the following topics:","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/monitor-windows-event-logs-with-datadog/","SourceName":"Datadog"}},{"node":{"ID":4147,"Title":"DevOps Solution Brief","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/resources/devops-solution-brief/solutions/devopssolutionbrief_shortened.png\" width=\"100%\"/>Adopting DevOps cultures has enabled organizations to increase development velocity, quickly respond to market needs, and drive efficiency amongst teams.In this solution brief, you’ll learn how Datadog supports helps organizations adopt or transform DevOps cultures and enables you to:Foster cultures of observability, collaboration, and data sharing Elevate development velocity and agility Easily implement automation into monitoring strategies Gain organization-wide visibility","PublishedAt":"2023-07-06 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/resources/devops-solution-brief/","SourceName":"Datadog"}},{"node":{"ID":4085,"Title":"AWS Application Migration Service Major Updates: Global View, Import and Export from Local Disk, and Additional Post-launch Actions","Description":"AWS Application Migration Service simplifies, expedites, and reduces the cost of migrating your applications to AWS. It allows you to lift and shift many physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows.&nbsp;You can minimize time-intensive, error-prone manual processes by automating replication and conversion of your source servers from physical, […]","PublishedAt":"2023-07-05 17:12:55+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/aws-application-migration-service-major-updates-global-view-import-and-export-from-local-disk-and-additional-post-launch-actions/","SourceName":"AWS"}},{"node":{"ID":4086,"Title":"Deploy Consul cluster peering locally with Minikube","Description":"Use Minikube to create multiple Kubernetes clusters with Consul and test cluster peering configurations in your local development environment.","PublishedAt":"2023-07-05 16:00:00+00:00","OriginURL":"https://www.hashicorp.com/blog/deploy-consul-cluster-peering-locally-with-minikube","SourceName":"HashiCorp"}},{"node":{"ID":4088,"Title":"Vector search in Elasticsearch: The rationale behind the design","Description":"","PublishedAt":"2023-07-05 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/vector-search-elasticsearch-rationale","SourceName":"Elastic"}},{"node":{"ID":4084,"Title":"Developers use AI tools, they just don’t trust them (Ep. 586)","Description":"<p>The home team shares what our Developer Survey respondents said about AI, spicy opinions about recent Apple unveilings, and an update on crypto regulation.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/07/04/developers-use-ai-tools-they-just-dont-trust-them-ep-586/\">Developers use AI tools, they just don’t trust them (Ep. 586)</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-07-04 04:40:00+00:00","OriginURL":"https://stackoverflow.blog/2023/07/04/developers-use-ai-tools-they-just-dont-trust-them-ep-586/","SourceName":"Stack Overflow"}},{"node":{"ID":4083,"Title":"HashiCorp State of Cloud Strategy Survey 2023: Inside the maturity model","Description":"HashiCorp’s 2023 State of Cloud Strategy Survey debuts a new cloud-maturity model, which reveals how more-mature organizations enjoy better business outcomes.","PublishedAt":"2023-07-03 18:30:00+00:00","OriginURL":"https://www.hashicorp.com/blog/hashicorp-state-of-cloud-strategy-survey-2023-inside-the-maturity-model","SourceName":"HashiCorp"}},{"node":{"ID":4080,"Title":"AWS Week in Review – Generative AI with LLM Hands-on Course, Amazon SageMaker Data Wrangler Updates, and More – July 3, 2023","Description":"In last week’s AWS Week in Review post, Danilo mentioned that it’s summer in London. Well, I’m based in Singapore, and it’s mostly summer here. But, June is a special month here as it marks the start of durian season. Starting next week, I’ll be travelling to Thailand, Malaysia, and the Philippines. But before I […]","PublishedAt":"2023-07-03 18:20:12+00:00","OriginURL":"https://aws.amazon.com/blogs/aws/aws-week-in-review-generative-ai-with-llm-hands-on-course-amazon-sagemaker-data-wrangler-updates-and-more-july-3-2023/","SourceName":"AWS"}},{"node":{"ID":4081,"Title":"Do large language models know what they are talking about?","Description":"<p>Large language models seem to possess the ability to reason intelligently, but does that mean they actually know things?</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/07/03/do-large-language-models-know-what-they-are-talking-about/\">Do large language models know what they are talking about?</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-07-03 16:01:22+00:00","OriginURL":"https://stackoverflow.blog/2023/07/03/do-large-language-models-know-what-they-are-talking-about/","SourceName":"Stack Overflow"}},{"node":{"ID":4079,"Title":"Pruning incoming log volumes with Elastic","Description":"<p><em>To log or not to log?</em> has always been a difficult question that software engineers still struggle with, to the detriment of site reliability engineering, or SRE, colleagues. Developers don't always get the level or context of the warnings and errors they capture in applications right and often log messages that may not always be helpful for SREs. I can admit to being one of those developers! This confusion often leads to a flood of events being ingested into logging platforms, making application monitoring and issue investigation for SREs feel a bit like this: </p>\n<p><img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt79f8c650151adcf0/649c630d63cca619c8d295bd/i_love_lucy.gif\" alt=\"![I Love Lucy Conveyor Belt Gif](./images/1.gif)\" /></p>\n<p>Source: <a href=\"https://giphy.com/explore/i-love-lucy-chocolate\">GIPHY</a></p>\n<p>When looking to reduce your log volume, it is possible to drop information on two dimensions: fields within an event, or the entire event itself. Removing dimensions of interest ensures we can focus on known events of interest and unknown events that may be of interest. </p>\n<p><img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blteba8436b74285018/649c69b49c69d8ce93c1cf33/event_vs_field.jpeg\" alt=\"![Log event versus field](./images/event-vs-field.jpg)\" />\nIn this blog, we will discuss various approaches for dropping known irrelevant events and fields from logs via various collectors. Specifically we will focus on <a href=\"https://www.elastic.co/beats/\">Beats</a>, <a href=\"https://www.elastic.co/logstash/\">Logstash</a>, <a href=\"https://www.elastic.co/guide/en/fleet/current/fleet-overview.html\">Elastic Agent</a>, <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html\">Ingest Pipelines</a>, and filtering with <a href=\"https://opentelemetry.io/docs/collector/\">OpenTelemetry Collectors</a>.</p>\n<h2 id=\"beats\">Beats</h2>\n<p><a href=\"https://www.elastic.co/beats/\">Beats</a> are a family of lightweight shippers that allows for the forwarding of events from a particular source. They are commonly used to ingest the events from a source into not just Elasticsearch, but also <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/configuring-output.html\">other outputs such as Logstash, Kafka, or Redis as shown in the Filebeat documentation</a>. There are six types of Beat available, which are summarized <a href=\"https://www.elastic.co/beats/\">here</a>.</p>\n<p>Our example will focus on Filebeat specifically, but both drop processors discussed here apply to all Beats. After following the <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html\">quick start guide within the Filebeat documentation</a>, you will have a running process using configuration file <code>filebeat.yml</code> dictating which log files you are monitoring with Filebeat from <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html#filebeat-input-types\">any of the supported input types</a>. Hopefully your configuration specifies a series of inputs in a format similar to the below:</p>\n<pre><code class=\"yml language-yml\">filebeat.inputs:\n- type: filestream\n  id: my-logging-app\n  paths:\n    - /var/log/*.log\n</code></pre>\n<p>Filebeat has many options available to configure, of which a full listing is given in the <a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html\"><code>filebeat.reference.yml</code></a> in the documentation. However, it is the <code>drop_event</code> and <code>drop_fields</code> processors in particular that can help us exclude unhelpful messages and isolate only the relevant fields in a given event respectively.\n&nbsp;\nWhen using the <code>drop_event</code> processor, you need to make sure at least one <a href=\"(https://www.elastic.co/guide/en/beats/filebeat/current/defining-processors.html#conditions\">condition</a>) is present to receive the messages you want; otherwise, if no condition is specified, the processor will drop all messages. If no condition is specified in the <code>drop_event</code> processor, all events will be dropped. Please ensure at least one condition is present to ensure you receive the messages you want. \nFor example, if we are not interested in HTTP requests against the <code>/profile</code> endpoint, we can amend the configuration to use the following condition:</p>\n<pre><code class=\"yml language-yml\">filebeat.inputs:\n- type: filestream\n  id: my-logging-app\n  paths:\n    - /var/tmp/other.log\n    - /var/log/*.log\nprocessors:\n  - drop_event:\n      when:\n          and:\n            - equals:\n              url.scheme: http\n            - equals:\n              url.path: /profile\n</code></pre>\n<p>Meanwhile, the <code>drop_fields</code> processor will drop the specified fields, except for the <code>@timestamp</code> and <code>type</code> fields, if the specified condition is fulfilled. Similar to the <code>drop_event</code> processor, if the condition is missing then the fields will always be dropped. If we wanted to exclude the error message field for successful HTTP requests, we could configure a processor similar to the below:</p>\n<pre><code class=\"yml language-yml\">filebeat.inputs:\n- type: filestream\n  id: my-logging-app\n  paths:\n    - /var/tmp/other.log\n    - /var/log/*.log\nprocessors:\n  - drop_fields:\n      when:\n          and:\n            - equals:\n              url.scheme: http\n            - equals:\n              http.response.status_code: 200\n          fields: [\"event.message\"]\n          ignore_missing: false\n</code></pre>\n<p>When dropping fields, there is always a possibility that the field might not exist on a given log message. If the field does not exist in all events being processed in Filebeat, an error will be raised if <code>ignore_missing</code> is specified as <code>true</code> rather than the default value of <code>false</code>.</p>\n<h2 id=\"logstashfiltering\">Logstash filtering</h2>\n<p><a href=\"https://www.elastic.co/logstash/\">Logstash</a> is a free and open data processing pipeline tool that allows you to ingest, transform, and output data between a myriad of sources. It sits within the Extract, Transform, and Load (or ETL) domain. With the prior discussion of Beats, it's important to note that usage of Logstash over Beats would be recommended if you want to centralize the transformation logic. Meanwhile, Beats or Elastic Agent allow dropping events early, which can reduce network traffic requirements early. \nLogstash provides a variety of transformation plugins out of the box that can be used to format and transform events from any source that Logstash is connected to. A typical pipeline within the Logstash configuration file <code>logstash.yml</code> contains three main sections: </p>\n<ol>\n<li><code>input</code> denotes the source of data to the pipeline.</li>\n<li><code>filter</code> contains the relevant data transformation logic.</li>\n<li>The target for the transformed data is configured in the <code>output</code> attribute.\nTo prevent events from making it to the output, the <a href=\"https://www.elastic.co/guide/en/logstash/current/plugins-filters-drop.html\">drop filter plugin will drop any events that meet the stated condition</a>. A typical example of reading from an input file, dropping INFO level events, and outputting to Elasticsearch is as follows:</li>\n</ol>\n<pre><code class=\"yml language-yml\">input {\n  file {\n    id =&gt; \"my-logging-app\"\n    path =&gt; [ \"/var/tmp/other.log\", \"/var/log/*.log\" ]\n  }\n}\nfilter {\n  if [url.scheme] == \"http\" &amp;&amp; [url.path] == \"/profile\" {\n    drop {\n      percentage =&gt; 80\n    }\n  }\n}\noutput {\n  elasticsearch {\n        hosts =&gt; \"https://my-elasticsearch:9200\"\n        data_stream =&gt; \"true\"\n    }\n}\n</code></pre>\n<p>One of the lesser-known options of this filter is the ability to configure a drop rate using the <a href=\"https://www.elastic.co/guide/en/logstash/current/plugins-filters-drop.html#plugins-filters-drop-percentage\"><code>percentage</code></a> option. One of the scary things about filtering out log events is the fear that you will inadvertently drop unknown but relevant entries that could be useful in an outage situation. Or, there is the possibility that your software sends a large volume of messages that could flood your instance, take up vital hot storage, and increase your costs. The percentage attribute covers this case by allowing a subset of the events to be ingested into Elasticsearch, which can address these challenges. In our example above, we ingest 20% of messages matching the criteria to Elasticsearch.\nSimilar to the <code>drop_fields</code> processor found in Beats, Logstash has a <code>remove_field</code> filter for removing individual fields. Although these can be used within many Logstash plugins, they are commonly used within the <code>mutate</code> plugin to transform events, similar to the below:</p>\n<pre><code class=\"yml language-yml\"># Input configuration omitted\nfilter {\n  if [url.scheme] == \"http\" &amp;&amp; [http.response.status_code] == 200 {\n    drop {\n      percentage =&gt; 80\n    }\n    mutate {\n      remove_field: [ \"event.message\" ]\n    }\n  }\n}\n# Output configuration omitted\n</code></pre>\n<p>Just like our Beats example, this will remove <code>event.message</code> from the events that are retained from the drop filter.</p>\n<h2 id=\"agent\">Agent</h2>\n<p>Elastic Agent is a single agent for logs, metrics, and security data that can execute on your host and send events from multiple services and infrastructure to Elasticsearch.\nSimilar to Beats, you can use the <a href=\"https://www.elastic.co/guide/en/fleet/current/drop_event-processor.html\"><code>drop_event</code></a> and <a href=\"https://www.elastic.co/guide/en/fleet/current/drop_fields-processor.html\"><code>drop_fields</code></a> processors in any integrations that support processors. For standalone installations, you should specify the processors within your <code>elastic-agent.yml</code> config. When using Fleet, the processing transforms are normally specified when configuring the integration under the <em>Advanced options</em> pop-out section, as shown below:</p>\n<p><img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/bltaf0b98692dc7f215/649c6d8befa20db5d04bd8ba/elastic-agent-kafka-processor.png\" alt=\"![Elastic Agent Kafka Integration Sample Processor](./images/elastic-agent-kafka-processor.png)\" /></p>\n<p>Comparing the above example with our Beats example, you'll notice they are using the same YAML-based format for both processors. There are some limitations to be aware of when using Elastic Agent processors, which are covered in the <a href=\"https://www.elastic.co/guide/en/fleet/current/elastic-agent-processor-configuration.html#limitations\">Fleet documentation</a>. If you are unsure if processing data via Elastic Agent processors is the right thing for your use case, check out this <a href=\"https://www.elastic.co/guide/en/fleet/current/elastic-agent-processor-configuration.html#processing-options\">handy matrix</a>.  </p>\n<h2 id=\"ingestpipelines\">Ingest pipelines</h2>\n<p>The Elastic Agent processors discussed in the previous section will process raw event data, meaning they execute before ingest pipelines. As a result, when using both approaches, proceed with caution as removing or altering fields expected by an ingest pipeline can cause the pipeline to break.\nAs covered in the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html#create-manage-ingest-pipelines\">Create and manage pipeline documentation</a>, new pipelines can be created either within the <strong>Stack Management > Ingest Pipelines</strong> screen or via the <code>_ingest</code> API, which we will use.\nJust like the other tools covered in this piece, the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/drop-processor.html\"><code>drop</code> processor</a> will allow for any event that meets the required condition to be dropped. It's also the case that if no condition is specified, all events coming through will be dropped. What is different is that the conditional logic is written using <a href=\"https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-guide.html\">Painless</a>, a Java-like scripting language, rather than the YAML syntax we have used previously:</p>\n<pre><code class=\"yml language-yml\">PUT _ingest/pipeline/my-logging-app-pipeline\n{\n  \"description\": \"Event and field dropping for my-logging-app\",\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\" : \"Drop event\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.url?.path == '/profile'\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"remove\": {\n        \"description\" : \"Drop field\",\n        \"field\" : \"event.message\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.http?.response?.status_code == 200\",\n        \"ignore_failure\": false\n      }\n    }\n  ]\n}\n</code></pre>\n<p>The <a href=\"https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-ingest-processor-context.html\"><code>ctx</code> variable</a> is a map representation of the fields within the document coming through the pipeline, meaning our example will compare the values of the <code>url.scheme</code> and <code>http.response.status_code</code> fields. JavaScript developers will recognize the <a href=\"https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-reference.html#null-safe-operator\"><code>?</code> denoted null safe operator</a>, which performs not null checks against the field access.\nAs visible in the second processor in the above example, the use of Painless conditional logic is also relevant to the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html\"><code>remove</code> processor</a>. This processor will drop the specified fields from the event when they match the specified condition.\nOne of the capabilities that give ingest processors an edge over the other approaches is the ability to specify failure processors, either on the pipeline or a specified processor. Although Beat's does have an <code>ignore_missing</code> option as discussed previously, Ingest Processors allow us to add exception handling such as adding error messages to give details of the processor exception:</p>\n<pre><code class=\"yml language-yml\">PUT _ingest/pipeline/my-logging-app-pipeline\n{\n  \"description\": \"Event and field dropping for my-logging-app with failures\",\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\" : \"Drop event\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.url?.path == '/profile'\",\n        \"ignore_failure\": true\n      }\n    },\n    {\n      \"remove\": {\n        \"description\" : \"Drop field\",\n        \"field\" : \"event.message\",\n        \"if\": \"ctx?.url?.scheme == 'http' &amp;&amp; ctx?.http?.response?.status_code == 200\",\n        \"ignore_failure\": false\n      }\n    }\n  ],\n  \"on_failure\": [\n    {\n      \"set\": {\n        \"description\": \"Set 'ingest.failure.message'\",\n        \"field\": \"ingest.failure.message\",\n        \"value\": \"Ingestion issue\"\n        }\n      }\n  ]\n}\n</code></pre>\n<p>The pipeline can then be used on a <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#add-pipeline-to-indexing-request\">single indexing request</a>, <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#set-default-pipeline\">set as the default pipeline</a> for an index, or even used alongside <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#pipelines-for-beats\">Beats</a> and <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html#pipelines-for-fleet-elastic-agent\">Elastic Agent</a>.</p>\n<h2 id=\"opentelemetrycollectors\">OpenTelemetry collectors</h2>\n<p><a href=\"https://opentelemetry.io/docs/collector/transforming-telemetry/\">OpenTelemetry</a>, or OTel, is an open standard that provides APIs, tooling, and integrations to enable the capture of telemetry data such as logs, metrics, and traces from applications. Application developers commonly use the OpenTelemetry agent for their programming language of choice to send trace data and metrics directly to the Elastic Stack, as Elastic supports the OpenTelemetry protocol (OTLP).\nIn some cases, having every application send behavioral information directly to the observability platform may be unwise. Large enterprise ecosystems may have centralized observability capabilities or may run large microservice ecosystems where adopting a standard tracing practice can be difficult. However, the sanitization of events and traces is also challenging as the number of applications and services grows. These are the situations where using one or more collectors as a router of data to the Elastic Stack makes sense.\nAs demonstrated in the <a href=\"https://opentelemetry.io/docs/collector/configuration/\">OpenTelemetry documentation</a> and the <a href=\"https://www.elastic.co/guide/en/apm/guide/current/open-telemetry-direct.html#connect-open-telemetry-collector\">example collector in the Elastic APM documentation</a>, the basic configuration for an OTel collector has four main sections:</p>\n<ol>\n<li><code>receivers</code> that define the sources of data, which can be push or pull-based.</li>\n<li><code>processors</code> that can filter or transform the received data before export, which is what we are interested in doing.</li>\n<li><code>exporters</code> which define how the data is sent to the final destination, in this case Elastic!</li>\n<li>A <code>service</code> section to define the components enabled in the collector that is needed for the other elements.\nDropping events and fields can be achieved using the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor\"><code>filter</code></a> and <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor\"><code>attributes</code></a> processors, respectively, in the collector config. A selection of examples of both filters is shown below:</li>\n</ol>\n<pre><code class=\"yml language-yml\">receivers: \n  filelog:\n    include: [ /var/tmp/other.log, /var/log/*.log ]\nprocessors: \n  filter/denylist:\n    error_mode: ignore\n    logs:\n      log_record:\n        - 'url.scheme == \"info\"'\n        - 'url.path == \"/profile\"'\n        - 'http.response.status_code == 200'\n  attributes/errors:\n    actions:\n      - key: error.message\n        action: delete\n  memory_limiter:\n    check_interval: 1s\n    limit_mib: 2000\n  batch:\nexporters:\n  # Exporters configuration omitted \nservice:\n  pipelines:\n    # Pipelines configuration omitted\n</code></pre>\n<p>The <code>filter</code> processor applied to the telemetry type (logs, metrics, or traces) will drop the event if it matches any of the specified conditions. Meanwhile, the <code>attributes</code> processor applied to our error fields will delete the <code>error.message</code> attribute from all events. The <code>pattern</code> attribute can also be used in place of the <code>key</code> option to remove fields matching a specified regular expression. Field deletion based on conditions as we have done in our Beats, Logstash, and ingest pipeline examples is not part of the specification. However, an alternative would be to use the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor\"><code>transform</code> processor</a> to specify a complex transformation to set the value of a field and then delete. </p>\n<h2 id=\"conclusions\">Conclusions</h2>\n<p>The aim of the DevOps movement is to align the processes and practices of software engineering and SRE. That includes working together to ensure that relevant logs, metrics, and traces are sent from applications to our Observability platform.\nAs we have seen first-hand with <a href=\"https://www.elastic.co/beats/\">Beats</a>, <a href=\"https://www.elastic.co/logstash/\">Logstash</a>, <a href=\"https://www.elastic.co/guide/en/fleet/current/fleet-overview.html\">Elastic Agent</a>, <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html\">Ingest Pipelines</a>, and <a href=\"https://opentelemetry.io/docs/collector/\">OTel collectors</a>, the approaches for dropping events and individual fields vary according to the tool used.\nYou may be wondering, which option is right for you? </p>\n<ol>\n<li>If the overhead of sending large messages over the network is a concern, transforming closer to the source using Beats or Logstash is the better option. If you're looking to minimize the system resources used in your collection and transformation, Beats may be preferred over Logstash as they have a small footprint.</li>\n<li>For centralizing transformation logic to apply to many application logs, using processors in an OTel collector may be the right approach.</li>\n<li>If you want to make use of centrally managed ingestion and transformation policies with popular services and systems such as Kafka, Nginx, or AWS, using Elastic Agent with Fleet is recommended.</li>\n<li>Ingest pipelines are great for transforming events at ingestion if you are less concerned about network overhead and would like your logic centralized within Elasticsearch. \nAlthough not covered here, other techniques such as runtime fields, index level compression, and <code>_synthetic_source</code> usage can also be used to reduce disk storage requirements and CPU overhead. If your favorite way to drop events or fields is not listed here, do let us know!</li>\n</ol>\n<h2 id=\"resources\">Resources</h2>\n<ol>\n<li><a href=\"https://www.elastic.co/beats/\">Elastic Beats</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html\">Filebeat | Filter and enhance data with processors</a></li>\n<li><a href=\"https://www.elastic.co/logstash/\">Logstash</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/logstash/current/filter-plugins.html\">Logstash | Filter plugins</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/fleet/current/fleet-overview.html\">Elastic Agent</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/fleet/current/elastic-agent-processor-configuration.html\">Elastic Agent | Processors</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html\">Ingest Pipelines</a> </li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/processors.html\">Elaticsearch | Ingest processor reference</a></li>\n<li><a href=\"https://opentelemetry.io/docs/collector/\">OTel Collectors</a> </li>\n<li><a href=\"https://opentelemetry.io/docs/collector/transforming-telemetry/#advanced-transformations\">OTel Transforming telemetry</a></li>\n</ol>","PublishedAt":"2023-07-03 13:00:00+00:00","OriginURL":"https://www.elastic.co/blog/pruning-incoming-log-volumes-with-elastic","SourceName":"Elastic"}},{"node":{"ID":4082,"Title":"How to get the best of lexical and AI-powered search with Elastic’s vector database","Description":"","PublishedAt":"2023-07-03 00:00:00+00:00","OriginURL":"https://www.elastic.co/blog/lexical-ai-powered-search-elastic-vector-database","SourceName":"Elastic"}},{"node":{"ID":4073,"Title":"How Instacart Measures the True Value of Advertising: The Methodology of Ad Incrementality","Description":"","PublishedAt":"2023-06-30 17:47:22+00:00","OriginURL":"https://tech.instacart.com/how-instacart-measures-the-true-value-of-advertising-the-methodology-of-ad-incrementality-aac6c58e627c?source=rss----587883b5d2ee---4","SourceName":"Instacart"}},{"node":{"ID":4070,"Title":"The Overflow #184: Stress test your code","Description":"<p>Vertical farming, pure math for dummies, and git for solo devs.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/06/30/the-overflow-184-stress-test-your-code/\">The Overflow #184: Stress test your code</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-06-30 13:00:00+00:00","OriginURL":"https://stackoverflow.blog/2023/06/30/the-overflow-184-stress-test-your-code/","SourceName":"Stack Overflow"}},{"node":{"ID":4077,"Title":"Mercari Hack Fest #7 : Introducing the Winners!","Description":"<p>Hello, my name is @afroscript from the Engineering Office. Mercari Hack Fest (“Hack Fest”), a technology festival for engineers was held for three days from April 19th-21st. *Related article : Organizing a Successful Internal Hackathon: Mercari Hack Fest Spring 2023 This article explains how the “Showcase Day”, the concluding event of Hack Fest was like, [&hellip;]</p>\n","PublishedAt":"2023-06-30 11:47:48+00:00","OriginURL":"https://engineering.mercari.com/en/blog/entry/20230621-e066032084/","SourceName":"Mercari"}},{"node":{"ID":4068,"Title":"Making computer science more humane at Carnegie Mellon (ep. 585)","Description":"<p>On this episode of the podcast, Ben and Ryan chat with Martial Hebert, dean of the School of Computer Science at Ryan’s alma mater, Carnegie Mellon University.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://stackoverflow.blog/2023/06/30/making-computer-science-more-humane-at-carnegie-mellon-ep-585/\">Making computer science more humane at Carnegie Mellon (ep. 585)</a> appeared first on <a rel=\"nofollow\" href=\"https://stackoverflow.blog\">Stack Overflow Blog</a>.</p>\n","PublishedAt":"2023-06-30 04:40:00+00:00","OriginURL":"https://stackoverflow.blog/2023/06/30/making-computer-science-more-humane-at-carnegie-mellon-ep-585/","SourceName":"Stack Overflow"}},{"node":{"ID":4067,"Title":"How We Migrated Our Acceptance Tests to Use Synthetic Monitoring","Description":"<img class=\"webfeedsFeaturedVisual rss\" src=\"https://imgix.datadoghq.com/img/blog/engineering/migrating-acceptance-tests-to-synthetic-monitoring/migrating-acceptance-tests-to-synthetic-monitoring.png\" width=\"100%\"/>The Frontend Developer Experience team strives to improve the lives of 300 frontend engineers at Datadog. We cover build systems, tests, deployments, code health, internal tools, and more—we’re here to remove any friction and pain points from our engineers’ workflows.One such pain point was difficult-to-maintain acceptance tests. This is the story of how we migrated a codebase from flaky, unmanageable acceptance testing with Puppeteer (Chromium Headless Browser) to more robust and maintainable Synthetic tests.","PublishedAt":"2023-06-30 00:00:00+00:00","OriginURL":"https://www.datadoghq.com/blog/engineering/migrating-acceptance-tests-to-synthetic-monitoring/","SourceName":"Datadog"}}]}},"pageContext":{"limit":30,"skip":600,"numPages":158,"currentPage":21}},"staticQueryHashes":["3649515864"]}