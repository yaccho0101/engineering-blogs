{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/page/144","result":{"data":{"allPost":{"edges":[{"node":{"ID":368,"Title":"The Curious Case of the Table-Locking UPDATE Query","Description":"<blockquote>\n<p>Update: On closer inspection, the lock type was not on the table, but on a tuple. For more information on this locking mechanism see the <a href=\"https://github.com/postgres/postgres/blob/master/src/backend/access/heap/README.tuplock\">internal Postgresql tuple locking documentation</a>. Postgres does not have lock promotion as suggested in the debugging section of this post.</p>\n</blockquote>\n\n<p>I maintain an internal-facing service at Heroku that does metadata processing. It's not real-time, so there's plenty of slack for when things go wrong. Recently I discovered that the system was getting bogged down to the point where no jobs were being executed at all. After hours of debugging, I found the problem was an <code>UPDATE</code> on a single row on a single table was causing the entire table to lock, which caused a lock queue and ground the whole process to a halt. This post is a story about how the problem was debugged and fixed and why such a seemingly simple query caused so much harm.</p>\n<h2 class=\"anchored\">\n  <a name=\"no-jobs-processing\" href=\"#no-jobs-processing\">No jobs processing</a>\n</h2>\n\n<p>I started debugging when the backlog on our system began to grow, and the number of jobs being processed fell to nearly zero. The system has been running in production for years, and while there have been occasional performance issues, nothing stood out as a huge problem. I checked our datastores, and they were well under their limits, I checked our error tracker and didn't see any smoking guns. My best guess was the database where the results were being stored was having problems.</p>\n\n<p>The first thing I did was run <code>heroku pg:diagnose</code>, which shows \"red\" (critical) and \"yellow\" (important but less critical) issues. It showed that I had queries that had been running for DAYS:</p>\n\n<pre><code>68698   5 days 18:01:26.446979  UPDATE \"table\" SET &lt;values&gt; WHERE (\"uuid\" = '&lt;uuid&gt;')\n</code></pre>\n\n<p>Which seemed odd. The query in question was a simple update, and it's not even on the most massive table in the DB. When I checked <code>heroku pg:outliers</code> from the <a href=\"https://github.com/heroku/heroku-pg-extras\">pg extras CLI plugin</a> I was surprised to see this update taking up 80%+ of the time even though it is smaller than the largest table in the database by a factor of 200. So what gives?</p>\n\n<p>Running the update statement manually didn't reproduce the issue, so I was fresh out of ideas. If it had, then I could have run with <code>EXPLAIN ANALYZE</code> to see why it was so slow. Luckily I work with some pretty fantastic database engineers, and I pinged them for possible ideas. They mentioned that there might be a locking issue with the database. The idea was strange to me since it had been running relatively unchanged for an extremely long time and only now started to see problems, but I decided to look into it.</p>\n\n<pre><code class=\"language-sql\">SELECT\n  S.pid,\n  age(clock_timestamp(), query_start),\n  query,\n  L.mode,\n  L.locktype,\n  L.granted\nFROM pg_stat_activity S\ninner join pg_locks L on S.pid = L.pid\norder by L.granted, L.pid DESC;\n-----------------------------------\npid      | 127624\nage      | 2 days 01:45:00.416267\nquery    | UPDATE \"table\" SET &lt;values&gt; WHERE (\"uuid\" = '&lt;uuid&gt;')\nmode     | AccessExclusiveLock\nlocktype | tuple\ngranted  | f\n</code></pre>\n\n<p>I saw a ton of queries that were hung for quite some time, and most of them pointed to my seemingly teeny <code>UPDATE</code> statement.</p>\n<h2 class=\"anchored\">\n  <a name=\"all-about-locks\" href=\"#all-about-locks\">All about locks</a>\n</h2>\n\n<p>Up until this point, I basically knew nothing about how PostgreSQL uses locking other than in an explicit advisory lock, which can be used via a gem like <a href=\"https://github.com/heroku/pg_lock\">pg_lock</a> (That I maintain). Luckily Postgres has excellent docs around locks, but it's a bit much if you're new to the field: <a href=\"https://www.postgresql.org/docs/11/explicit-locking.html#LOCKING-TABLES\">Postgresql Lock documentation</a></p>\n\n<p>Looking up the name of the lock from before <code>Access Exclusive Lock</code> I saw that it locks the whole table:</p>\n\n<blockquote>\n<p>ACCESS EXCLUSIVE\nConflicts with locks of all modes (ACCESS SHARE, ROW SHARE, ROW EXCLUSIVE, SHARE UPDATE EXCLUSIVE, SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE). This mode guarantees that the holder is the only transaction accessing the table in any way.\nAcquired by the DROP TABLE, TRUNCATE, REINDEX, CLUSTER, VACUUM FULL, and REFRESH MATERIALIZED VIEW (without CONCURRENTLY) commands. Many forms of ALTER TABLE also acquire a lock at this level (see ALTER TABLE). This is also the default lock mode for LOCK TABLE statements that do not specify a mode explicitly.</p>\n</blockquote>\n\n<p>From the docs, this lock is not typically triggered by an <code>UPDATE</code>, so what gives? Grepping through the docs showed me that an <code>UPDATE</code> should trigger a <code>ROW SHARE</code> lock:</p>\n\n<pre><code>ROW EXCLUSIVE\nConflicts with the SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE lock modes.\n\nThe commands UPDATE, DELETE, and INSERT acquire this lock mode on the target table (in addition to ACCESS SHARE locks on any other referenced tables). In general, this lock mode will be acquired by any command that modifies data in a table.\n</code></pre>\n\n<blockquote>\n<p>A database engineer directly told me what kind of lock an <code>UPDATE</code> should use, but you could find it in the docs if you don't have access to some excellent database professionals.</p>\n</blockquote>\n\n<p>Mostly what happens when you try to <code>UPDATE</code> is that Postgres will acquire a lock on the row that you want to change. If you have two update statements running at the same time on the same row, then the second must wait for the first to process. So why on earth, if an <code>UPDATE</code> is supposed only to take out a row lock, was my query taking out a lock against the whole table?</p>\n<h2 class=\"anchored\">\n  <a name=\"unmasking-a-locking-mystery\" href=\"#unmasking-a-locking-mystery\">Unmasking a locking mystery</a>\n</h2>\n\n<p>I would love to tell you that I have a really great debugging tool to tell you about here, but I mostly duck-duck-go-ed (searched) a ton and eventually found <a href=\"https://grokbase.com/t/postgresql/pgsql-general/124s02j3jy/updates-sharelocks-rowexclusivelocks-and-deadlocks\">this forum post</a>. In the post someone is complaining about a similar behavior, they're using an update but are seeing more aggressive lock being used sometimes.</p>\n\n<p>Based on the responses to the forum it sounded like if there is more than a few <code>UPDATE</code> queries that are trying to modify the same row at the same time what happens is that one of the queries will try to acquire the lock, see it is taken then it will instead acquire a larger lock on the table. Postgres queues locks, so if this happens for multiple rows with similar contention, then multiple queries would be taking out locks on the whole table, which somewhat could explain the behavior I was seeing. It seemed plausible, but why was there such a problem?</p>\n\n<p>I combed over my codebase and couldn't find anything. Then as I was laying down to go to bed that evening, I had a moment of inspiration where I remembered that we were updating the database in parallel for the same UUID using threads:</p>\n\n<pre><code class=\"language-ruby\">@things.map do |thing|\n  Concurrent::Promise.execute(executor: :fast) do\n    store_results!(thing)\n  end\nend.each(&amp;:value!)\n</code></pre>\n\n<p>In every loop, we were creating a promise that would concurrently update values (using a thread pool). Due to a design decision from years ago, each loop causes an <code>UPDATE</code> to the same row in the database for each job being run. This programming pattern was never a problem before because, as I mentioned earlier, there's another table with more than 200x the number of records, so we've never had any issues with this scheme until recently.</p>\n\n<p>With this new theory, I removed the concurrency, which meant that each <code>UPDATE</code> call would be sequential instead of in parallel:</p>\n\n<pre><code class=\"language-ruby\">@things.map do |thing|\n  store_results!(thing)\nend\n</code></pre>\n\n<p>While the code is less efficiently in the use of IO on the Ruby program, it means that the chance that the same row will try to be updated at the same time is drastically decreased.</p>\n\n<p>I manually killed the long-running locked queries using <code>SELECT pg_cancel_backend(&lt;pid&gt;);</code> and I deployed this change (in the morning after a code review).</p>\n\n<p>Once the old stuck queries were aborted, and the new code was in place, then the system promptly got back up and running, churning through plenty of backlog.</p>\n<h2 class=\"anchored\">\n  <a name=\"locks-and-stuff\" href=\"#locks-and-stuff\">Locks and stuff</a>\n</h2>\n\n<p>While this somewhat obscure debugging story might not be directly relevant to your database, here are some things you can take away from this article. Your database has locks (think mutexes but with varying scope), and those locks can mess up your day if they're doing something different than you're expecting. You can see the locks that your database is currently using by running the <code>heroku pg:locks</code> command (may need to install the <code>pg:extras</code> plugin). You can also see which queries are taking out which locks using the SQL query I posted earlier.</p>\n\n<p>The next thing I want to cover is documentation. If it weren't for several very experienced Postgres experts and a seemingly random forum post about how multiple <code>UPDATE</code> statements can trigger a more aggressive lock type, then I never would have figured this out. If you're familiar with the Postgres documentation, is this behavior written down anywhere? If so, then could we make it easier to find or understand somehow? If it's not written down, can you help me document it? I don't mind writing documentation, but I'm not totally sure what the expected behavior is. For instance, why does a lock queue for a row that goes above a specific threshold trigger a table lock? And what exactly is that threshold? I'm sure this behavior makes total sense from an implementation point of view, but as an end-user, I would like it to be spelled out and officially documented.</p>\n\n<p>I hope you either learned a thing or two or at least got a kick out of my misery. This issue was a pain to debug, but in hindsight, a quirky bug to blog about. Thanks for reading!</p>\n\n<p>And to learn about another potential database issue, check out this other blog post by Heroku Engineer Ben Fritsch, <a href=\"https://blog.heroku.com/know-your-database-types\">Know Your Database Types</a>.</p>\n\n<p>Special thanks to <a href=\"https://github.com/mble\">Matthew Blewitt</a> and <a href=\"https://github.com/andscoop\">Andy Cooper</a> for helping me debug this!</p>","PublishedAt":"2019-12-18 18:07:00+00:00","OriginURL":"https://blog.heroku.com/curious-case-table-locking-update-query","SourceName":"Heroku"}},{"node":{"ID":369,"Title":"Let It Crash: Best Practices for Handling Node.js Errors on Shutdown","Description":"<p><em>This blog post is adapted from a talk given by Julián Duque at NodeConf EU 2019 titled \"<a href=\"https://youtu.be/Fguac8pIAtU\">Let it crash!</a>.\"</em></p>\n\n<p>Before coming to Heroku, I did some consulting work as a Node.js solutions architect. My job was to visit various companies and make sure that they were successful in designing production-ready Node applications. Unfortunately, I witnessed many different problems when it came to error handling, especially on process shutdown. When an error occurred, there was often not enough visibility on why it happened, a lack of logging details, and bouts of downtime as applications attempted to recover from crashes.</p>\n\n<div class=\"embedded-video-wrapper\">\n<iframe title=\"Let it Crash!\" src=\"https://www.youtube-nocookie.com/embed/Fguac8pIAtU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n\n<p>We started to assemble a collection of best practices and recommendations on error handling, to ensure they were aligned with the overall Node.js community. In this post, I'll walk through some of the background on the Node.js process lifecycle and some strategies to properly handle graceful shutdown and quickly restart your application after a catastrophic error terminates your program. </p>\n<h2 class=\"anchored\">\n  <a name=\"the-node-js-process-lifecycle\" href=\"#the-node-js-process-lifecycle\">The Node.js process lifecycle</a>\n</h2>\n\n<p>Let's first explore briefly how Node.js operates. A Node.js process is very lightweight and has a small memory footprint. Because crashes are an inevitable part of programming, your primary goal when architecting an application is to keep the startup process very lean, so that your application can quickly boot up. If your startup operations include CPU intensive work or synchronous operations, it might affect the ability of your Node.js processes to quickly restart.</p>\n\n<p>A strategy you can use here is to prebuild as much as possible. That might mean preparing data or compiling assets during the building process. It may increase your deployment times, but it's better to spend more time outside of the startup process. Ultimately, this ensures that when a crash does happen, you can exit a process and start a new one without much downtime.</p>\n<h3 class=\"anchored\">\n  <a name=\"node-js-exit-methods\" href=\"#node-js-exit-methods\">Node.js exit methods</a>\n</h3>\n\n<p>Let's take a look at several ways you can terminate a Node.js process and the differences between them.</p>\n\n<p>The most common function to use is <a href=\"https://nodejs.org/api/process.html#process_process_exit_code\"><code>process.exit()</code></a>, which takes a single argument, an integer. If the argument is <code>0</code>, it represents a successful exit state. If it's greater than that, it indicates that an error occurred; <code>1</code> is a common exit code for failures here.</p>\n\n<p>Another option is <a href=\"https://nodejs.org/api/process.html#process_process_abort\"><code>process.abort()</code></a>. When this method is called, the Node.js process terminates immediately. More importantly, if your operating system allows it, Node will also generate a core dump file, which contains a ton of useful information about the process. You can use this core dump to do some postmortem debugging using tools like <a href=\"https://github.com/nodejs/llnode\"><code>llnode</code></a>.</p>\n<h3 class=\"anchored\">\n  <a name=\"node-js-exit-events\" href=\"#node-js-exit-events\">Node.js exit events</a>\n</h3>\n\n<p>As Node.js is built on top of JavaScript, it has an event loop, which allows you to listen for events that occur and act on them. When Node.js exits, it also emits several types of events.</p>\n\n<p>One of these is <code>beforeExit</code>, and as its name implies, it is emitted right before a Node process exits. You can provide an event handler which can make asynchronous calls, and the event loop will continue to perform the work until it's all finished. It's important to note that this event is <em>not</em> emitted on <code>process.exit()</code> calls or <code>uncaughtException</code>s; we'll get into when you might use this event a little later.</p>\n\n<p>Another event is <code>exit</code>, which is emitted only when <code>process.exit()</code> is explicitly called. As it fires after the event loop has been terminated, you can't do any asynchronous work in this handler.</p>\n\n<p>The code sample below illustrates the differences between the two events:</p>\n\n<pre><code class=\"language-js\">process.on('beforeExit', code =&gt; {\n  // Can make asynchronous calls\n  setTimeout(() =&gt; {\n    console.log(`Process will exit with code: ${code}`)\n    process.exit(code)\n  }, 100)\n})\n\nprocess.on('exit', code =&gt; {\n  // Only synchronous calls\n  console.log(`Process exited with code: ${code}`)\n})\n</code></pre>\n<h3 class=\"anchored\">\n  <a name=\"os-signal-events\" href=\"#os-signal-events\">OS signal events</a>\n</h3>\n\n<p>Your operating system emits events to your Node.js process, too, depending on the circumstances occurring outside of your program. These are referred to as <a href=\"https://en.wikipedia.org/wiki/Signal_(IPC)\">signals</a>. Two of the more common signals are <code>SIGTERM</code> and <code>SIGINT</code>.</p>\n\n<p><code>SIGTERM</code> is normally sent by a process monitor to tell Node.js to expect a successful termination. If you're running <code>systemd</code> or <code>upstart</code> to manage your Node application, and you stop the service, it sends a <code>SIGTERM</code> event so that you can handle the process shutdown.</p>\n\n<p><code>SIGINT</code> is emitted when a Node.js process is interrupted, usually as the result of a control-C (<code>^-C</code>) keyboard event. You can also capture that event and do some work around it.</p>\n\n<p>Here is an example showing how you may act on these signal events:</p>\n\n<pre><code class=\"language-js\">process.on('SIGTERM', signal =&gt; {\n  console.log(`Process ${process.pid} received a SIGTERM signal`)\n  process.exit(0)\n})\n\nprocess.on('SIGINT', signal =&gt; {\n  console.log(`Process ${process.pid} has been interrupted`)\n  process.exit(0)\n})\n</code></pre>\n\n<p>Since these two events are considered a successful termination, we call <code>process.exit</code> and pass an argument of <code>0</code> because it is something that is expected.</p>\n<h3 class=\"anchored\">\n  <a name=\"javascript-error-events\" href=\"#javascript-error-events\">JavaScript error events</a>\n</h3>\n\n<p>At last, we arrive at higher-level error types: the error events thrown by JavaScript itself.</p>\n\n<p>When a JavaScript error is not properly handled, an <code>uncaughtException</code> is emitted. These suggest the programmer has made an error, and they should be treated with the utmost priority. Usually, it means a bug occurred on a piece of logic that needed more testing, such as calling a method on a <code>null</code> type.</p>\n\n<p>An <code>unhandledRejection</code> error is a newer concept. It is emitted when a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise\">promise</a> is not satisfied; in other words, a promise was rejected (it failed), and there was no handler attached to respond. These errors can indicate an operational error or a programmer error, and they should also be treated as high priority.</p>\n\n<p>In both of these cases, you should do something counterintuitive and <strong>let your program crash</strong>! Please don't try to be clever and introduce some complex logic trying to prevent a process restart. Doing so will almost always leave your application in a bad state, whether that's having a memory leak or leaving sockets hanging. It's simpler to let it crash, start a new process from scratch, and continue receiving more requests.</p>\n\n<p>Here's some code indicating how you might best handle these events:</p>\n\n<pre><code class=\"language-js\">process.on('uncaughtException', err =&gt; {\n  console.log(`Uncaught Exception: ${err.message}`)\n  process.exit(1)\n})\n</code></pre>\n\n<p>We’re explicitly “crashing” the Node.js process here! Don’t be afraid of this! It is more likely than not unsafe to continue. The <a href=\"https://nodejs.org/api/process.html#process_warning_using_uncaughtexception_correctly\">Node.js documentation</a> says,</p>\n\n<blockquote>\n<p>Unhandled exceptions inherently mean that an application is in an undefined state...The correct use of 'uncaughtException' is to perform synchronous cleanup of allocated resources (e.g. file descriptors, handles, etc) before shutting down the process. It is not safe to resume normal operation after 'uncaughtException'.</p>\n</blockquote>\n\n<pre><code class=\"language-js\">process.on('unhandledRejection', (reason, promise) =&gt; {\n  console.log('Unhandled rejection at ', promise, `reason: ${err.message}`)\n  process.exit(1)\n})\n</code></pre>\n\n<p><code>unhandledRejection</code> is such a common error, that the Node.js maintainers have decided it should really crash the process, and they warn us that in a future version of Node.js <code>unhandledRejection</code>s will crash the process.</p>\n\n<blockquote>\n<p>[DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.</p>\n</blockquote>\n<h2 class=\"anchored\">\n  <a name=\"run-more-than-one-process\" href=\"#run-more-than-one-process\">Run more than one process</a>\n</h2>\n\n<p>Even if your process startup time is extremely quick, running just a single process is a risk to safe and uninterrupted application operation. We recommend running more than one process and to use a load balancer to handle the scheduling. That way, if one of the processes crashes, there is another process that is alive and able to receive new requests. This is going to give you a little bit more leverage and prevent downtime.</p>\n\n<p>Use whatever you have on-hand for the load balancing. You can configure a reverse proxy like nginx or HAProxy to do this. If you're on Heroku, you can <a href=\"https://devcenter.heroku.com/articles/how-heroku-works#http-routing\">scale your application</a> to increase the number of dynos. If you're on Kubernetes, you can use <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> or other load balancer strategies for your application.</p>\n<h2 class=\"anchored\">\n  <a name=\"monitor-your-processes\" href=\"#monitor-your-processes\">Monitor your processes</a>\n</h2>\n\n<p>You should have process monitoring in-place, something running in your operating system or an application environment that's constantly checking if your Node.js process is alive or not. If the process crashes due to a failure, the process monitor is in charge of restarting the process.</p>\n\n<p>Our recommendation is to always use the native process monitoring that's available on your operating system. For example, if you're running on Unix or Linux, you can use the <a href=\"https://en.wikipedia.org/wiki/Systemd\"><code>systemd</code></a> or <a href=\"https://en.wikipedia.org/wiki/Upstart_(software)\"><code>upstart</code></a> commands. If you're using containers, Docker has a <a href=\"https://docs.docker.com/engine/reference/run/#restart-policies---restart\"><code>--restart</code> flag</a>, and Kubernetes has <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy\"><code>restartPolicy</code></a>, both of which are useful.</p>\n\n<p>If you can't use any existing tools, use a Node.js process monitor like <a href=\"https://pm2.keymetrics.io/\">PM2</a> or <a href=\"https://github.com/foreversd/forever\">forever</a> as a last resort. These tools are okay for development environments, but I can't really recommend them for production use.</p>\n\n<p>If your application is running on Heroku, don’t worry—we take care of the restart for you!</p>\n<h2 class=\"anchored\">\n  <a name=\"graceful-shutdowns\" href=\"#graceful-shutdowns\">Graceful shutdowns</a>\n</h2>\n\n<p>Let's say we have a server running. It's receiving requests and establishing connections with clients. But what happens if the process crashes?  If we're not performing a graceful shutdown, some of those sockets are going to hang around and keep waiting for a response until a timeout has been reached. That unnecessary time spent consumes resources, eventually leading to downtime and a degraded experience for your users.</p>\n\n<p>It's best to explicitly stop receiving connections, so that the server can disconnect connections while it's recovering. Any new connections will go to the other Node.js processes running through the load balancer</p>\n\n<p>To do this, you can call <a href=\"https://nodejs.org/api/http.html#http_server_close_callback\"><code>server.close()</code></a>, which tells the server to stop accepting new connections. Most Node servers implement this class, and it accepts a callback function as an argument.</p>\n\n<p>Now, imagine that your server has many clients connected, and the majority of them have not experienced an error or crashed. How can you close the server while not abruptly disconnecting valid clients? We'll need to use a timeout to build a system to indicate that if all the connections don't close within a certain limit, we will completely shutdown the server. We do this because we want to give existing, healthy clients time to finish up but don't want the server to wait for an excessively long time to shutdown.</p>\n\n<p>Here's some sample code of what that might look like:</p>\n\n<pre><code class=\"language-js\">process.on('&lt;signal or error event&gt;', _ =&gt; {\n  server.close(() =&gt; {\n    process.exit(0)\n  })\n  // If server hasn't finished in 1000ms, shut down process\n  setTimeout(() =&gt; {\n    process.exit(0)\n  }, 1000).unref() // Prevents the timeout from registering on event loop\n})\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"logging\" href=\"#logging\">Logging</a>\n</h2>\n\n<p>Chances are you have already implemented a robust logging strategy for your running application, so I won't get into it too much about that here. Just remember to log with the same rigorous quality and amount of information for when the application shuts down!</p>\n\n<p>If a crash occurs, log as much relevant information as possible, including the errors and stack trace. Rely on libraries like <a href=\"https://github.com/pinojs/pino\"><code>pino</code></a> or <a href=\"https://github.com/winstonjs/winston\"><code>winston</code></a> in your application, and store these logs using one of their transports for better visibility. You can also take a look at <a href=\"https://elements.heroku.com/addons#logging\">our various logging add-ons</a> to find a provider which matches your application’s needs.</p>\n<h2 class=\"anchored\">\n  <a name=\"make-sure-everything-is-still-good\" href=\"#make-sure-everything-is-still-good\">Make sure everything is still good</a>\n</h2>\n\n<p>Last, and certainly not least, we recommend that you add a health check route. This is a simple endpoint that returns a <code>200</code> status code if your application is running:</p>\n\n<pre><code class=\"language-js\">// Add a health check route in express\napp.get('/_health', (req, res) =&gt; {\n  res.status(200).send('ok')\n})\n</code></pre>\n\n<p>You can have a separate service continuously monitor that route. You can configure this in a number of ways, whether by using a reverse proxy, such as nginx or HAProxy, or a load balancer, like ELB or ALB.</p>\n\n<p>Any application that acts as the top layer of your Node.js process can be used to constantly monitor that the health check is returning. These will also give you way more visibility around the health of your Node.js processes, and you can rest easy knowing that your Node processes are running properly. There are some great great monitoring services to help you with this in the <a href=\"https://elements.heroku.com/addons#monitoring\">Add-ons section of our Elements Marketplace</a>.</p>\n<h2 class=\"anchored\">\n  <a name=\"putting-it-all-together\" href=\"#putting-it-all-together\">Putting it all together</a>\n</h2>\n\n<p>Whenever I work on a new Node.js project, I use the same function to ensure that my crashes are logged and my recoveries are guaranteed. It looks something like this:</p>\n\n<pre><code class=\"language-js\">function terminate (server, options = { coredump: false, timeout: 500 }) {\n  // Exit function\n  const exit = code =&gt; {\n    options.coredump ? process.abort() : process.exit(code)\n  }\n\n  return (code, reason) =&gt; (err, promise) =&gt; {\n    if (err &amp;&amp; err instanceof Error) {\n    // Log error information, use a proper logging library here :)\n    console.log(err.message, err.stack)\n    }\n\n    // Attempt a graceful shutdown\n    server.close(exit)\n    setTimeout(exit, options.timeout).unref()\n  }\n}\n\nmodule.exports = terminate\n</code></pre>\n\n<p>Here, I've created a module called <code>terminate</code>. I pass the instance of that server that I'm going to be closing, and some configuration options, such as whether I want to enable core dumps, as well as the timeout. I usually use an environment variable to control when I want to enable a core dump. I enable them only when I am going to do some performance testing on my application or whenever I want to replicate the error.</p>\n\n<p>This exported function can then be set to listen to our error events:</p>\n\n<pre><code class=\"language-js\">const http = require('http')\nconst terminate = require('./terminate')\nconst server = http.createServer(...)\n\nconst exitHandler = terminate(server, {\n  coredump: false,\n  timeout: 500\n})\n\nprocess.on('uncaughtException', exitHandler(1, 'Unexpected Error'))\nprocess.on('unhandledRejection', exitHandler(1, 'Unhandled Promise'))\nprocess.on('SIGTERM', exitHandler(0, 'SIGTERM'))\nprocess.on('SIGINT', exitHandler(0, 'SIGINT'))\n</code></pre>\n<h2 class=\"anchored\">\n  <a name=\"additional-resources\" href=\"#additional-resources\">Additional resources</a>\n</h2>\n\n<p>There are a number of existing npm modules that pretty much solve the aforementioned issues in a similar ways. You can check these out as well:</p>\n\n<ul>\n<li><a href=\"https://github.com/godaddy/terminus\">@godaddy/terminus</a></li>\n<li><a href=\"https://github.com/hunterloftis/stoppable\">stoppable</a></li>\n<li><a href=\"https://github.com/sebhildebrandt/http-graceful-shutdown\">http-graceful-shutdown</a></li>\n</ul>\n\n<p>Hopefully, this information will simplify your life and enable your Node app to run better and safer in production!</p>","PublishedAt":"2019-12-17 18:12:00+00:00","OriginURL":"https://blog.heroku.com/best-practices-nodejs-errors","SourceName":"Heroku"}},{"node":{"ID":645,"Title":"Getting Ready For The Big Data Apocalypse","Description":"","PublishedAt":"2019-12-16 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-12-16-gettingreadyforbigdataapocalypse/","SourceName":"Trivago"}},{"node":{"ID":382,"Title":"Instagram Data Saver Mode","Description":"<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://instagram-engineering.com/instagram-data-saver-mode-ffb01fd5a6bd?source=rss----37dc2a3034f2---4\"><img src=\"https://cdn-images-1.medium.com/max/912/1*DFLQYzOGL6yMpXmN28iP7g.png\" width=\"912\"></a></p><p class=\"medium-feed-snippet\">We recently shipped Data Saver Mode, a new feature on Instagram for Android that helps the app consume less mobile data. In this post&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://instagram-engineering.com/instagram-data-saver-mode-ffb01fd5a6bd?source=rss----37dc2a3034f2---4\">Continue reading on Instagram Engineering »</a></p></div>","PublishedAt":"2019-12-13 18:09:34+00:00","OriginURL":"https://instagram-engineering.com/instagram-data-saver-mode-ffb01fd5a6bd?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":270,"Title":"NY MusicTech @ GIPHY","Description":"December 10, 2019 GIPHY HQ – New York, NY Join us and the MusicTech meetup for a night of demos and musical performances! MusicTech is a community that&#8217;s supports the people who are defining what music technology will look like in the future. Demos by:&#8211; Christian Rutledge, Muzooka&#8211; Hazmin Valdes, BillFold POS&#8211; Deane Marcus, RIFF [&#8230;]","PublishedAt":"2019-12-03 14:50:26+00:00","OriginURL":"https://engineering.giphy.com/ny-musictech-giphy/","SourceName":"GIPHY"}},{"node":{"ID":383,"Title":"Powered by AI: Instagram’s Explore recommender system","Description":"<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://instagram-engineering.com/powered-by-ai-instagrams-explore-recommender-system-7ca901d2a882?source=rss----37dc2a3034f2---4\"><img src=\"https://cdn-images-1.medium.com/max/1800/0*L8XlYDF2i6ziTrH4\" width=\"1800\"></a></p><p class=\"medium-feed-snippet\">This post was originally published on Facebook AI blog: https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/.</p><p class=\"medium-feed-link\"><a href=\"https://instagram-engineering.com/powered-by-ai-instagrams-explore-recommender-system-7ca901d2a882?source=rss----37dc2a3034f2---4\">Continue reading on Instagram Engineering »</a></p></div>","PublishedAt":"2019-11-26 13:48:39+00:00","OriginURL":"https://instagram-engineering.com/powered-by-ai-instagrams-explore-recommender-system-7ca901d2a882?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":646,"Title":"Open Source? trivago.","Description":"","PublishedAt":"2019-11-20 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-11-20-opensourcetrivago/","SourceName":"Trivago"}},{"node":{"ID":647,"Title":"Automation-First Approach Using the Karate API Testing Framework","Description":"","PublishedAt":"2019-11-14 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-11-14-apitestautomationusingkarate/","SourceName":"Trivago"}},{"node":{"ID":384,"Title":"10 Questions with Shupin Mao, Well-being tech lead","Description":"","PublishedAt":"2019-11-08 16:24:40+00:00","OriginURL":"https://instagram-engineering.com/10-questions-with-shupin-mao-well-being-tech-lead-3b19f19b168d?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":815,"Title":"Implementing Dark Mode Using the Observer Pattern","Description":"Last week’s update to the SoundCloud iOS app includes support for Dark Mode. This took several months of work and collaboration between…","PublishedAt":"2019-11-08 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/dark-mode-observer-pattern","SourceName":"Soundcloud"}},{"node":{"ID":385,"Title":"Making instagram.com faster: Code size and execution optimizations (Part 4)","Description":"","PublishedAt":"2019-11-01 13:03:12+00:00","OriginURL":"https://instagram-engineering.com/making-instagram-com-faster-code-size-and-execution-optimizations-part-4-57668be796a8?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":271,"Title":"Pupline: GIPHY’s Media Metadata Pipeline","Description":"Here at GIPHY, we receive thousands of GIF uploads each day. Like any tech company, we love data: we want to attach as much data as we can to these GIFs so that we can improve their search performance and the user experience. We also want to do so in an organized, centralized, and asynchronous [&#8230;]","PublishedAt":"2019-10-29 17:07:14+00:00","OriginURL":"https://engineering.giphy.com/pupline-giphys-media-metadata-pipeline/","SourceName":"GIPHY"}},{"node":{"ID":648,"Title":"triversity - An Interview with two trivago Tech Camp Participants","Description":"","PublishedAt":"2019-10-23 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-10-23-triversityinterview/","SourceName":"Trivago"}},{"node":{"ID":386,"Title":"Python at Scale: Strict Modules","Description":"","PublishedAt":"2019-10-17 15:01:07+00:00","OriginURL":"https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":272,"Title":"GIPHY Gets Tagged by K-Nearest Neighbors","Description":"When a brand new GIF gets uploaded onto GIPHY.com, there’s usually not a lot of data associated with it to make sure that it’s discoverable via search. While we do rely on machine learning for tag generation, we also allow uploading users and GIPHY’s content team to manually add “tags”, which are keywords that help [&#8230;]","PublishedAt":"2019-10-16 15:14:01+00:00","OriginURL":"https://engineering.giphy.com/giphy-gets-tagged-by-k-nearest-neighbors/","SourceName":"GIPHY"}},{"node":{"ID":387,"Title":"Making instagram.com faster: Part 3 — cache first","Description":"<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://instagram-engineering.com/making-instagram-com-faster-part-3-cache-first-6f3f130b9669?source=rss----37dc2a3034f2---4\"><img src=\"https://cdn-images-1.medium.com/max/1664/1*NGABlwJjUR2g0T23xTvWoQ.png\" width=\"1664\"></a></p><p class=\"medium-feed-snippet\">In recent years instagram.com has seen a lot of changes&#x200A;&#x2014;&#x200A;we&#x2019;ve launched stories, filters, creation tools, notifications, and direct&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://instagram-engineering.com/making-instagram-com-faster-part-3-cache-first-6f3f130b9669?source=rss----37dc2a3034f2---4\">Continue reading on Instagram Engineering »</a></p></div>","PublishedAt":"2019-10-11 00:04:26+00:00","OriginURL":"https://instagram-engineering.com/making-instagram-com-faster-part-3-cache-first-6f3f130b9669?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":388,"Title":"Implementing Dark Mode in iOS 13","Description":"","PublishedAt":"2019-10-08 16:30:51+00:00","OriginURL":"https://instagram-engineering.com/instagram-darkmode-58802b43c0f2?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":273,"Title":"DevOps and Drinks : Discussing Spinnaker with GIPHY and Armory","Description":"October 10, 2019 GIPHY HQ – New York, NY Come join us on October 10th for another iteration of DevOps and Drinks, this time featuring our GIPHY Engineering&#8217;s very own Site Reliability Engineer, Bryant Rockoff, who will discuss how GIPHY has deployed and is using Spinnaker to help ensure we help to keep the internet [&#8230;]","PublishedAt":"2019-10-04 17:33:31+00:00","OriginURL":"https://engineering.giphy.com/devops-and-drinks-discussing-spinnaker-with-giphy-and-armory/","SourceName":"GIPHY"}},{"node":{"ID":274,"Title":"The Round","Description":"October 3rd, 2019 @ 26 Bridge, Brooklyn NY Come see us at The Round NYC on October 3rd where we&#8217;ll be participating in a panel, sharing what it&#8217;s like to work at GIPHY Engineering and chatting with candidates about opportunities to join the team! The event is completely free. You can find more info and [&#8230;]","PublishedAt":"2019-10-01 17:20:04+00:00","OriginURL":"https://engineering.giphy.com/the-round/","SourceName":"GIPHY"}},{"node":{"ID":816,"Title":"Solving Remote Build Cache Misses by Annoying Your Colleagues","Description":"This is part two in a series about solving Gradle remote build cache misses. Solving build cache misses is important to both avoid work that…","PublishedAt":"2019-10-01 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/gradle-remote-build-cache-misses-part-2","SourceName":"Soundcloud"}},{"node":{"ID":389,"Title":"Interview with Tamar Shapiro, Instagram’s Head of Analytics","Description":"","PublishedAt":"2019-09-24 15:51:03+00:00","OriginURL":"https://instagram-engineering.com/interview-with-tamar-shapiro-instagrams-head-of-analytics-c81946d02b90?source=rss----37dc2a3034f2---4","SourceName":"Instagram"}},{"node":{"ID":649,"Title":"How to Analyze SurveyMonkey Data in Python","Description":"","PublishedAt":"2019-09-23 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-09-23-howtoanalyzesurveymonkeydatainpython/","SourceName":"Trivago"}},{"node":{"ID":275,"Title":"3D Print Fashion x WOW: Fashion of the Future @ GIPHY","Description":"September 25, 2019 GIPHY HQ – New York, NY Join us at GIPHY where 3D Print Fashion, in conjunction with Women of Wearables, are putting on a visual presentation and interactive panel dedicated to the future of fashion tech. Learn more about the people behind fashion&#8217;s most future forward innovations and exciting designs. oin us [&#8230;]","PublishedAt":"2019-09-18 18:50:52+00:00","OriginURL":"https://engineering.giphy.com/3d-print-fashion-x-wow-fashion-of-the-future-giphy/","SourceName":"GIPHY"}},{"node":{"ID":817,"Title":"Gradle Remote Build Cache Misses","Description":"Until recently, one of the top technical risks facing SoundCloud’s Android team was increasing build times. Our engineering leadership was well aware of the problem, and it was highlighted in our company’s quarterly goals and objectives as modularization. Faster build times means more productive developers. More productive developers are happier and can iterate on products more quickly. Modularization is key to decreasing build times, but avoiding work is another important part of the puzzle, and build caching is one way to avoid that work. Gradle, our tool for building Android, has a local file system cache that reuses outputs of previously performed tasks. We have been using the Gradle remote build cache in order to save our developers’ time. It helps us avoid redoing work that other teammates have already done or switching to old branches. However, to get the full benefits of caching, you have to go beyond simply setting it up.","PublishedAt":"2019-08-30 00:00:00+00:00","OriginURL":"https://developers.soundcloud.com/blog/gradle-remote-build-cache-misses","SourceName":"Soundcloud"}},{"node":{"ID":650,"Title":"Machine Learning and Bathtubs - How Small Visual Changes Improve User Experience","Description":"","PublishedAt":"2019-08-21 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-08-21-spajourneywithimageconcepts/","SourceName":"Trivago"}},{"node":{"ID":276,"Title":"The Brooklyn iOS August Meetup @ GIPHY","Description":"August 20, 2019 GIPHY HQ – New York, NY Join us at GIPHY on Tuesday, August 20th for two exciting talks featuring our very own iOS developer, Chris Maier as well as Mike Sanderson of Tendigi. Doors open at 6:30, and presentations begin at 7. In Chris&#8217; talk, &#8220;Building a GIF Carousel with UICollectionViewLayout&#8221; he&#8217;ll [&#8230;]","PublishedAt":"2019-08-14 17:08:00+00:00","OriginURL":"https://engineering.giphy.com/the-brooklyn-ios-august-meetup-giphy/","SourceName":"GIPHY"}},{"node":{"ID":277,"Title":"What’s that Spike?","Description":"Weekly Search Trends at GIPHY The Product Analytics team at GIPHY sits at the interface of technical data engineering teams and the rest of our business. We’re tasked with making data easily accessible across the org and uncovering insights to enhance our product and sales strategies. This can take the form of building data visualizations, [&#8230;]","PublishedAt":"2019-08-13 19:34:21+00:00","OriginURL":"https://engineering.giphy.com/whats-that-spike/","SourceName":"GIPHY"}},{"node":{"ID":651,"Title":"The Web Performance Impact Of Lossy Network Conditions","Description":"","PublishedAt":"2019-08-08 00:00:00+00:00","OriginURL":"https://tech.trivago.com/post/2019-08-08-thewebperformanceimpactoflossynetworkcon/","SourceName":"Trivago"}},{"node":{"ID":370,"Title":"The Time Has Come! Share Videos up to 60 Seconds Long on Imgur","Description":"If 60 seconds is long enough for Nicolas Cage to steal a car, what will you do with 60 seconds? 60 second video upload &#8212; with audio! &#8212; is now...","PublishedAt":"2019-07-31 20:22:08+00:00","OriginURL":"https://blog.imgur.com/2019/07/31/the-time-has-come-share-videos-up-to-60-seconds-long-on-imgur/?utm_source=rss&utm_medium=rss&utm_campaign=the-time-has-come-share-videos-up-to-60-seconds-long-on-imgur","SourceName":"Imgur"}},{"node":{"ID":4443,"Title":"The Time Has Come! Share Videos up to 60 Seconds Long on Imgur","Description":"If 60 seconds is long enough for Nicolas Cage to steal a car, what will you do with 60 seconds? 60 second video upload &#8212; with audio! &#8212; is now...","PublishedAt":"2019-07-31 20:22:08+00:00","OriginURL":"https://blog.imgur.com/2019/07/31/the-time-has-come-share-videos-up-to-60-seconds-long-on-imgur/","SourceName":"Imgur"}}]}},"pageContext":{"limit":30,"skip":4290,"numPages":158,"currentPage":144}},"staticQueryHashes":["3649515864"]}